# Comparing `tmp/qworker-1.7.8-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.zip` & `tmp/qworker-1.8.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.zip`

## zipinfo {}

```diff
@@ -1,32 +1,37 @@
-Zip file size: 314766 bytes, number of entries: 30
-drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-02 00:28 qw/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-02 00:28 qworker-1.7.8.dist-info/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-02 00:28 qworker.libs/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-02 00:28 qw/wrappers/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-02 00:28 qw/utils/
--rw-r--r--  2.0 unx      137 b- defN 23-Jun-02 00:28 qw/__init__.py
--rw-r--r--  2.0 unx      622 b- defN 23-Jun-02 00:28 qw/version.py
--rw-r--r--  2.0 unx     7462 b- defN 23-Jun-02 00:28 qw/process.py
--rw-r--r--  2.0 unx     2370 b- defN 23-Jun-02 00:28 qw/conf.py
--rw-r--r--  2.0 unx     4129 b- defN 23-Jun-02 00:28 qw/protocols.py
--rw-r--r--  2.0 unx     2392 b- defN 23-Jun-02 00:28 qw/__main__.py
--rwxr-xr-x  2.0 unx   568544 b- defN 23-Jun-02 00:28 qw/exceptions.cpython-39-x86_64-linux-gnu.so
--rw-r--r--  2.0 unx    15590 b- defN 23-Jun-02 00:28 qw/client.py
--rw-r--r--  2.0 unx    21225 b- defN 23-Jun-02 00:28 qw/server.py
--rw-r--r--  2.0 unx     2160 b- defN 23-Jun-02 00:28 qw/discovery.py
--rw-r--r--  2.0 unx      379 b- defN 23-Jun-02 00:28 qw/decorators.py
--rw-r--r--  2.0 unx      320 b- defN 23-Jun-02 00:28 qw/wrappers/__init__.py
--rw-r--r--  2.0 unx     4180 b- defN 23-Jun-02 00:28 qw/wrappers/di_task.py
--rw-r--r--  2.0 unx      920 b- defN 23-Jun-02 00:28 qw/wrappers/base.py
--rw-r--r--  2.0 unx      853 b- defN 23-Jun-02 00:28 qw/wrappers/func.py
--rw-r--r--  2.0 unx      597 b- defN 23-Jun-02 00:28 qw/utils/versions.py
--rw-r--r--  2.0 unx       46 b- defN 23-Jun-02 00:28 qw/utils/__init__.py
--rw-r--r--  2.0 unx      512 b- defN 23-Jun-02 00:28 qw/utils/functions.py
--rwxr-xr-x  2.0 unx   434800 b- defN 23-Jun-02 00:28 qw/utils/json.cpython-39-x86_64-linux-gnu.so
--rw-r--r--  2.0 unx     3170 b- defN 23-Jun-02 00:28 qworker-1.7.8.dist-info/METADATA
--rw-r--r--  2.0 unx     1070 b- defN 23-Jun-02 00:28 qworker-1.7.8.dist-info/LICENSE
--rw-rw-r--  2.0 unx     1951 b- defN 23-Jun-02 00:28 qworker-1.7.8.dist-info/RECORD
--rw-r--r--  2.0 unx       40 b- defN 23-Jun-02 00:28 qworker-1.7.8.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        3 b- defN 23-Jun-02 00:28 qworker-1.7.8.dist-info/top_level.txt
--rw-r--r--  2.0 unx      217 b- defN 23-Jun-02 00:28 qworker-1.7.8.dist-info/WHEEL
-30 files, 1073689 bytes uncompressed, 311212 bytes compressed:  71.0%
+Zip file size: 316906 bytes, number of entries: 35
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-04 01:21 qw/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-04 01:21 qworker.libs/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-04 01:21 qworker-1.8.0.dist-info/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-04 01:21 qw/wrappers/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-04 01:21 qw/utils/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-04 01:21 qw/queues/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-04 01:21 qw/executor/
+-rw-r--r--  2.0 unx      137 b- defN 23-Jun-04 01:21 qw/__init__.py
+-rw-r--r--  2.0 unx      622 b- defN 23-Jun-04 01:21 qw/version.py
+-rw-r--r--  2.0 unx     7462 b- defN 23-Jun-04 01:21 qw/process.py
+-rw-r--r--  2.0 unx     2598 b- defN 23-Jun-04 01:21 qw/conf.py
+-rw-r--r--  2.0 unx     4129 b- defN 23-Jun-04 01:21 qw/protocols.py
+-rw-r--r--  2.0 unx     2392 b- defN 23-Jun-04 01:21 qw/__main__.py
+-rwxr-xr-x  2.0 unx   568544 b- defN 23-Jun-04 01:21 qw/exceptions.cpython-39-x86_64-linux-gnu.so
+-rw-r--r--  2.0 unx    15767 b- defN 23-Jun-04 01:21 qw/client.py
+-rw-r--r--  2.0 unx    15939 b- defN 23-Jun-04 01:21 qw/server.py
+-rw-r--r--  2.0 unx     2160 b- defN 23-Jun-04 01:21 qw/discovery.py
+-rw-r--r--  2.0 unx      379 b- defN 23-Jun-04 01:21 qw/decorators.py
+-rw-r--r--  2.0 unx      320 b- defN 23-Jun-04 01:21 qw/wrappers/__init__.py
+-rw-r--r--  2.0 unx     4315 b- defN 23-Jun-04 01:21 qw/wrappers/di_task.py
+-rw-r--r--  2.0 unx     1447 b- defN 23-Jun-04 01:21 qw/wrappers/base.py
+-rw-r--r--  2.0 unx     1246 b- defN 23-Jun-04 01:21 qw/wrappers/func.py
+-rw-r--r--  2.0 unx      597 b- defN 23-Jun-04 01:21 qw/utils/versions.py
+-rw-r--r--  2.0 unx       46 b- defN 23-Jun-04 01:21 qw/utils/__init__.py
+-rw-r--r--  2.0 unx      512 b- defN 23-Jun-04 01:21 qw/utils/functions.py
+-rwxr-xr-x  2.0 unx   434800 b- defN 23-Jun-04 01:21 qw/utils/json.cpython-39-x86_64-linux-gnu.so
+-rw-r--r--  2.0 unx       62 b- defN 23-Jun-04 01:21 qw/queues/__init__.py
+-rw-r--r--  2.0 unx     4827 b- defN 23-Jun-04 01:21 qw/queues/manager.py
+-rw-r--r--  2.0 unx     1845 b- defN 23-Jun-04 01:21 qw/executor/__init__.py
+-rw-r--r--  2.0 unx     3170 b- defN 23-Jun-04 01:21 qworker-1.8.0.dist-info/METADATA
+-rw-r--r--  2.0 unx     1070 b- defN 23-Jun-04 01:21 qworker-1.8.0.dist-info/LICENSE
+-rw-rw-r--  2.0 unx     2189 b- defN 23-Jun-04 01:21 qworker-1.8.0.dist-info/RECORD
+-rw-r--r--  2.0 unx       40 b- defN 23-Jun-04 01:21 qworker-1.8.0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        3 b- defN 23-Jun-04 01:21 qworker-1.8.0.dist-info/top_level.txt
+-rw-r--r--  2.0 unx      217 b- defN 23-Jun-04 01:21 qworker-1.8.0.dist-info/WHEEL
+35 files, 1076835 bytes uncompressed, 312800 bytes compressed:  71.0%
```

## zipnote {}

```diff
@@ -1,22 +1,28 @@
 Filename: qw/
 Comment: 
 
-Filename: qworker-1.7.8.dist-info/
+Filename: qworker.libs/
 Comment: 
 
-Filename: qworker.libs/
+Filename: qworker-1.8.0.dist-info/
 Comment: 
 
 Filename: qw/wrappers/
 Comment: 
 
 Filename: qw/utils/
 Comment: 
 
+Filename: qw/queues/
+Comment: 
+
+Filename: qw/executor/
+Comment: 
+
 Filename: qw/__init__.py
 Comment: 
 
 Filename: qw/version.py
 Comment: 
 
 Filename: qw/process.py
@@ -66,26 +72,35 @@
 
 Filename: qw/utils/functions.py
 Comment: 
 
 Filename: qw/utils/json.cpython-39-x86_64-linux-gnu.so
 Comment: 
 
-Filename: qworker-1.7.8.dist-info/METADATA
+Filename: qw/queues/__init__.py
+Comment: 
+
+Filename: qw/queues/manager.py
+Comment: 
+
+Filename: qw/executor/__init__.py
+Comment: 
+
+Filename: qworker-1.8.0.dist-info/METADATA
 Comment: 
 
-Filename: qworker-1.7.8.dist-info/LICENSE
+Filename: qworker-1.8.0.dist-info/LICENSE
 Comment: 
 
-Filename: qworker-1.7.8.dist-info/RECORD
+Filename: qworker-1.8.0.dist-info/RECORD
 Comment: 
 
-Filename: qworker-1.7.8.dist-info/entry_points.txt
+Filename: qworker-1.8.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: qworker-1.7.8.dist-info/top_level.txt
+Filename: qworker-1.8.0.dist-info/top_level.txt
 Comment: 
 
-Filename: qworker-1.7.8.dist-info/WHEEL
+Filename: qworker-1.8.0.dist-info/WHEEL
 Comment: 
 
 Zip file comment:
```

## qw/version.py

```diff
@@ -2,15 +2,15 @@
    QueueWorker is a asyncio-based Worker for distributed functions.
 """
 
 __title__ = 'qworker'
 __description__ = ('QueueWorker is asynchronous Task Queue implementation '
                    'built on top of Asyncio.'
                    'Can you spawn distributed workers to run functions inside workers.')
-__version__ = '1.7.8'
+__version__ = '1.8.0'
 __author__ = 'Jesus Lara'
 __author_email__ = 'jesuslarag@gmail.com'
 __license__ = 'MIT'
 
 def get_version() -> tuple:  # pragma: no cover
     """ Get Queue Worker version as tuple.
     """
```

## qw/conf.py

```diff
@@ -11,14 +11,17 @@
 ### Worker Configuration
 WORKER_DEFAULT_HOST = config.get('WORKER_DEFAULT_HOST', fallback='0.0.0.0')
 WORKER_DEFAULT_PORT = config.getint('WORKER_DEFAULT_PORT', fallback=8888)
 WORKER_DEFAULT_QTY = config.getint('WORKER_DEFAULT_QTY', fallback=4)
 WORKER_QUEUE_SIZE = config.getint('WORKER_QUEUE_SIZE', fallback=8)
 RESOURCE_THRESHOLD = config.getint('RESOURCE_THRESHOLD', fallback=90)
 CHECK_RESOURCE_USAGE = config.getboolean('CHECK_RESOURCE_USAGE', fallback=True)
+WORKER_RETRY_INTERVAL = config.getint('WORKER_RETRY_INTERVAL', fallback=10)
+WORKER_RETRY_COUNT = config.getint('WORKER_RETRY_COUNT', fallback=3)
+WORKER_CONCURRENCY_NUMBER = config.getint('WORKER_CONCURRENCY_NUMBER', fallback=4)
 
 ## ID for saving worker list on Redis
 QW_WORKER_LIST = 'QW_WORKER_LIST'
 
 ## Network Discovery:
 USE_DISCOVERY = config.getboolean('USE_DISCOVERY', fallback=True)
 WORKER_DISCOVERY_HOST = config.get('WORKER_DISCOVERY_HOST')
```

## qw/client.py

```diff
@@ -1,12 +1,13 @@
 """QueueWorker Client."""
 import asyncio
 import itertools
 import random
 import warnings
+import inspect
 from typing import Any
 from collections.abc import Callable
 from collections import defaultdict
 from functools import partial
 import aioredis
 import pickle
 import cloudpickle
@@ -135,15 +136,19 @@
 
     def get_servers(self) -> list:
         return self._workers
 
     def register_pickle_module(self, module: Any):
         cloudpickle.register_pickle_by_value(module)
 
-    async def validate_connection(self, reader: asyncio.StreamReader, writer: asyncio.StreamWriter) -> tuple:
+    async def validate_connection(
+        self,
+        reader: asyncio.StreamReader,
+        writer: asyncio.StreamWriter
+    ) -> tuple:
         signature = make_signature(expected_message, WORKER_SECRET_KEY)
         writer.write(b'%d\n' % len(signature))
         writer.write(signature)
         # sending data to worker:
         await writer.drain()
         response = None
         response = await reader.readexactly(8)
@@ -207,15 +212,15 @@
                 )
                 await asyncio.sleep(WAIT_TIME)
                 continue
             except OSError as err:
                 self._num_retries[worker] += 1
                 if self._num_retries[worker] > MAX_RETRY_COUNT:
                     raise ConnectionError(
-                        f'Max number of retries is reached for {worker!r}, error: {err!s}'
+                        f'Max number of retries reached for {worker!r}, error: {err!s}'
                     ) from err
                 warnings.warn(
                     f"Can't connect to {worker!r}. Retrying..."
                 )
                 await asyncio.sleep(WAIT_TIME)
                 continue
             except Exception as err:
@@ -343,15 +348,15 @@
         elif isinstance(task_result, list):
             # try to convert into object:
             res = []
             for el in task_result:
                 try:
                     a = orjson.loads(el)
                     res.append(a)
-                except (TypeError, ValueError) as err:
+                except (TypeError, ValueError):
                     res.append(el)
             return res
         elif isinstance(task_result, dict):
             # check if result is and error:
             if 'exception' in task_result:
                 ex = task_result['exception']
                 if isinstance(ex, BaseException):
@@ -396,22 +401,25 @@
             self.logger.error(err)
             raise
         self.logger.debug(f'Sending function {fn!s} to Worker')
         host, *_ = writer.get_extra_info('sockname')
         if isinstance(fn, (FuncWrapper, TaskWrapper)):
             # Function was wrapped or is already wrapped
             func = fn
+        elif not asyncio.iscoroutinefunction(fn):
+            func = fn(*args, **kwargs)
         else:
             func = FuncWrapper(
                 host,
                 fn,
                 *args,
                 **kwargs
             )
         # serializing
+        print('FN: ', func, func.__class__.__name__)
         try:
             # getting data
             serialized_task = cloudpickle.dumps(func)
             writer.write(serialized_task)
             # sending data to worker:
             if writer.can_write_eof():
                 writer.write_eof()
```

## qw/server.py

```diff
@@ -1,53 +1,50 @@
 """QueueWorker Server Implementation"""
 import os
+import time
 import socket
 import uuid
 import asyncio
 import inspect
 from typing import Any
 from collections.abc import Callable
 import multiprocessing as mp
-from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
-from functools import partial
-import resource
-import psutil
 import cloudpickle
 from navconfig.logging import logging
 from qw.exceptions import (
     QWException,
     ParserError,
     DiscardedTask
 )
 from qw.utils import make_signature
-# from .protocols import QueueProtocol
 from .conf import (
     WORKER_DEFAULT_HOST,
     WORKER_DEFAULT_PORT,
     WORKER_DEFAULT_QTY,
-    WORKER_QUEUE_SIZE,
     expected_message,
-    WORKER_SECRET_KEY,
-    RESOURCE_THRESHOLD,
-    CHECK_RESOURCE_USAGE
+    WORKER_SECRET_KEY
 )
 from .utils.json import json_encoder
 from .utils.versions import get_versions
 from .utils import cPrint
+from .queues import QueueManager
 from .wrappers import (
-    QueueWrapper,
-    FuncWrapper,
-    TaskWrapper
+    QueueWrapper
 )
-
+from .executor import TaskExecutor
 
 DEFAULT_HOST = WORKER_DEFAULT_HOST
 if not DEFAULT_HOST:
     DEFAULT_HOST = socket.gethostbyname(socket.gethostname())
 
+
+# Initialize a semaphore with Worker Limit
+semaphore = asyncio.Semaphore(WORKER_DEFAULT_QTY)
+
+
 class QWorker:
     """Queue Task Worker server.
 
     Attributes:
         host: Hostname of the server.
         port: Port number of the server.
         loop: Event loop to run in.
@@ -63,24 +60,19 @@
             debug: bool = False,
             protocol: Any = None
     ):
         self.host = host
         self.port = port
         self.debug = debug
         self.queue = None
-        self.consumers = []
-        self.executor = None
         self._id = worker_id
         if name:
             self._name = name
         else:
             self._name = mp.current_process().name
-        self._executor = ThreadPoolExecutor(
-            max_workers=WORKER_DEFAULT_QTY
-        )
         self._loop = event_loop if event_loop else asyncio.new_event_loop()
         self._server: Callable = None
         self._pid = os.getpid()
         self._protocol = protocol
         # logging:
         self.logger = logging.getLogger(
             f'QW:Server:{self._name}:{self._id}'
@@ -88,18 +80,15 @@
 
     @property
     def name(self):
         return self._name
 
     async def start(self):
         """Starts Queue Manager."""
-        self.queue = asyncio.Queue(maxsize=WORKER_QUEUE_SIZE)
-        self.executor = ProcessPoolExecutor(
-            max_workers=WORKER_DEFAULT_QTY
-        )
+        self.queue = QueueManager(worker_name=self._name)
         try:
             if self._protocol:
                 self._server = await self._loop.create_server(
                     self._protocol,
                     host=self.host,
                     port=self.port,
                     family=socket.AF_INET,
@@ -123,222 +112,86 @@
             )
         except Exception as err:
             raise QWException(
                 f"Error: {err}"
             ) from err
         # Serve requests until Ctrl+C is pressed
         try:
-            await self.fire_consumers()
+            await self.queue.fire_consumers(
+                done_callback=self.task_callback
+            )
             async with self._server:
                 await self._server.serve_forever()
         except (RuntimeError, KeyboardInterrupt) as err:
             self.logger.exception(err, stack_info=True)
 
-    async def fire_consumers(self):
-        """Fire up the Task consumers."""
-        self.consumers = [
-            asyncio.create_task(
-                self.task_handler(self.queue)) for _ in range(WORKER_QUEUE_SIZE - 1)
-        ]
-
-    def get_resource_usage(self):
-        if CHECK_RESOURCE_USAGE is True:
-            soft, _ = resource.getrlimit(resource.RLIMIT_NOFILE)
-            processes = psutil.process_iter()
-            used = 0
-            try:
-                for ps in processes:
-                    try:
-                        num_fds = len(ps.open_files())
-                        used += num_fds
-                    except (
-                        psutil.NoSuchProcess,
-                        psutil.AccessDenied,
-                        psutil.ZombieProcess
-                    ):
-                        pass
-                return (used / soft) * 100
-            except (ValueError, RuntimeError):
-                pass
-        else:
-            return True
-
-    async def empty_queue(self, q: asyncio.Queue):
-        """Processing and shutting down the Queue."""
-        while not q.empty():
-            q.get_nowait()
-            q.task_done()
-        await q.join()
-
     async def shutdown(self):
         if self.debug is True:
             cPrint(
                 f'Shutting down worker {self.name!s}'
             )
         try:
             # forcing close the queue
-            await self.empty_queue(self.queue)
+            await self.queue.empty_queue()
         except KeyboardInterrupt:
             pass
-        # also: cancel the idle consumers:
-        for c in self.consumers:
-            try:
-                c.cancel()
-            except asyncio.CancelledError:
-                pass
         try:
             self._server.close()
             await self._server.wait_closed()
         except RuntimeError as err:
-            self.logger.exception(err, stack_info=True)
+            self.logger.exception(
+                err, stack_info=True
+            )
         except Exception as exc:
             raise QWException(
                 f"Error closing Worker: {exc}"
             )
         finally:
             self._loop.stop()
         if self.debug is True:
             cPrint(
                 '::: QueueWorker Server Closed ::: ',
                 level='INFO'
             )
 
-    def run_process(self, fn, loop):
-        """Unpickles task, runs it and pickles result."""
-        # loop = asyncio.new_event_loop()
-        asyncio.set_event_loop(loop)
-        fn.set_loop(loop)
-        try:
-            result = loop.run_until_complete(
-                self.run_function(fn)
-            )
-            print("run_process completed normally")
-            return result
-        except Exception as err:
-            raise QWException(
-                f"Error: {err}"
-            ) from err
-
-    async def run_function(self, fn):
-        result = None
-        self.logger.notice(
-            f'Running Task {fn!s} in worker {self.name!s}'
-        )
-        try:
-            if isinstance(fn, FuncWrapper):
-                result = await fn()
-            elif inspect.isawaitable(fn) or asyncio.iscoroutinefunction(fn):
-                result = await fn()
-            else:
-                result = fn()
-        except Exception as err:  # pylint: disable=W0703
-            print(err)
-            result = err
-        finally:
-            return result
-
-    async def run_task(self, task: TaskWrapper):
-        result = None
-        self.logger.debug(
-            f"Running Task: {task!s}"
-        )
-        try:
-            await task.create()
-            result = await task.run()
-        except Exception as err:  # pylint: disable=W0703
-            result = err
-        finally:
-            await task.close()
-        return result
-
-    async def task_handler(self, q: asyncio.Queue):
-        """Method for handling the tasks received by the connection handler."""
-        while True:
-            task = await q.get()
-            if self.debug:
-                cPrint(
-                    f'Running Queued Task {task!s}', level='DEBUG'
-                )
-            # try:
-            #    loop = asyncio.new_event_loop()
-            # except RuntimeError:
-            loop = asyncio.get_running_loop()
-            # processing the task received
-            if isinstance(task, TaskWrapper):
-                # Running a FlowTask Task
-                task.set_loop(loop)
-                task.debug = True
-                self.logger.debug(
-                    f'Running Queued Task: {task!s}'
-                )
-                try:
-                    fn = loop.create_task(self.run_task(task))
-                    result = await fn
-                    # with ThreadPoolExecutor(max_workers=2) as pool:
-                    #     future = loop.run_in_executor(pool, fn)
-                    #     result = await future
-                except (RuntimeError) as err:
-                    raise QWException(
-                        f"Error: {err}"
-                    ) from err
-                except Exception as err:
-                    print(err)
-            elif isinstance(task, FuncWrapper):
-                # running a FuncWrapper
-                result = None
-                try:
-                    result = await self.run_function(task)
-                except Exception as err:  # pylint: disable=W0703
-                    result = err
-                self.logger.debug(f'{task!s} Result: {result!r}')
-            else:
-                # TODO: try to Execute the object deserialized
-                pass
-            q.task_done()
-            self.logger.debug(
-                f'consumed: {task}'
-            )
-            self.logger.debug(
-                f'QUEUE Size after Work: {self.queue.qsize()}'
-            )
+    async def task_callback(self, task, **kwargs):
+        self.logger.debug('RUNNING TASK >>> {task}')
 
     def check_signature(self, payload: bytes) -> bool:
         signature = make_signature(expected_message, WORKER_SECRET_KEY)
         if signature == payload:
             return True
         else:
             return False
 
     async def response_keepalive(
         self,
         writer: asyncio.StreamWriter,
         status: dict = None
     ) -> None:
         addrs = ', '.join(str(sock.getsockname()) for sock in self._server.sockets)
-        prct_used = self.get_resource_usage()
         if not status:
             status = {
                 "pong": "Empty data",
                 "worker": {
                     "name": self.name,
-                    "serving": addrs,
-                    "resource": f"{prct_used:.2f}%"
+                    "serving": addrs
                 }
             }
         result = json_encoder(status)
         await self.closing_writer(writer, result.encode('utf-8'))
 
     async def worker_health(self, writer: asyncio.StreamWriter):
         addrs = ', '.join(str(sock.getsockname()) for sock in self._server.sockets)
         status = {
             "queue": {
-                "size": self.queue.qsize(),
+                "size": self.queue.size(),
                 "full": self.queue.full(),
                 "empty": self.queue.empty(),
-                "consumers": len(self.consumers)
+                "consumers": len(self.queue.consumers)
             },
             "worker": {
                 "name": self.name,
                 "address": self.server_address,
                 "serving": addrs
             }
         }
@@ -351,18 +204,18 @@
             "versions": get_versions(),
             "worker": {
                 "name": self.name,
                 "address": self.server_address,
                 "serving": addrs
             },
             "queue": {
-                "size": self.queue.qsize(),
+                "size": self.queue.size(),
                 "full": self.queue.full(),
                 "empty": self.queue.empty(),
-                "consumers": len(self.consumers)
+                "consumers": len(self.queue.consumers)
             },
         }
         await self.response_keepalive(
             status=status,
             writer=writer
         )
 
@@ -442,18 +295,22 @@
             if reader.at_eof():
                 break
         return serialized_task
 
     async def deserialize_task(self, serialized_task, writer: asyncio.StreamWriter):
         try:
             task = cloudpickle.loads(serialized_task)
-            self.logger.debug(f'TASK RECEIVED: {task}')
+            self.logger.debug(
+                f'TASK RECEIVED: {task} at {int(time.time())}'
+            )
             return task
         except RuntimeError as ex:
-            ex = ParserError(f"Error Decoding Serialized Task: {ex}")
+            ex = ParserError(
+                f"Error Decoding Serialized Task: {ex}"
+            )
             result = cloudpickle.dumps(ex)
             await self.closing_writer(writer, result)
             return False
 
     async def handle_queue_wrapper(
         self,
         task: QueueWrapper,
@@ -463,33 +320,27 @@
         """Handle QueueWrapper Tasks.
         """
         # Set Debug level of task:
         task.debug = self.debug
         if task.queued is True:
             try:
                 task.id = uid
-                await self.queue.put(task)
-                self.logger.debug(
-                    f'Current QUEUE Size: {self.queue.qsize()}'
+                await self.queue.put(
+                    task, id=task.id
                 )
                 return f'Task {task!s} with id {uid} was queued.'.encode('utf-8')
             except asyncio.QueueFull:
                 return await self.discard_task(
                     f"Worker {self.name!s} Queue is Full, discarding Task {task!r}"
                 )
         else:
             try:
                 # executed and send result to client
-                loop = asyncio.get_running_loop()
-                task.id = uid
-                task.set_loop(loop)
-                fn = loop.create_task(
-                    self.run_function(task)
-                )
-                return await fn
+                executor = TaskExecutor(task)
+                return await executor.run()
             except Exception as err:  # pylint: disable=W0703
                 try:
                     result = cloudpickle.dumps(err)
                 except Exception as ex:  # pylint: disable=W0703
                     result = cloudpickle.dumps(
                         QWException(
                             f'Error on Deal with Exception: {ex!s}'
@@ -508,57 +359,57 @@
         Args:
             reader: asyncio StreamReader, client information
             writer: asyncio StreamWriter, infor to send to client.
         Returns:
             Task Result.
         """
         # # TODO: task can select which executor to use, else use default:
-        addr = writer.get_extra_info("peername")
+        addr = writer.get_extra_info(
+            "peername"
+        )
         # first time: check signature authentication of payload:
         if not await self.signature_validation(reader, writer):
             return False
         self.logger.debug(
             f"Received Data from {addr!r} to worker {self.name!s} pid: {self._pid}"
         )
         # after: deserialize Task:
         serialized_task = await self._read_task(reader)
         task = None
         result = None
+        task = await self.deserialize_task(
+            serialized_task, writer
+        )
         task_uuid = uuid.uuid4()
-        task = await self.deserialize_task(serialized_task, writer)
         if not task:
             return False
         elif isinstance(task, QueueWrapper):
             if not (result := await self.handle_queue_wrapper(task, task_uuid, writer)):
                 return False
         elif callable(task):
-            result = await self.run_function(
-                task
-            )
+            executor = TaskExecutor(task)
+            result = await executor.run()
         else:
             # put work in Queue:
             try:
-                await self.queue.put(task)
-                await asyncio.sleep(.1)
-                self.logger.debug(
-                    f'Current QUEUE Size: {self.queue.qsize()}'
-                )
-                result = f'Task {task!s}:{task_uuid} was Queued.'.encode('utf-8')
+                await self.queue.put(task, id=task_uuid)
+                result = f'Task {task!s} was Queued.'.encode('utf-8')
             except asyncio.QueueFull:
-                self.logger.error(
-                    f"Worker Queue is Full, discarding Task {task!r}"
-                )
                 return await self.discard_task(
                     message=f'Task {task!s} was discarded, queue full',
                     writer=writer
                 )
         if result is None:
             # Not always a Task returns Value, sometimes returns None.
             result = [
-                {"uuid": task_uuid, "worker": self.name}
+                {
+                    "task": task,
+                    "uuid": task_uuid,
+                    "worker": self.name
+                }
             ]
         try:
             if isinstance(result, BaseException):
                 try:
                     msg = result.message
                 except Exception:
                     msg = str(result)
@@ -587,18 +438,22 @@
         """Sending results and closing the streamer."""
         try:
             writer.write(result)
             await writer.drain()
             if writer.can_write_eof():
                 writer.write_eof()
             if self.debug is True:
-                cPrint(f"Closing client socket, pid: {self._pid}", level='DEBUG')
+                cPrint(
+                    f"Closing client socket, pid: {self._pid}", level='DEBUG'
+                )
             writer.close()
         except Exception as e:
-            self.logger.error(f"Error while closing writer: {str(e)}")
+            self.logger.error(
+                f"Error while closing writer: {str(e)}"
+            )
 
 
 ### Start Server ###
 def start_server(num_worker, host, port, debug: bool):
     """thread worker function"""
     loop = None
     worker = None
```

## qw/wrappers/di_task.py

```diff
@@ -1,25 +1,26 @@
 """TaskWrapper.
 
-Wrapping a DI-task to be executed by Worker.
+Wrapping a Flowtask-task to be executed by Worker.
 """
+import asyncio
 import multiprocessing as mp
 from navconfig.logging import logging
 try:
     from flowtask.tasks.task import Task
     from flowtask.exceptions import (
         TaskNotFound,
         TaskError,
         TaskFailed
     )
 except ImportError:
     logging.warning(
-        "Unable to Load DataIntegrator Task Component, we can't send DI Tasks to any Worker."
+        "Unable to Load FlowTask Task Component, we can't send Tasks to any Worker."
     )
-from qw.exceptions import ProcessNotFound, QWException
+from qw.exceptions import QWException
 from .base import QueueWrapper
 
 class TaskWrapper(QueueWrapper):
     """Wraps a DI Task and arguments"""
     def __init__(self, program, task, *args, task_id: str = None, **kwargs):
         super(TaskWrapper, self).__init__(*args, **kwargs)
         try:
@@ -47,26 +48,30 @@
         return f'{self.id!s}'
 
     def __repr__(self):
         return f'Task(task={self.task}, program={self.program}, debug={self._debug})'
 
     async def create(self):
         try:
+            loop = self.loop
+        except AttributeError:
+            loop = asyncio.get_running_loop()
+        try:
             self._task = Task(
                 task=self.task,
                 program=self.program,
                 task_id=self.id,
-                loop=self.loop,
+                loop=loop,
                 worker=mp.current_process(),
                 new_args=self.new_args,
                 debug=self._debug,
                 **self.kwargs
             )
         except TaskNotFound as ex:
-            raise ProcessNotFound(
+            raise TaskNotFound(
                 f"Task Not Found: {ex}"
             )
         except TaskError:
             raise
         except Exception as err:
             logging.exception(err, stack_info=True)
             raise QWException(
@@ -89,15 +94,17 @@
                 stats = None
             result = {
                 "result": result,
                 "stats": stats
             }
             return result
         except Exception as err:  # pylint: disable=W0703
-            return TaskFailed(str(err))
+            raise TaskFailed(
+                f"{err}"
+            )
         finally:
             await self.close()
 
     async def run(self):
         """ Running the Task in the loop."""
         result = None
         print(f':: Starting Task {self.program}.{self.task}')
```

## qw/wrappers/base.py

```diff
@@ -1,28 +1,45 @@
 """
 Abstract Wrapper Base.
 
 Any other wrapper extends this.
 """
+from typing import Callable, Coroutine, Any
 import uuid
-from abc import ABC
 
+# coro = Callable[[int], Coroutine[Any, Any, str]]
 
-class QueueWrapper(ABC):
+class QueueWrapper:
     _queued: bool = True
     _debug: bool = False
 
-    def __init__(self, *args, **kwargs):
+    def __init__(self, coro=None, *args, **kwargs):
         if 'queued' in kwargs:
             self._queued = kwargs['queued']
             del kwargs['queued']
         self._id = uuid.uuid4()
         self.args = args
         self.kwargs = kwargs
         self.loop = None
+        ## retry functionality
+        self.retries = 0
+        # function to be handled:
+        self.coro = coro
+
+    async def call(self):
+        # Call the async function stored in the args[0] with *args[1:] and **kwargs
+        await self.coro(*self.args[1:], **self.kwargs)
+
+    async def __call__(self):
+        return await self.coro(
+            *self.args, **self.kwargs
+        )
+
+    def add_retries(self):
+        self.retries += 1
 
     @property
     def queued(self):
         return self._queued
 
     @queued.setter
     def queued(self, value):
```

## qw/wrappers/func.py

```diff
@@ -1,23 +1,32 @@
 """Functional Wrapper."""
 import asyncio
+from concurrent.futures import ThreadPoolExecutor
+from functools import partial
 from .base import QueueWrapper
 
 class FuncWrapper(QueueWrapper):
     """Wraps function and it's arguments."""
     def __init__(self, host, func, *args, **kwargs):
         super(FuncWrapper, self).__init__(*args, **kwargs)
         self.host = host
         self.func, self.args, self.kwargs = func, args, kwargs
 
     async def __call__(self):
         print(f'Calling Function {self.func.__name__}')
         if asyncio.iscoroutinefunction(self.func):
             return await self.func(*self.args, **self.kwargs)
         else:
-            return self.func(*self.args, **self.kwargs)
+            try:
+                loop = asyncio.get_running_loop()
+            except RuntimeError:
+                loop = asyncio.get_event_loop()
+            fn = partial(self.func, *self.args, **self.kwargs)
+            with ThreadPoolExecutor(max_workers=2) as executor:
+                future = loop.run_in_executor(executor, fn)
+            return await future
 
     def __repr__(self) -> str:
-        return '<%s> from %s' % (self.func.__name__, self.host) # pylint: disable=C0209
+        return '<%s> from %s' % (self.func.__name__, self.host)  # pylint: disable=C0209
 
     def __str__(self):
-        return '<%s> from %s' % (self.func.__name__, self.host) # pylint: disable=C0209
+        return '<%s> from %s' % (self.func.__name__, self.host)  # pylint: disable=C0209
```

## Comparing `qworker-1.7.8.dist-info/METADATA` & `qworker-1.8.0.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: qworker
-Version: 1.7.8
+Version: 1.8.0
 Summary: QueueWorker is asynchronous Task Queue implementation built on top of Asyncio.Can you spawn distributed workers to run functions inside workers.
 Home-page: https://github.com/phenobarbital/qworker
 Author: Jesus Lara
 Author-email: jesuslara@phenobarbital.info
 License: MIT
 Project-URL: Source, https://github.com/phenobarbital/qworker
 Project-URL: Funding, https://paypal.me/phenobarbital
```

## Comparing `qworker-1.7.8.dist-info/LICENSE` & `qworker-1.8.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `qworker-1.7.8.dist-info/RECORD` & `qworker-1.8.0.dist-info/RECORD`

 * *Files 14% similar despite different names*

```diff
@@ -1,25 +1,28 @@
 qw/__init__.py,sha256=awMNjg7WGznbrNuJLoEJV6BbnREk4BPD8k9My8BD7Uo,137
-qw/version.py,sha256=D0vRiiPxapQdVyRpMBKng_WqTaJbu1gWKGM9HWd7IS4,622
+qw/version.py,sha256=NWxw2FqlWBfP9A3096yNY2O3wPHQfZsZBUuQ4kPF7H4,622
 qw/process.py,sha256=5hTfos_sBg81tSfkNRRBQxwwMJMyWjLlZs4KSAAmmVA,7462
-qw/conf.py,sha256=NwJmDyUJipLs70ygOONO3Z6HikecnBLzigMvJJPP3L0,2370
+qw/conf.py,sha256=hl1C4K6P79mSSKqQcpbNODLMa11Ymvl-Go0ceBaHqfI,2598
 qw/protocols.py,sha256=FkBzIGCRjDUiYPzoUrvIXOHCq_LSplEJXVtLfsn8B04,4129
 qw/__main__.py,sha256=oUCTmeloFK83is_ipZ3yoRreu23TIcAELKhpOiIt_Xc,2392
 qw/exceptions.cpython-39-x86_64-linux-gnu.so,sha256=31p1G11akVAHtoIku71L-pdOUKnmz0rZrK3glmOCQ9A,568544
-qw/client.py,sha256=RqMTMbATZOhUD5lGdN6MIFOlzk5_pGzq9nM_eUuGzDU,15590
-qw/server.py,sha256=dUAvf4BzkcXjkdd4bwKmvOnCp7PWNEqsAY_YmWvGLEA,21225
+qw/client.py,sha256=5R8eAv-9S7ozi5VQxIXrq9bUQbBS1gQgBpscoUWSPJg,15767
+qw/server.py,sha256=NwxnJR7i6vWCpcLwbJELKkSL-h0OD0dSA36kbTzfsPY,15939
 qw/discovery.py,sha256=l_Lb3Bmni6WTTu5fxzj4-9KquiRak1rq8k9I70f_hSI,2160
 qw/decorators.py,sha256=m53pMJcotaf6XLJXbaSO6ywIgN8TWvbk__DbbsLWvkE,379
 qw/wrappers/__init__.py,sha256=Ot_f0GTaDB50Za4Hxsz2FZSkZDn4zZoDHjXOvqj9T9k,320
-qw/wrappers/di_task.py,sha256=lTLHpDhy3YK08Hd4jdSJkFfEaahrwHTmNlqo02AfpmM,4180
-qw/wrappers/base.py,sha256=d9jR9pbNUt9B6cIlHtUokJXkSaPT0S0iI_AIbg4WoY0,920
-qw/wrappers/func.py,sha256=TOSZWL6_eynS0FqdQwSYDu_oW88DLZuj_i6QfWUHEkI,853
+qw/wrappers/di_task.py,sha256=rjxsQfOeAd_awocCx6st8e0-J4bFLMamUCi98F5jpTM,4315
+qw/wrappers/base.py,sha256=FeeaDDGLZH1Q6PN4dxlshBHSB5fMGEoVXRrhQB3xtBA,1447
+qw/wrappers/func.py,sha256=sL43tB6BWPbz-iOpkySTaOIpGtd6J1F37R8V7WfwcE8,1246
 qw/utils/versions.py,sha256=d8AdLmhM1bPc82vTAshCJBljGr-Ur3_hiZ_GtTbbORA,597
 qw/utils/__init__.py,sha256=bYf_I4ymTf8vsqMjK00NJi2-A-lVijPTwN6HDOVjcjQ,46
 qw/utils/functions.py,sha256=9iXVvYLtQzOK0tRecOV2Oqrl-2raxAHwBCNilLui1xQ,512
 qw/utils/json.cpython-39-x86_64-linux-gnu.so,sha256=cL7bP26M2Hib8DD0QMRY-AL6izzzBUWrJxxWHbIEkhA,434800
-qworker-1.7.8.dist-info/METADATA,sha256=CtPTlx1CtZYcz1hvuxSdYsh_x6xw2WF8fZv6O5G41iU,3170
-qworker-1.7.8.dist-info/LICENSE,sha256=EW8vB8vWRFvBxGC3soG2HCxkuNrQpOGUgNFd81h7u54,1070
-qworker-1.7.8.dist-info/RECORD,,
-qworker-1.7.8.dist-info/entry_points.txt,sha256=ooHTYYyEjHI9Rj79mHQmU_s4HP-PTvD3g3xSa26vIGc,40
-qworker-1.7.8.dist-info/top_level.txt,sha256=2NxbFCeI_G1kzzf628VnbDchanCX6rcTJORdENY-rHI,3
-qworker-1.7.8.dist-info/WHEEL,sha256=G1-Tt_WO10x9oXjmzv4wXxMLbyVnQaK1ig1QydJbPuA,217
+qw/queues/__init__.py,sha256=itGqt8q7feqZePbUWJTeA82Q2AwU0B2agUFXRmpLuDc,62
+qw/queues/manager.py,sha256=pZ4CyWLJQJxzVd3BN7n2dtawJK1FYcI_ug8iKSt2afM,4827
+qw/executor/__init__.py,sha256=o7IudTVdo2Z-353vuZBVj1gsPNELHnv0Ko5PC1VrqMM,1845
+qworker-1.8.0.dist-info/METADATA,sha256=u35jKt4OU8XGTeqB08DcX25Mv4dxMUzS1pSK43H4bVo,3170
+qworker-1.8.0.dist-info/LICENSE,sha256=EW8vB8vWRFvBxGC3soG2HCxkuNrQpOGUgNFd81h7u54,1070
+qworker-1.8.0.dist-info/RECORD,,
+qworker-1.8.0.dist-info/entry_points.txt,sha256=ooHTYYyEjHI9Rj79mHQmU_s4HP-PTvD3g3xSa26vIGc,40
+qworker-1.8.0.dist-info/top_level.txt,sha256=2NxbFCeI_G1kzzf628VnbDchanCX6rcTJORdENY-rHI,3
+qworker-1.8.0.dist-info/WHEEL,sha256=G1-Tt_WO10x9oXjmzv4wXxMLbyVnQaK1ig1QydJbPuA,217
```

