# Comparing `tmp/phiflow-2.3.4.tar.gz` & `tmp/phiflow-2.4.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dist\phiflow-2.3.4.tar", last modified: Thu Apr 20 17:29:57 2023, max compression
+gzip compressed data, was "dist\phiflow-2.4.0.tar", last modified: Sun Jun  4 18:42:18 2023, max compression
```

## Comparing `phiflow-2.3.4.tar` & `phiflow-2.4.0.tar`

### file list

```diff
@@ -1,116 +1,122 @@
-drwxrwxrwx   0        0        0        0 2023-04-20 17:29:57.395654 phiflow-2.3.4/
--rw-rw-rw-   0        0        0       95 2022-04-20 10:42:57.000000 phiflow-2.3.4/MANIFEST.in
--rw-rw-rw-   0        0        0     2453 2023-04-20 17:29:57.395654 phiflow-2.3.4/PKG-INFO
--rw-rw-rw-   0        0        0     8330 2023-02-26 20:09:34.000000 phiflow-2.3.4/README.md
-drwxrwxrwx   0        0        0        0 2023-04-20 17:29:57.075367 phiflow-2.3.4/phi/
--rw-rw-rw-   0        0        0        5 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/VERSION
--rw-rw-rw-   0        0        0     1944 2023-02-26 20:09:34.000000 phiflow-2.3.4/phi/__init__.py
--rw-rw-rw-   0        0        0     8988 2023-02-26 20:09:34.000000 phiflow-2.3.4/phi/_troubleshoot.py
-drwxrwxrwx   0        0        0        0 2023-04-20 17:29:57.106392 phiflow-2.3.4/phi/field/
--rw-rw-rw-   0        0        0     2349 2023-02-26 20:09:34.000000 phiflow-2.3.4/phi/field/__init__.py
--rw-rw-rw-   0        0        0     2499 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/field/_angular_velocity.py
--rw-rw-rw-   0        0        0     3553 2023-02-26 20:09:34.000000 phiflow-2.3.4/phi/field/_embed.py
--rw-rw-rw-   0        0        0    17883 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/field/_field.py
--rw-rw-rw-   0        0        0     5129 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/field/_field_io.py
--rw-rw-rw-   0        0        0    40467 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/field/_field_math.py
--rw-rw-rw-   0        0        0    33185 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/field/_grid.py
--rw-rw-rw-   0        0        0     1584 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/field/_mask.py
--rw-rw-rw-   0        0        0     2837 2023-02-26 20:09:34.000000 phiflow-2.3.4/phi/field/_noise.py
--rw-rw-rw-   0        0        0    12380 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/field/_point_cloud.py
--rw-rw-rw-   0        0        0    21286 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/field/_scene.py
--rw-rw-rw-   0        0        0     1549 2023-03-17 11:48:14.000000 phiflow-2.3.4/phi/flow.py
-drwxrwxrwx   0        0        0        0 2023-04-20 17:29:57.131415 phiflow-2.3.4/phi/geom/
--rw-rw-rw-   0        0        0      538 2023-01-12 19:29:54.000000 phiflow-2.3.4/phi/geom/__init__.py
--rw-rw-rw-   0        0        0    21250 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/geom/_box.py
--rw-rw-rw-   0        0        0    19954 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/geom/_geom.py
--rw-rw-rw-   0        0        0     4456 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/geom/_sphere.py
--rw-rw-rw-   0        0        0     3929 2023-02-26 20:09:34.000000 phiflow-2.3.4/phi/geom/_stack.py
--rw-rw-rw-   0        0        0     7916 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/geom/_transform.py
--rw-rw-rw-   0        0        0     3458 2023-02-26 20:09:34.000000 phiflow-2.3.4/phi/geom/_union.py
-drwxrwxrwx   0        0        0        0 2023-04-20 17:29:57.144427 phiflow-2.3.4/phi/jax/
--rw-rw-rw-   0        0        0      463 2022-04-20 10:42:57.000000 phiflow-2.3.4/phi/jax/__init__.py
--rw-rw-rw-   0        0        0    20075 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/jax/_jax_backend.py
--rw-rw-rw-   0        0        0      907 2022-04-20 10:42:57.000000 phiflow-2.3.4/phi/jax/flow.py
-drwxrwxrwx   0        0        0        0 2023-04-20 17:29:57.151433 phiflow-2.3.4/phi/jax/stax/
--rw-rw-rw-   0        0        0        0 2022-04-20 10:42:57.000000 phiflow-2.3.4/phi/jax/stax/__init__.py
--rw-rw-rw-   0        0        0      795 2023-03-17 11:48:14.000000 phiflow-2.3.4/phi/jax/stax/flow.py
--rw-rw-rw-   0        0        0    46154 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/jax/stax/nets.py
-drwxrwxrwx   0        0        0        0 2023-04-20 17:29:57.179459 phiflow-2.3.4/phi/math/
--rw-rw-rw-   0        0        0     4221 2023-04-04 14:12:05.000000 phiflow-2.3.4/phi/math/__init__.py
--rw-rw-rw-   0        0        0     3842 2023-02-26 18:31:51.000000 phiflow-2.3.4/phi/math/_fit.py
--rw-rw-rw-   0        0        0    57009 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/math/_functional.py
--rw-rw-rw-   0        0        0    35833 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/math/_magic_ops.py
--rw-rw-rw-   0        0        0    32377 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/math/_nd.py
--rw-rw-rw-   0        0        0   109916 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/math/_ops.py
--rw-rw-rw-   0        0        0    33308 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/math/_optimize.py
--rw-rw-rw-   0        0        0    73123 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/math/_shape.py
--rw-rw-rw-   0        0        0    37406 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/math/_sparse.py
--rw-rw-rw-   0        0        0   109375 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/math/_tensors.py
--rw-rw-rw-   0        0        0    18445 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/math/_trace.py
-drwxrwxrwx   0        0        0        0 2023-04-20 17:29:57.200477 phiflow-2.3.4/phi/math/backend/
--rw-rw-rw-   0        0        0      787 2022-04-20 10:42:57.000000 phiflow-2.3.4/phi/math/backend/__init__.py
--rw-rw-rw-   0        0        0    59048 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/math/backend/_backend.py
--rw-rw-rw-   0        0        0     6172 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/math/backend/_dtype.py
--rw-rw-rw-   0        0        0    22325 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/math/backend/_linalg.py
--rw-rw-rw-   0        0        0     9054 2023-02-26 20:09:34.000000 phiflow-2.3.4/phi/math/backend/_minimize.py
--rw-rw-rw-   0        0        0    21139 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/math/backend/_numpy_backend.py
--rw-rw-rw-   0        0        0    22490 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/math/backend/_profile.py
--rw-rw-rw-   0        0        0    59082 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/math/extrapolation.py
--rw-rw-rw-   0        0        0    31622 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/math/magic.py
-drwxrwxrwx   0        0        0        0 2023-04-20 17:29:57.210486 phiflow-2.3.4/phi/physics/
--rw-rw-rw-   0        0        0      480 2023-02-26 20:09:34.000000 phiflow-2.3.4/phi/physics/__init__.py
--rw-rw-rw-   0        0        0    18286 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/physics/_boundaries.py
--rw-rw-rw-   0        0        0     8477 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/physics/advect.py
--rw-rw-rw-   0        0        0     4850 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/physics/diffuse.py
--rw-rw-rw-   0        0        0    15532 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/physics/fluid.py
-drwxrwxrwx   0        0        0        0 2023-04-20 17:29:57.249522 phiflow-2.3.4/phi/tf/
--rw-rw-rw-   0        0        0     1224 2022-05-20 17:17:38.000000 phiflow-2.3.4/phi/tf/__init__.py
--rw-rw-rw-   0        0        0     4228 2022-04-20 10:42:57.000000 phiflow-2.3.4/phi/tf/_compile_cuda.py
--rw-rw-rw-   0        0        0     2085 2022-04-20 10:42:57.000000 phiflow-2.3.4/phi/tf/_profiling.py
--rw-rw-rw-   0        0        0    29927 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/tf/_tf_backend.py
--rw-rw-rw-   0        0        0     3280 2022-04-20 10:42:57.000000 phiflow-2.3.4/phi/tf/_tf_cuda_resample.py
--rw-rw-rw-   0        0        0     1175 2023-03-17 11:48:14.000000 phiflow-2.3.4/phi/tf/flow.py
--rw-rw-rw-   0        0        0    34622 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/tf/nets.py
-drwxrwxrwx   0        0        0        0 2023-04-20 17:29:57.262534 phiflow-2.3.4/phi/torch/
--rw-rw-rw-   0        0        0      498 2022-04-20 10:42:57.000000 phiflow-2.3.4/phi/torch/__init__.py
--rw-rw-rw-   0        0        0    55689 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/torch/_torch_backend.py
--rw-rw-rw-   0        0        0     1145 2023-03-17 11:48:14.000000 phiflow-2.3.4/phi/torch/flow.py
--rw-rw-rw-   0        0        0    39392 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/torch/nets.py
-drwxrwxrwx   0        0        0        0 2023-04-20 17:29:57.284554 phiflow-2.3.4/phi/vis/
--rw-rw-rw-   0        0        0     1089 2023-03-17 11:48:08.000000 phiflow-2.3.4/phi/vis/__init__.py
-drwxrwxrwx   0        0        0        0 2023-04-20 17:29:57.312580 phiflow-2.3.4/phi/vis/_console/
--rw-rw-rw-   0        0        0       36 2022-04-20 10:42:57.000000 phiflow-2.3.4/phi/vis/_console/__init__.py
--rw-rw-rw-   0        0        0     6865 2022-04-20 10:42:57.000000 phiflow-2.3.4/phi/vis/_console/_console_gui.py
--rw-rw-rw-   0        0        0     3140 2022-04-20 10:42:57.000000 phiflow-2.3.4/phi/vis/_console/_console_plot.py
--rw-rw-rw-   0        0        0     2360 2022-04-20 10:42:57.000000 phiflow-2.3.4/phi/vis/_console/_console_util.py
-drwxrwxrwx   0        0        0        0 2023-04-20 17:29:57.372633 phiflow-2.3.4/phi/vis/_dash/
--rw-rw-rw-   0        0        0        0 2022-04-20 10:42:57.000000 phiflow-2.3.4/phi/vis/_dash/__init__.py
--rw-rw-rw-   0        0        0    21982 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/vis/_dash/_plotly_plots.py
--rw-rw-rw-   0        0        0     7826 2022-04-20 10:42:57.000000 phiflow-2.3.4/phi/vis/_dash/board.py
--rw-rw-rw-   0        0        0     4222 2023-03-17 11:48:08.000000 phiflow-2.3.4/phi/vis/_dash/colormaps.py
--rw-rw-rw-   0        0        0     2954 2022-04-20 10:42:57.000000 phiflow-2.3.4/phi/vis/_dash/dash_app.py
--rw-rw-rw-   0        0        0     7143 2022-04-20 10:42:57.000000 phiflow-2.3.4/phi/vis/_dash/dash_gui.py
--rw-rw-rw-   0        0        0     2075 2022-04-20 10:42:57.000000 phiflow-2.3.4/phi/vis/_dash/info.py
--rw-rw-rw-   0        0        0     1228 2022-04-20 10:42:57.000000 phiflow-2.3.4/phi/vis/_dash/log.py
--rw-rw-rw-   0        0        0     5359 2022-04-20 10:42:57.000000 phiflow-2.3.4/phi/vis/_dash/model_controls.py
--rw-rw-rw-   0        0        0     3493 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/vis/_dash/player_controls.py
--rw-rw-rw-   0        0        0     2710 2023-02-26 20:09:34.000000 phiflow-2.3.4/phi/vis/_dash/viewer.py
--rw-rw-rw-   0        0        0     5890 2022-04-20 10:42:57.000000 phiflow-2.3.4/phi/vis/_dash/viewsettings.py
--rw-rw-rw-   0        0        0     3046 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/vis/_log.py
-drwxrwxrwx   0        0        0        0 2023-04-20 17:29:57.379642 phiflow-2.3.4/phi/vis/_matplotlib/
--rw-rw-rw-   0        0        0      108 2023-02-26 20:09:34.000000 phiflow-2.3.4/phi/vis/_matplotlib/__init__.py
--rw-rw-rw-   0        0        0    23781 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/vis/_matplotlib/_matplotlib_plots.py
--rw-rw-rw-   0        0        0     7439 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/vis/_matplotlib/_scalars.py
--rw-rw-rw-   0        0        0      793 2023-03-17 11:48:08.000000 phiflow-2.3.4/phi/vis/_plot_util.py
--rw-rw-rw-   0        0        0     7184 2022-04-20 10:42:57.000000 phiflow-2.3.4/phi/vis/_user_namespace.py
--rw-rw-rw-   0        0        0    10690 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/vis/_viewer.py
--rw-rw-rw-   0        0        0    27058 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/vis/_vis.py
--rw-rw-rw-   0        0        0    17070 2023-04-20 17:29:41.000000 phiflow-2.3.4/phi/vis/_vis_base.py
-drwxrwxrwx   0        0        0        0 2023-04-20 17:29:57.394654 phiflow-2.3.4/phiflow.egg-info/
--rw-rw-rw-   0        0        0     2453 2023-04-20 17:29:56.000000 phiflow-2.3.4/phiflow.egg-info/PKG-INFO
--rw-rw-rw-   0        0        0     2278 2023-04-20 17:29:56.000000 phiflow-2.3.4/phiflow.egg-info/SOURCES.txt
--rw-rw-rw-   0        0        0        1 2023-04-20 17:29:56.000000 phiflow-2.3.4/phiflow.egg-info/dependency_links.txt
--rw-rw-rw-   0        0        0       30 2023-04-20 17:29:56.000000 phiflow-2.3.4/phiflow.egg-info/requires.txt
--rw-rw-rw-   0        0        0        4 2023-04-20 17:29:56.000000 phiflow-2.3.4/phiflow.egg-info/top_level.txt
--rw-rw-rw-   0        0        0       86 2023-04-20 17:29:57.396656 phiflow-2.3.4/setup.cfg
--rw-rw-rw-   0        0        0     7119 2023-02-26 18:31:51.000000 phiflow-2.3.4/setup.py
+drwxrwxrwx   0        0        0        0 2023-06-04 18:42:18.000000 phiflow-2.4.0/
+-rw-rw-rw-   0        0        0       97 2022-09-01 13:34:14.000000 phiflow-2.4.0/MANIFEST.in
+-rw-rw-rw-   0        0        0     2453 2023-06-04 18:42:18.000000 phiflow-2.4.0/PKG-INFO
+-rw-rw-rw-   0        0        0     8587 2023-06-04 10:30:49.000000 phiflow-2.4.0/README.md
+drwxrwxrwx   0        0        0        0 2023-06-04 18:42:17.000000 phiflow-2.4.0/phi/
+-rw-rw-rw-   0        0        0        5 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/VERSION
+-rw-rw-rw-   0        0        0     2052 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/__init__.py
+-rw-rw-rw-   0        0        0     9238 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/_troubleshoot.py
+drwxrwxrwx   0        0        0        0 2023-06-04 18:42:17.000000 phiflow-2.4.0/phi/field/
+-rw-rw-rw-   0        0        0     2440 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/field/__init__.py
+-rw-rw-rw-   0        0        0     2558 2023-05-02 11:45:13.000000 phiflow-2.4.0/phi/field/_angular_velocity.py
+-rw-rw-rw-   0        0        0     3652 2023-05-02 11:45:13.000000 phiflow-2.4.0/phi/field/_embed.py
+-rw-rw-rw-   0        0        0    19026 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/field/_field.py
+-rw-rw-rw-   0        0        0     5246 2023-05-02 11:45:13.000000 phiflow-2.4.0/phi/field/_field_io.py
+-rw-rw-rw-   0        0        0    43236 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/field/_field_math.py
+-rw-rw-rw-   0        0        0    34208 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/field/_grid.py
+-rw-rw-rw-   0        0        0     1628 2023-05-02 11:20:46.000000 phiflow-2.4.0/phi/field/_mask.py
+-rw-rw-rw-   0        0        0     5063 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/field/_mesh.py
+-rw-rw-rw-   0        0        0     2901 2023-05-02 11:45:13.000000 phiflow-2.4.0/phi/field/_noise.py
+-rw-rw-rw-   0        0        0    12751 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/field/_point_cloud.py
+-rw-rw-rw-   0        0        0    22580 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/field/_scene.py
+-rw-rw-rw-   0        0        0     1602 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/flow.py
+drwxrwxrwx   0        0        0        0 2023-06-04 18:42:17.000000 phiflow-2.4.0/phi/geom/
+-rw-rw-rw-   0        0        0      582 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/geom/__init__.py
+-rw-rw-rw-   0        0        0    22188 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/geom/_box.py
+-rw-rw-rw-   0        0        0    20545 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/geom/_geom.py
+-rw-rw-rw-   0        0        0    10406 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/geom/_poly_surface.py
+-rw-rw-rw-   0        0        0     4574 2023-05-02 11:20:46.000000 phiflow-2.4.0/phi/geom/_sphere.py
+-rw-rw-rw-   0        0        0     4031 2023-05-02 11:45:13.000000 phiflow-2.4.0/phi/geom/_stack.py
+-rw-rw-rw-   0        0        0     8150 2023-05-02 11:20:46.000000 phiflow-2.4.0/phi/geom/_transform.py
+-rw-rw-rw-   0        0        0     3563 2023-03-12 22:23:08.000000 phiflow-2.4.0/phi/geom/_union.py
+drwxrwxrwx   0        0        0        0 2023-06-04 18:42:17.000000 phiflow-2.4.0/phi/jax/
+-rw-rw-rw-   0        0        0      481 2022-07-04 14:35:24.000000 phiflow-2.4.0/phi/jax/__init__.py
+-rw-rw-rw-   0        0        0    24362 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/jax/_jax_backend.py
+-rw-rw-rw-   0        0        0      933 2022-09-01 13:34:14.000000 phiflow-2.4.0/phi/jax/flow.py
+drwxrwxrwx   0        0        0        0 2023-06-04 18:42:17.000000 phiflow-2.4.0/phi/jax/stax/
+-rw-rw-rw-   0        0        0        0 2022-09-01 13:34:14.000000 phiflow-2.4.0/phi/jax/stax/__init__.py
+-rw-rw-rw-   0        0        0      812 2023-05-02 11:20:46.000000 phiflow-2.4.0/phi/jax/stax/flow.py
+-rw-rw-rw-   0        0        0    47255 2023-05-02 11:20:46.000000 phiflow-2.4.0/phi/jax/stax/nets.py
+drwxrwxrwx   0        0        0        0 2023-06-04 18:42:17.000000 phiflow-2.4.0/phi/math/
+-rw-rw-rw-   0        0        0     5003 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/math/__init__.py
+-rw-rw-rw-   0        0        0     3877 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/math/_fit.py
+-rw-rw-rw-   0        0        0    57933 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/math/_functional.py
+-rw-rw-rw-   0        0        0    38057 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/math/_magic_ops.py
+-rw-rw-rw-   0        0        0    33016 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/math/_nd.py
+-rw-rw-rw-   0        0        0   128710 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/math/_ops.py
+-rw-rw-rw-   0        0        0    43583 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/math/_optimize.py
+-rw-rw-rw-   0        0        0    77701 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/math/_shape.py
+-rw-rw-rw-   0        0        0    53863 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/math/_sparse.py
+-rw-rw-rw-   0        0        0   110117 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/math/_tensors.py
+-rw-rw-rw-   0        0        0    18876 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/math/_trace.py
+drwxrwxrwx   0        0        0        0 2023-06-04 18:42:17.000000 phiflow-2.4.0/phi/math/backend/
+-rw-rw-rw-   0        0        0     1147 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/math/backend/__init__.py
+-rw-rw-rw-   0        0        0    66425 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/math/backend/_backend.py
+-rw-rw-rw-   0        0        0     6338 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/math/backend/_dtype.py
+-rw-rw-rw-   0        0        0    38986 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/math/backend/_linalg.py
+-rw-rw-rw-   0        0        0     9232 2023-03-12 22:23:08.000000 phiflow-2.4.0/phi/math/backend/_minimize.py
+-rw-rw-rw-   0        0        0    20677 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/math/backend/_numpy_backend.py
+-rw-rw-rw-   0        0        0     1355 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/math/backend/_object.py
+-rw-rw-rw-   0        0        0    12821 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/math/backend/_partition.py
+-rw-rw-rw-   0        0        0    18656 2023-06-01 12:45:39.000000 phiflow-2.4.0/phi/math/backend/_partition_draft.py
+-rw-rw-rw-   0        0        0    23053 2023-05-02 11:20:46.000000 phiflow-2.4.0/phi/math/backend/_profile.py
+-rw-rw-rw-   0        0        0    60504 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/math/extrapolation.py
+-rw-rw-rw-   0        0        0    33589 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/math/magic.py
+drwxrwxrwx   0        0        0        0 2023-06-04 18:42:17.000000 phiflow-2.4.0/phi/physics/
+-rw-rw-rw-   0        0        0      489 2023-03-12 22:23:08.000000 phiflow-2.4.0/phi/physics/__init__.py
+-rw-rw-rw-   0        0        0    18654 2023-05-02 11:45:13.000000 phiflow-2.4.0/phi/physics/_boundaries.py
+-rw-rw-rw-   0        0        0     8808 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/physics/advect.py
+-rw-rw-rw-   0        0        0     4969 2023-05-02 11:45:13.000000 phiflow-2.4.0/phi/physics/diffuse.py
+-rw-rw-rw-   0        0        0    15813 2023-05-02 11:45:13.000000 phiflow-2.4.0/phi/physics/fluid.py
+drwxrwxrwx   0        0        0        0 2023-06-04 18:42:17.000000 phiflow-2.4.0/phi/tf/
+-rw-rw-rw-   0        0        0     1260 2022-09-01 13:34:14.000000 phiflow-2.4.0/phi/tf/__init__.py
+-rw-rw-rw-   0        0        0     4334 2022-07-04 14:35:24.000000 phiflow-2.4.0/phi/tf/_compile_cuda.py
+-rw-rw-rw-   0        0        0     2145 2022-07-04 14:35:24.000000 phiflow-2.4.0/phi/tf/_profiling.py
+-rw-rw-rw-   0        0        0    33868 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/tf/_tf_backend.py
+-rw-rw-rw-   0        0        0     3373 2022-09-01 13:34:14.000000 phiflow-2.4.0/phi/tf/_tf_cuda_resample.py
+-rw-rw-rw-   0        0        0     1206 2023-05-02 11:20:46.000000 phiflow-2.4.0/phi/tf/flow.py
+-rw-rw-rw-   0        0        0    35305 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/tf/nets.py
+drwxrwxrwx   0        0        0        0 2023-06-04 18:42:17.000000 phiflow-2.4.0/phi/torch/
+-rw-rw-rw-   0        0        0      515 2022-07-04 14:35:24.000000 phiflow-2.4.0/phi/torch/__init__.py
+-rw-rw-rw-   0        0        0    60304 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/torch/_torch_backend.py
+-rw-rw-rw-   0        0        0     1174 2023-05-02 11:20:46.000000 phiflow-2.4.0/phi/torch/flow.py
+-rw-rw-rw-   0        0        0    40805 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/torch/nets.py
+drwxrwxrwx   0        0        0        0 2023-06-04 18:42:17.000000 phiflow-2.4.0/phi/vis/
+-rw-rw-rw-   0        0        0     1186 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/vis/__init__.py
+drwxrwxrwx   0        0        0        0 2023-06-04 18:42:17.000000 phiflow-2.4.0/phi/vis/_console/
+-rw-rw-rw-   0        0        0       36 2022-07-04 14:35:24.000000 phiflow-2.4.0/phi/vis/_console/__init__.py
+-rw-rw-rw-   0        0        0     7021 2022-07-04 14:35:24.000000 phiflow-2.4.0/phi/vis/_console/_console_gui.py
+-rw-rw-rw-   0        0        0     3225 2022-09-01 13:34:14.000000 phiflow-2.4.0/phi/vis/_console/_console_plot.py
+-rw-rw-rw-   0        0        0     2455 2022-07-04 14:35:24.000000 phiflow-2.4.0/phi/vis/_console/_console_util.py
+drwxrwxrwx   0        0        0        0 2023-06-04 18:42:17.000000 phiflow-2.4.0/phi/vis/_dash/
+-rw-rw-rw-   0        0        0        0 2022-07-04 14:35:24.000000 phiflow-2.4.0/phi/vis/_dash/__init__.py
+-rw-rw-rw-   0        0        0    22162 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/vis/_dash/_plotly_plots.py
+-rw-rw-rw-   0        0        0     8009 2022-09-01 13:34:14.000000 phiflow-2.4.0/phi/vis/_dash/board.py
+-rw-rw-rw-   0        0        0     4326 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/vis/_dash/colormaps.py
+-rw-rw-rw-   0        0        0     3047 2022-07-04 14:35:24.000000 phiflow-2.4.0/phi/vis/_dash/dash_app.py
+-rw-rw-rw-   0        0        0     7322 2022-09-01 13:34:14.000000 phiflow-2.4.0/phi/vis/_dash/dash_gui.py
+-rw-rw-rw-   0        0        0     2158 2022-07-04 14:35:24.000000 phiflow-2.4.0/phi/vis/_dash/info.py
+-rw-rw-rw-   0        0        0     1262 2022-07-04 14:35:24.000000 phiflow-2.4.0/phi/vis/_dash/log.py
+-rw-rw-rw-   0        0        0     5471 2022-09-01 13:34:14.000000 phiflow-2.4.0/phi/vis/_dash/model_controls.py
+-rw-rw-rw-   0        0        0     3589 2023-05-02 11:20:46.000000 phiflow-2.4.0/phi/vis/_dash/player_controls.py
+-rw-rw-rw-   0        0        0     2765 2023-05-02 11:45:13.000000 phiflow-2.4.0/phi/vis/_dash/viewer.py
+-rw-rw-rw-   0        0        0     6009 2023-05-02 11:45:13.000000 phiflow-2.4.0/phi/vis/_dash/viewsettings.py
+-rw-rw-rw-   0        0        0     2356 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/vis/_io.py
+-rw-rw-rw-   0        0        0     3453 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/vis/_log.py
+drwxrwxrwx   0        0        0        0 2023-06-04 18:42:18.000000 phiflow-2.4.0/phi/vis/_matplotlib/
+-rw-rw-rw-   0        0        0      110 2023-03-12 22:23:08.000000 phiflow-2.4.0/phi/vis/_matplotlib/__init__.py
+-rw-rw-rw-   0        0        0    36951 2023-06-04 15:54:57.000000 phiflow-2.4.0/phi/vis/_matplotlib/_matplotlib_plots.py
+-rw-rw-rw-   0        0        0     7757 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/vis/_matplotlib/_scalars.py
+-rw-rw-rw-   0        0        0     1604 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/vis/_plot_util.py
+-rw-rw-rw-   0        0        0     7396 2022-09-01 13:34:14.000000 phiflow-2.4.0/phi/vis/_user_namespace.py
+-rw-rw-rw-   0        0        0    11057 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/vis/_viewer.py
+-rw-rw-rw-   0        0        0    31233 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/vis/_vis.py
+-rw-rw-rw-   0        0        0    19518 2023-06-04 10:30:49.000000 phiflow-2.4.0/phi/vis/_vis_base.py
+drwxrwxrwx   0        0        0        0 2023-06-04 18:42:18.000000 phiflow-2.4.0/phiflow.egg-info/
+-rw-rw-rw-   0        0        0     2453 2023-06-04 18:42:17.000000 phiflow-2.4.0/phiflow.egg-info/PKG-INFO
+-rw-rw-rw-   0        0        0     2434 2023-06-04 18:42:17.000000 phiflow-2.4.0/phiflow.egg-info/SOURCES.txt
+-rw-rw-rw-   0        0        0        1 2023-06-04 18:42:17.000000 phiflow-2.4.0/phiflow.egg-info/dependency_links.txt
+-rw-rw-rw-   0        0        0       47 2023-06-04 18:42:17.000000 phiflow-2.4.0/phiflow.egg-info/requires.txt
+-rw-rw-rw-   0        0        0        4 2023-06-04 18:42:17.000000 phiflow-2.4.0/phiflow.egg-info/top_level.txt
+-rw-rw-rw-   0        0        0       86 2023-06-04 18:42:18.000000 phiflow-2.4.0/setup.cfg
+-rw-rw-rw-   0        0        0     7338 2023-06-04 10:30:49.000000 phiflow-2.4.0/setup.py
```

### filetype from file(1)

```diff
@@ -1 +1 @@
-POSIX tar archive
+POSIX tar archive (GNU)
```

### Comparing `phiflow-2.3.4/PKG-INFO` & `phiflow-2.4.0/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 Metadata-Version: 2.1
 Name: phiflow
-Version: 2.3.4
+Version: 2.4.0
 Summary: Differentiable PDE solving framework for machine learning
 Home-page: https://github.com/tum-pbs/PhiFlow
 Author: Philipp Holl
 Author-email: philipp.holl@tum.de
 License: MIT
-Download-URL: https://github.com/tum-pbs/PhiFlow/archive/2.3.4.tar.gz
+Download-URL: https://github.com/tum-pbs/PhiFlow/archive/2.4.0.tar.gz
 Description: # PhiFlow
         
         [**Homepage**](https://github.com/tum-pbs/PhiFlow)
         &nbsp;&nbsp;&nbsp; [**Documentation**](https://tum-pbs.github.io/PhiFlow/)
         &nbsp;&nbsp;&nbsp; [**API**](https://tum-pbs.github.io/PhiFlow/phi)
         &nbsp;&nbsp;&nbsp; [**Demos**](https://github.com/tum-pbs/PhiFlow/tree/master/demos)
         &nbsp;&nbsp;&nbsp; [<img src="https://www.tensorflow.org/images/colab_logo_32px.png" height=16> **Fluids Tutorial**](https://colab.research.google.com/github/tum-pbs/PhiFlow/blob/develop/docs/Fluids_Tutorial.ipynb#offline=true&sandboxMode=true)
```

### Comparing `phiflow-2.3.4/README.md` & `phiflow-2.4.0/README.md`

 * *Files 8% similar despite different names*

```diff
@@ -1,98 +1,101 @@
-# ![PhiFlow](docs/figures/Logo_DallE2_3_layout.png)
-
-![Build Status](https://github.com/tum-pbs/PhiFlow/actions/workflows/unit-tests.yml/badge.svg)
-[![PyPI pyversions](https://img.shields.io/pypi/pyversions/phiflow.svg)](https://pypi.org/project/phiflow/)
-[![PyPI license](https://img.shields.io/pypi/l/phiflow.svg)](https://pypi.org/project/phiflow/)
-[![Code Coverage](https://codecov.io/gh/tum-pbs/PhiFlow/branch/develop/graph/badge.svg)](https://codecov.io/gh/tum-pbs/PhiFlow/branch/develop/)
-[![Google Collab Book](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tum-pbs/PhiFlow/blob/develop/docs/Fluids_Tutorial.ipynb)
-
-Φ<sub>Flow</sub> is an open-source simulation toolkit built for optimization and machine learning applications.
-It is written mostly in Python and can be used with
-[NumPy](https://numpy.org/),
-[PyTorch](https://pytorch.org/),
-[Jax](https://github.com/google/jax)
-or [TensorFlow](https://www.tensorflow.org/).
-The close integration with these machine learning frameworks allows it to leverage their automatic differentiation functionality,
-making it easy to build end-to-end differentiable functions involving both learning models and physics simulations.
-
-[//]: # (![Gui]&#40;https://tum-pbs.github.io/PhiFlow/figures/WebInterface.png&#41;)
-
-| <img src="docs/figures/RenderedSmoke.gif">  | <img src="docs/figures/Animations.gif">                                                                                                                                                                                                                                                                       |
-|------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
-| [Fluids Tutorial](https://tum-pbs.github.io/PhiFlow/Fluids_Tutorial.html) &nbsp; • &nbsp; [Φ<sub>Flow</sub> to Blender](https://github.com/intergalactic-mammoth/phiflow2blender) | [Animation Gallery](https://tum-pbs.github.io/PhiFlow/Animations.html) &nbsp; • &nbsp; [Solar System](https://tum-pbs.github.io/PhiFlow/Planets_Tutorial.html) &nbsp; • &nbsp; [Learning to Throw](https://tum-pbs.github.io/PhiFlow/Learn_to_Throw_Tutorial.html) |
-
-
-## Features
-
-* Variety of built-in PDE operations with focus on fluid phenomena, allowing for concise formulation of simulations.
-* Tight integration with PyTorch, Jax and TensorFlow for straightforward neural network training with fully differentiable simulations that can [run on the GPU](https://tum-pbs.github.io/PhiFlow/GPU_Execution.html#enabling-gpu-execution).
-* Flexible, easy-to-use [web interface](https://tum-pbs.github.io/PhiFlow/Web_Interface.html) featuring live visualizations and interactive controls that can affect simulations or network training on the fly.
-* Object-oriented, vectorized design for expressive code, ease of use, flexibility and extensibility.
-* Reusable simulation code, independent of backend and dimensionality, i.e. the exact same code can run a 2D fluid sim using NumPy and a 3D fluid sim on the GPU using TensorFlow or PyTorch.
-* High-level linear equation solver with automated sparse matrix generation.
-
-
-## Installation
-
-Installation with [pip](https://pypi.org/project/pip/) on [Python 3.6](https://www.python.org/downloads/) and above:
-``` bash
-$ pip install phiflow
-```
-Install [PyTorch](https://pytorch.org/), [TensorFlow](https://www.tensorflow.org/install) or [Jax](https://github.com/google/jax#installation) in addition to Φ<sub>Flow</sub> to enable machine learning capabilities and GPU execution.
-To enable the web UI, also install [Dash](https://pypi.org/project/dash/).
-For optimal GPU performance, you may compile the custom CUDA operators, see the [detailed installation instructions](https://tum-pbs.github.io/PhiFlow/Installation_Instructions.html).
-
-You can verify your installation by running
-```bash
-$ python3 -c "import phi; phi.verify()"
-```
-This will check for compatible PyTorch, Jax and TensorFlow installations as well.
-
-## Documentation and Tutorials
-[**Documentation Overview**](https://tum-pbs.github.io/PhiFlow/)
-&nbsp; • &nbsp; [**▶ YouTube Tutorials**](https://www.youtube.com/playlist?list=PLYLhRkuWBmZ5R6hYzusA2JBIUPFEE755O)
-&nbsp; • &nbsp; [**API**](https://tum-pbs.github.io/PhiFlow/phi/)
-&nbsp; • &nbsp; [**Demos**](https://github.com/tum-pbs/PhiFlow/tree/master/demos)
-&nbsp; • &nbsp; [<img src="https://www.tensorflow.org/images/colab_logo_32px.png" height=16> **Playground**](https://colab.research.google.com/drive/1zBlQbmNguRt-Vt332YvdTqlV4DBcus2S#offline=true&sandboxMode=true)
-
-To get started, check out our YouTube tutorial series and the following Jupyter notebooks:
-
-* [<img src="https://www.tensorflow.org/images/colab_logo_32px.png" height=16>](https://colab.research.google.com/github/tum-pbs/PhiFlow/blob/develop/docs/Math_Introduction.ipynb) [Tensors](https://tum-pbs.github.io/PhiFlow/Math_Introduction.html): Introduction to tensors.
-* [<img src="https://www.tensorflow.org/images/colab_logo_32px.png" height=16>](https://colab.research.google.com/github/tum-pbs/PhiFlow/blob/develop/docs/Fluids_Tutorial.ipynb) [Fluids](https://tum-pbs.github.io/PhiFlow/Fluids_Tutorial.html): Introduction to core classes and fluid-related functions.
-* [<img src="https://www.tensorflow.org/images/colab_logo_32px.png" height=16>](https://colab.research.google.com/github/tum-pbs/PhiFlow/blob/develop/docs/Planets_Tutorial.ipynb) [Solar System](https://tum-pbs.github.io/PhiFlow/Planets_Tutorial.html): Visualize a many-body system with Newtonian gravity.
-* [<img src="https://www.tensorflow.org/images/colab_logo_32px.png" height=16>](https://colab.research.google.com/github/tum-pbs/PhiFlow/blob/develop/docs/Learn_to_Throw_Tutorial.ipynb) [Learn to Throw](https://tum-pbs.github.io/PhiFlow/Learn_to_Throw_Tutorial.html): Train a neural network to hit a target, comparing supervised and differentiable physics losses.
-
-If you like to work with an IDE, like PyCharm or VS Code, the following demos will also be helpful:
-
-* [smoke_plume.py](demos/smoke_plume.py) runs a smoke simulation and displays it in the web interface.
-* [optimize_pressure.py](demos/differentiate_pressure.py) uses TensorFlow to optimize a velocity field and displays it in the web interface.
-
-## Publications
-
-We will upload a whitepaper, soon.
-In the meantime, please cite the ICLR 2020 paper.
-
-* [Learning to Control PDEs with Differentiable Physics](https://ge.in.tum.de/publications/2020-iclr-holl/), *Philipp Holl, Vladlen Koltun, Nils Thuerey*, ICLR 2020.
-* [Solver-in-the-Loop: Learning from Differentiable Physics to Interact with Iterative PDE-Solvers](https://arxiv.org/abs/2007.00016), *Kiwon Um, Raymond Fei, Philipp Holl, Robert Brand, Nils Thuerey*, NeurIPS 2020.
-* [Φ<sub>Flow</sub>: A Differentiable PDE Solving Framework for Deep Learning via Physical Simulations](https://montrealrobotics.ca/diffcvgp/), *Nils Thuerey, Kiwon Um, Philipp Holl*, DiffCVGP workshop at NeurIPS 2020.
-* [Physics-based Deep Learning](https://physicsbaseddeeplearning.org/intro.html) (book), *Nils Thuerey, Philipp Holl, Maximilian Mueller, Patrick Schnell, Felix Trost, Kiwon Um*, DiffCVGP workshop at NeurIPS 2020.
-* [Half-Inverse Gradients for Physical Deep Learning](https://arxiv.org/abs/2203.10131), *Patrick Schnell, Philipp Holl, Nils Thuerey*, ICLR 2022.
-* [Scale-invariant Learning by Physics Inversion](https://arxiv.org/abs/2109.15048), *Philipp Holl, Vladlen Koltun, Nils Thuerey*, NeurIPS 2022.
-
-Φ<sub>Flow</sub> has been used in the following data sets:
-
-* [PDEBench](https://github.com/pdebench/PDEBench)
-* [PDEarena](https://microsoft.github.io/pdearena/)
-
-## Version History
-
-The [Version history](https://github.com/tum-pbs/PhiFlow/releases) lists all major changes since release.
-The releases are also listed on [PyPI](https://pypi.org/project/phiflow/).
-
-## Contributions
-
-Contributions are welcome! Check out [this document](CONTRIBUTING.md) for guidelines.
-
-## Acknowledgements
-
-This work is supported by the ERC Starting Grant realFlow (StG-2015-637014) and the Intel Intelligent Systems Lab.
+# ![PhiFlow](docs/figures/Logo_DallE2_3_layout.png)
+
+![Build Status](https://github.com/tum-pbs/PhiFlow/actions/workflows/unit-tests.yml/badge.svg)
+[![PyPI pyversions](https://img.shields.io/pypi/pyversions/phiflow.svg)](https://pypi.org/project/phiflow/)
+[![PyPI license](https://img.shields.io/pypi/l/phiflow.svg)](https://pypi.org/project/phiflow/)
+[![Code Coverage](https://codecov.io/gh/tum-pbs/PhiFlow/branch/develop/graph/badge.svg)](https://codecov.io/gh/tum-pbs/PhiFlow/branch/develop/)
+[![Google Collab Book](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tum-pbs/PhiFlow/blob/develop/docs/Fluids_Tutorial.ipynb)
+
+Φ<sub>Flow</sub> is an open-source simulation toolkit built for optimization and machine learning applications.
+It is written mostly in Python and can be used with
+[NumPy](https://numpy.org/),
+[PyTorch](https://pytorch.org/),
+[Jax](https://github.com/google/jax)
+or [TensorFlow](https://www.tensorflow.org/).
+The close integration with these machine learning frameworks allows it to leverage their automatic differentiation functionality,
+making it easy to build end-to-end differentiable functions involving both learning models and physics simulations.
+
+[//]: # (![Gui]&#40;https://tum-pbs.github.io/PhiFlow/figures/WebInterface.png&#41;)
+
+| <img src="docs/figures/RenderedSmoke.gif">  | <img src="docs/figures/Animations.gif">                                                                                                                                                                                                                                                                       |
+|------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
+| [Fluids Tutorial](https://tum-pbs.github.io/PhiFlow/Fluids_Tutorial.html) &nbsp; • &nbsp; [Φ<sub>Flow</sub> to Blender](https://github.com/intergalactic-mammoth/phiflow2blender) | [Animation Gallery](https://tum-pbs.github.io/PhiFlow/Animations.html) &nbsp; • &nbsp; [Solar System](https://tum-pbs.github.io/PhiFlow/Planets_Tutorial.html) &nbsp; • &nbsp; [Learning to Throw](https://tum-pbs.github.io/PhiFlow/Learn_to_Throw_Tutorial.html) |
+
+
+## Features
+
+* Variety of built-in PDE operations with focus on fluid phenomena, allowing for concise formulation of simulations.
+* Tight integration with PyTorch, Jax and TensorFlow for straightforward neural network training with fully differentiable simulations that can [run on the GPU](https://tum-pbs.github.io/PhiFlow/GPU_Execution.html#enabling-gpu-execution).
+* Flexible, easy-to-use [web interface](https://tum-pbs.github.io/PhiFlow/Web_Interface.html) featuring live visualizations and interactive controls that can affect simulations or network training on the fly.
+* Object-oriented, vectorized design for expressive code, ease of use, flexibility and extensibility.
+* Reusable simulation code, independent of backend and dimensionality, i.e. the exact same code can run a 2D fluid sim using NumPy and a 3D fluid sim on the GPU using TensorFlow or PyTorch.
+* High-level linear equation solver with automated sparse matrix generation.
+
+
+## Installation
+
+Installation with [pip](https://pypi.org/project/pip/) on [Python 3.6](https://www.python.org/downloads/) and above:
+``` bash
+$ pip install phiflow
+```
+Install [PyTorch](https://pytorch.org/), [TensorFlow](https://www.tensorflow.org/install) or [Jax](https://github.com/google/jax#installation) in addition to Φ<sub>Flow</sub> to enable machine learning capabilities and GPU execution.
+To enable the web UI, also install [Dash](https://pypi.org/project/dash/).
+For optimal GPU performance, you may compile the custom CUDA operators, see the [detailed installation instructions](https://tum-pbs.github.io/PhiFlow/Installation_Instructions.html).
+
+You can verify your installation by running
+```bash
+$ python3 -c "import phi; phi.verify()"
+```
+This will check for compatible PyTorch, Jax and TensorFlow installations as well.
+
+## Documentation and Tutorials
+[**Documentation Overview**](https://tum-pbs.github.io/PhiFlow/)
+&nbsp; • &nbsp; [**▶ YouTube Tutorials**](https://www.youtube.com/playlist?list=PLYLhRkuWBmZ5R6hYzusA2JBIUPFEE755O)
+&nbsp; • &nbsp; [**API**](https://tum-pbs.github.io/PhiFlow/phi/)
+&nbsp; • &nbsp; [**Demos**](https://github.com/tum-pbs/PhiFlow/tree/master/demos)
+&nbsp; • &nbsp; [<img src="https://www.tensorflow.org/images/colab_logo_32px.png" height=16> **Playground**](https://colab.research.google.com/drive/1zBlQbmNguRt-Vt332YvdTqlV4DBcus2S#offline=true&sandboxMode=true)
+
+To get started, check out our YouTube tutorial series and the following Jupyter notebooks:
+
+* [<img src="https://www.tensorflow.org/images/colab_logo_32px.png" height=16>](https://colab.research.google.com/github/tum-pbs/PhiFlow/blob/develop/docs/Math_Introduction.ipynb) [Tensors](https://tum-pbs.github.io/PhiFlow/Math_Introduction.html): Introduction to tensors.
+* [<img src="https://www.tensorflow.org/images/colab_logo_32px.png" height=16>](https://colab.research.google.com/github/tum-pbs/PhiFlow/blob/develop/docs/Fluids_Tutorial.ipynb) [Fluids](https://tum-pbs.github.io/PhiFlow/Fluids_Tutorial.html): Introduction to core classes and fluid-related functions.
+* [<img src="https://www.tensorflow.org/images/colab_logo_32px.png" height=16>](https://colab.research.google.com/github/tum-pbs/PhiFlow/blob/develop/docs/Planets_Tutorial.ipynb) [Solar System](https://tum-pbs.github.io/PhiFlow/Planets_Tutorial.html): Visualize a many-body system with Newtonian gravity.
+* [<img src="https://www.tensorflow.org/images/colab_logo_32px.png" height=16>](https://colab.research.google.com/github/tum-pbs/PhiFlow/blob/develop/docs/Learn_to_Throw_Tutorial.ipynb) [Learn to Throw](https://tum-pbs.github.io/PhiFlow/Learn_to_Throw_Tutorial.html): Train a neural network to hit a target, comparing supervised and differentiable physics losses.
+
+If you like to work with an IDE, like PyCharm or VS Code, the following demos will also be helpful:
+
+* [smoke_plume.py](demos/smoke_plume.py) runs a smoke simulation and displays it in the web interface.
+* [optimize_pressure.py](demos/differentiate_pressure.py) uses TensorFlow to optimize a velocity field and displays it in the web interface.
+
+## Publications
+
+We will upload a whitepaper, soon.
+In the meantime, please cite the ICLR 2020 paper.
+
+* [Learning to Control PDEs with Differentiable Physics](https://ge.in.tum.de/publications/2020-iclr-holl/), *Philipp Holl, Vladlen Koltun, Nils Thuerey*, ICLR 2020.
+* [Solver-in-the-Loop: Learning from Differentiable Physics to Interact with Iterative PDE-Solvers](https://arxiv.org/abs/2007.00016), *Kiwon Um, Raymond Fei, Philipp Holl, Robert Brand, Nils Thuerey*, NeurIPS 2020.
+* [Φ<sub>Flow</sub>: A Differentiable PDE Solving Framework for Deep Learning via Physical Simulations](https://montrealrobotics.ca/diffcvgp/), *Nils Thuerey, Kiwon Um, Philipp Holl*, DiffCVGP workshop at NeurIPS 2020.
+* [Physics-based Deep Learning](https://physicsbaseddeeplearning.org/intro.html) (book), *Nils Thuerey, Philipp Holl, Maximilian Mueller, Patrick Schnell, Felix Trost, Kiwon Um*, DiffCVGP workshop at NeurIPS 2020.
+* [Half-Inverse Gradients for Physical Deep Learning](https://arxiv.org/abs/2203.10131), *Patrick Schnell, Philipp Holl, Nils Thuerey*, ICLR 2022.
+* [Scale-invariant Learning by Physics Inversion](https://arxiv.org/abs/2109.15048), *Philipp Holl, Vladlen Koltun, Nils Thuerey*, NeurIPS 2022.
+
+
+## Benchmarks & Data Sets
+
+Φ<sub>Flow</sub> has been used in the creation of various public data sets, such as
+[PDEBench](https://github.com/pdebench/PDEBench) and [PDEarena](https://microsoft.github.io/pdearena/).
+
+[See more packages that use Φ<sub>Flow</sub>](https://github.com/tum-pbs/PhiFlow/network/dependents)
+
+## Version History
+
+The [Version history](https://github.com/tum-pbs/PhiFlow/releases) lists all major changes since release.
+The releases are also listed on [PyPI](https://pypi.org/project/phiflow/).
+
+## Contributions
+
+Contributions are welcome! Check out [this document](CONTRIBUTING.md) for guidelines.
+
+## Acknowledgements
+
+This work is supported by the ERC Starting Grant realFlow (StG-2015-637014) and the Intel Intelligent Systems Lab.
```

### Comparing `phiflow-2.3.4/phi/__init__.py` & `phiflow-2.4.0/phi/__init__.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,73 +1,73 @@
-"""
-Open-source simulation toolkit built for optimization and machine learning applications.
-
-Use one of the following imports:
-
-* `from phi.flow import *`  for NumPy mode
-* `from phi.tf.flow import *`  for TensorFlow mode
-* `from phi.torch.flow import *` for PyTorch mode
-* `from phi.jax.flow import *` for Jax mode
-
-Project homepage: https://github.com/tum-pbs/PhiFlow
-
-Documentation overview: https://tum-pbs.github.io/PhiFlow
-
-PyPI: https://pypi.org/project/phiflow/
-"""
-
-import os as _os
-
-
-with open(_os.path.join(_os.path.dirname(__file__), 'VERSION'), 'r') as version_file:
-    __version__ = version_file.read()
-
-
-def verify():
-    """
-    Checks your configuration for potential problems and prints a summary.
-
-    To run verify without importing `phi`, run the script `tests/verify.py` included in the source distribution.
-    """
-    import sys
-    from ._troubleshoot import assert_minimal_config, troubleshoot
-    try:
-        assert_minimal_config()
-    except AssertionError as fail_err:
-        print("\n".join(fail_err.args), file=sys.stderr)
-        return
-    print(troubleshoot())
-
-
-def detect_backends() -> tuple:
-    """
-    Registers all available backends and returns them.
-    This includes only backends for which the minimal requirements are fulfilled.
-
-    Returns:
-        `tuple` of `phi.math.backend.Backend`
-    """
-    try:
-        from .jax import JAX
-    except ImportError:
-        pass
-    try:
-        from .torch import TORCH
-    except ImportError:
-        pass
-    try:
-        from .tf import TENSORFLOW
-    except ImportError:
-        pass
-    from .math.backend import BACKENDS
-    return tuple(BACKENDS)
-
-
-def set_logging_level(level='debug'):
-    """
-    Sets the logging level for PhiFlow functions.
-
-    Args:
-        level: Logging level, one of `'critical', 'fatal', 'error', 'warning', 'info', 'debug'`
-    """
-    from phi.math.backend import PHI_LOGGER
-    PHI_LOGGER.setLevel(level.upper())
+"""
+Open-source simulation toolkit built for optimization and machine learning applications.
+
+Use one of the following imports:
+
+* `from phi.flow import *`  for NumPy mode
+* `from phi.tf.flow import *`  for TensorFlow mode
+* `from phi.torch.flow import *` for PyTorch mode
+* `from phi.jax.flow import *` for Jax mode
+
+Project homepage: https://github.com/tum-pbs/PhiFlow
+
+Documentation overview: https://tum-pbs.github.io/PhiFlow
+
+PyPI: https://pypi.org/project/phiflow/
+"""
+
+import os as _os
+
+
+with open(_os.path.join(_os.path.dirname(__file__), 'VERSION'), 'r') as version_file:
+    __version__ = version_file.read()
+
+
+def verify():
+    """
+    Checks your configuration for potential problems and prints a summary.
+
+    To run verify without importing `phi`, run the script `tests/verify.py` included in the source distribution.
+    """
+    import sys
+    from ._troubleshoot import assert_minimal_config, troubleshoot
+    try:
+        assert_minimal_config()
+    except AssertionError as fail_err:
+        print("\n".join(fail_err.args), file=sys.stderr)
+        return
+    print(troubleshoot())
+
+
+def detect_backends() -> tuple:
+    """
+    Registers all available backends and returns them.
+    This includes only backends for which the minimal requirements are fulfilled.
+
+    Returns:
+        `tuple` of `phi.math.backend.Backend`
+    """
+    try:
+        from .jax import JAX
+    except ImportError:
+        pass
+    try:
+        from .torch import TORCH
+    except ImportError:
+        pass
+    try:
+        from .tf import TENSORFLOW
+    except ImportError:
+        pass
+    from .math.backend import BACKENDS
+    return tuple([b for b in BACKENDS if b.name != 'Python'])
+
+
+def set_logging_level(level='debug'):
+    """
+    Sets the logging level for PhiFlow functions.
+
+    Args:
+        level: Logging level, one of `'critical', 'fatal', 'error', 'warning', 'info', 'debug'`
+    """
+    from phi.math.backend import PHI_LOGGER
+    PHI_LOGGER.setLevel(level.upper())
```

### Comparing `phiflow-2.3.4/phi/field/__init__.py` & `phiflow-2.4.0/phi/field/__init__.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,58 +1,59 @@
-"""
-The fields module provides a number of data structures and functions to represent continuous, spatially varying data.
-
-All fields are subclasses of `Field` which provides abstract functions for sampling field values at physical locations.
-
-The most important field types are:
-
-* `CenteredGrid` embeds a tensor in the physical space. Uses linear interpolation between grid points.
-* `StaggeredGrid` samples the vector components at face centers instead of at cell centers.
-* `Noise` is a function that produces a procedurally generated noise field
-
-Use `grid()` to create a `Grid` from data or by sampling another `Field` or `phi.geom.Geometry`.
-Alternatively, the `phi.physics.Domain` class provides convenience methods for grid creation.
-
-All fields can be sampled at physical locations or volumes using `sample()` or `reduce_sample()`.
-
-See the `phi.field` module documentation at https://tum-pbs.github.io/PhiFlow/Fields.html
-"""
-
-from ._field import Field, SampledField, sample, reduce_sample, resample, as_extrapolation
-from ._mask import HardGeometryMask, SoftGeometryMask as GeometryMask, SoftGeometryMask
-from ._grid import Grid, CenteredGrid, StaggeredGrid
-from ._point_cloud import PointCloud
-from ._noise import Noise
-from ._angular_velocity import AngularVelocity
-from phi.math import (
-    abs, sign, round, ceil, floor, sqrt, exp, isfinite, is_finite, real, imag, sin, cos, cast, to_float, to_int32, to_int64, convert,
-    stop_gradient,
-    jit_compile, jit_compile_linear, functional_gradient, jacobian, gradient,
-    solve_linear, solve_nonlinear, minimize,
-    l2_loss, l1_loss, frequency_loss,
-    unstack, stack, concat  # expand, rename_dims, pack_dims, unpack_dims
-)
-from ._field_math import (
-    assert_close,
-    bake_extrapolation,
-    laplace, spatial_gradient, divergence, stagger, curl,  # spatial operators
-    fourier_poisson, fourier_laplace,
-    mean, pad, shift, normalize, center_of_mass,
-    concat, stack,
-    where, maximum, minimum,
-    vec_squared, vec_length as vec_abs, vec_length,
-    downsample2x, upsample2x,
-    finite_fill,
-    native_call,
-    integrate,
-    pack_dims,
-    support, mask,
-)
-from ._field_io import write, read
-from ._scene import Scene
-
-__all__ = [key for key in globals().keys() if not key.startswith('_')]
-
-__pdoc__ = {
-    'Grid.__init__': False,
-    'Scene.__init__': False,
-}
+"""
+The fields module provides a number of data structures and functions to represent continuous, spatially varying data.
+
+All fields are subclasses of `Field` which provides abstract functions for sampling field values at physical locations.
+
+The most important field types are:
+
+* `CenteredGrid` embeds a tensor in the physical space. Uses linear interpolation between grid points.
+* `StaggeredGrid` samples the vector components at face centers instead of at cell centers.
+* `Noise` is a function that produces a procedurally generated noise field
+
+Use `grid()` to create a `Grid` from data or by sampling another `Field` or `phi.geom.Geometry`.
+Alternatively, the `phi.physics.Domain` class provides convenience methods for grid creation.
+
+All fields can be sampled at physical locations or volumes using `sample()` or `reduce_sample()`.
+
+See the `phi.field` module documentation at https://tum-pbs.github.io/PhiFlow/Fields.html
+"""
+
+from ._field import Field, SampledField, sample, reduce_sample, resample, as_extrapolation
+from ._mask import HardGeometryMask, SoftGeometryMask as GeometryMask, SoftGeometryMask
+from ._grid import Grid, CenteredGrid, StaggeredGrid
+from ._point_cloud import PointCloud
+from ._noise import Noise
+from ._angular_velocity import AngularVelocity
+from phi.math import (
+    abs, sign, round, ceil, floor, sqrt, exp, isfinite, is_finite, real, imag, sin, cos, cast, to_float, to_int32, to_int64, convert,
+    stop_gradient,
+    jit_compile, jit_compile_linear, functional_gradient, jacobian, gradient,
+    solve_linear, solve_nonlinear, minimize,
+    l2_loss, l1_loss, frequency_loss,
+    unstack, stack, concat  # expand, rename_dims, pack_dims, unpack_dims
+)
+from ._field_math import (
+    assert_close,
+    bake_extrapolation,
+    laplace, spatial_gradient, divergence, stagger, curl,  # spatial operators
+    fourier_poisson, fourier_laplace,
+    mean, pad, shift, normalize, center_of_mass,
+    concat, stack,
+    where, maximum, minimum,
+    vec_squared, vec_length as vec_abs, vec_length,
+    downsample2x, upsample2x,
+    finite_fill,
+    native_call,
+    integrate,
+    pack_dims,
+    support, mask,
+    connect, connect_neighbors,
+)
+from ._field_io import write, read
+from ._scene import Scene
+
+__all__ = [key for key in globals().keys() if not key.startswith('_')]
+
+__pdoc__ = {
+    'Grid.__init__': False,
+    'Scene.__init__': False,
+}
```

### Comparing `phiflow-2.3.4/phi/field/_angular_velocity.py` & `phiflow-2.4.0/phi/field/_angular_velocity.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,59 +1,59 @@
-from typing import Callable, Union
-from numbers import Number
-
-from phi import math
-
-from ._field import Field
-from ..geom import Geometry
-from ..math import Shape, spatial, instance, Tensor, wrap
-
-
-class AngularVelocity(Field):
-    """
-    Model of a single vortex or set of vortices.
-    The falloff of the velocity magnitude can be controlled.
-
-    Without a specified falloff, the velocity increases linearly with the distance from the vortex center.
-    This is the case with rotating rigid bodies, for example.
-    """
-
-    def __init__(self,
-                 location: Union[Tensor, tuple, list, Number],
-                 strength: Union[Tensor, Number] = 1.0,
-                 falloff: Callable = None,
-                 component: str = None):
-        location = wrap(location)
-        strength = wrap(strength)
-        assert location.shape.channel.names == ('vector',), "location must have a single channel dimension called 'vector'"
-        assert location.shape.spatial.is_empty, "location tensor cannot have any spatial dimensions"
-        assert not instance(location), "AngularVelocity does not support instance dimensions"
-        self.location = location
-        self.strength = strength
-        self.falloff = falloff
-        self.component = component
-        spatial_names = location.vector.item_names
-        assert spatial_names is not None, "location.vector must list spatial dimensions as item names"
-        self._shape = location.shape & spatial(**{dim: 1 for dim in spatial_names})
-
-    def _sample(self, geometry: Geometry, **kwargs) -> Tensor:
-        points = geometry.center
-        distances = points - self.location
-        strength = self.strength if self.falloff is None else self.strength * self.falloff(distances)
-        velocity = math.cross_product(strength, distances)
-        velocity = math.sum(velocity, self.location.shape.batch.without(points.shape))
-        if self.component:
-            velocity = velocity.vector[self.component]
-        return velocity
-
-    @property
-    def shape(self) -> Shape:
-        return self._shape
-
-    def __getitem__(self, item: dict):
-        assert all(dim == 'vector' for dim in item), f"Cannot slice AngularVelocity with {item}"
-        if 'vector' in item:
-            assert item['vector'] == 0 or self.component is None
-            component = self.shape.spatial.names[item['vector']]
-            return AngularVelocity(self.location, self.strength, self.falloff, component)
-        else:
-            return self
+from typing import Callable, Union
+from numbers import Number
+
+from phi import math
+
+from ._field import Field
+from ..geom import Geometry
+from ..math import Shape, spatial, instance, Tensor, wrap
+
+
+class AngularVelocity(Field):
+    """
+    Model of a single vortex or set of vortices.
+    The falloff of the velocity magnitude can be controlled.
+
+    Without a specified falloff, the velocity increases linearly with the distance from the vortex center.
+    This is the case with rotating rigid bodies, for example.
+    """
+
+    def __init__(self,
+                 location: Union[Tensor, tuple, list, Number],
+                 strength: Union[Tensor, Number] = 1.0,
+                 falloff: Callable = None,
+                 component: str = None):
+        location = wrap(location)
+        strength = wrap(strength)
+        assert location.shape.channel.names == ('vector',), "location must have a single channel dimension called 'vector'"
+        assert location.shape.spatial.is_empty, "location tensor cannot have any spatial dimensions"
+        assert not instance(location), "AngularVelocity does not support instance dimensions"
+        self.location = location
+        self.strength = strength
+        self.falloff = falloff
+        self.component = component
+        spatial_names = location.vector.item_names
+        assert spatial_names is not None, "location.vector must list spatial dimensions as item names"
+        self._shape = location.shape & spatial(**{dim: 1 for dim in spatial_names})
+
+    def _sample(self, geometry: Geometry, **kwargs) -> Tensor:
+        points = geometry.center
+        distances = points - self.location
+        strength = self.strength if self.falloff is None else self.strength * self.falloff(distances)
+        velocity = math.cross_product(strength, distances)
+        velocity = math.sum(velocity, self.location.shape.batch.without(points.shape))
+        if self.component:
+            velocity = velocity.vector[self.component]
+        return velocity
+
+    @property
+    def shape(self) -> Shape:
+        return self._shape
+
+    def __getitem__(self, item: dict):
+        assert all(dim == 'vector' for dim in item), f"Cannot slice AngularVelocity with {item}"
+        if 'vector' in item:
+            assert item['vector'] == 0 or self.component is None
+            component = self.shape.spatial.names[item['vector']]
+            return AngularVelocity(self.location, self.strength, self.falloff, component)
+        else:
+            return self
```

### Comparing `phiflow-2.3.4/phi/field/_embed.py` & `phiflow-2.4.0/phi/field/_embed.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,99 +1,99 @@
-from phi.geom import GridCell, Box
-from phi.math import Tensor, spatial, Extrapolation, Shape, stack
-from ._field import Field, sample
-from ..math.extrapolation import Undefined, ConstantExtrapolation
-
-
-class FieldEmbedding(Extrapolation):
-
-    def __init__(self, field: Field):
-        super().__init__(pad_rank=1)
-        self.field = field
-
-    def to_dict(self) -> dict:
-        raise NotImplementedError("FieldEmbedding cannot be converted to dict")
-
-    def __value_attrs__(self):
-        return 'field',
-
-    def __getitem__(self, item):
-        return FieldEmbedding(self.field[item])
-
-    @staticmethod
-    def __stack__(values: tuple, dim: Shape, **kwargs) -> 'ConstantExtrapolation':
-        if all(isinstance(v, FieldEmbedding) for v in values):
-            return ConstantExtrapolation(stack([v.field for v in values], dim, **kwargs))
-        else:
-            return NotImplemented
-
-    def spatial_gradient(self) -> 'Extrapolation':
-        return NotImplemented
-        from ._field_math import spatial_gradient
-        return FieldEmbedding(spatial_gradient(self.field))  # this is not supported for all fields
-
-    def valid_outer_faces(self, dim) -> tuple:
-        return False, False
-
-    def pad_values(self, value: Tensor, width: int, dim: str, upper_edge: bool, bounds: Box = None, already_padded: dict = None, **kwargs) -> Tensor:
-        assert bounds is not None, f"{type(self)}.pad() requires 'bounds' argument"
-        if already_padded:
-            padded_res = spatial(**{dim: lo + up for dim, (lo, up) in already_padded.items()})
-            resolution = spatial(value) - padded_res
-            value_grid = GridCell(resolution, bounds).padded(already_padded)
-        else:
-            value_grid = GridCell(spatial(value), bounds)
-        if upper_edge:
-            pad_grid = value_grid.padded({dim: (0, width)})[{dim: slice(-width, None)}]
-        else:
-            pad_grid = value_grid.padded({dim: (width, 0)})[{dim: slice(0, width)}]
-        result = sample(self.field, pad_grid)
-        return result
-    
-    @property
-    def is_flexible(self) -> bool:
-        return False
-
-    def __repr__(self):
-        return repr(self.field)
-
-    def __abs__(self):
-        return Undefined(self)
-
-    def __neg__(self):
-        return Undefined(self)
-
-    def _op(self, other, op):
-        if isinstance(other, ConstantExtrapolation):  # some operations can be handled by ConstantExtrapolation, e.g. * 0
-            op = getattr(other, op.__name__)
-            result = op(self)
-            return Undefined(self) if result is NotImplemented else result
-        elif isinstance(other, FieldEmbedding):
-            if other.field is self.field:
-                return self
-            return Undefined(self)
-        else:
-            return Undefined(self)
-
-    def __add__(self, other):
-        return self._op(other, ConstantExtrapolation.__add__)
-
-    def __radd__(self, other):
-        return self._op(other, ConstantExtrapolation.__add__)
-
-    def __mul__(self, other):
-        return self._op(other, ConstantExtrapolation.__mul__)
-
-    def __rmul__(self, other):
-        return self._op(other, ConstantExtrapolation.__mul__)
-
-    def __sub__(self, other):
-        return self._op(other, ConstantExtrapolation.__rsub__)
-
-    def __rsub__(self, other):
-        return self._op(other, ConstantExtrapolation.__sub__)
-
-    def __truediv__(self, other):
-        return self._op(other, ConstantExtrapolation.__rtruediv__)
-
-    def __rtruediv__(self, other):
-        return self._op(other, ConstantExtrapolation.__truediv__)
+from phi.geom import GridCell, Box
+from phi.math import Tensor, spatial, Extrapolation, Shape, stack
+from ._field import Field, sample
+from ..math.extrapolation import Undefined, ConstantExtrapolation
+
+
+class FieldEmbedding(Extrapolation):
+
+    def __init__(self, field: Field):
+        super().__init__(pad_rank=1)
+        self.field = field
+
+    def to_dict(self) -> dict:
+        raise NotImplementedError("FieldEmbedding cannot be converted to dict")
+
+    def __value_attrs__(self):
+        return 'field',
+
+    def __getitem__(self, item):
+        return FieldEmbedding(self.field[item])
+
+    @staticmethod
+    def __stack__(values: tuple, dim: Shape, **kwargs) -> 'ConstantExtrapolation':
+        if all(isinstance(v, FieldEmbedding) for v in values):
+            return ConstantExtrapolation(stack([v.field for v in values], dim, **kwargs))
+        else:
+            return NotImplemented
+
+    def spatial_gradient(self) -> 'Extrapolation':
+        return NotImplemented
+        from ._field_math import spatial_gradient
+        return FieldEmbedding(spatial_gradient(self.field))  # this is not supported for all fields
+
+    def valid_outer_faces(self, dim) -> tuple:
+        return False, False
+
+    def pad_values(self, value: Tensor, width: int, dim: str, upper_edge: bool, bounds: Box = None, already_padded: dict = None, **kwargs) -> Tensor:
+        assert bounds is not None, f"{type(self)}.pad() requires 'bounds' argument"
+        if already_padded:
+            padded_res = spatial(**{dim: lo + up for dim, (lo, up) in already_padded.items()})
+            resolution = spatial(value) - padded_res
+            value_grid = GridCell(resolution, bounds).padded(already_padded)
+        else:
+            value_grid = GridCell(spatial(value), bounds)
+        if upper_edge:
+            pad_grid = value_grid.padded({dim: (0, width)})[{dim: slice(-width, None)}]
+        else:
+            pad_grid = value_grid.padded({dim: (width, 0)})[{dim: slice(0, width)}]
+        result = sample(self.field, pad_grid)
+        return result
+    
+    @property
+    def is_flexible(self) -> bool:
+        return False
+
+    def __repr__(self):
+        return repr(self.field)
+
+    def __abs__(self):
+        return Undefined(self)
+
+    def __neg__(self):
+        return Undefined(self)
+
+    def _op(self, other, op):
+        if isinstance(other, ConstantExtrapolation):  # some operations can be handled by ConstantExtrapolation, e.g. * 0
+            op = getattr(other, op.__name__)
+            result = op(self)
+            return Undefined(self) if result is NotImplemented else result
+        elif isinstance(other, FieldEmbedding):
+            if other.field is self.field:
+                return self
+            return Undefined(self)
+        else:
+            return Undefined(self)
+
+    def __add__(self, other):
+        return self._op(other, ConstantExtrapolation.__add__)
+
+    def __radd__(self, other):
+        return self._op(other, ConstantExtrapolation.__add__)
+
+    def __mul__(self, other):
+        return self._op(other, ConstantExtrapolation.__mul__)
+
+    def __rmul__(self, other):
+        return self._op(other, ConstantExtrapolation.__mul__)
+
+    def __sub__(self, other):
+        return self._op(other, ConstantExtrapolation.__rsub__)
+
+    def __rsub__(self, other):
+        return self._op(other, ConstantExtrapolation.__sub__)
+
+    def __truediv__(self, other):
+        return self._op(other, ConstantExtrapolation.__rtruediv__)
+
+    def __rtruediv__(self, other):
+        return self._op(other, ConstantExtrapolation.__truediv__)
```

### Comparing `phiflow-2.3.4/phi/field/_field.py` & `phiflow-2.4.0/phi/field/_field.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,465 +1,483 @@
-import warnings
-from numbers import Number
-from typing import TypeVar, Callable, Union
-
-from phi import math
-from phi.math import Shape, Tensor, channel
-from phi.math.extrapolation import Extrapolation
-from phi.geom import Geometry, Box, Point
-from phi.math.magic import BoundDim
-
-
-class Field:
-    """
-    Base class for all fields.
-    
-    Important implementations:
-    
-    * CenteredGrid
-    * StaggeredGrid
-    * PointCloud
-    * Noise
-    
-    See the `phi.field` module documentation at https://tum-pbs.github.io/PhiFlow/Fields.html
-    """
-
-    @property
-    def shape(self) -> Shape:
-        """
-        Returns a shape with the following properties
-        
-        * The spatial dimension names match the dimensions of this Field
-        * The batch dimensions match the batch dimensions of this Field
-        * The channel dimensions match the channels of this Field
-        """
-        raise NotImplementedError
-
-    @property
-    def spatial_rank(self) -> int:
-        """
-        Spatial rank of the field (1 for 1D, 2 for 2D, 3 for 3D).
-        This is equal to the spatial rank of the `data`.
-        """
-        raise NotImplementedError
-
-    @property
-    def bounds(self) -> Box:
-        """
-        The bounds represent the area inside which the values of this `Field` are valid.
-        The bounds will also be used as axis limits for plots.
-
-        The bounds can be set manually in the constructor, otherwise default bounds will be generated.
-
-        For fields that are valid without bounds, the lower and upper limit of `bounds` is set to `-inf` and `inf`, respectively.
-
-        Fields whose spatial rank is determined only during sampling return an empty `Box`.
-        """
-        raise NotImplementedError
-
-    def _sample(self, geometry: Geometry, **kwargs) -> math.Tensor:
-        """ For internal use only. Use `sample()` instead. """
-        raise NotImplementedError(self)
-
-    def at(self, representation: 'SampledField', keep_extrapolation=False, **kwargs) -> 'SampledFieldType':
-        """
-        Short for `resample(self, representation)`
-
-        See Also
-            `resample()`.
-
-        Returns:
-            Field object of same type as `representation`
-        """
-        return resample(self, representation, keep_extrapolation, **kwargs)
-
-    def __matmul__(self, other: 'SampledField'):  # value @ representation
-        # Deprecated. Use `resample(value, field)` instead.
-        warnings.warn("value @ field is deprecated. Use resample(value, field) instead.", DeprecationWarning)
-        return self.at(other, keep_extrapolation=False)
-
-    def __rmatmul__(self, other):  # values @ representation
-        if not isinstance(self, SampledField):
-            return NotImplemented
-        if isinstance(other, (Geometry, Number, tuple, list)):
-            return self.with_values(other)
-        return NotImplemented
-
-    def __rshift__(self, other):
-        warnings.warn(">> operator for Fields is deprecated. Use field.at(), the constructor or obj @ field instead.", SyntaxWarning, stacklevel=2)
-        return self.at(other, keep_extrapolation=False)
-
-    def __rrshift__(self, other):
-        warnings.warn(">> operator for Fields is deprecated. Use field.at(), the constructor or obj @ field instead.", SyntaxWarning, stacklevel=2)
-        if not isinstance(self, SampledField):
-            return NotImplemented
-        if isinstance(other, (Geometry, float, int, complex, tuple, list)):
-            return self.with_values(other)
-        return NotImplemented
-
-    def __getitem__(self, item) -> 'Field':
-        """
-        Access a slice of the Field.
-        The returned `Field` may be of a different type than `self`.
-
-        Args:
-            item: `dict` mapping dimensions (`str`) to selections (`int` or `slice`) or other supported type, such as `int` or `str`.
-
-        Returns:
-            Sliced `Field`.
-        """
-        raise NotImplementedError(self)
-
-    def __getattr__(self, name: str) -> BoundDim:
-        return BoundDim(self, name)
-
-    def dimension(self, name: str):
-        """
-        Returns a reference to one of the dimensions of this field.
-
-        The dimension reference can be used the same way as a `Tensor` dimension reference.
-        Notable properties and methods of a dimension reference are:
-        indexing using `[index]`, `unstack()`, `size`, `exists`, `is_batch`, `is_spatial`, `is_channel`.
-
-        A shortcut to calling this function is the syntax `field.<dim_name>` which calls `field.dimension(<dim_name>)`.
-
-        Args:
-            name: dimension name
-
-        Returns:
-            dimension reference
-
-        """
-        return BoundDim(self, name)
-
-    def __repr__(self):
-        return f"{self.__class__.__name__} {self.shape}"
-
-
-class SampledField(Field):
-    """
-    Base class for fields that are sampled at specific locations such as grids or point clouds.
-    """
-
-    def __init__(self,
-                 elements: Union[Geometry, Tensor],
-                 values: Tensor,
-                 extrapolation: Union[float, Extrapolation, Field, None],
-                 bounds: Union[Box, None]):
-        """
-        Args:
-          elements: Geometry object specifying the sample points and sizes
-          values: values corresponding to elements
-          extrapolation: values outside elements
-        """
-        if isinstance(elements, Tensor):
-            elements = Point(elements)
-        assert isinstance(elements, Geometry), elements
-        assert isinstance(values, Tensor), f"Values must be a Tensor but got {values}."
-        assert bounds is None or isinstance(bounds, Box), 'Invalid bounds.'
-        self._bounds = bounds
-        self._elements: Geometry = elements
-        self._values: Tensor = values
-        self._extrapolation: Extrapolation = as_extrapolation(extrapolation)
-
-    @property
-    def bounds(self) -> Box:
-        raise NotImplementedError(self.__class__)
-
-    def _sample(self, geometry: Geometry, **kwargs) -> math.Tensor:
-        raise NotImplementedError(self.__class__)
-
-    def with_values(self, values):
-        """ Returns a copy of this field with `values` replaced. """
-        raise NotImplementedError(self)
-
-    def with_extrapolation(self, extrapolation: Extrapolation):
-        """ Returns a copy of this field with `values` replaced. """
-        raise NotImplementedError(self)
-
-    @property
-    def shape(self):
-        raise NotImplementedError()
-
-    @property
-    def spatial_rank(self) -> int:
-        return self._elements.spatial_rank
-
-    def __getitem__(self: 'FieldType', item) -> 'FieldType':
-        raise NotImplementedError(self)
-
-    @staticmethod
-    def __stack__(values: tuple, dim: Shape, **kwargs) -> 'FieldType':
-        from ._field_math import stack
-        return stack(values, dim, kwargs.get('bounds', None))
-
-    @staticmethod
-    def __concat__(values: tuple, dim: str, **kwargs) -> 'FieldType':
-        from ._field_math import concat
-        return concat(values, dim)
-
-    @property
-    def elements(self) -> Geometry:
-        """
-        Returns a geometrical representation of the discrete volume elements.
-        The result is a tuple of Geometry objects, each of which can have additional spatial (but not batch) dimensions.
-        
-        For grids, the geometries are boxes while particle fields may be represented as spheres.
-        
-        If this Field has no discrete points, this method returns an empty geometry.
-        """
-        return self._elements
-
-    @property
-    def points(self) -> Tensor:
-        return self.elements.center
-
-    @property
-    def values(self) -> Tensor:
-        return self._values
-
-    data = values
-
-    @property
-    def extrapolation(self) -> Extrapolation:
-        return self._extrapolation
-
-    def __mul__(self, other):
-        return self._op2(other, lambda d1, d2: d1 * d2)
-
-    __rmul__ = __mul__
-
-    def __truediv__(self, other):
-        return self._op2(other, lambda d1, d2: d1 / d2)
-
-    def __rtruediv__(self, other):
-        return self._op2(other, lambda d1, d2: d2 / d1)
-
-    def __sub__(self, other):
-        return self._op2(other, lambda d1, d2: d1 - d2)
-
-    def __rsub__(self, other):
-        return self._op2(other, lambda d1, d2: d2 - d1)
-
-    def __add__(self, other):
-        return self._op2(other, lambda d1, d2: d1 + d2)
-
-    __radd__ = __add__
-
-    def __pow__(self, power, modulo=None):
-        return self._op2(power, lambda f, p: f ** p)
-
-    def __neg__(self):
-        return self._op1(lambda x: -x)
-
-    def __gt__(self, other):
-        return self._op2(other, lambda x, y: x > y)
-
-    def __ge__(self, other):
-        return self._op2(other, lambda x, y: x >= y)
-
-    def __lt__(self, other):
-        return self._op2(other, lambda x, y: x < y)
-
-    def __le__(self, other):
-        return self._op2(other, lambda x, y: x <= y)
-
-    def __abs__(self):
-        return self._op1(lambda x: abs(x))
-
-    def _op1(self: 'SampledFieldType', operator: Callable) -> 'SampledFieldType':
-        """
-        Perform an operation on the data of this field.
-
-        Args:
-          operator: function that accepts tensors and extrapolations and returns objects of the same type and dimensions
-
-        Returns:
-          Field of same type
-        """
-        values = operator(self.values)
-        extrapolation_ = operator(self._extrapolation)
-        return self.with_values(values).with_extrapolation(extrapolation_)
-
-    def _op2(self, other, operator) -> 'SampledField':
-        if isinstance(other, Geometry):
-            raise ValueError(f"Cannot combine {self.__class__.__name__} with a Geometry, got {type(other)}")
-        if isinstance(other, Field):
-            other_values = reduce_sample(other, self._elements)
-            values = operator(self._values, other_values)
-            extrapolation_ = operator(self._extrapolation, other.extrapolation)
-            return self.with_values(values).with_extrapolation(extrapolation_)
-        else:
-            if isinstance(other, (tuple, list)) and len(other) == self.spatial_rank:
-                other = math.wrap(other, self.points.shape['vector'])
-            else:
-                other = math.wrap(other)
-            values = operator(self._values, other)
-            return self.with_values(values)
-
-
-def sample(field: Union[Field, Geometry],
-           geometry: Union[Geometry, SampledField, Tensor],
-           **kwargs) -> math.Tensor:
-    """
-    Computes the field value inside the volume of the (batched) `geometry`.
-
-    The field value may be determined by integrating over the volume, sampling the central value or any other way.
-
-    The batch dimensions of `geometry` are matched with this field.
-    The `geometry` must not share any channel dimensions with this field.
-    Spatial dimensions of `geometry` can be used to sample a grid of geometries.
-
-    See Also:
-        `reduce_sample()`, `Field.at()`, [Resampling overview](https://tum-pbs.github.io/PhiFlow/Fields.html#resampling-fields).
-
-    Args:
-        field: Source `Field` to sample.
-        geometry: Single or batched `phi.geom.Geometry` or `SampledField` or location `Tensor`.
-            When passing a `SampledField`, its `elements` are used as sample points.
-            When passing a vector-valued `Tensor`, a `Point` geometry will be created.
-        **kwargs: Sampling arguments, e.g. to specify the numerical scheme.
-            By default, linear interpolation is used.
-            Grids also support 6th order implicit sampling at mid-points.
-
-    Returns:
-        Sampled values as a `phi.math.Tensor`
-    """
-    geometry = _get_geometry(geometry)
-    if isinstance(field, Geometry):
-        from ._field_math import mask
-        field = mask(field)
-    geom_ch = channel(geometry).without('vector')
-    assert all(dim not in field.shape for dim in geom_ch)
-    if isinstance(field, SampledField) and field.elements.shallow_equals(geometry) and not geom_ch:
-        return field.values
-    if geom_ch:
-        sampled = [field._sample(p, **kwargs) for p in geometry.unstack(geom_ch.name)]
-        return math.stack(sampled, geom_ch)
-    else:
-        return field._sample(geometry, **kwargs)
-
-
-def reduce_sample(field: Union[Field, Geometry],
-                  geometry: Union[Geometry, SampledField, Tensor],
-                  dim=channel('vector'),
-                  **kwargs) -> math.Tensor:
-    """
-    Similar to `sample()`, but matches channel dimensions of `geometry` with channel dimensions of this field.
-    Currently, `geometry` may have at most one channel dimension.
-
-    See Also:
-        `sample()`, `Field.at()`, [Resampling overview](https://tum-pbs.github.io/PhiFlow/Fields.html#resampling-fields).
-
-    Args:
-        field: Source `Field` to sample.
-        geometry: Single or batched `phi.geom.Geometry` or `SampledField` or location `Tensor`.
-            When passing a `SampledField`, its `elements` are used as sample points.
-            When passing a vector-valued `Tensor`, a `Point` geometry will be created.
-        dim: Dimension of result, resulting from reduction of channel dimensions.
-        **kwargs: Sampling arguments, e.g. to specify the numerical scheme.
-            By default, linear interpolation is used.
-            Grids also support 6th order implicit sampling at mid-points.
-
-    Returns:
-        Sampled values as a `phi.math.Tensor`
-    """
-    geometry = _get_geometry(geometry)
-    if isinstance(field, Geometry):
-        from ._field_math import mask
-        field = mask(field)
-    if isinstance(field, SampledField) and field.elements.shallow_equals(geometry):
-        return field.values
-    if channel(geometry).without('vector'):  # Reduce this dimension
-        geom_ch = channel(geometry).without('vector')
-        assert geom_ch.rank == 1, "Only single-dimension reduction supported."
-        if field.shape.channel.volume > 1:
-            assert field.shape.channel.volume == geom_ch.volume, f"Cannot sample field with channels {field.shape.channel} at elements with channels {geometry.shape.channel}."
-            components = math.unstack(field, field.shape.channel.name)
-            sampled = [c._sample(p, **kwargs) for c, p in zip(components, geometry.unstack(geom_ch.name))]
-        else:
-            sampled = [field._sample(p, **kwargs) for p in geometry.unstack(channel(geometry).without('vector').name)]
-        dim = dim.with_size(geometry.shape.channel.item_names[0])
-        return math.stack(sampled, dim)
-    else:  # Nothing to reduce
-        return field._sample(geometry, **kwargs)
-
-
-def resample(value: Union[Field, Geometry, Tensor, float], to: SampledField, keep_extrapolation=False, **kwargs):
-    """
-    Samples a `Field`, `Geometry` or value at the sample points of the field `to`.
-    The result will approximate `value` on the data structure of `to`.
-    Unlike `sample()`, this method returns a `Field` object, not a `Tensor`.
-
-    Aliases:
-        `value.at(to)`, (and the deprecated `value @ to`).
-
-    See Also:
-        `sample()`, `reduce_sample()`, `Field.at()`, [Resampling overview](https://tum-pbs.github.io/PhiFlow/Fields.html#resampling-fields).
-
-    Args:
-        value: Object containing values to resample.
-            This can be
-        to: `SampledField` (`CenteredGrid`, `StaggeredGrid` or `PointCloud`) object defining the sample points.
-            The current values of `to` are ignored.
-        keep_extrapolation: Only available if `self` is a `SampledField`.
-            If True, the resampled field will inherit the extrapolation from `self` instead of `representation`.
-            This can result in non-compatible value tensors for staggered grids where the tensor size depends on the extrapolation type.
-        **kwargs: Sampling arguments, e.g. to specify the numerical scheme.
-            By default, linear interpolation is used.
-            Grids also support 6th order implicit sampling at mid-points.
-
-    Returns:
-        Field object of same type as `representation`
-
-    Examples:
-        >>> grid = CenteredGrid(x=64, y=32)
-        >>> field.resample(Noise(), to=grid)
-        CenteredGrid[(xˢ=64, yˢ=32), size=(x=64, y=32), extrapolation=float64 0.0]
-        >>> field.resample(1, to=grid)
-        CenteredGrid[(xˢ=64, yˢ=32), size=(x=64, y=32), extrapolation=float64 0.0]
-        >>> field.resample(Box(x=1, y=2), to=grid)
-        CenteredGrid[(xˢ=64, yˢ=32), size=(x=64, y=32), extrapolation=float64 0.0]
-        >>> field.resample(grid, to=grid) == grid
-        True
-    """
-    if not isinstance(value, (Field, Geometry)):
-        return to.with_values(value)
-    resampled = reduce_sample(value, to.elements, **kwargs)
-    extrap = value.extrapolation if isinstance(value, SampledField) and keep_extrapolation else to.extrapolation
-    return to.with_values(resampled).with_extrapolation(extrap)
-
-
-def _get_geometry(geometry):
-    if isinstance(geometry, SampledField):
-        return geometry.elements
-    elif isinstance(geometry, Tensor) and 'vector' in geometry.shape:
-        return Point(geometry)
-    elif isinstance(geometry, Geometry):
-        return geometry
-    else:
-        raise ValueError(f"A Geometry, SampledField or location Tensor is required but got {geometry}")
-
-
-FieldType = TypeVar('FieldType', bound=Field)
-SampledFieldType = TypeVar('SampledFieldType', bound=SampledField)
-
-
-def as_extrapolation(obj: Union[Extrapolation, float, Field, None]) -> Extrapolation:
-    """
-    Returns an `Extrapolation` representing `obj`.
-
-    Args:
-        obj: One of
-
-            * `float` or `Tensor`: Extrapolate with a constant value
-            * `Extrapolation`: Use as-is.
-            * `Field`: Sample values from `obj`, embedding another field inside `obj`.
-
-    Returns:
-        `Extrapolation`
-    """
-    if isinstance(obj, Field):
-        from ._embed import FieldEmbedding
-        return FieldEmbedding(obj)
-    else:
-        return math.extrapolation.as_extrapolation(obj)
+import warnings
+from numbers import Number
+from typing import TypeVar, Callable, Union
+
+from phi import math
+from phi.math import Shape, Tensor, channel
+from phi.math.extrapolation import Extrapolation
+from phi.geom import Geometry, Box, Point
+from phi.math.magic import BoundDim
+
+
+class Field:
+    """
+    Base class for all fields.
+    
+    Important implementations:
+    
+    * CenteredGrid
+    * StaggeredGrid
+    * PointCloud
+    * Noise
+    
+    See the `phi.field` module documentation at https://tum-pbs.github.io/PhiFlow/Fields.html
+    """
+
+    @property
+    def shape(self) -> Shape:
+        """
+        Returns a shape with the following properties
+        
+        * The spatial dimension names match the dimensions of this Field
+        * The batch dimensions match the batch dimensions of this Field
+        * The channel dimensions match the channels of this Field
+        """
+        raise NotImplementedError
+
+    @property
+    def spatial_rank(self) -> int:
+        """
+        Spatial rank of the field (1 for 1D, 2 for 2D, 3 for 3D).
+        This is equal to the spatial rank of the `data`.
+        """
+        raise NotImplementedError
+
+    @property
+    def bounds(self) -> Box:
+        """
+        The bounds represent the area inside which the values of this `Field` are valid.
+        The bounds will also be used as axis limits for plots.
+
+        The bounds can be set manually in the constructor, otherwise default bounds will be generated.
+
+        For fields that are valid without bounds, the lower and upper limit of `bounds` is set to `-inf` and `inf`, respectively.
+
+        Fields whose spatial rank is determined only during sampling return an empty `Box`.
+        """
+        raise NotImplementedError
+
+    def _sample(self, geometry: Geometry, **kwargs) -> math.Tensor:
+        """ For internal use only. Use `sample()` instead. """
+        raise NotImplementedError(self)
+
+    def at(self, representation: 'SampledField', keep_extrapolation=False, **kwargs) -> 'SampledFieldType':
+        """
+        Short for `resample(self, representation)`
+
+        See Also
+            `resample()`.
+
+        Returns:
+            Field object of same type as `representation`
+        """
+        return resample(self, representation, keep_extrapolation, **kwargs)
+
+    def __matmul__(self, other: 'SampledField'):  # value @ representation
+        # Deprecated. Use `resample(value, field)` instead.
+        warnings.warn("value @ field is deprecated. Use resample(value, field) instead.", DeprecationWarning)
+        return self.at(other, keep_extrapolation=False)
+
+    def __rmatmul__(self, other):  # values @ representation
+        if not isinstance(self, SampledField):
+            return NotImplemented
+        if isinstance(other, (Geometry, Number, tuple, list)):
+            return self.with_values(other)
+        return NotImplemented
+
+    def __rshift__(self, other):
+        warnings.warn(">> operator for Fields is deprecated. Use field.at(), the constructor or obj @ field instead.", SyntaxWarning, stacklevel=2)
+        return self.at(other, keep_extrapolation=False)
+
+    def __rrshift__(self, other):
+        warnings.warn(">> operator for Fields is deprecated. Use field.at(), the constructor or obj @ field instead.", SyntaxWarning, stacklevel=2)
+        if not isinstance(self, SampledField):
+            return NotImplemented
+        if isinstance(other, (Geometry, float, int, complex, tuple, list)):
+            return self.with_values(other)
+        return NotImplemented
+
+    def __getitem__(self, item) -> 'Field':
+        """
+        Access a slice of the Field.
+        The returned `Field` may be of a different type than `self`.
+
+        Args:
+            item: `dict` mapping dimensions (`str`) to selections (`int` or `slice`) or other supported type, such as `int` or `str`.
+
+        Returns:
+            Sliced `Field`.
+        """
+        raise NotImplementedError(self)
+
+    def __getattr__(self, name: str) -> BoundDim:
+        return BoundDim(self, name)
+
+    def dimension(self, name: str):
+        """
+        Returns a reference to one of the dimensions of this field.
+
+        The dimension reference can be used the same way as a `Tensor` dimension reference.
+        Notable properties and methods of a dimension reference are:
+        indexing using `[index]`, `unstack()`, `size`, `exists`, `is_batch`, `is_spatial`, `is_channel`.
+
+        A shortcut to calling this function is the syntax `field.<dim_name>` which calls `field.dimension(<dim_name>)`.
+
+        Args:
+            name: dimension name
+
+        Returns:
+            dimension reference
+
+        """
+        return BoundDim(self, name)
+
+    def __repr__(self):
+        return f"{self.__class__.__name__} {self.shape}"
+
+    @property
+    def is_grid(self):
+        """ `isinstance(self, Grid)`. Added for forward compatibility with 2.5. """
+        from ._grid import Grid
+        return isinstance(self, Grid)
+
+    @property
+    def is_point_cloud(self):
+        """ `isinstance(self, PointCloud)`. Added for forward compatibility with 2.5. """
+        from ._point_cloud import PointCloud
+        return isinstance(self, PointCloud)
+
+    @property
+    def is_staggered(self):
+        """ Whether the field values are sampled between elements. Added for forward compatibility with 2.5. """
+        from ._grid import StaggeredGrid
+        return isinstance(self, StaggeredGrid)
+
+
+class SampledField(Field):
+    """
+    Base class for fields that are sampled at specific locations such as grids or point clouds.
+    """
+
+    def __init__(self,
+                 elements: Union[Geometry, Tensor],
+                 values: Tensor,
+                 extrapolation: Union[float, Extrapolation, Field, None],
+                 bounds: Union[Box, None]):
+        """
+        Args:
+          elements: Geometry object specifying the sample points and sizes
+          values: values corresponding to elements
+          extrapolation: values outside elements
+        """
+        if isinstance(elements, Tensor):
+            elements = Point(elements)
+        assert isinstance(elements, Geometry), elements
+        assert isinstance(values, Tensor), f"Values must be a Tensor but got {values}."
+        assert bounds is None or isinstance(bounds, Box), 'Invalid bounds.'
+        self._bounds = bounds
+        self._elements: Geometry = elements
+        self._values: Tensor = values
+        self._extrapolation: Extrapolation = as_extrapolation(extrapolation)
+
+    @property
+    def bounds(self) -> Box:
+        raise NotImplementedError(self.__class__)
+
+    def _sample(self, geometry: Geometry, **kwargs) -> math.Tensor:
+        raise NotImplementedError(self.__class__)
+
+    def with_values(self, values):
+        """ Returns a copy of this field with `values` replaced. """
+        raise NotImplementedError(self)
+
+    def with_extrapolation(self, extrapolation: Extrapolation):
+        """ Returns a copy of this field with `values` replaced. """
+        raise NotImplementedError(self)
+
+    @property
+    def shape(self):
+        raise NotImplementedError()
+
+    @property
+    def spatial_rank(self) -> int:
+        return self._elements.spatial_rank
+
+    def __getitem__(self: 'FieldType', item) -> 'FieldType':
+        raise NotImplementedError(self)
+
+    @staticmethod
+    def __stack__(values: tuple, dim: Shape, **kwargs) -> 'FieldType':
+        from ._field_math import stack
+        return stack(values, dim, kwargs.get('bounds', None))
+
+    @staticmethod
+    def __concat__(values: tuple, dim: str, **kwargs) -> 'FieldType':
+        from ._field_math import concat
+        return concat(values, dim)
+
+    @property
+    def elements(self) -> Geometry:
+        """
+        Returns a geometrical representation of the discrete volume elements.
+        The result is a tuple of Geometry objects, each of which can have additional spatial (but not batch) dimensions.
+        
+        For grids, the geometries are boxes while particle fields may be represented as spheres.
+        
+        If this Field has no discrete points, this method returns an empty geometry.
+        """
+        return self._elements
+
+    @property
+    def points(self) -> Tensor:
+        return self.elements.center
+
+    @property
+    def values(self) -> Tensor:
+        return self._values
+
+    data = values
+
+    @property
+    def extrapolation(self) -> Extrapolation:
+        return self._extrapolation
+
+    def __mul__(self, other):
+        return self._op2(other, lambda d1, d2: d1 * d2)
+
+    __rmul__ = __mul__
+
+    def __truediv__(self, other):
+        return self._op2(other, lambda d1, d2: d1 / d2)
+
+    def __rtruediv__(self, other):
+        return self._op2(other, lambda d1, d2: d2 / d1)
+
+    def __sub__(self, other):
+        return self._op2(other, lambda d1, d2: d1 - d2)
+
+    def __rsub__(self, other):
+        return self._op2(other, lambda d1, d2: d2 - d1)
+
+    def __add__(self, other):
+        return self._op2(other, lambda d1, d2: d1 + d2)
+
+    __radd__ = __add__
+
+    def __pow__(self, power, modulo=None):
+        return self._op2(power, lambda f, p: f ** p)
+
+    def __neg__(self):
+        return self._op1(lambda x: -x)
+
+    def __gt__(self, other):
+        return self._op2(other, lambda x, y: x > y)
+
+    def __ge__(self, other):
+        return self._op2(other, lambda x, y: x >= y)
+
+    def __lt__(self, other):
+        return self._op2(other, lambda x, y: x < y)
+
+    def __le__(self, other):
+        return self._op2(other, lambda x, y: x <= y)
+
+    def __abs__(self):
+        return self._op1(lambda x: abs(x))
+
+    def _op1(self: 'SampledFieldType', operator: Callable) -> 'SampledFieldType':
+        """
+        Perform an operation on the data of this field.
+
+        Args:
+          operator: function that accepts tensors and extrapolations and returns objects of the same type and dimensions
+
+        Returns:
+          Field of same type
+        """
+        values = operator(self.values)
+        extrapolation_ = operator(self._extrapolation)
+        return self.with_values(values).with_extrapolation(extrapolation_)
+
+    def _op2(self, other, operator) -> 'SampledField':
+        if isinstance(other, Geometry):
+            raise ValueError(f"Cannot combine {self.__class__.__name__} with a Geometry, got {type(other)}")
+        if isinstance(other, Field):
+            other_values = reduce_sample(other, self._elements)
+            values = operator(self._values, other_values)
+            extrapolation_ = operator(self._extrapolation, other.extrapolation)
+            return self.with_values(values).with_extrapolation(extrapolation_)
+        else:
+            if isinstance(other, (tuple, list)) and len(other) == self.spatial_rank:
+                other = math.wrap(other, self.points.shape['vector'])
+            else:
+                other = math.wrap(other)
+            values = operator(self._values, other)
+            return self.with_values(values)
+
+
+def sample(field: Union[Field, Geometry],
+           geometry: Union[Geometry, SampledField, Tensor],
+           **kwargs) -> math.Tensor:
+    """
+    Computes the field value inside the volume of the (batched) `geometry`.
+
+    The field value may be determined by integrating over the volume, sampling the central value or any other way.
+
+    The batch dimensions of `geometry` are matched with this field.
+    The `geometry` must not share any channel dimensions with this field.
+    Spatial dimensions of `geometry` can be used to sample a grid of geometries.
+
+    See Also:
+        `reduce_sample()`, `Field.at()`, [Resampling overview](https://tum-pbs.github.io/PhiFlow/Fields.html#resampling-fields).
+
+    Args:
+        field: Source `Field` to sample.
+        geometry: Single or batched `phi.geom.Geometry` or `SampledField` or location `Tensor`.
+            When passing a `SampledField`, its `elements` are used as sample points.
+            When passing a vector-valued `Tensor`, a `Point` geometry will be created.
+        **kwargs: Sampling arguments, e.g. to specify the numerical scheme.
+            By default, linear interpolation is used.
+            Grids also support 6th order implicit sampling at mid-points.
+
+    Returns:
+        Sampled values as a `phi.math.Tensor`
+    """
+    geometry = _get_geometry(geometry)
+    if isinstance(field, Geometry):
+        from ._field_math import mask
+        field = mask(field)
+    geom_ch = channel(geometry).without('vector')
+    assert all(dim not in field.shape for dim in geom_ch)
+    if isinstance(field, SampledField) and field.elements.shallow_equals(geometry) and not geom_ch:
+        return field.values
+    if geom_ch:
+        sampled = [field._sample(p, **kwargs) for p in geometry.unstack(geom_ch.name)]
+        return math.stack(sampled, geom_ch)
+    else:
+        return field._sample(geometry, **kwargs)
+
+
+def reduce_sample(field: Union[Field, Geometry],
+                  geometry: Union[Geometry, SampledField, Tensor],
+                  dim=channel('vector'),
+                  **kwargs) -> math.Tensor:
+    """
+    Similar to `sample()`, but matches channel dimensions of `geometry` with channel dimensions of this field.
+    Currently, `geometry` may have at most one channel dimension.
+
+    See Also:
+        `sample()`, `Field.at()`, [Resampling overview](https://tum-pbs.github.io/PhiFlow/Fields.html#resampling-fields).
+
+    Args:
+        field: Source `Field` to sample.
+        geometry: Single or batched `phi.geom.Geometry` or `SampledField` or location `Tensor`.
+            When passing a `SampledField`, its `elements` are used as sample points.
+            When passing a vector-valued `Tensor`, a `Point` geometry will be created.
+        dim: Dimension of result, resulting from reduction of channel dimensions.
+        **kwargs: Sampling arguments, e.g. to specify the numerical scheme.
+            By default, linear interpolation is used.
+            Grids also support 6th order implicit sampling at mid-points.
+
+    Returns:
+        Sampled values as a `phi.math.Tensor`
+    """
+    geometry = _get_geometry(geometry)
+    if isinstance(field, Geometry):
+        from ._field_math import mask
+        field = mask(field)
+    if isinstance(field, SampledField) and field.elements.shallow_equals(geometry):
+        return field.values
+    if channel(geometry).without('vector'):  # Reduce this dimension
+        geom_ch = channel(geometry).without('vector')
+        assert geom_ch.rank == 1, "Only single-dimension reduction supported."
+        if field.shape.channel.volume > 1:
+            assert field.shape.channel.volume == geom_ch.volume, f"Cannot sample field with channels {field.shape.channel} at elements with channels {geometry.shape.channel}."
+            components = math.unstack(field, field.shape.channel.name)
+            sampled = [c._sample(p, **kwargs) for c, p in zip(components, geometry.unstack(geom_ch.name))]
+        else:
+            sampled = [field._sample(p, **kwargs) for p in geometry.unstack(channel(geometry).without('vector').name)]
+        dim = dim.with_size(geometry.shape.channel.item_names[0])
+        return math.stack(sampled, dim)
+    else:  # Nothing to reduce
+        return field._sample(geometry, **kwargs)
+
+
+def resample(value: Union[Field, Geometry, Tensor, float], to: SampledField, keep_extrapolation=False, **kwargs):
+    """
+    Samples a `Field`, `Geometry` or value at the sample points of the field `to`.
+    The result will approximate `value` on the data structure of `to`.
+    Unlike `sample()`, this method returns a `Field` object, not a `Tensor`.
+
+    Aliases:
+        `value.at(to)`, (and the deprecated `value @ to`).
+
+    See Also:
+        `sample()`, `reduce_sample()`, `Field.at()`, [Resampling overview](https://tum-pbs.github.io/PhiFlow/Fields.html#resampling-fields).
+
+    Args:
+        value: Object containing values to resample.
+            This can be
+        to: `SampledField` (`CenteredGrid`, `StaggeredGrid` or `PointCloud`) object defining the sample points.
+            The current values of `to` are ignored.
+        keep_extrapolation: Only available if `self` is a `SampledField`.
+            If True, the resampled field will inherit the extrapolation from `self` instead of `representation`.
+            This can result in non-compatible value tensors for staggered grids where the tensor size depends on the extrapolation type.
+        **kwargs: Sampling arguments, e.g. to specify the numerical scheme.
+            By default, linear interpolation is used.
+            Grids also support 6th order implicit sampling at mid-points.
+
+    Returns:
+        Field object of same type as `representation`
+
+    Examples:
+        >>> grid = CenteredGrid(x=64, y=32)
+        >>> field.resample(Noise(), to=grid)
+        CenteredGrid[(xˢ=64, yˢ=32), size=(x=64, y=32), extrapolation=float64 0.0]
+        >>> field.resample(1, to=grid)
+        CenteredGrid[(xˢ=64, yˢ=32), size=(x=64, y=32), extrapolation=float64 0.0]
+        >>> field.resample(Box(x=1, y=2), to=grid)
+        CenteredGrid[(xˢ=64, yˢ=32), size=(x=64, y=32), extrapolation=float64 0.0]
+        >>> field.resample(grid, to=grid) == grid
+        True
+    """
+    if not isinstance(value, (Field, Geometry)):
+        return to.with_values(value)
+    resampled = reduce_sample(value, to.elements, **kwargs)
+    extrap = value.extrapolation if isinstance(value, SampledField) and keep_extrapolation else to.extrapolation
+    return to.with_values(resampled).with_extrapolation(extrap)
+
+
+def _get_geometry(geometry):
+    if isinstance(geometry, SampledField):
+        return geometry.elements
+    elif isinstance(geometry, Tensor) and 'vector' in geometry.shape:
+        return Point(geometry)
+    elif isinstance(geometry, Geometry):
+        return geometry
+    else:
+        raise ValueError(f"A Geometry, SampledField or location Tensor is required but got {geometry}")
+
+
+FieldType = TypeVar('FieldType', bound=Field)
+SampledFieldType = TypeVar('SampledFieldType', bound=SampledField)
+
+
+def as_extrapolation(obj: Union[Extrapolation, float, Field, None]) -> Extrapolation:
+    """
+    Returns an `Extrapolation` representing `obj`.
+
+    Args:
+        obj: One of
+
+            * `float` or `Tensor`: Extrapolate with a constant value
+            * `Extrapolation`: Use as-is.
+            * `Field`: Sample values from `obj`, embedding another field inside `obj`.
+
+    Returns:
+        `Extrapolation`
+    """
+    if isinstance(obj, Field):
+        from ._embed import FieldEmbedding
+        return FieldEmbedding(obj)
+    else:
+        return math.extrapolation.as_extrapolation(obj)
```

### Comparing `phiflow-2.3.4/phi/field/_field_io.py` & `phiflow-2.4.0/phi/field/_field_io.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,117 +1,117 @@
-from typing import Union
-
-import numpy as np
-
-from phi import geom
-from ._field import SampledField
-from ._grid import Grid, CenteredGrid, StaggeredGrid, unstack_staggered_tensor
-from ._field_math import stack
-from ..math import extrapolation, wrap, tensor, Shape, channel, Tensor, spatial
-
-
-def write(field: SampledField, file: Union[str, Tensor]):
-    """
-    Writes a field to disc using a NumPy file format.
-    Depending on `file`, the data may be split up into multiple files.
-
-    All characteristics of the field are serialized so that it can be fully restored using `read()`.
-
-    See Also:
-        `read()`
-
-    Args:
-        field: Field to be saved.
-        file: Single file as `str` or `Tensor` of string type.
-            If `file` is a tensor, the dimensions of `field` are matched to the dimensions of `file`.
-            Dimensions of `file` that are missing in `field` result in data duplication.
-            Dimensions of `field` that are missing in `file` result in larger files.
-    """
-    if isinstance(file, str):
-        write_single_field(field, file)
-    elif isinstance(file, Tensor):
-        if file.rank == 0:
-            write_single_field(field, file.native())
-        else:
-            dim = file.shape.names[0]
-            files = file.unstack(dim)
-            fields = field.dimension(dim).unstack(file.shape.get_size(dim))
-            for field_, file_ in zip(fields, files):
-                write(field_, file_)
-    else:
-        raise ValueError(file)
-
-
-def write_single_field(field: SampledField, file: str):
-    if isinstance(field, StaggeredGrid):
-        data = field.staggered_tensor().numpy(field.values.shape.names)
-    else:
-        data = field.values.numpy(field.values.shape.names)
-    dim_names = field.values.shape.names
-    if isinstance(field, Grid):
-        lower = field.bounds.lower.numpy()
-        upper = field.bounds.upper.numpy()
-        bounds_item_names = field.bounds.size.vector.item_names
-        extrap = field.extrapolation.to_dict()
-        np.savez_compressed(file,
-                            dim_names=dim_names,
-                            dim_types=field.values.shape.types,
-                            dim_item_names=np.asarray(field.values.shape.item_names, dtype=object),
-                            field_type=type(field).__name__,
-                            lower=lower,
-                            upper=upper,
-                            bounds_item_names=bounds_item_names,
-                            extrapolation=extrap,
-                            data=data)
-    else:
-        raise NotImplementedError(f"{type(field)} not implemented. Only Grid allowed.")
-
-
-def read(file: Union[str, Tensor], convert_to_backend=True) -> SampledField:
-    """
-    Loads a previously saved `SampledField` from disc.
-
-    See Also:
-        `write()`.
-
-    Args:
-        file: Single file as `str` or `Tensor` of string type.
-            If `file` is a tensor, all contained files are loaded an stacked according to the dimensions of `file`.
-        convert_to_backend: Whether to convert the read data to the data format of the default backend, e.g. TensorFlow tensors.
-
-    Returns:
-        Loaded `SampledField`.
-    """
-    if isinstance(file, str):
-        return read_single_field(file, convert_to_backend=convert_to_backend)
-    if isinstance(file, Tensor):
-        if file.rank == 0:
-            return read_single_field(file.native(), convert_to_backend=convert_to_backend)
-        else:
-            dim = file.shape[0]
-            files = file.unstack(dim.name)
-            fields = [read(file_, convert_to_backend=convert_to_backend) for file_ in files]
-            return stack(fields, dim)
-    else:
-        raise ValueError(file)
-
-
-def read_single_field(file: str, convert_to_backend=True) -> SampledField:
-    stored = np.load(file, allow_pickle=True)
-    ftype = stored['field_type']
-    implemented_types = ('CenteredGrid', 'StaggeredGrid')
-    if ftype in implemented_types:
-        data_arr = stored['data']
-        dim_item_names = stored.get('dim_item_names', (None,) * len(data_arr.shape))
-        data = tensor(data_arr, Shape(data_arr.shape, tuple(stored['dim_names']), tuple(stored['dim_types']), tuple(dim_item_names)), convert=convert_to_backend)
-        bounds_item_names = stored.get('bounds_item_names', None)
-        if bounds_item_names is None or bounds_item_names.shape == ():  # None or empty array
-            bounds_item_names = spatial(data).names
-        lower = wrap(stored['lower'], channel(vector=tuple(bounds_item_names))) if stored['lower'].ndim > 0 else wrap(stored['lower'])
-        upper = wrap(stored['upper'], channel(vector=tuple(bounds_item_names)))
-        extr = extrapolation.from_dict(stored['extrapolation'][()])
-        if ftype == 'CenteredGrid':
-            return CenteredGrid(data, bounds=geom.Box(lower, upper), extrapolation=extr)
-        elif ftype == 'StaggeredGrid':
-            data_ = unstack_staggered_tensor(data, extr)
-            return StaggeredGrid(data_, bounds=geom.Box(lower, upper), extrapolation=extr)
-    raise NotImplementedError(f"{ftype} not implemented ({implemented_types})")
+from typing import Union
+
+import numpy as np
+
+from phi import geom
+from ._field import SampledField
+from ._grid import Grid, CenteredGrid, StaggeredGrid, unstack_staggered_tensor
+from ._field_math import stack
+from ..math import extrapolation, wrap, tensor, Shape, channel, Tensor, spatial
+
+
+def write(field: SampledField, file: Union[str, Tensor]):
+    """
+    Writes a field to disc using a NumPy file format.
+    Depending on `file`, the data may be split up into multiple files.
+
+    All characteristics of the field are serialized so that it can be fully restored using `read()`.
+
+    See Also:
+        `read()`
+
+    Args:
+        field: Field to be saved.
+        file: Single file as `str` or `Tensor` of string type.
+            If `file` is a tensor, the dimensions of `field` are matched to the dimensions of `file`.
+            Dimensions of `file` that are missing in `field` result in data duplication.
+            Dimensions of `field` that are missing in `file` result in larger files.
+    """
+    if isinstance(file, str):
+        write_single_field(field, file)
+    elif isinstance(file, Tensor):
+        if file.rank == 0:
+            write_single_field(field, file.native())
+        else:
+            dim = file.shape.names[0]
+            files = file.unstack(dim)
+            fields = field.dimension(dim).unstack(file.shape.get_size(dim))
+            for field_, file_ in zip(fields, files):
+                write(field_, file_)
+    else:
+        raise ValueError(file)
+
+
+def write_single_field(field: SampledField, file: str):
+    if isinstance(field, StaggeredGrid):
+        data = field.staggered_tensor().numpy(field.values.shape.names)
+    else:
+        data = field.values.numpy(field.values.shape.names)
+    dim_names = field.values.shape.names
+    if isinstance(field, Grid):
+        lower = field.bounds.lower.numpy()
+        upper = field.bounds.upper.numpy()
+        bounds_item_names = field.bounds.size.vector.item_names
+        extrap = field.extrapolation.to_dict()
+        np.savez_compressed(file,
+                            dim_names=dim_names,
+                            dim_types=field.values.shape.types,
+                            dim_item_names=np.asarray(field.values.shape.item_names, dtype=object),
+                            field_type=type(field).__name__,
+                            lower=lower,
+                            upper=upper,
+                            bounds_item_names=bounds_item_names,
+                            extrapolation=extrap,
+                            data=data)
+    else:
+        raise NotImplementedError(f"{type(field)} not implemented. Only Grid allowed.")
+
+
+def read(file: Union[str, Tensor], convert_to_backend=True) -> SampledField:
+    """
+    Loads a previously saved `SampledField` from disc.
+
+    See Also:
+        `write()`.
+
+    Args:
+        file: Single file as `str` or `Tensor` of string type.
+            If `file` is a tensor, all contained files are loaded an stacked according to the dimensions of `file`.
+        convert_to_backend: Whether to convert the read data to the data format of the default backend, e.g. TensorFlow tensors.
+
+    Returns:
+        Loaded `SampledField`.
+    """
+    if isinstance(file, str):
+        return read_single_field(file, convert_to_backend=convert_to_backend)
+    if isinstance(file, Tensor):
+        if file.rank == 0:
+            return read_single_field(file.native(), convert_to_backend=convert_to_backend)
+        else:
+            dim = file.shape[0]
+            files = file.unstack(dim.name)
+            fields = [read(file_, convert_to_backend=convert_to_backend) for file_ in files]
+            return stack(fields, dim)
+    else:
+        raise ValueError(file)
+
+
+def read_single_field(file: str, convert_to_backend=True) -> SampledField:
+    stored = np.load(file, allow_pickle=True)
+    ftype = stored['field_type']
+    implemented_types = ('CenteredGrid', 'StaggeredGrid')
+    if ftype in implemented_types:
+        data_arr = stored['data']
+        dim_item_names = stored.get('dim_item_names', (None,) * len(data_arr.shape))
+        data = tensor(data_arr, Shape(data_arr.shape, tuple(stored['dim_names']), tuple(stored['dim_types']), tuple(dim_item_names)), convert=convert_to_backend)
+        bounds_item_names = stored.get('bounds_item_names', None)
+        if bounds_item_names is None or bounds_item_names.shape == ():  # None or empty array
+            bounds_item_names = spatial(data).names
+        lower = wrap(stored['lower'], channel(vector=tuple(bounds_item_names))) if stored['lower'].ndim > 0 else wrap(stored['lower'])
+        upper = wrap(stored['upper'], channel(vector=tuple(bounds_item_names)))
+        extr = extrapolation.from_dict(stored['extrapolation'][()])
+        if ftype == 'CenteredGrid':
+            return CenteredGrid(data, bounds=geom.Box(lower, upper), extrapolation=extr)
+        elif ftype == 'StaggeredGrid':
+            data_ = unstack_staggered_tensor(data, extr)
+            return StaggeredGrid(data_, bounds=geom.Box(lower, upper), extrapolation=extr)
+    raise NotImplementedError(f"{ftype} not implemented ({implemented_types})")
```

### Comparing `phiflow-2.3.4/phi/field/_field_math.py` & `phiflow-2.4.0/phi/field/_field_math.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,845 +1,893 @@
-import warnings
-from numbers import Number
-from typing import Callable, List, Tuple, Optional, Union
-
-from phi import geom
-from phi import math
-from phi.geom import Box, Geometry
-from phi.math import Tensor, spatial, instance, tensor, channel, Shape, unstack, solve_linear, jit_compile_linear, shape, Solve, extrapolation, jit_compile
-from ._field import Field, SampledField, SampledFieldType, as_extrapolation
-from ._grid import CenteredGrid, Grid, StaggeredGrid, GridType
-from ._point_cloud import PointCloud
-from ..math.extrapolation import Extrapolation, SYMMETRIC, REFLECT, ANTIREFLECT, ANTISYMMETRIC, combine_by_direction
-
-
-def bake_extrapolation(grid: GridType) -> GridType:
-    """
-    Pads `grid` with its current extrapolation.
-    For `StaggeredGrid`s, the resulting grid will have a consistent shape, independent of the original extrapolation.
-
-    Args:
-        grid: `CenteredGrid` or `StaggeredGrid`.
-
-    Returns:
-        Padded grid with extrapolation `phi.math.extrapolation.NONE`.
-    """
-    if grid.extrapolation == math.extrapolation.NONE:
-        return grid
-    if isinstance(grid, StaggeredGrid):
-        values = grid.values.unstack('vector')
-        padded = []
-        for dim, value in zip(grid.shape.spatial.names, values):
-            lower, upper = grid.extrapolation.valid_outer_faces(dim)
-            padded.append(math.pad(value, {dim: (0 if lower else 1, 0 if upper else 1)}, grid.extrapolation[{'vector': dim}], bounds=grid.bounds))
-        return StaggeredGrid(math.stack(padded, grid.shape['vector']), bounds=grid.bounds, extrapolation=math.extrapolation.NONE)
-    elif isinstance(grid, CenteredGrid):
-        return pad(grid, 1).with_extrapolation(math.extrapolation.NONE)
-    else:
-        raise ValueError(f"Not a valid grid: {grid}")
-
-
-def laplace(field: GridType,
-            axes=spatial,
-            order=2,
-            implicit: math.Solve = None,
-            weights: Union[Tensor, Field] = None) -> GridType:
-    """
-    Spatial Laplace operator for scalar grid.
-    If a vector grid is passed, it is assumed to be centered and the laplace is computed component-wise.
-
-    Args:
-        field: n-dimensional `CenteredGrid`
-        axes: The second derivative along these dimensions is summed over
-        weights: (Optional) Multiply the axis terms by these factors before summation.
-            Must be a `phi.math.Tensor` or `phi.field.Field` with a single channel dimension that lists all laplace axes by name.
-        order: Spatial order of accuracy.
-            Higher orders entail larger stencils and more computation time but result in more accurate results assuming a large enough resolution.
-            Supported: 2 explicit, 4 explicit, 6 implicit.
-        implicit: When a `Solve` object is passed, performs an implicit operation with the specified solver and tolerances.
-            Otherwise, an explicit stencil is used.
-
-    Returns:
-        laplacian field as `CenteredGrid`
-    """
-    if isinstance(weights, Field):
-        weights = weights.at(field).values
-    axes_names = field.shape.only(axes).names
-    extrap_map = {}
-    if not implicit:
-        if order == 2:
-                values, needed_shifts = [1, -2, 1], (-1, 0, 1)
-
-        elif order == 4:
-                values, needed_shifts = [-1/12, 4/3, -5/2, 4/3, -1/12], (-2, -1, 0, 1, 2)
-    else:
-        extrap_map_rhs = {}
-        if order == 6:
-            values, needed_shifts = [3/44, 12/11, -51/22, 12/11, 3/44], (-2, -1, 0, 1, 2)
-            extrap_map['symmetric'] = combine_by_direction(REFLECT, SYMMETRIC)
-            values_rhs, needed_shifts_rhs = [2/11, 1, 2/11], (-1, 0, 1)
-            extrap_map_rhs['symmetric'] = combine_by_direction(REFLECT, SYMMETRIC)
-    base_widths = (abs(min(needed_shifts)), max(needed_shifts))
-    field.with_extrapolation(extrapolation.map(_ex_map_f(extrap_map), field.extrapolation))
-    padded_components = [pad(field, {dim: base_widths}) for dim in axes_names]
-    shifted_components = [shift(padded_component, needed_shifts, None, pad=False, dims=dim) for padded_component, dim in zip(padded_components, axes_names)]
-    result_components = [sum([value * shift_ for value, shift_ in zip(values, shifted_component)]) / field.dx.vector[dim]**2 for shifted_component, dim in zip(shifted_components, axes_names)]
-    if implicit:
-        result_components = stack(result_components, channel('laplacian'))
-        result_components.with_values(result_components.values._cache())
-        result_components = result_components.with_extrapolation(extrapolation.map(_ex_map_f(extrap_map_rhs), field.extrapolation))
-        implicit.x0 = result_components
-        result_components = solve_linear(_lhs_for_implicit_scheme, result_components, solve=implicit, values_rhs=values_rhs, needed_shifts_rhs=needed_shifts_rhs, stack_dim=channel('laplacian'))
-        result_components = unstack(result_components, 'laplacian')
-        extrap_map = extrap_map_rhs
-    result_components = [component.with_bounds(field.bounds) for component in result_components]
-    if weights is not None:
-        assert channel(weights).rank == 1 and channel(weights).item_names is not None, f"weights must have one channel dimension listing the laplace dims but got {shape(weights)}"
-        assert set(channel(weights).item_names[0]) >= set(axes_names), f"the channel dim of weights must contain all laplace dims {axes_names} but only has {channel(weights).item_names}"
-        result_components = [c * weights[ax] for c, ax in zip(result_components, axes_names)]
-    result = sum(result_components)
-    result = result.with_extrapolation(extrapolation.map(_ex_map_f(extrap_map), field.extrapolation))
-    return result
-
-
-def spatial_gradient(field: CenteredGrid,
-                     gradient_extrapolation: Extrapolation = None,
-                     type: type = CenteredGrid,
-                     dims: math.DimFilter = spatial,
-                     stack_dim: Shape = channel('vector'),
-                     order=2,
-                     implicit: Solve = None):
-    """
-    Finite difference spatial_gradient.
-
-    This function can operate in two modes:
-
-    * `type=CenteredGrid` approximates the spatial_gradient at cell centers using central differences
-    * `type=StaggeredGrid` computes the spatial_gradient at face centers of neighbouring cells
-
-    Args:
-        field: centered grid of any number of dimensions (scalar field, vector field, tensor field)
-        gradient_extrapolation: Extrapolation of the output
-        type: either `CenteredGrid` or `StaggeredGrid`
-        dims: Along which dimensions to compute the spatial gradient. Only supported when `type==CenteredGrid`.
-        stack_dim: Dimension to be added. This dimension lists the spatial_gradient w.r.t. the spatial dimensions.
-            The `field` must not have a dimension of the same name.
-        order: Spatial order of accuracy.
-            Higher orders entail larger stencils and more computation time but result in more accurate results assuming a large enough resolution.
-            Supported: 2 explicit, 4 explicit, 6 implicit.
-        implicit: When a `Solve` object is passed, performs an implicit operation with the specified solver and tolerances.
-            Otherwise, an explicit stencil is used.
-
-    Returns:
-        spatial_gradient field of type `type`.
-    """
-    if gradient_extrapolation is None:
-        gradient_extrapolation = field.extrapolation.spatial_gradient()
-    extrap_map = {}
-    if not implicit:
-        if order == 2:
-            if type == CenteredGrid:
-                values, needed_shifts = [-1/2, 1/2], (-1, 1)
-            else:
-                values, needed_shifts = [-1, 1], (0, 1)
-        elif order == 4:
-            if type == CenteredGrid:
-                values, needed_shifts = [1/12, -2/3, 2/3, -1/12], (-2, -1, 1, 2)
-            else:
-                values, needed_shifts = [1/24, -27/24, 27/24, -1/24], (-1, 0, 1, 2)
-        else:
-            raise NotImplementedError(f"explicit {order}th-order not supported")
-    else:
-        extrap_map_rhs = {}
-        if order == 6:
-            if type == CenteredGrid:
-                values, needed_shifts = [-1/36, -14/18, 14/18, 1/36], (-2, -1, 1, 2)
-                values_rhs, needed_shifts_rhs = [1/3, 1, 1/3], (-1, 0, 1)
-            else:
-                values, needed_shifts = [-17/186, -63/62, 63/62, 17/186], (-1, 0, 1, 2)
-                extrap_map['symmetric'] = combine_by_direction(REFLECT, SYMMETRIC)
-                values_rhs, needed_shifts_rhs = [9/62, 1, 9/62], (-1, 0, 1)
-                extrap_map_rhs['symmetric'] = combine_by_direction(ANTIREFLECT, ANTISYMMETRIC)
-        else:
-            raise NotImplementedError(f"implicit {order}th-order not supported")
-    base_widths = (abs(min(needed_shifts)), max(needed_shifts))
-    field.with_extrapolation(extrapolation.map(_ex_map_f(extrap_map), field.extrapolation))  # ToDo does this line do anything?
-    if implicit:
-        gradient_extrapolation = extrapolation.map(_ex_map_f(extrap_map_rhs), gradient_extrapolation)
-    spatial_dims = field.shape.only(dims).names
-    stack_dim = stack_dim._with_item_names((spatial_dims,))
-    if type == CenteredGrid:
-        # ToDo if extrapolation == math.extrapolation.NONE, extend size by 1
-        # pad = 1 if extrapolation == math.extrapolation.NONE else 0
-        # bounds = Box(field.bounds.lower - field.dx, field.bounds.upper + field.dx) if extrapolation == math.extrapolation.NONE else field.bounds
-        std_widths = (0, 0)
-        if gradient_extrapolation == math.extrapolation.NONE:
-            base_widths = (abs(min(needed_shifts))+1, max(needed_shifts)+1)
-            std_widths = (1, 1)
-        padded_components = [pad(field, {dim_: base_widths if dim_ == dim else std_widths for dim_ in spatial_dims}) for dim in spatial_dims]
-    elif type == StaggeredGrid:
-        assert spatial_dims == field.shape.spatial.names, f"spatial_gradient with type=StaggeredGrid requires dims=spatial, i.e. dims='{','.join(field.shape.spatial.names)}'"
-        base_widths = (base_widths[0], base_widths[1]-1)
-        padded_components = pad_for_staggered_output(field, gradient_extrapolation, field.shape.spatial.names, base_widths)
-    else:
-        raise ValueError(type)
-    shifted_components = [shift(padded_component, needed_shifts, stack_dim=None, pad=False, dims=dim) for padded_component, dim in zip(padded_components, spatial_dims)]
-    result_components = [sum([value * shift_ for value, shift_ in zip(values, shifted_component)]) / field.dx.vector[dim] for shifted_component, dim in zip(shifted_components, field.shape.spatial.names)]
-    if type == CenteredGrid:
-        result = stack(result_components, stack_dim)
-    else:
-        assert stack_dim.name == 'vector', f"spatial_gradient with type=StaggeredGrid requires stack_dim.name == 'vector' but got '{stack_dim.name}'"
-        result = StaggeredGrid(math.stack([component.values for component in result_components], channel(vector=spatial_dims)), bounds=field.bounds, extrapolation=gradient_extrapolation)
-    result = result.with_extrapolation(gradient_extrapolation)
-    if implicit:
-        implicit.x0 = result
-        result = solve_linear(_lhs_for_implicit_scheme, result, solve=implicit, values_rhs=values_rhs, needed_shifts_rhs=needed_shifts_rhs, stack_dim=stack_dim, staggered_output=type != CenteredGrid)
-    if type == CenteredGrid and gradient_extrapolation == math.extrapolation.NONE:
-        result = result.with_bounds(Box(field.bounds.lower - field.dx, field.bounds.upper + field.dx))
-    else:
-        result = result.with_bounds(field.bounds)
-    return result
-
-
-def _ex_map_f(ext_dict: dict):
-    def f(ext: Extrapolation):
-        return ext_dict[ext.__repr__()] if ext.__repr__() in ext_dict else ext
-    return f
-
-
-@jit_compile_linear(auxiliary_args="values_rhs, needed_shifts_rhs, stack_dim, staggered_output")
-def _lhs_for_implicit_scheme(x, values_rhs, needed_shifts_rhs, stack_dim, staggered_output=False):
-    result = []
-    for dim, component in zip(x.shape.only(math.spatial).names, unstack(x, stack_dim.name)):
-        shifted = shift(component, needed_shifts_rhs, stack_dim=None, dims=dim)
-        result.append(sum([value * shift_ for value, shift_ in zip(values_rhs, shifted)]))
-
-    if staggered_output:
-        result = x.with_values(math.stack([component.values for component in result], channel('vector')))
-    else:
-        result = stack(result, stack_dim)
-
-    return result
-
-
-def pad_for_staggered_output(field: CenteredGrid, output_extrapolation: Extrapolation, dims: tuple, base_widths: tuple):
-    padded_components = []
-    for dim in dims:
-        border_valid = output_extrapolation.valid_outer_faces(dim)
-        padding_widths = (border_valid[0] + base_widths[0], border_valid[1] + base_widths[1])
-        padded_components.append(pad(field, {dim: padding_widths}))
-
-    return padded_components
-
-
-def shift(grid: CenteredGrid, offsets: tuple, stack_dim: Optional[Shape] = channel('shift'), dims=spatial, pad=True):
-    """
-    Wraps :func:`math.shift` for CenteredGrid.
-
-    Args:
-      grid: CenteredGrid: 
-      offsets: tuple: 
-      stack_dim:  (Default value = 'shift')
-    """
-    if pad:
-        padding = grid.extrapolation
-        new_bounds = grid.bounds
-    else:
-        padding = None
-        max_lower_shift = min(offsets) if min(offsets) < 0 else 0
-        max_upper_shift = max(offsets) if max(offsets) > 0 else 0
-        w_lower = math.wrap([max_lower_shift if dim in dims else 0 for dim in grid.shape.spatial.names])
-        w_upper = math.wrap([max_upper_shift if dim in dims else 0 for dim in grid.shape.spatial.names])
-        new_bounds = Box(grid.box.lower - w_lower * grid.dx, grid.box.upper - w_upper * grid.dx)
-    data = math.shift(grid.values, offsets, dims=dims, padding=padding, stack_dim=stack_dim)
-    return [type(grid)(data[i], bounds=new_bounds, extrapolation=grid.extrapolation) for i in range(len(offsets))]
-
-
-def stagger(field: CenteredGrid,
-            face_function: Callable,
-            extrapolation: Union[float, math.extrapolation.Extrapolation],
-            type: type = StaggeredGrid):
-    """
-    Creates a new grid by evaluating `face_function` given two neighbouring cells.
-    One layer of missing cells is inferred from the extrapolation.
-    
-    This method returns a Field of type `type` which must be either StaggeredGrid or CenteredGrid.
-    When returning a StaggeredGrid, the new values are sampled at the faces of neighbouring cells.
-    When returning a CenteredGrid, the new grid has the same resolution as `field`.
-
-    Args:
-      field: centered grid
-      face_function: function mapping (value1: Tensor, value2: Tensor) -> center_value: Tensor
-      extrapolation: extrapolation mode of the returned grid. Has no effect on the values.
-      type: one of (StaggeredGrid, CenteredGrid)
-      field: CenteredGrid: 
-      face_function: Callable:
-      extrapolation: math.extrapolation.Extrapolation: 
-      type: type:  (Default value = StaggeredGrid)
-
-    Returns:
-      grid of type matching the `type` argument
-
-    """
-    extrapolation = as_extrapolation(extrapolation)
-    all_lower = []
-    all_upper = []
-    if type == StaggeredGrid:
-        for dim in field.resolution.names:
-            valid_lo, valid_up = extrapolation.valid_outer_faces(dim)
-            if valid_lo and valid_up:
-                width_lower, width_upper = {dim: (1, 0)}, {dim: (0, 1)}
-            elif valid_lo and not valid_up:
-                width_lower, width_upper = {dim: (1, -1)}, {dim: (0, 0)}
-            elif not valid_lo and valid_up:
-                width_lower, width_upper = {dim: (0, 0)}, {dim: (-1, 1)}
-            else:
-                width_lower, width_upper = {dim: (0, -1)}, {dim: (-1, 0)}
-            all_lower.append(math.pad(field.values, width_lower, field.extrapolation, bounds=field.bounds))
-            all_upper.append(math.pad(field.values, width_upper, field.extrapolation, bounds=field.bounds))
-        all_upper = math.stack(all_upper, channel('vector'))
-        all_lower = math.stack(all_lower, channel('vector'))
-        values = face_function(all_lower, all_upper)
-        result = StaggeredGrid(values, bounds=field.bounds, extrapolation=extrapolation)
-        assert result.shape.spatial == field.shape.spatial
-        return result
-    elif type == CenteredGrid:
-        left, right = math.shift(field.values, (-1, 1), padding=field.extrapolation, stack_dim=channel('vector'))
-        values = face_function(left, right)
-        return CenteredGrid(values, bounds=field.bounds, extrapolation=extrapolation)
-    else:
-        raise ValueError(type)
-
-
-def divergence(field: Grid, order=2, implicit: Solve = None) -> CenteredGrid:
-    """
-    Computes the divergence of a grid using finite differences.
-
-    This function can operate in two modes depending on the type of `field`:
-
-    * `CenteredGrid` approximates the divergence at cell centers using central differences
-    * `StaggeredGrid` exactly computes the divergence at cell centers
-
-    Args:
-        field: vector field as `CenteredGrid` or `StaggeredGrid`
-        order: Spatial order of accuracy.
-            Higher orders entail larger stencils and more computation time but result in more accurate results assuming a large enough resolution.
-            Supported: 2 explicit, 4 explicit, 6 implicit.
-        implicit: When a `Solve` object is passed, performs an implicit operation with the specified solver and tolerances.
-            Otherwise, an explicit stencil is used.
-
-    Returns:
-        Divergence field as `CenteredGrid`
-    """
-
-    extrap_map = {}
-    if not implicit:
-        if order == 2:
-            if isinstance(field, CenteredGrid):
-                values, needed_shifts = [-1 / 2, 1 / 2], (-1, 1)
-            else:
-                values, needed_shifts = [-1, 1], (0, 1)
-
-        elif order == 4:
-            if isinstance(field, CenteredGrid):
-                values, needed_shifts = [1 / 12, -2 / 3, 2 / 3, -1 / 12], (-2, -1, 1, 2)
-            else:
-                values, needed_shifts = [1 / 24, -27 / 24, 27 / 24, -1 / 24], (-1, 0, 1, 2)
-    else:
-        extrap_map_rhs = {}
-        if order == 6:
-            extrap_map['symmetric'] = combine_by_direction(REFLECT, SYMMETRIC)
-            extrap_map_rhs['symmetric'] = combine_by_direction(ANTIREFLECT, ANTISYMMETRIC)
-
-            if isinstance(field, CenteredGrid):
-                values, needed_shifts = [-1 / 36, -14 / 18, 14 / 18, 1 / 36], (-2, -1, 1, 2)
-                values_rhs, needed_shifts_rhs = [1 / 3, 1, 1 / 3], (-1, 0, 1)
-
-            else:
-                values, needed_shifts = [-17 / 186, -63 / 62, 63 / 62, 17 / 186], (-1, 0, 1, 2)
-                values_rhs, needed_shifts_rhs = [9 / 62, 1, 9 / 62], (-1, 0, 1)
-    base_widths = (abs(min(needed_shifts)), max(needed_shifts))
-    field.with_extrapolation(extrapolation.map(_ex_map_f(extrap_map), field.extrapolation))  # ToDo does this line do anything?
-    spatial_dims = field.shape.spatial.names
-    if isinstance(field, StaggeredGrid):
-        base_widths = (base_widths[0]+1, base_widths[1])
-        padded_components = []
-        for dim, component in zip(field.shape.spatial.names, unstack(field, 'vector')):
-            border_valid = field.extrapolation.valid_outer_faces(dim)
-            padding_widths = (base_widths[0] - border_valid[0], base_widths[1] - border_valid[1])
-            padded_components.append(pad(component, {dim: padding_widths}))
-    elif isinstance(field, CenteredGrid):
-        padded_components = [pad(component, {dim: base_widths}) for dim, component in zip(spatial_dims, unstack(field, 'vector'))]
-        if field.extrapolation == math.extrapolation.NONE:
-            padded_components = [pad(component, {dim_: (0, 0) if dim_ == dim else (-1, -1) for dim_ in spatial_dims}) for dim, component in zip(spatial_dims, padded_components)]
-    shifted_components = [shift(padded_component, needed_shifts, None, pad=False, dims=dim) for padded_component, dim in zip(padded_components, spatial_dims)]
-    result_components = [sum([value * shift for value, shift in zip(values, shifted_component)]) / field.dx.vector[dim] for shifted_component, dim in zip(shifted_components, spatial_dims)]
-    if implicit:
-        result_components = stack(result_components, channel('vector'))
-        result_components.with_values(result_components.values._cache())
-        implicit.x0 = field
-        result_components = solve_linear(_lhs_for_implicit_scheme, result_components, solve=implicit, values_rhs=values_rhs, needed_shifts_rhs=needed_shifts_rhs, stack_dim=channel('vector'))
-        result_components = unstack(result_components, 'vector')
-    result_components = [component.with_bounds(field.bounds) for component in result_components]
-    result = sum(result_components)
-    if field.extrapolation == math.extrapolation.NONE and isinstance(field, CenteredGrid):
-        result = result.with_bounds(Box(field.bounds.lower + field.dx, field.bounds.upper - field.dx))
-    return result
-
-
-def curl(field: Grid, type: type = CenteredGrid):
-    """ Computes the finite-difference curl of the give 2D `StaggeredGrid`. """
-    assert field.spatial_rank in (2, 3), "curl is only defined in 2 and 3 spatial dimensions."
-    if isinstance(field, CenteredGrid) and field.spatial_rank == 2:
-        if 'vector' not in field.shape and type == StaggeredGrid:
-            # 2D curl of scalar field
-            grad = math.spatial_gradient(field.values, dx=field.dx, difference='forward', padding=None, stack_dim=channel('vector'))
-            result = grad.vector.flip() * (1, -1)  # (d/dy, -d/dx)
-            bounds = Box(field.bounds.lower + 0.5 * field.dx, field.bounds.upper - 0.5 * field.dx)  # lose 1 cell per dimension
-            return StaggeredGrid(result, bounds=bounds, extrapolation=field.extrapolation.spatial_gradient())
-        if 'vector' in field.shape and type == CenteredGrid:
-            # 2D curl of vector field
-            x, y = field.shape.spatial.names
-            vy_dx = math.spatial_gradient(field.values.vector[1], dx=field.dx.vector[0], padding=field.extrapolation, dims=x, stack_dim=None)
-            vx_dy = math.spatial_gradient(field.values.vector[0], dx=field.dx.vector[1], padding=field.extrapolation, dims=y, stack_dim=None)
-            c = vy_dx - vx_dy
-            return field.with_values(c)
-    elif isinstance(field, StaggeredGrid) and field.spatial_rank == 2:
-        if type == CenteredGrid:
-            for dim in field.resolution.names:
-                l, u = field.extrapolation.valid_outer_faces(dim)
-                assert l == u, "periodic extrapolation not yet supported"
-            values = bake_extrapolation(field).values
-            x_padded = math.pad(values.vector['x'], {'y': (1, 1)}, field.extrapolation)
-            y_padded = math.pad(values.vector['y'], {'x': (1, 1)}, field.extrapolation)
-            vx_dy = math.spatial_gradient(x_padded, field.dx, 'forward', None, dims='y', stack_dim=None)
-            vy_dx = math.spatial_gradient(y_padded, field.dx, 'forward', None, dims='x', stack_dim=None)
-            result = vy_dx - vx_dy
-            return CenteredGrid(result, field.extrapolation.spatial_gradient(), bounds=field.bounds)
-    raise NotImplementedError()
-
-
-def fourier_laplace(grid: GridType, times=1) -> GridType:
-    """ See `phi.math.fourier_laplace()` """
-    assert grid.extrapolation.spatial_gradient() == math.extrapolation.PERIODIC
-    values = math.fourier_laplace(grid.values, dx=grid.dx, times=times)
-    return type(grid)(values=values, bounds=grid.bounds, extrapolation=grid.extrapolation)
-
-
-def fourier_poisson(grid: GridType, times=1) -> GridType:
-    """ See `phi.math.fourier_poisson()` """
-    assert grid.extrapolation.spatial_gradient() == math.extrapolation.PERIODIC
-    values = math.fourier_poisson(grid.values, dx=grid.dx, times=times)
-    return type(grid)(values=values, bounds=grid.bounds, extrapolation=grid.extrapolation)
-
-
-def native_call(f, *inputs, channels_last=None, channel_dim='vector', extrapolation=None) -> Union[SampledField, Tensor]:
-    """
-    Similar to `phi.math.native_call()`.
-
-    Args:
-        f: Function to be called on native tensors of `inputs.values`.
-            The function output must have the same dimension layout as the inputs and the batch size must be identical.
-        *inputs: `SampledField` or `phi.Tensor` instances.
-        extrapolation: (Optional) Extrapolation of the output field. If `None`, uses the extrapolation of the first input field.
-
-    Returns:
-        `SampledField` matching the first `SampledField` in `inputs`.
-    """
-    input_tensors = [i.values if isinstance(i, SampledField) else tensor(i) for i in inputs]
-    values = math.native_call(f, *input_tensors, channels_last=channels_last, channel_dim=channel_dim)
-    for i in inputs:
-        if isinstance(i, SampledField):
-            result = i.with_values(values=values)
-            if extrapolation is not None:
-                result = result.with_extrapolation(extrapolation)
-            return result
-    else:
-        raise AssertionError("At least one input must be a SampledField.")
-
-
-def data_bounds(loc: Union[SampledField, Tensor]) -> Box:
-    if isinstance(loc, SampledField):
-        loc = loc.points
-    assert isinstance(loc, Tensor), f"loc must be a Tensor or SampledField but got {type(loc)}"
-    min_vec = math.min(loc, dim=loc.shape.non_batch.non_channel)
-    max_vec = math.max(loc, dim=loc.shape.non_batch.non_channel)
-    return Box(min_vec, max_vec)
-
-
-def mean(field: SampledField) -> Tensor:
-    """
-    Computes the mean value by reducing all spatial / instance dimensions.
-
-    Args:
-        field: `SampledField`
-
-    Returns:
-        `phi.Tensor`
-    """
-    return math.mean(field.values, field.shape.non_channel.non_batch)
-
-
-def normalize(field: SampledField, norm: SampledField, epsilon=1e-5):
-    """ Multiplies the values of `field` so that its sum matches the source. """
-    data = math.normalize_to(field.values, norm.values, epsilon)
-    return field.with_values(data)
-
-
-def center_of_mass(density: SampledField):
-    """
-    Compute the center of mass of a density field.
-
-    Args:
-        density: Scalar `SampledField`
-
-    Returns:
-        `Tensor` holding only batch dimensions.
-    """
-    assert 'vector' not in density.shape
-    return mean(density.points * density) / mean(density)
-
-
-def pad(grid: GridType, widths: Union[int, tuple, list, dict]) -> GridType:
-    """
-    Pads a `Grid` using its extrapolation.
-
-    Unlike `phi.math.pad()`, this function also affects the `bounds` of the grid, changing its size and origin depending on `widths`.
-
-    Args:
-        grid: `CenteredGrid` or `StaggeredGrid`
-        widths: Either `int` or `(lower, upper)` to pad the same number of cells in all spatial dimensions
-            or `dict` mapping dimension names to `(lower, upper)`.
-
-    Returns:
-        `Grid` of the same type as `grid`
-    """
-    if isinstance(widths, int):
-        widths = {axis: (widths, widths) for axis in grid.shape.spatial.names}
-    elif isinstance(widths, (tuple, list)):
-        widths = {axis: (width if isinstance(width, (tuple, list)) else (width, width)) for axis, width in zip(grid.shape.spatial.names, widths)}
-    else:
-        assert isinstance(widths, dict)
-    widths_list = [widths[axis] if axis in widths.keys() else (0, 0) for axis in grid.shape.spatial.names]
-    if isinstance(grid, Grid):
-        data = math.pad(grid.values, widths, grid.extrapolation, bounds=grid.bounds)
-        w_lower = math.wrap([w[0] for w in widths_list])
-        w_upper = math.wrap([w[1] for w in widths_list])
-        bounds = Box(grid.box.lower - w_lower * grid.dx, grid.box.upper + w_upper * grid.dx)
-        return type(grid)(values=data, bounds=bounds, extrapolation=grid.extrapolation)
-    raise NotImplementedError(f"{type(grid)} not supported. Only Grid instances allowed.")
-
-
-def downsample2x(grid: Grid) -> GridType:
-    """
-    Reduces the number of sample points by a factor of 2 in each spatial dimension.
-    The new values are determined via linear interpolation.
-
-    See Also:
-        `upsample2x()`.
-
-    Args:
-        grid: `CenteredGrid` or `StaggeredGrid`.
-
-    Returns:
-        `Grid` of same type as `grid`.
-    """
-    if isinstance(grid, CenteredGrid):
-        values = math.downsample2x(grid.values, grid.extrapolation)
-        return CenteredGrid(values, bounds=grid.bounds, extrapolation=grid.extrapolation)
-    elif isinstance(grid, StaggeredGrid):
-        values = []
-        for dim, centered_grid in zip(grid.shape.spatial.names, unstack(grid, 'vector')):
-            odd_discarded = centered_grid.values[{dim: slice(None, None, 2)}]
-            others_interpolated = math.downsample2x(odd_discarded, grid.extrapolation, dims=grid.shape.spatial.without(dim))
-            values.append(others_interpolated)
-        return StaggeredGrid(math.stack(values, channel('vector')), bounds=grid.bounds, extrapolation=grid.extrapolation)
-    else:
-        raise ValueError(type(grid))
-
-
-def upsample2x(grid: GridType) -> GridType:
-    """
-    Increases the number of sample points by a factor of 2 in each spatial dimension.
-    The new values are determined via linear interpolation.
-
-    See Also:
-        `downsample2x()`.
-
-    Args:
-        grid: `CenteredGrid` or `StaggeredGrid`.
-
-    Returns:
-        `Grid` of same type as `grid`.
-    """
-    if isinstance(grid, CenteredGrid):
-        values = math.upsample2x(grid.values, grid.extrapolation)
-        return CenteredGrid(values, bounds=grid.bounds, extrapolation=grid.extrapolation)
-    elif isinstance(grid, StaggeredGrid):
-        raise NotImplementedError()
-    else:
-        raise ValueError(type(grid))
-
-
-def concat(fields: Union[List[SampledFieldType], Tuple[SampledFieldType, ...]], dim: Union[str, Shape]) -> SampledFieldType:
-    """
-    Concatenates the given `SampledField`s along `dim`.
-
-    See Also:
-        `stack()`.
-
-    Args:
-        fields: List of matching `SampledField` instances.
-        dim: Concatenation dimension as `Shape`. Size is ignored.
-
-    Returns:
-        `SampledField` matching concatenated fields.
-    """
-    assert all(isinstance(f, SampledField) for f in fields)
-    assert all(isinstance(f, type(fields[0])) for f in fields)
-    if any(f.extrapolation != fields[0].extrapolation for f in fields):
-        raise NotImplementedError("Concatenating extrapolations not supported")
-    if isinstance(fields[0], Grid):
-        values = math.concat([f.values for f in fields], dim)
-        return fields[0].with_values(values)
-    elif isinstance(fields[0], PointCloud):
-        elements = geom.concat([f.elements for f in fields], dim)
-        values = math.concat([math.expand(f.values, f.shape.only(dim)) for f in fields], dim)
-        return PointCloud(elements=elements, values=values, extrapolation=fields[0].extrapolation, add_overlapping=fields[0]._add_overlapping, bounds=fields[0]._bounds)
-    raise NotImplementedError(type(fields[0]))
-
-
-def stack(fields, dim: Shape, dim_bounds: Box = None):
-    """
-    Stacks the given `SampledField`s along `dim`.
-
-    See Also:
-        `concat()`.
-
-    Args:
-        fields: List of matching `SampledField` instances.
-        dim: Stack dimension as `Shape`. Size is ignored.
-        dim_bounds: `Box` defining the physical size for `dim`.
-
-    Returns:
-        `SampledField` matching stacked fields.
-    """
-    assert all(isinstance(f, SampledField) for f in fields), f"All fields must be SampledFields of the same type but got {fields}"
-    assert all(isinstance(f, type(fields[0])) for f in fields), f"All fields must be SampledFields of the same type but got {fields}"
-    if any(f.extrapolation != fields[0].extrapolation for f in fields):
-        raise NotImplementedError("Concatenating extrapolations not supported")
-    if isinstance(fields[0], Grid):
-        values = math.stack([f.values for f in fields], dim)
-        if spatial(dim):
-            if dim_bounds is None:
-                dim_bounds = Box(**{dim.name: len(fields)})
-            return type(fields[0])(values, extrapolation=fields[0].extrapolation, bounds=fields[0].bounds * dim_bounds)
-        else:
-            return fields[0].with_values(values)
-    elif isinstance(fields[0], PointCloud):
-        elements = geom.stack([f.elements for f in fields], dim)
-        values = math.stack([f.values for f in fields], dim)
-        return PointCloud(elements=elements, values=values, extrapolation=fields[0].extrapolation, add_overlapping=fields[0]._add_overlapping, bounds=fields[0]._bounds)
-    raise NotImplementedError(type(fields[0]))
-
-
-def assert_close(*fields: Union[SampledField, Tensor, Number],
-                 rel_tolerance: float = 1e-5,
-                 abs_tolerance: float = 0,
-                 msg: str = "",
-                 verbose: bool = True):
-    """ Raises an AssertionError if the `values` of the given fields are not close. See `phi.math.assert_close()`. """
-    f0 = next(filter(lambda t: isinstance(t, SampledField), fields))
-    values = [(f @ f0).values if isinstance(f, SampledField) else math.wrap(f) for f in fields]
-    math.assert_close(*values, rel_tolerance=rel_tolerance, abs_tolerance=abs_tolerance, msg=msg, verbose=verbose)
-
-
-def where(mask: Union[Field, Geometry, float], field_true: Union[Field, float], field_false: Union[Field, float]) -> SampledFieldType:
-    """
-    Element-wise where operation.
-    Picks the value of `field_true` where `mask=1 / True` and the value of `field_false` where `mask=0 / False`.
-
-    The fields are automatically resampled if necessary, preferring the sample points of `mask`.
-    At least one of the arguments must be a `SampledField`.
-
-    Args:
-        mask: `Field` or `Geometry` object.
-        field_true: `Field`
-        field_false: `Field`
-
-    Returns:
-        `SampledField`
-    """
-    field_true, field_false, mask = _auto_resample(field_true, field_false, mask)
-    values = math.where(mask.values, field_true.values, field_false.values)
-    return field_true.with_values(values)
-
-
-def maximum(f1: Union[Field, Geometry, float], f2: Union[Field, Geometry, float]):
-    """
-    Element-wise maximum.
-    One of the given fields needs to be an instance of `SampledField` and the the result will be sampled at the corresponding points.
-    If both are `SampledFields` but have different points, `f1` takes priority.
-
-    Args:
-        f1: `Field` or `Geometry` or constant.
-        f2: `Field` or `Geometry` or constant.
-
-    Returns:
-        `SampledField`
-    """
-    f1, f2 = _auto_resample(f1, f2)
-    return f1.with_values(math.maximum(f1.values, f2.values))
-
-
-def minimum(f1: Union[Field, Geometry, float], f2: Union[Field, Geometry, float]):
-    """
-    Element-wise minimum.
-    One of the given fields needs to be an instance of `SampledField` and the the result will be sampled at the corresponding points.
-    If both are `SampledFields` but have different points, `f1` takes priority.
-
-    Args:
-        f1: `Field` or `Geometry` or constant.
-        f2: `Field` or `Geometry` or constant.
-
-    Returns:
-        `SampledField`
-    """
-    f1, f2 = _auto_resample(f1, f2)
-    return f1.with_values(math.minimum(f1.values, f2.values))
-
-
-def _auto_resample(*fields: Field):
-    """ Prefers extrapolation from first SampledField """
-    for sampled_field in fields:
-        if isinstance(sampled_field, SampledField):
-            return [f @ sampled_field for f in fields]
-    raise AssertionError(f"At least one argument must be a SampledField but got {fields}")
-
-
-def vec_length(field: SampledField):
-    """ See `phi.math.vec_abs()` """
-    assert isinstance(field, SampledField), f"SampledField required but got {type(field).__name__}"
-    if isinstance(field, StaggeredGrid):
-        field = field.at_centers()
-    return field.with_values(math.vec_abs(field.values))
-
-
-def vec_squared(field: SampledField):
-    """ See `phi.math.vec_squared()` """
-    if isinstance(field, StaggeredGrid):
-        field = field.at_centers()
-    return field.with_values(math.vec_squared(field.values))
-
-
-def finite_fill(grid: GridType, distance=1, diagonal=True) -> GridType:
-    """
-    Extrapolates values of `grid` which are marked by nonzero values in `valid` using `phi.math.masked_fill().
-    If `values` is a StaggeredGrid, its components get extrapolated independently.
-
-    Args:
-        grid: Grid holding the values for extrapolation and possible non-finite values to be filled.
-        distance: Number of extrapolation steps, i.e. how far a cell can be from the closest finite value to get filled.
-        diagonal: Whether to extrapolate values to their diagonal neighbors per step.
-
-    Returns:
-        grid: Grid with extrapolated values.
-        valid: binary Grid marking all valid values after extrapolation.
-    """
-    if isinstance(grid, CenteredGrid):
-        new_values = math.finite_fill(grid.values, distance=distance, diagonal=diagonal, padding=grid.extrapolation)
-        return grid.with_values(new_values)
-    elif isinstance(grid, StaggeredGrid):
-        new_values = [finite_fill(c, distance=distance, diagonal=diagonal).values for c in grid.vector]
-        return grid.with_values(math.stack(new_values, channel(grid)))
-    else:
-        raise ValueError(grid)
-
-
-def discretize(grid: Grid, filled_fraction=0.25):
-    """ Treats channel dimensions as batch dimensions. """
-    import numpy as np
-    data = math.reshaped_native(grid.values, [grid.shape.non_spatial, grid.shape.spatial])
-    ranked_idx = np.argsort(data, axis=-1)
-    filled_idx = ranked_idx[:, int(round(grid.shape.spatial.volume * (1 - filled_fraction))):]
-    filled = np.zeros_like(data)
-    np.put_along_axis(filled, filled_idx, 1, axis=-1)
-    filled_t = math.reshaped_tensor(filled, [grid.shape.non_spatial, grid.shape.spatial])
-    return grid.with_values(filled_t)
-
-
-def integrate(field: Field, region: Geometry, **kwargs) -> Tensor:
-    """
-    Computes *∫<sub>R</sub> f(x) dx<sup>d</sup>* , where *f* denotes the `Field`, *R* the `region` and *d* the number of spatial dimensions (`d=field.shape.spatial_rank`).
-    Depending on the `sample` implementation for `field`, the integral may be a rough approximation.
-
-    This method is currently only implemented for `CenteredGrid`.
-
-    Args:
-        field: `Field` to integrate.
-        region: Region to integrate over.
-        **kwargs: Specify numerical scheme.
-
-    Returns:
-        Integral as `phi.Tensor`
-    """
-    if not isinstance(field, CenteredGrid):
-        raise NotImplementedError()
-    return field._sample(region, **kwargs) * region.volume
-
-
-def pack_dims(field: SampledFieldType,
-              dims: Union[Shape, tuple, list, str],
-              packed_dim: Shape,
-              pos: Union[int, None] = None) -> SampledFieldType:
-    """
-    Currently only supports grids and non-spatial dimensions.
-
-    See Also:
-        `phi.math.pack_dims()`.
-
-    Args:
-        field: `SampledField`
-
-    Returns:
-        `SampledField` of same type as `field`.
-    """
-    if isinstance(field, Grid):
-        if spatial(field.shape.only(dims)):
-            raise NotImplementedError("Packing spatial dimensions not supported for grids")
-        return field.with_values(math.pack_dims(field.values, dims, packed_dim, pos))
-    else:
-        raise NotImplementedError()
-
-
-def support(field: SampledField, list_dim: Union[Shape, str] = instance('nonzero')) -> Tensor:
-    """
-    Returns the points at which the field values are non-zero.
-
-    Args:
-        field: `SampledField`
-        list_dim: Dimension to list the non-zero values.
-
-    Returns:
-        `Tensor` with shape `(list_dim, vector)`
-    """
-    return field.points[math.nonzero(field.values, list_dim=list_dim)]
-
-
-def mask(obj: Union[SampledFieldType, Geometry]) -> SampledFieldType:
-    """
-    Returns a `Field` that masks the inside (or non-zero values when `obj` is a grid) of a physical object.
-    The mask takes the value 1 inside the object and 0 outside.
-    For `CenteredGrid` and `StaggeredGrid`, the mask labels non-zero non-NaN entries as 1 and all other values as 0
-
-    Returns:
-        `Grid` type or `PointCloud`
-    """
-    if isinstance(obj, PointCloud):
-        return PointCloud(obj.elements, 1, math.extrapolation.remove_constant_offset(obj.extrapolation), bounds=obj.bounds)
-    elif isinstance(obj, Geometry):
-        return PointCloud(obj, 1, 0)
-    elif isinstance(obj, CenteredGrid):
-        values = math.cast(obj.values != 0, int)
-        return obj.with_values(values)
-    else:
-        raise ValueError(obj)
+import warnings
+from numbers import Number
+from typing import Callable, List, Tuple, Optional, Union
+
+from phi import geom
+from phi import math
+from phi.geom import Box, Geometry
+from phi.math import Tensor, spatial, instance, tensor, channel, Shape, unstack, solve_linear, jit_compile_linear, shape, Solve, extrapolation, jit_compile, rename_dims, flatten, batch
+from ._field import Field, SampledField, SampledFieldType, as_extrapolation
+from ._grid import CenteredGrid, Grid, StaggeredGrid, GridType
+from ._mesh import Mesh
+from ._point_cloud import PointCloud
+from ..math.extrapolation import Extrapolation, SYMMETRIC, REFLECT, ANTIREFLECT, ANTISYMMETRIC, combine_by_direction
+
+
+def bake_extrapolation(grid: GridType) -> GridType:
+    """
+    Pads `grid` with its current extrapolation.
+    For `StaggeredGrid`s, the resulting grid will have a consistent shape, independent of the original extrapolation.
+
+    Args:
+        grid: `CenteredGrid` or `StaggeredGrid`.
+
+    Returns:
+        Padded grid with extrapolation `phi.math.extrapolation.NONE`.
+    """
+    if grid.extrapolation == math.extrapolation.NONE:
+        return grid
+    if isinstance(grid, StaggeredGrid):
+        values = grid.values.unstack('vector')
+        padded = []
+        for dim, value in zip(grid.shape.spatial.names, values):
+            lower, upper = grid.extrapolation.valid_outer_faces(dim)
+            padded.append(math.pad(value, {dim: (0 if lower else 1, 0 if upper else 1)}, grid.extrapolation[{'vector': dim}], bounds=grid.bounds))
+        return StaggeredGrid(math.stack(padded, grid.shape['vector']), bounds=grid.bounds, extrapolation=math.extrapolation.NONE)
+    elif isinstance(grid, CenteredGrid):
+        return pad(grid, 1).with_extrapolation(math.extrapolation.NONE)
+    else:
+        raise ValueError(f"Not a valid grid: {grid}")
+
+
+def laplace(field: GridType,
+            axes=spatial,
+            order=2,
+            implicit: math.Solve = None,
+            weights: Union[Tensor, Field] = None) -> GridType:
+    """
+    Spatial Laplace operator for scalar grid.
+    If a vector grid is passed, it is assumed to be centered and the laplace is computed component-wise.
+
+    Args:
+        field: n-dimensional `CenteredGrid`
+        axes: The second derivative along these dimensions is summed over
+        weights: (Optional) Multiply the axis terms by these factors before summation.
+            Must be a `phi.math.Tensor` or `phi.field.Field` with a single channel dimension that lists all laplace axes by name.
+        order: Spatial order of accuracy.
+            Higher orders entail larger stencils and more computation time but result in more accurate results assuming a large enough resolution.
+            Supported: 2 explicit, 4 explicit, 6 implicit.
+        implicit: When a `Solve` object is passed, performs an implicit operation with the specified solver and tolerances.
+            Otherwise, an explicit stencil is used.
+
+    Returns:
+        laplacian field as `CenteredGrid`
+    """
+    if isinstance(weights, Field):
+        weights = weights.at(field).values
+    axes_names = field.shape.only(axes).names
+    extrap_map = {}
+    if not implicit:
+        if order == 2:
+                values, needed_shifts = [1, -2, 1], (-1, 0, 1)
+
+        elif order == 4:
+                values, needed_shifts = [-1/12, 4/3, -5/2, 4/3, -1/12], (-2, -1, 0, 1, 2)
+    else:
+        extrap_map_rhs = {}
+        if order == 6:
+            values, needed_shifts = [3/44, 12/11, -51/22, 12/11, 3/44], (-2, -1, 0, 1, 2)
+            extrap_map['symmetric'] = combine_by_direction(REFLECT, SYMMETRIC)
+            values_rhs, needed_shifts_rhs = [2/11, 1, 2/11], (-1, 0, 1)
+            extrap_map_rhs['symmetric'] = combine_by_direction(REFLECT, SYMMETRIC)
+    base_widths = (abs(min(needed_shifts)), max(needed_shifts))
+    field.with_extrapolation(extrapolation.map(_ex_map_f(extrap_map), field.extrapolation))
+    padded_components = [pad(field, {dim: base_widths}) for dim in axes_names]
+    shifted_components = [shift(padded_component, needed_shifts, None, pad=False, dims=dim) for padded_component, dim in zip(padded_components, axes_names)]
+    result_components = [sum([value * shift_ for value, shift_ in zip(values, shifted_component)]) / field.dx.vector[dim]**2 for shifted_component, dim in zip(shifted_components, axes_names)]
+    if implicit:
+        result_components = stack(result_components, channel('laplacian'))
+        result_components.with_values(result_components.values._cache())
+        result_components = result_components.with_extrapolation(extrapolation.map(_ex_map_f(extrap_map_rhs), field.extrapolation))
+        implicit.x0 = result_components
+        result_components = solve_linear(_lhs_for_implicit_scheme, result_components, solve=implicit, values_rhs=values_rhs, needed_shifts_rhs=needed_shifts_rhs, stack_dim=channel('laplacian'))
+        result_components = unstack(result_components, 'laplacian')
+        extrap_map = extrap_map_rhs
+    result_components = [component.with_bounds(field.bounds) for component in result_components]
+    if weights is not None:
+        assert channel(weights).rank == 1 and channel(weights).item_names is not None, f"weights must have one channel dimension listing the laplace dims but got {shape(weights)}"
+        assert set(channel(weights).item_names[0]) >= set(axes_names), f"the channel dim of weights must contain all laplace dims {axes_names} but only has {channel(weights).item_names}"
+        result_components = [c * weights[ax] for c, ax in zip(result_components, axes_names)]
+    result = sum(result_components)
+    result = result.with_extrapolation(extrapolation.map(_ex_map_f(extrap_map), field.extrapolation))
+    return result
+
+
+def spatial_gradient(field: CenteredGrid,
+                     gradient_extrapolation: Extrapolation = None,
+                     type: type = CenteredGrid,
+                     dims: math.DimFilter = spatial,
+                     stack_dim: Shape = channel('vector'),
+                     order=2,
+                     implicit: Solve = None):
+    """
+    Finite difference spatial_gradient.
+
+    This function can operate in two modes:
+
+    * `type=CenteredGrid` approximates the spatial_gradient at cell centers using central differences
+    * `type=StaggeredGrid` computes the spatial_gradient at face centers of neighbouring cells
+
+    Args:
+        field: centered grid of any number of dimensions (scalar field, vector field, tensor field)
+        gradient_extrapolation: Extrapolation of the output
+        type: either `CenteredGrid` or `StaggeredGrid`
+        dims: Along which dimensions to compute the spatial gradient. Only supported when `type==CenteredGrid`.
+        stack_dim: Dimension to be added. This dimension lists the spatial_gradient w.r.t. the spatial dimensions.
+            The `field` must not have a dimension of the same name.
+        order: Spatial order of accuracy.
+            Higher orders entail larger stencils and more computation time but result in more accurate results assuming a large enough resolution.
+            Supported: 2 explicit, 4 explicit, 6 implicit.
+        implicit: When a `Solve` object is passed, performs an implicit operation with the specified solver and tolerances.
+            Otherwise, an explicit stencil is used.
+
+    Returns:
+        spatial_gradient field of type `type`.
+    """
+    if gradient_extrapolation is None:
+        gradient_extrapolation = field.extrapolation.spatial_gradient()
+    extrap_map = {}
+    if not implicit:
+        if order == 2:
+            if type == CenteredGrid:
+                values, needed_shifts = [-1/2, 1/2], (-1, 1)
+            else:
+                values, needed_shifts = [-1, 1], (0, 1)
+        elif order == 4:
+            if type == CenteredGrid:
+                values, needed_shifts = [1/12, -2/3, 2/3, -1/12], (-2, -1, 1, 2)
+            else:
+                values, needed_shifts = [1/24, -27/24, 27/24, -1/24], (-1, 0, 1, 2)
+        else:
+            raise NotImplementedError(f"explicit {order}th-order not supported")
+    else:
+        extrap_map_rhs = {}
+        if order == 6:
+            if type == CenteredGrid:
+                values, needed_shifts = [-1/36, -14/18, 14/18, 1/36], (-2, -1, 1, 2)
+                values_rhs, needed_shifts_rhs = [1/3, 1, 1/3], (-1, 0, 1)
+            else:
+                values, needed_shifts = [-17/186, -63/62, 63/62, 17/186], (-1, 0, 1, 2)
+                extrap_map['symmetric'] = combine_by_direction(REFLECT, SYMMETRIC)
+                values_rhs, needed_shifts_rhs = [9/62, 1, 9/62], (-1, 0, 1)
+                extrap_map_rhs['symmetric'] = combine_by_direction(ANTIREFLECT, ANTISYMMETRIC)
+        else:
+            raise NotImplementedError(f"implicit {order}th-order not supported")
+    base_widths = (abs(min(needed_shifts)), max(needed_shifts))
+    field.with_extrapolation(extrapolation.map(_ex_map_f(extrap_map), field.extrapolation))  # ToDo does this line do anything?
+    if implicit:
+        gradient_extrapolation = extrapolation.map(_ex_map_f(extrap_map_rhs), gradient_extrapolation)
+    spatial_dims = field.shape.only(dims).names
+    stack_dim = stack_dim.with_size(spatial_dims)
+    if type == CenteredGrid:
+        # ToDo if extrapolation == math.extrapolation.NONE, extend size by 1
+        # pad = 1 if extrapolation == math.extrapolation.NONE else 0
+        # bounds = Box(field.bounds.lower - field.dx, field.bounds.upper + field.dx) if extrapolation == math.extrapolation.NONE else field.bounds
+        std_widths = (0, 0)
+        if gradient_extrapolation == math.extrapolation.NONE:
+            base_widths = (abs(min(needed_shifts))+1, max(needed_shifts)+1)
+            std_widths = (1, 1)
+        padded_components = [pad(field, {dim_: base_widths if dim_ == dim else std_widths for dim_ in spatial_dims}) for dim in spatial_dims]
+    elif type == StaggeredGrid:
+        assert spatial_dims == field.shape.spatial.names, f"spatial_gradient with type=StaggeredGrid requires dims=spatial, i.e. dims='{','.join(field.shape.spatial.names)}'"
+        base_widths = (base_widths[0], base_widths[1]-1)
+        padded_components = pad_for_staggered_output(field, gradient_extrapolation, field.shape.spatial.names, base_widths)
+    else:
+        raise ValueError(type)
+    shifted_components = [shift(padded_component, needed_shifts, stack_dim=None, pad=False, dims=dim) for padded_component, dim in zip(padded_components, spatial_dims)]
+    result_components = [sum([value * shift_ for value, shift_ in zip(values, shifted_component)]) / field.dx.vector[dim] for shifted_component, dim in zip(shifted_components, field.shape.spatial.names)]
+    if type == CenteredGrid:
+        result = stack(result_components, stack_dim)
+    else:
+        assert stack_dim.name == 'vector', f"spatial_gradient with type=StaggeredGrid requires stack_dim.name == 'vector' but got '{stack_dim.name}'"
+        result = StaggeredGrid(math.stack([component.values for component in result_components], channel(vector=spatial_dims)), bounds=field.bounds, extrapolation=gradient_extrapolation)
+    result = result.with_extrapolation(gradient_extrapolation)
+    if implicit:
+        implicit.x0 = result
+        result = solve_linear(_lhs_for_implicit_scheme, result, solve=implicit, values_rhs=values_rhs, needed_shifts_rhs=needed_shifts_rhs, stack_dim=stack_dim, staggered_output=type != CenteredGrid)
+    if type == CenteredGrid and gradient_extrapolation == math.extrapolation.NONE:
+        result = result.with_bounds(Box(field.bounds.lower - field.dx, field.bounds.upper + field.dx))
+    else:
+        result = result.with_bounds(field.bounds)
+    return result
+
+
+def _ex_map_f(ext_dict: dict):
+    def f(ext: Extrapolation):
+        return ext_dict[ext.__repr__()] if ext.__repr__() in ext_dict else ext
+    return f
+
+
+@jit_compile_linear(auxiliary_args="values_rhs, needed_shifts_rhs, stack_dim, staggered_output")
+def _lhs_for_implicit_scheme(x, values_rhs, needed_shifts_rhs, stack_dim, staggered_output=False):
+    result = []
+    for dim, component in zip(x.shape.only(math.spatial).names, unstack(x, stack_dim.name)):
+        shifted = shift(component, needed_shifts_rhs, stack_dim=None, dims=dim)
+        result.append(sum([value * shift_ for value, shift_ in zip(values_rhs, shifted)]))
+
+    if staggered_output:
+        result = x.with_values(math.stack([component.values for component in result], channel('vector')))
+    else:
+        result = stack(result, stack_dim)
+
+    return result
+
+
+def pad_for_staggered_output(field: CenteredGrid, output_extrapolation: Extrapolation, dims: tuple, base_widths: tuple):
+    padded_components = []
+    for dim in dims:
+        border_valid = output_extrapolation.valid_outer_faces(dim)
+        padding_widths = (border_valid[0] + base_widths[0], border_valid[1] + base_widths[1])
+        padded_components.append(pad(field, {dim: padding_widths}))
+
+    return padded_components
+
+
+def shift(grid: CenteredGrid, offsets: tuple, stack_dim: Optional[Shape] = channel('shift'), dims=spatial, pad=True):
+    """
+    Wraps :func:`math.shift` for CenteredGrid.
+
+    Args:
+      grid: CenteredGrid: 
+      offsets: tuple: 
+      stack_dim:  (Default value = 'shift')
+    """
+    if pad:
+        padding = grid.extrapolation
+        new_bounds = grid.bounds
+    else:
+        padding = None
+        max_lower_shift = min(offsets) if min(offsets) < 0 else 0
+        max_upper_shift = max(offsets) if max(offsets) > 0 else 0
+        w_lower = math.wrap([max_lower_shift if dim in dims else 0 for dim in grid.shape.spatial.names])
+        w_upper = math.wrap([max_upper_shift if dim in dims else 0 for dim in grid.shape.spatial.names])
+        new_bounds = Box(grid.box.lower - w_lower * grid.dx, grid.box.upper - w_upper * grid.dx)
+    data = math.shift(grid.values, offsets, dims=dims, padding=padding, stack_dim=stack_dim)
+    return [type(grid)(data[i], bounds=new_bounds, extrapolation=grid.extrapolation) for i in range(len(offsets))]
+
+
+def stagger(field: CenteredGrid,
+            face_function: Callable,
+            extrapolation: Union[float, math.extrapolation.Extrapolation],
+            type: type = StaggeredGrid):
+    """
+    Creates a new grid by evaluating `face_function` given two neighbouring cells.
+    One layer of missing cells is inferred from the extrapolation.
+    
+    This method returns a Field of type `type` which must be either StaggeredGrid or CenteredGrid.
+    When returning a StaggeredGrid, the new values are sampled at the faces of neighbouring cells.
+    When returning a CenteredGrid, the new grid has the same resolution as `field`.
+
+    Args:
+      field: centered grid
+      face_function: function mapping (value1: Tensor, value2: Tensor) -> center_value: Tensor
+      extrapolation: extrapolation mode of the returned grid. Has no effect on the values.
+      type: one of (StaggeredGrid, CenteredGrid)
+      field: CenteredGrid: 
+      face_function: Callable:
+      extrapolation: math.extrapolation.Extrapolation: 
+      type: type:  (Default value = StaggeredGrid)
+
+    Returns:
+      grid of type matching the `type` argument
+
+    """
+    extrapolation = as_extrapolation(extrapolation)
+    all_lower = []
+    all_upper = []
+    if type == StaggeredGrid:
+        for dim in field.resolution.names:
+            valid_lo, valid_up = extrapolation.valid_outer_faces(dim)
+            if valid_lo and valid_up:
+                width_lower, width_upper = {dim: (1, 0)}, {dim: (0, 1)}
+            elif valid_lo and not valid_up:
+                width_lower, width_upper = {dim: (1, -1)}, {dim: (0, 0)}
+            elif not valid_lo and valid_up:
+                width_lower, width_upper = {dim: (0, 0)}, {dim: (-1, 1)}
+            else:
+                width_lower, width_upper = {dim: (0, -1)}, {dim: (-1, 0)}
+            all_lower.append(math.pad(field.values, width_lower, field.extrapolation, bounds=field.bounds))
+            all_upper.append(math.pad(field.values, width_upper, field.extrapolation, bounds=field.bounds))
+        all_upper = math.stack(all_upper, channel('vector'))
+        all_lower = math.stack(all_lower, channel('vector'))
+        values = face_function(all_lower, all_upper)
+        result = StaggeredGrid(values, bounds=field.bounds, extrapolation=extrapolation)
+        assert result.shape.spatial == field.shape.spatial
+        return result
+    elif type == CenteredGrid:
+        left, right = math.shift(field.values, (-1, 1), padding=field.extrapolation, stack_dim=channel('vector'))
+        values = face_function(left, right)
+        return CenteredGrid(values, bounds=field.bounds, extrapolation=extrapolation)
+    else:
+        raise ValueError(type)
+
+
+def divergence(field: Grid, order=2, implicit: Solve = None) -> CenteredGrid:
+    """
+    Computes the divergence of a grid using finite differences.
+
+    This function can operate in two modes depending on the type of `field`:
+
+    * `CenteredGrid` approximates the divergence at cell centers using central differences
+    * `StaggeredGrid` exactly computes the divergence at cell centers
+
+    Args:
+        field: vector field as `CenteredGrid` or `StaggeredGrid`
+        order: Spatial order of accuracy.
+            Higher orders entail larger stencils and more computation time but result in more accurate results assuming a large enough resolution.
+            Supported: 2 explicit, 4 explicit, 6 implicit.
+        implicit: When a `Solve` object is passed, performs an implicit operation with the specified solver and tolerances.
+            Otherwise, an explicit stencil is used.
+
+    Returns:
+        Divergence field as `CenteredGrid`
+    """
+
+    extrap_map = {}
+    if not implicit:
+        if order == 2:
+            if isinstance(field, CenteredGrid):
+                values, needed_shifts = [-1 / 2, 1 / 2], (-1, 1)
+            else:
+                values, needed_shifts = [-1, 1], (0, 1)
+
+        elif order == 4:
+            if isinstance(field, CenteredGrid):
+                values, needed_shifts = [1 / 12, -2 / 3, 2 / 3, -1 / 12], (-2, -1, 1, 2)
+            else:
+                values, needed_shifts = [1 / 24, -27 / 24, 27 / 24, -1 / 24], (-1, 0, 1, 2)
+    else:
+        extrap_map_rhs = {}
+        if order == 6:
+            extrap_map['symmetric'] = combine_by_direction(REFLECT, SYMMETRIC)
+            extrap_map_rhs['symmetric'] = combine_by_direction(ANTIREFLECT, ANTISYMMETRIC)
+
+            if isinstance(field, CenteredGrid):
+                values, needed_shifts = [-1 / 36, -14 / 18, 14 / 18, 1 / 36], (-2, -1, 1, 2)
+                values_rhs, needed_shifts_rhs = [1 / 3, 1, 1 / 3], (-1, 0, 1)
+
+            else:
+                values, needed_shifts = [-17 / 186, -63 / 62, 63 / 62, 17 / 186], (-1, 0, 1, 2)
+                values_rhs, needed_shifts_rhs = [9 / 62, 1, 9 / 62], (-1, 0, 1)
+    base_widths = (abs(min(needed_shifts)), max(needed_shifts))
+    field.with_extrapolation(extrapolation.map(_ex_map_f(extrap_map), field.extrapolation))  # ToDo does this line do anything?
+    spatial_dims = field.shape.spatial.names
+    if isinstance(field, StaggeredGrid):
+        base_widths = (base_widths[0]+1, base_widths[1])
+        padded_components = []
+        for dim, component in zip(field.shape.spatial.names, unstack(field, 'vector')):
+            border_valid = field.extrapolation.valid_outer_faces(dim)
+            padding_widths = (base_widths[0] - border_valid[0], base_widths[1] - border_valid[1])
+            padded_components.append(pad(component, {dim: padding_widths}))
+    elif isinstance(field, CenteredGrid):
+        padded_components = [pad(component, {dim: base_widths}) for dim, component in zip(spatial_dims, unstack(field, 'vector'))]
+        if field.extrapolation == math.extrapolation.NONE:
+            padded_components = [pad(component, {dim_: (0, 0) if dim_ == dim else (-1, -1) for dim_ in spatial_dims}) for dim, component in zip(spatial_dims, padded_components)]
+    shifted_components = [shift(padded_component, needed_shifts, None, pad=False, dims=dim) for padded_component, dim in zip(padded_components, spatial_dims)]
+    result_components = [sum([value * shift for value, shift in zip(values, shifted_component)]) / field.dx.vector[dim] for shifted_component, dim in zip(shifted_components, spatial_dims)]
+    if implicit:
+        result_components = stack(result_components, channel('vector'))
+        result_components.with_values(result_components.values._cache())
+        implicit.x0 = field
+        result_components = solve_linear(_lhs_for_implicit_scheme, result_components, solve=implicit, values_rhs=values_rhs, needed_shifts_rhs=needed_shifts_rhs, stack_dim=channel('vector'))
+        result_components = unstack(result_components, 'vector')
+    result_components = [component.with_bounds(field.bounds) for component in result_components]
+    result = sum(result_components)
+    if field.extrapolation == math.extrapolation.NONE and isinstance(field, CenteredGrid):
+        result = result.with_bounds(Box(field.bounds.lower + field.dx, field.bounds.upper - field.dx))
+    return result
+
+
+def curl(field: Grid, type: type = CenteredGrid):
+    """ Computes the finite-difference curl of the give 2D `StaggeredGrid`. """
+    assert field.spatial_rank in (2, 3), "curl is only defined in 2 and 3 spatial dimensions."
+    if isinstance(field, CenteredGrid) and field.spatial_rank == 2:
+        if 'vector' not in field.shape and type == StaggeredGrid:
+            # 2D curl of scalar field
+            grad = math.spatial_gradient(field.values, dx=field.dx, difference='forward', padding=None, stack_dim=channel('vector'))
+            result = grad.vector.flip() * (1, -1)  # (d/dy, -d/dx)
+            bounds = Box(field.bounds.lower + 0.5 * field.dx, field.bounds.upper - 0.5 * field.dx)  # lose 1 cell per dimension
+            return StaggeredGrid(result, bounds=bounds, extrapolation=field.extrapolation.spatial_gradient())
+        if 'vector' in field.shape and type == CenteredGrid:
+            # 2D curl of vector field
+            x, y = field.shape.spatial.names
+            vy_dx = math.spatial_gradient(field.values.vector[1], dx=field.dx.vector[0], padding=field.extrapolation, dims=x, stack_dim=None)
+            vx_dy = math.spatial_gradient(field.values.vector[0], dx=field.dx.vector[1], padding=field.extrapolation, dims=y, stack_dim=None)
+            c = vy_dx - vx_dy
+            return field.with_values(c)
+    elif isinstance(field, StaggeredGrid) and field.spatial_rank == 2:
+        if type == CenteredGrid:
+            values = bake_extrapolation(field).values
+            x_padded = math.pad(values.vector['x'], {'y': (1, 1)}, field.extrapolation)
+            y_padded = math.pad(values.vector['y'], {'x': (1, 1)}, field.extrapolation)
+            vx_dy = math.spatial_gradient(x_padded, field.dx, 'forward', None, dims='y', stack_dim=None)
+            vy_dx = math.spatial_gradient(y_padded, field.dx, 'forward', None, dims='x', stack_dim=None)
+            result = vy_dx - vx_dy
+            return CenteredGrid(result, field.extrapolation.spatial_gradient(), bounds=field.bounds)
+    raise NotImplementedError()
+
+
+def fourier_laplace(grid: GridType, times=1) -> GridType:
+    """ See `phi.math.fourier_laplace()` """
+    assert grid.extrapolation.spatial_gradient() == math.extrapolation.PERIODIC
+    values = math.fourier_laplace(grid.values, dx=grid.dx, times=times)
+    return type(grid)(values=values, bounds=grid.bounds, extrapolation=grid.extrapolation)
+
+
+def fourier_poisson(grid: GridType, times=1) -> GridType:
+    """ See `phi.math.fourier_poisson()` """
+    assert grid.extrapolation.spatial_gradient() == math.extrapolation.PERIODIC
+    values = math.fourier_poisson(grid.values, dx=grid.dx, times=times)
+    return type(grid)(values=values, bounds=grid.bounds, extrapolation=grid.extrapolation)
+
+
+def native_call(f, *inputs, channels_last=None, channel_dim='vector', extrapolation=None) -> Union[SampledField, Tensor]:
+    """
+    Similar to `phi.math.native_call()`.
+
+    Args:
+        f: Function to be called on native tensors of `inputs.values`.
+            The function output must have the same dimension layout as the inputs and the batch size must be identical.
+        *inputs: `SampledField` or `phi.Tensor` instances.
+        extrapolation: (Optional) Extrapolation of the output field. If `None`, uses the extrapolation of the first input field.
+
+    Returns:
+        `SampledField` matching the first `SampledField` in `inputs`.
+    """
+    input_tensors = [i.values if isinstance(i, SampledField) else tensor(i) for i in inputs]
+    values = math.native_call(f, *input_tensors, channels_last=channels_last, channel_dim=channel_dim)
+    for i in inputs:
+        if isinstance(i, SampledField):
+            result = i.with_values(values=values)
+            if extrapolation is not None:
+                result = result.with_extrapolation(extrapolation)
+            return result
+    else:
+        raise AssertionError("At least one input must be a SampledField.")
+
+
+def data_bounds(loc: Union[SampledField, Tensor]) -> Box:
+    if isinstance(loc, SampledField):
+        loc = loc.points
+    assert isinstance(loc, Tensor), f"loc must be a Tensor or SampledField but got {type(loc)}"
+    min_vec = math.min(loc, dim=loc.shape.non_batch.non_channel)
+    max_vec = math.max(loc, dim=loc.shape.non_batch.non_channel)
+    return Box(min_vec, max_vec)
+
+
+def mean(field: SampledField) -> Tensor:
+    """
+    Computes the mean value by reducing all spatial / instance dimensions.
+
+    Args:
+        field: `SampledField`
+
+    Returns:
+        `phi.Tensor`
+    """
+    return math.mean(field.values, field.shape.non_channel.non_batch)
+
+
+def normalize(field: SampledField, norm: SampledField, epsilon=1e-5):
+    """ Multiplies the values of `field` so that its sum matches the source. """
+    data = math.normalize_to(field.values, norm.values, epsilon)
+    return field.with_values(data)
+
+
+def center_of_mass(density: SampledField):
+    """
+    Compute the center of mass of a density field.
+
+    Args:
+        density: Scalar `SampledField`
+
+    Returns:
+        `Tensor` holding only batch dimensions.
+    """
+    assert 'vector' not in density.shape
+    return mean(density.points * density) / mean(density)
+
+
+def pad(grid: GridType, widths: Union[int, tuple, list, dict]) -> GridType:
+    """
+    Pads a `Grid` using its extrapolation.
+
+    Unlike `phi.math.pad()`, this function also affects the `bounds` of the grid, changing its size and origin depending on `widths`.
+
+    Args:
+        grid: `CenteredGrid` or `StaggeredGrid`
+        widths: Either `int` or `(lower, upper)` to pad the same number of cells in all spatial dimensions
+            or `dict` mapping dimension names to `(lower, upper)`.
+
+    Returns:
+        `Grid` of the same type as `grid`
+    """
+    if isinstance(widths, int):
+        widths = {axis: (widths, widths) for axis in grid.shape.spatial.names}
+    elif isinstance(widths, (tuple, list)):
+        widths = {axis: (width if isinstance(width, (tuple, list)) else (width, width)) for axis, width in zip(grid.shape.spatial.names, widths)}
+    else:
+        assert isinstance(widths, dict)
+    widths_list = [widths[axis] if axis in widths.keys() else (0, 0) for axis in grid.shape.spatial.names]
+    if isinstance(grid, Grid):
+        data = math.pad(grid.values, widths, grid.extrapolation, bounds=grid.bounds)
+        w_lower = math.wrap([w[0] for w in widths_list])
+        w_upper = math.wrap([w[1] for w in widths_list])
+        bounds = Box(grid.box.lower - w_lower * grid.dx, grid.box.upper + w_upper * grid.dx)
+        return type(grid)(values=data, bounds=bounds, extrapolation=grid.extrapolation)
+    raise NotImplementedError(f"{type(grid)} not supported. Only Grid instances allowed.")
+
+
+def downsample2x(grid: Grid) -> GridType:
+    """
+    Reduces the number of sample points by a factor of 2 in each spatial dimension.
+    The new values are determined via linear interpolation.
+
+    See Also:
+        `upsample2x()`.
+
+    Args:
+        grid: `CenteredGrid` or `StaggeredGrid`.
+
+    Returns:
+        `Grid` of same type as `grid`.
+    """
+    if isinstance(grid, CenteredGrid):
+        values = math.downsample2x(grid.values, grid.extrapolation)
+        return CenteredGrid(values, bounds=grid.bounds, extrapolation=grid.extrapolation)
+    elif isinstance(grid, StaggeredGrid):
+        values = []
+        for dim, centered_grid in zip(grid.shape.spatial.names, unstack(grid, 'vector')):
+            odd_discarded = centered_grid.values[{dim: slice(None, None, 2)}]
+            others_interpolated = math.downsample2x(odd_discarded, grid.extrapolation, dims=grid.shape.spatial.without(dim))
+            values.append(others_interpolated)
+        return StaggeredGrid(math.stack(values, channel('vector')), bounds=grid.bounds, extrapolation=grid.extrapolation)
+    else:
+        raise ValueError(type(grid))
+
+
+def upsample2x(grid: GridType) -> GridType:
+    """
+    Increases the number of sample points by a factor of 2 in each spatial dimension.
+    The new values are determined via linear interpolation.
+
+    See Also:
+        `downsample2x()`.
+
+    Args:
+        grid: `CenteredGrid` or `StaggeredGrid`.
+
+    Returns:
+        `Grid` of same type as `grid`.
+    """
+    if isinstance(grid, CenteredGrid):
+        values = math.upsample2x(grid.values, grid.extrapolation)
+        return CenteredGrid(values, bounds=grid.bounds, extrapolation=grid.extrapolation)
+    elif isinstance(grid, StaggeredGrid):
+        raise NotImplementedError()
+    else:
+        raise ValueError(type(grid))
+
+
+def concat(fields: Union[List[SampledFieldType], Tuple[SampledFieldType, ...]], dim: Union[str, Shape]) -> SampledFieldType:
+    """
+    Concatenates the given `SampledField`s along `dim`.
+
+    See Also:
+        `stack()`.
+
+    Args:
+        fields: List of matching `SampledField` instances.
+        dim: Concatenation dimension as `Shape`. Size is ignored.
+
+    Returns:
+        `SampledField` matching concatenated fields.
+    """
+    assert all(isinstance(f, SampledField) for f in fields)
+    assert all(isinstance(f, type(fields[0])) for f in fields)
+    if any(f.extrapolation != fields[0].extrapolation for f in fields):
+        raise NotImplementedError("Concatenating extrapolations not supported")
+    if isinstance(fields[0], Grid):
+        values = math.concat([f.values for f in fields], dim)
+        return fields[0].with_values(values)
+    elif isinstance(fields[0], PointCloud):
+        elements = geom.concat([f.elements for f in fields], dim)
+        values = math.concat([math.expand(f.values, f.shape.only(dim)) for f in fields], dim)
+        return PointCloud(elements=elements, values=values, extrapolation=fields[0].extrapolation, add_overlapping=fields[0]._add_overlapping, bounds=fields[0]._bounds)
+    raise NotImplementedError(type(fields[0]))
+
+
+def stack(fields, dim: Shape, dim_bounds: Box = None):
+    """
+    Stacks the given `SampledField`s along `dim`.
+
+    See Also:
+        `concat()`.
+
+    Args:
+        fields: List of matching `SampledField` instances.
+        dim: Stack dimension as `Shape`. Size is ignored.
+        dim_bounds: `Box` defining the physical size for `dim`.
+
+    Returns:
+        `SampledField` matching stacked fields.
+    """
+    assert all(isinstance(f, SampledField) for f in fields), f"All fields must be SampledFields of the same type but got {fields}"
+    assert all(isinstance(f, type(fields[0])) for f in fields), f"All fields must be SampledFields of the same type but got {fields}"
+    if any(f.extrapolation != fields[0].extrapolation for f in fields):
+        raise NotImplementedError("Concatenating extrapolations not supported")
+    if isinstance(fields[0], Grid):
+        values = math.stack([f.values for f in fields], dim)
+        if spatial(dim):
+            if dim_bounds is None:
+                dim_bounds = Box(**{dim.name: len(fields)})
+            return type(fields[0])(values, extrapolation=fields[0].extrapolation, bounds=fields[0].bounds * dim_bounds)
+        else:
+            return fields[0].with_values(values)
+    elif isinstance(fields[0], PointCloud):
+        elements = geom.stack([f.elements for f in fields], dim)
+        values = math.stack([f.values for f in fields], dim)
+        return PointCloud(elements=elements, values=values, extrapolation=fields[0].extrapolation, add_overlapping=fields[0]._add_overlapping, bounds=fields[0]._bounds)
+    raise NotImplementedError(type(fields[0]))
+
+
+def assert_close(*fields: Union[SampledField, Tensor, Number],
+                 rel_tolerance: float = 1e-5,
+                 abs_tolerance: float = 0,
+                 msg: str = "",
+                 verbose: bool = True):
+    """ Raises an AssertionError if the `values` of the given fields are not close. See `phi.math.assert_close()`. """
+    f0 = next(filter(lambda t: isinstance(t, SampledField), fields))
+    values = [(f @ f0).values if isinstance(f, SampledField) else math.wrap(f) for f in fields]
+    math.assert_close(*values, rel_tolerance=rel_tolerance, abs_tolerance=abs_tolerance, msg=msg, verbose=verbose)
+
+
+def where(mask: Union[Field, Geometry, float], field_true: Union[Field, float], field_false: Union[Field, float]) -> SampledFieldType:
+    """
+    Element-wise where operation.
+    Picks the value of `field_true` where `mask=1 / True` and the value of `field_false` where `mask=0 / False`.
+
+    The fields are automatically resampled if necessary, preferring the sample points of `mask`.
+    At least one of the arguments must be a `SampledField`.
+
+    Args:
+        mask: `Field` or `Geometry` object.
+        field_true: `Field`
+        field_false: `Field`
+
+    Returns:
+        `SampledField`
+    """
+    field_true, field_false, mask = _auto_resample(field_true, field_false, mask)
+    values = math.where(mask.values, field_true.values, field_false.values)
+    return field_true.with_values(values)
+
+
+def maximum(f1: Union[Field, Geometry, float], f2: Union[Field, Geometry, float]):
+    """
+    Element-wise maximum.
+    One of the given fields needs to be an instance of `SampledField` and the the result will be sampled at the corresponding points.
+    If both are `SampledFields` but have different points, `f1` takes priority.
+
+    Args:
+        f1: `Field` or `Geometry` or constant.
+        f2: `Field` or `Geometry` or constant.
+
+    Returns:
+        `SampledField`
+    """
+    f1, f2 = _auto_resample(f1, f2)
+    return f1.with_values(math.maximum(f1.values, f2.values))
+
+
+def minimum(f1: Union[Field, Geometry, float], f2: Union[Field, Geometry, float]):
+    """
+    Element-wise minimum.
+    One of the given fields needs to be an instance of `SampledField` and the the result will be sampled at the corresponding points.
+    If both are `SampledFields` but have different points, `f1` takes priority.
+
+    Args:
+        f1: `Field` or `Geometry` or constant.
+        f2: `Field` or `Geometry` or constant.
+
+    Returns:
+        `SampledField`
+    """
+    f1, f2 = _auto_resample(f1, f2)
+    return f1.with_values(math.minimum(f1.values, f2.values))
+
+
+def _auto_resample(*fields: Field):
+    """ Prefers extrapolation from first SampledField """
+    for sampled_field in fields:
+        if isinstance(sampled_field, SampledField):
+            return [f @ sampled_field for f in fields]
+    raise AssertionError(f"At least one argument must be a SampledField but got {fields}")
+
+
+def vec_length(field: SampledField):
+    """ See `phi.math.vec_abs()` """
+    assert isinstance(field, SampledField), f"SampledField required but got {type(field).__name__}"
+    if isinstance(field, StaggeredGrid):
+        field = field.at_centers()
+    return field.with_values(math.vec_abs(field.values))
+
+
+def vec_squared(field: SampledField):
+    """ See `phi.math.vec_squared()` """
+    if isinstance(field, StaggeredGrid):
+        field = field.at_centers()
+    return field.with_values(math.vec_squared(field.values))
+
+
+def finite_fill(grid: GridType, distance=1, diagonal=True) -> GridType:
+    """
+    Extrapolates values of `grid` which are marked by nonzero values in `valid` using `phi.math.masked_fill().
+    If `values` is a StaggeredGrid, its components get extrapolated independently.
+
+    Args:
+        grid: Grid holding the values for extrapolation and possible non-finite values to be filled.
+        distance: Number of extrapolation steps, i.e. how far a cell can be from the closest finite value to get filled.
+        diagonal: Whether to extrapolate values to their diagonal neighbors per step.
+
+    Returns:
+        grid: Grid with extrapolated values.
+        valid: binary Grid marking all valid values after extrapolation.
+    """
+    if isinstance(grid, CenteredGrid):
+        new_values = math.finite_fill(grid.values, distance=distance, diagonal=diagonal, padding=grid.extrapolation)
+        return grid.with_values(new_values)
+    elif isinstance(grid, StaggeredGrid):
+        new_values = [finite_fill(c, distance=distance, diagonal=diagonal).values for c in grid.vector]
+        return grid.with_values(math.stack(new_values, channel(grid)))
+    else:
+        raise ValueError(grid)
+
+
+def discretize(grid: Grid, filled_fraction=0.25):
+    """ Treats channel dimensions as batch dimensions. """
+    import numpy as np
+    data = math.reshaped_native(grid.values, [grid.shape.non_spatial, grid.shape.spatial])
+    ranked_idx = np.argsort(data, axis=-1)
+    filled_idx = ranked_idx[:, int(round(grid.shape.spatial.volume * (1 - filled_fraction))):]
+    filled = np.zeros_like(data)
+    np.put_along_axis(filled, filled_idx, 1, axis=-1)
+    filled_t = math.reshaped_tensor(filled, [grid.shape.non_spatial, grid.shape.spatial])
+    return grid.with_values(filled_t)
+
+
+def integrate(field: Field, region: Geometry, **kwargs) -> Tensor:
+    """
+    Computes *∫<sub>R</sub> f(x) dx<sup>d</sup>* , where *f* denotes the `Field`, *R* the `region` and *d* the number of spatial dimensions (`d=field.shape.spatial_rank`).
+    Depending on the `sample` implementation for `field`, the integral may be a rough approximation.
+
+    This method is currently only implemented for `CenteredGrid`.
+
+    Args:
+        field: `Field` to integrate.
+        region: Region to integrate over.
+        **kwargs: Specify numerical scheme.
+
+    Returns:
+        Integral as `phi.Tensor`
+    """
+    if not isinstance(field, CenteredGrid):
+        raise NotImplementedError()
+    return field._sample(region, **kwargs) * region.volume
+
+
+def pack_dims(field: SampledFieldType,
+              dims: Union[Shape, tuple, list, str],
+              packed_dim: Shape,
+              pos: Union[int, None] = None) -> SampledFieldType:
+    """
+    Currently only supports grids and non-spatial dimensions.
+
+    See Also:
+        `phi.math.pack_dims()`.
+
+    Args:
+        field: `SampledField`
+
+    Returns:
+        `SampledField` of same type as `field`.
+    """
+    if isinstance(field, Grid):
+        if spatial(field.shape.only(dims)):
+            raise NotImplementedError("Packing spatial dimensions not supported for grids")
+        return field.with_values(math.pack_dims(field.values, dims, packed_dim, pos))
+    else:
+        raise NotImplementedError()
+
+
+def support(field: SampledField, list_dim: Union[Shape, str] = instance('nonzero')) -> Tensor:
+    """
+    Returns the points at which the field values are non-zero.
+
+    Args:
+        field: `SampledField`
+        list_dim: Dimension to list the non-zero values.
+
+    Returns:
+        `Tensor` with shape `(list_dim, vector)`
+    """
+    return field.points[math.nonzero(field.values, list_dim=list_dim)]
+
+
+def mask(obj: Union[SampledFieldType, Geometry]) -> SampledFieldType:
+    """
+    Returns a `Field` that masks the inside (or non-zero values when `obj` is a grid) of a physical object.
+    The mask takes the value 1 inside the object and 0 outside.
+    For `CenteredGrid` and `StaggeredGrid`, the mask labels non-zero non-NaN entries as 1 and all other values as 0
+
+    Returns:
+        `Grid` type or `PointCloud`
+    """
+    if isinstance(obj, PointCloud):
+        return PointCloud(obj.elements, 1, math.extrapolation.remove_constant_offset(obj.extrapolation), bounds=obj.bounds)
+    elif isinstance(obj, Geometry):
+        return PointCloud(obj, 1, 0)
+    elif isinstance(obj, CenteredGrid):
+        values = math.cast(obj.values != 0, int)
+        return obj.with_values(values)
+    else:
+        raise ValueError(obj)
+
+
+def connect(obj: SampledField, connections: Tensor) -> Mesh:
+    """
+    Build a `Mesh` by connecting elements from a field.
+
+    See Also:
+        `connect_neighbors()`.
+
+    Args:
+        obj: `PointCloud` or `Mesh`.
+        connections: Connectivity matrix. Any non-zero entry represents a connection.
+
+    Returns:
+        `Mesh`
+    """
+    if isinstance(obj, (PointCloud, Mesh)):
+        return Mesh(obj.elements, connections, obj.values, extrapolation=obj.extrapolation, bounds=obj.bounds)
+    else:
+        raise ValueError(f"connect requires a PointCloud or Mesh but got {type(obj)}")
+
+
+def connect_neighbors(obj: SampledField, max_distance: float or Tensor, format: str = 'dense') -> Mesh:
+    """
+    Build  a `Mesh` by connecting proximate elements of a `SampledField`.
+
+    See Also:
+        `connect()`.
+
+    Args:
+        obj: `PointCloud`, `Mesh`, `CenteredGrid` or `StaggeredGrid`.
+        max_distance: Connectivity threshold distance. Elements further apart than this will not be connected.
+        format: Connectivity matrix format, `'dense'`, `'coo'` or `'csr'`.
+
+    Returns:
+        `Mesh`.
+    """
+    if isinstance(obj, CenteredGrid):
+        elements = flatten(obj.elements, instance('elements'))
+        values = math.pack_dims(obj.values, spatial, instance('elements'))
+        obj = PointCloud(elements, values, obj.extrapolation, bounds=obj.bounds)
+    elif isinstance(obj, StaggeredGrid):
+        elements = flatten(obj.elements, instance('elements'), flatten_batch=True)
+        values = math.pack_dims(obj.values, spatial(obj.values).names + ('vector',), instance('elements'))
+        obj = PointCloud(elements, values, obj.extrapolation, bounds=obj.bounds)
+    assert isinstance(obj, (PointCloud, Mesh)), f"obj must be a PointCloud, Mesh or Grid but got {type(obj)}"
+    points = math.rename_dims(obj.elements, spatial, instance).center
+    dx = math.pairwise_distances(points, max_distance=max_distance, format=format)
+    con = math.vec_length(dx) > 0
+    return connect(obj, con)
```

### Comparing `phiflow-2.3.4/phi/field/_grid.py` & `phiflow-2.4.0/phi/field/_grid.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,612 +1,615 @@
-from typing import TypeVar, Any, Tuple, List, Union
-
-from phi.math import Solve
-
-from phi import math, geom
-from phi.geom import Box, Geometry, GridCell
-from ._embed import FieldEmbedding
-from ._field import SampledField, Field, sample, reduce_sample, as_extrapolation
-from ..geom._stack import GeometryStack
-from ..math import Shape, NUMPY
-from ..math._shape import spatial, channel, parse_dim_order
-from ..math._tensors import TensorStack, Tensor
-from ..math.extrapolation import Extrapolation
-from ..math.magic import slicing_dict
-
-
-class Grid(SampledField):
-    """
-    Base class for `CenteredGrid` and `StaggeredGrid`.
-    """
-
-    def __init__(self, elements: Geometry, values: Tensor, extrapolation: Union[float, Extrapolation], resolution: Union[Shape, int], bounds: Union[Box, float]):
-        assert isinstance(bounds, Box)
-        assert isinstance(resolution, Shape)
-        if bounds.size.vector.item_names is None:
-            with NUMPY:
-                bounds = bounds.shifted(math.zeros(channel(vector=spatial(values).names)))
-        SampledField.__init__(self, elements, values, extrapolation, bounds)
-        assert values.shape.spatial_rank == elements.spatial_rank, f"Spatial dimensions of values ({values.shape}) do not match elements {elements}"
-        assert values.shape.spatial_rank == bounds.spatial_rank, f"Spatial dimensions of values ({values.shape}) do not match elements {elements}"
-        assert values.shape.instance_rank == 0, f"Instance dimensions not supported for grids. Got values with shape {values.shape}"
-        self._resolution = resolution
-
-    def closest_values(self, points: Geometry):
-        """
-        Sample the closest grid point values of this field at the world-space locations (in physical units) given by `points`.
-        Points must have a single channel dimension named `vector`.
-        It may additionally contain any number of batch and spatial dimensions, all treated as batch dimensions.
-
-        Args:
-            points: world-space locations
-
-        Returns:
-            Closest grid point values as a `Tensor`.
-            For each dimension, the grid points immediately left and right of the sample points are evaluated.
-            For each point in `points`, a *2^d* cube of points is determined where *d* is the number of spatial dimensions of this field.
-            These values are stacked along the new dimensions `'closest_<dim>'` where `<dim>` refers to the name of a spatial dimension.
-        """
-        raise NotImplementedError(self)
-
-    def _sample(self, geometry: Geometry, **kwargs) -> math.Tensor:
-        raise NotImplementedError(self)
-
-    def with_values(self, values):
-        if isinstance(values, math.Tensor):
-            bounds = self.bounds.project(*values.shape.spatial.names)
-            return type(self)(values, extrapolation=self.extrapolation, bounds=bounds)
-        else:
-            return type(self)(values, extrapolation=self.extrapolation, bounds=self.bounds, resolution=self._resolution)
-
-    def with_extrapolation(self, extrapolation: Extrapolation):
-        return type(self)(self.values, extrapolation=extrapolation, bounds=self.bounds)
-
-    def with_bounds(self, bounds: Box):
-        return type(self)(self.values, extrapolation=self.extrapolation, bounds=bounds)
-
-    def __value_attrs__(self):
-        return '_values', '_extrapolation'
-
-    def __variable_attrs__(self):
-        return '_values',
-
-    def __expand__(self, dims: Shape, **kwargs) -> 'Grid':
-        return self.with_values(math.expand(self.values, dims, **kwargs))
-
-    def __replace_dims__(self, dims: Tuple[str, ...], new_dims: Shape, **kwargs) -> 'Grid':
-        for dim in dims:
-            if dim in self._resolution:
-                return NotImplemented
-        values = math.rename_dims(self.values, dims, new_dims)
-        extrapolation = math.rename_dims(self.extrapolation, dims, new_dims, **kwargs)
-        bounds = math.rename_dims(self.bounds, dims, new_dims, **kwargs)
-        return type(self)(values, extrapolation=extrapolation, bounds=bounds, resolution=self._resolution)
-
-
-    def __eq__(self, other):
-        if not type(self) == type(other):
-            return False
-        if not (self._bounds == other._bounds and self._resolution == other._resolution and self._extrapolation == other._extrapolation):
-            return False
-        if self.values is None:
-            return other.values is None
-        if other.values is None:
-            return False
-        if not math.all_available(self.values) or not math.all_available(other.values):  # tracers involved
-            if math.all_available(self.values) != math.all_available(other.values):
-                return False
-            else:  # both tracers
-                return self.values.shape == other.values.shape
-        return bool((self.values == other.values).all)
-
-    def __getitem__(self, item) -> 'Grid':
-        raise NotImplementedError(self)
-
-    @property
-    def shape(self):
-        return self._resolution & self._values.shape.non_spatial
-
-    @property
-    def bounds(self) -> Box:
-        return self._bounds
-
-    @property
-    def box(self) -> Box:
-        return self._bounds
-
-    @property
-    def resolution(self) -> Shape:
-        return self._resolution
-
-    @property
-    def dx(self) -> Tensor:
-        return self.bounds.size / self.resolution
-
-    def __repr__(self):
-        if self._values is not None:
-            return f"{self.__class__.__name__}[{self.shape.non_spatial & self.resolution}, size={self.box.size}, extrapolation={self._extrapolation}]"
-        else:
-            return f"{self.__class__.__name__}[{self.resolution}, size={self.box.size}, extrapolation={self._extrapolation}]"
-
-    def uniform_values(self):
-        """
-        Returns a uniform tensor containing `values`.
-
-        For periodic grids, which always have a uniform value tensor, `values' is returned directly.
-        If `values` is not uniform, it is padded as in `StaggeredGrid.staggered_tensor()`.
-        """
-        return self.values
-
-
-GridType = TypeVar('GridType', bound=Grid)
-
-
-class CenteredGrid(Grid):
-    """
-    N-dimensional grid with values sampled at the cell centers.
-    A centered grid is defined through its `CenteredGrid.values` `phi.math.Tensor`, its `CenteredGrid.bounds` `phi.geom.Box` describing the physical size, and its `CenteredGrid.extrapolation` (`phi.math.extrapolation.Extrapolation`).
-    
-    Centered grids support batch, spatial and channel dimensions.
-
-    See Also:
-        `StaggeredGrid`,
-        `Grid`,
-        `SampledField`,
-        `Field`,
-        module documentation at https://tum-pbs.github.io/PhiFlow/Fields.html
-    """
-
-    def __init__(self,
-                 values: Any = 0.,
-                 extrapolation: Any = 0.,
-                 bounds: Union[Box, float] = None,
-                 resolution: Union[int, Shape] = None,
-                 **resolution_: Union[int, Tensor]):
-        """
-        Args:
-            values: Values to use for the grid.
-                Has to be one of the following:
-
-                * `phi.geom.Geometry`: sets inside values to 1, outside to 0
-                * `Field`: resamples the Field to the staggered sample points
-                * `Number`: uses the value for all sample points
-                * `tuple` or `list`: interprets the sequence as vector, used for all sample points
-                * `phi.math.Tensor` compatible with grid dims: uses tensor values as grid values
-                * Function `values(x)` where `x` is a `phi.math.Tensor` representing the physical location.
-                    The spatial dimensions of the grid will be passed as batch dimensions to the function.
-
-            extrapolation: The grid extrapolation determines the value outside the `values` tensor.
-                Allowed types: `float`, `phi.math.Tensor`, `phi.math.extrapolation.Extrapolation`.
-            bounds: Physical size and location of the grid as `phi.geom.Box`.
-                If the resolution is determined through `resolution` of `values`, a `float` can be passed for `bounds` to create a unit box.
-            resolution: Grid resolution as purely spatial `phi.math.Shape`.
-                If `bounds` is given as a `Box`, the resolution may be specified as an `int` to be equal along all axes.
-            **resolution_: Spatial dimensions as keyword arguments. Typically either `resolution` or `spatial_dims` are specified.
-        """
-        if resolution is None and not resolution_:
-            assert isinstance(values, math.Tensor), "Grid resolution must be specified when 'values' is not a Tensor."
-            resolution = values.shape.spatial
-            bounds = _get_bounds(bounds, resolution)
-            elements = GridCell(resolution, bounds)
-        else:
-            resolution = _get_resolution(resolution, resolution_, bounds)
-            bounds = _get_bounds(bounds, resolution)
-            elements = GridCell(resolution, bounds)
-            if isinstance(values, math.Tensor):
-                values = math.expand(values, resolution)
-            elif isinstance(values, Geometry):
-                values = reduce_sample(values, elements)
-            elif isinstance(values, Field):
-                values = reduce_sample(values, elements)
-            elif callable(values):
-                values = _sample_function(values, elements)
-            else:
-                if isinstance(values, (tuple, list)) and len(values) == resolution.rank:
-                    values = math.tensor(values, channel(vector=resolution.names))
-                values = math.expand(math.tensor(values), resolution)
-        if values.dtype.kind not in (float, complex):
-            values = math.to_float(values)
-        assert resolution.spatial_rank == bounds.spatial_rank, f"Resolution {resolution} does not match bounds {bounds}"
-        Grid.__init__(self, elements, values, extrapolation, values.shape.spatial, bounds)
-
-    def __getitem__(self, item):
-        item = slicing_dict(self, item)
-        if not item:
-            return self
-        values = self._values[item]
-        extrapolation = self._extrapolation[item]
-        keep_dims = [dim for dim in self.resolution.names if dim not in item or not isinstance(item[dim], int)]
-        bounds = self.elements[item].bounds[{'vector': keep_dims}]
-        return CenteredGrid(values, bounds=bounds, extrapolation=extrapolation)
-
-    def _sample(self, geometry: Geometry, **kwargs) -> Tensor:
-        if geometry == self.bounds:
-            return math.mean(self._values, self._resolution)
-        if isinstance(geometry, GeometryStack):
-            sampled = [self._sample(g, **kwargs) for g in geometry.geometries]
-            return math.stack(sampled, geometry.geometries.shape)
-        if isinstance(geometry, GridCell):
-            if self.elements == geometry:
-                return self.values
-            elif math.close(self.dx, geometry.size):
-                if all([math.close(offset, geometry.half_size) or math.close(offset, 0)
-                        for offset in math.abs(self.bounds.lower - geometry.bounds.lower)]):
-                    dyadic_interpolated = self._dyadic_interplate(geometry.resolution, geometry.bounds, **kwargs)
-                    if dyadic_interpolated is not NotImplemented:
-                        return dyadic_interpolated
-                if 'order' in kwargs and kwargs['order'] != 2:
-                    raise NotImplementedError(f"Only 6th-order implicit and 2nd-order resampling supported but got order={kwargs['order']}")
-                fast_resampled = self._shift_resample(geometry.resolution, geometry.bounds)
-                if fast_resampled is not NotImplemented:
-                    return fast_resampled
-        points = geometry.center
-        local_points = self.box.global_to_local(points) * self.resolution - 0.5
-        resampled_values = math.grid_sample(self.values, local_points, self.extrapolation, bounds=self.bounds)
-        if isinstance(self._extrapolation, FieldEmbedding):
-            if isinstance(geometry, GridCell) and ((geometry.bounds.upper <= self.bounds.upper).all or (geometry.bounds.lower >= self.bounds.lower).all):
-                # geometry is a subgrid of self
-                return resampled_values
-            else:  # otherwise we also sample the extrapolation Field
-                ext_values = self._extrapolation.field._sample(geometry, **kwargs)
-                inside = self.bounds.lies_inside(points)
-                return math.where(inside, resampled_values, ext_values)
-        return resampled_values
-
-    def _dyadic_interplate(self, resolution: Shape, bounds: Box, order=2, implicit: Solve = None):
-        offsets = bounds.lower - self.bounds.lower
-        interpolation_dirs = [0 if math.close(offset, 0) else int(math.sign(offset)) for offset in offsets]
-        return _dyadic_interpolate(self.values, interpolation_dirs, self.extrapolation, order, implicit)
-
-    def _shift_resample(self, resolution: Shape, bounds: Box, threshold=1e-5, max_padding=20):
-        assert math.all_available(bounds.lower, bounds.upper), "Shift resampling requires 'bounds' to be available."
-        lower = math.to_int32(math.ceil(math.maximum(0, self.box.lower - bounds.lower) / self.dx - threshold))
-        upper = math.to_int32(math.ceil(math.maximum(0, bounds.upper - self.box.upper) / self.dx - threshold))
-        total_padding = (math.sum(lower) + math.sum(upper)).numpy()
-        if total_padding > max_padding and self.extrapolation.native_grid_sample_mode:
-            return NotImplemented
-        elif total_padding > 0:
-            from phi.field import pad
-            padded = pad(self, {dim: (int(lower[i]), int(upper[i])) for i, dim in enumerate(self.shape.spatial.names)})
-            grid_box, grid_resolution, grid_values = padded.box, padded.resolution, padded.values
-        else:
-            grid_box, grid_resolution, grid_values = self.box, self.resolution, self.values
-        origin_in_local = grid_box.global_to_local(bounds.lower) * grid_resolution
-        data = math.sample_subgrid(grid_values, origin_in_local, resolution)
-        return data
-
-    def closest_values(self, points: Geometry):
-        local_points = self.box.global_to_local(points.center) * self.resolution - 0.5
-        return math.closest_grid_values(self.values, local_points, self.extrapolation)
-
-
-class StaggeredGrid(Grid):
-    """
-    N-dimensional grid whose vector components are sampled at the respective face centers.
-    A staggered grid is defined through its values tensor, its bounds describing the physical size, and its extrapolation.
-    
-    Staggered grids support batch and spatial dimensions but only one channel dimension for the staggered vector components.
-
-    See Also:
-        `CenteredGrid`,
-        `Grid`,
-        `SampledField`,
-        `Field`,
-        module documentation at https://tum-pbs.github.io/PhiFlow/Fields.html
-    """
-
-    def __init__(self,
-                 values: Any = 0.,
-                 extrapolation: Union[float, Extrapolation] = 0,
-                 bounds: Union[Box, float] = None,
-                 resolution: Union[Shape, int] = None,
-                 **resolution_: Union[int, Tensor]):
-        """
-        Args:
-            values: Values to use for the grid.
-                Has to be one of the following:
-
-                * `phi.geom.Geometry`: sets inside values to 1, outside to 0
-                * `Field`: resamples the Field to the staggered sample points
-                * `Number`: uses the value for all sample points
-                * `tuple` or `list`: interprets the sequence as vector, used for all sample points
-                * `phi.math.Tensor` with staggered shape: uses tensor values as grid values.
-                  Must contain a `vector` dimension with each slice consisting of one more element along the dimension they describe.
-                  Use `phi.math.stack()` to manually create this non-uniform tensor.
-                * Function `values(x)` where `x` is a `phi.math.Tensor` representing the physical location.
-                    The spatial dimensions of the grid will be passed as batch dimensions to the function.
-
-            extrapolation: The grid extrapolation determines the value outside the `values` tensor.
-                Allowed types: `float`, `phi.math.Tensor`, `phi.math.extrapolation.Extrapolation`.
-            bounds: Physical size and location of the grid as `phi.geom.Box`.
-                If the resolution is determined through `resolution` of `values`, a `float` can be passed for `bounds` to create a unit box.
-            resolution: Grid resolution as purely spatial `phi.math.Shape`.
-                If `bounds` is given as a `Box`, the resolution may be specified as an `int` to be equal along all axes.
-            **resolution_: Spatial dimensions as keyword arguments. Typically either `resolution` or `spatial_dims` are specified.
-        """
-        extrapolation = as_extrapolation(extrapolation)
-        if resolution is None and not resolution_:
-            assert isinstance(values, Tensor), "Grid resolution must be specified when 'values' is not a Tensor."
-            if not all(extrapolation.valid_outer_faces(d)[0] != extrapolation.valid_outer_faces(d)[1] for d in spatial(values).names):  # non-uniform values required
-                if values.shape.is_uniform:
-                    values = unstack_staggered_tensor(values, extrapolation)
-                resolution = resolution_from_staggered_tensor(values, extrapolation)
-            else:
-                resolution = spatial(values)
-            bounds = _get_bounds(bounds, resolution)
-            bounds = bounds or Box(math.const_vec(0, resolution), math.wrap(resolution, channel('vector')))
-            elements = staggered_elements(resolution, bounds, extrapolation)
-        else:
-            resolution = _get_resolution(resolution, resolution_, bounds)
-            bounds = _get_bounds(bounds, resolution)
-            elements = staggered_elements(resolution, bounds, extrapolation)
-            if isinstance(values, math.Tensor):
-                if not spatial(values):
-                    values = expand_staggered(values, resolution, extrapolation)
-                if not all(extrapolation.valid_outer_faces(d)[0] != extrapolation.valid_outer_faces(d)[1] for d in resolution.names):  # non-uniform values required
-                    if values.shape.is_uniform:
-                        values = unstack_staggered_tensor(values, extrapolation)
-                    else:  # Keep dim order from data and check it matches resolution
-                        assert set(resolution_from_staggered_tensor(values, extrapolation)) == set(resolution), f"Failed to create StaggeredGrid: values {values.shape} do not match given resolution {resolution} for extrapolation {extrapolation}. See https://tum-pbs.github.io/PhiFlow/Staggered_Grids.html"
-            elif isinstance(values, Geometry):
-                values = reduce_sample(values, elements)
-            elif isinstance(values, Field):
-                values = reduce_sample(values, elements)
-            elif callable(values):
-                values = _sample_function(values, elements)
-                if elements.shape.shape.rank > 1:  # Different number of X and Y faces
-                    assert isinstance(values, TensorStack), f"values function must return a staggered Tensor but returned {type(values)}"
-                assert 'staggered_direction' in values.shape
-                if 'vector' in values.shape:
-                    values = math.stack([values.staggered_direction[i].vector[i] for i in range(resolution.rank)], channel(vector=resolution))
-                else:
-                    values = values.staggered_direction.as_channel('vector')
-            else:
-                values = expand_staggered(math.tensor(values), resolution, extrapolation)
-        if values.dtype.kind not in (float, complex):
-            values = math.to_float(values)
-        assert resolution.spatial_rank == bounds.spatial_rank, f"Resolution {resolution} does not match bounds {bounds}"
-        Grid.__init__(self, elements, values, extrapolation, resolution, bounds)
-
-    @property
-    def cells(self):
-        return GridCell(self.resolution, self.bounds)
-
-    def with_extrapolation(self, extrapolation: Extrapolation):
-        extrapolation = as_extrapolation(extrapolation)
-        if all([extrapolation.valid_outer_faces(dim) == self.extrapolation.valid_outer_faces(dim) for dim in self.resolution.names]):
-            return StaggeredGrid(self.values, extrapolation=extrapolation, bounds=self.bounds)
-        else:
-            values = []
-            for dim, component in zip(self.shape.spatial.names, self.values.unstack('vector')):
-                old_lo, old_hi = [int(v) for v in self.extrapolation.valid_outer_faces(dim)]
-                new_lo, new_hi = [int(v) for v in extrapolation.valid_outer_faces(dim)]
-                widths = (new_lo - old_lo, new_hi - old_hi)
-                values.append(math.pad(component, {dim: widths}, self.extrapolation, bounds=self.bounds))
-            values = math.stack(values, channel(vector=self.resolution))
-            return StaggeredGrid(values, extrapolation, bounds=self.bounds)
-
-    def _sample(self, geometry: Geometry, **kwargs) -> Tensor:
-        channels = [sample(component, geometry, **kwargs) for component in self.vector.unstack()]
-        return math.stack(channels, geometry.shape['vector'])
-
-    def closest_values(self, points: Geometry):
-        if 'staggered_direction' in points.shape:
-            points_ = points.unstack('staggered_direction')
-            channels = [component.closest_values(p) for p, component in zip(points_, self.vector.unstack())]
-        else:
-            channels = [component.closest_values(points) for component in self.vector.unstack()]
-        return math.stack(channels, points.shape['vector'])
-
-    def at_centers(self) -> CenteredGrid:
-        """
-        Interpolates the staggered values to the cell centers.
-
-        Returns:
-            `CenteredGrid` sampled at cell centers.
-        """
-        return CenteredGrid(self, resolution=self.resolution, bounds=self.bounds, extrapolation=self.extrapolation)
-
-    def __getitem__(self, item):
-        item = slicing_dict(self, item)
-        if not item:
-            return self
-        if 'vector' in item:
-            selection = item['vector']
-            if isinstance(selection, int):
-                item['vector'] = self.resolution.names[selection]
-        values = self._values[{dim: sel for dim, sel in item.items() if dim not in self.shape.spatial}]
-        for dim, sel in item.items():
-            if dim in self.shape.spatial:
-                raise AssertionError("Cannot slice StaggeredGrid along spatial dimensions.")
-                # sel = slice(sel, sel + 1) if isinstance(sel, int) else sel
-                # values = []
-                # for vdim, val in zip(self.shape.spatial.names, self.values.unstack('vector')):
-                #     if vdim == dim:
-                #         values.append(val[{dim: slice(sel.start, sel.stop + 1)}])
-                #     else:
-                #         values.append(val[{dim: sel}])
-                # values = math.stack(values, channel('vector'))
-        extrapolation = self._extrapolation[item]
-        bounds = GridCell(self._resolution, self._bounds)[item].bounds
-        if 'vector' in item:
-            selection = item['vector']
-            if isinstance(selection, str) and ',' in selection:
-                selection = parse_dim_order(selection)
-            if isinstance(selection, str):  # single item name
-                item_names = self.shape.get_item_names('vector', fallback_spatial=True)
-                assert selection in item_names, f"Accessing field.vector['{selection}'] failed. Item names are {item_names}."
-                selection = item_names.index(selection)
-            if isinstance(selection, int):
-                dim = self.resolution.names[selection]
-                comp_cells = GridCell(self.resolution, bounds).stagger(dim, *self.extrapolation.valid_outer_faces(dim))
-                return CenteredGrid(values, bounds=comp_cells.bounds, extrapolation=extrapolation)
-            else:
-                assert isinstance(selection, slice) and not selection.start and not selection.stop
-        return StaggeredGrid(values, bounds=bounds, extrapolation=extrapolation)
-
-    def uniform_values(self):
-        if self.values.shape.is_uniform:
-            return self.values
-        else:
-            return self.staggered_tensor()
-
-    def staggered_tensor(self) -> Tensor:
-        """
-        Stacks all component grids into a single uniform `phi.math.Tensor`.
-        The individual components are padded to a common (larger) shape before being stacked.
-        The shape of the returned tensor is exactly one cell larger than the grid `resolution` in every spatial dimension.
-
-        Returns:
-            Uniform `phi.math.Tensor`.
-        """
-        padded = []
-        for dim, component in zip(self.resolution.names, math.unstack(self.values, 'vector')):
-            widths = {d: (0, 1) for d in self.resolution.names}
-            lo_valid, up_valid = self.extrapolation.valid_outer_faces(dim)
-            widths[dim] = (int(not lo_valid), int(not up_valid))
-            padded.append(math.pad(component, widths, self.extrapolation[{'vector': dim}], bounds=self.bounds))
-        result = math.stack(padded, channel(vector=self.resolution))
-        assert result.shape.is_uniform
-        return result
-
-    def _op2(self, other, operator):
-        if isinstance(other, StaggeredGrid) and self.bounds == other.bounds and self.shape.spatial == other.shape.spatial:
-            values = operator(self._values, other.values)
-            extrapolation_ = operator(self._extrapolation, other.extrapolation)
-            return StaggeredGrid(values=values, extrapolation=extrapolation_, bounds=self.bounds)
-        else:
-            return SampledField._op2(self, other, operator)
-
-
-def unstack_staggered_tensor(data: Tensor, extrapolation: Extrapolation) -> TensorStack:
-    sliced = []
-    for dim, component in zip(data.shape.spatial.names, data.unstack('vector')):
-        lo_valid, up_valid = extrapolation.valid_outer_faces(dim)
-        slices = {d: slice(0, -1) for d in data.shape.spatial.names}
-        slices[dim] = slice(int(not lo_valid), - int(not up_valid) or None)
-        sliced.append(component[slices])
-    return math.stack(sliced, channel(vector=spatial(data)))
-
-
-def staggered_elements(resolution: Shape, bounds: Box, extrapolation: Extrapolation):
-    cells = GridCell(resolution, bounds)
-    grids = []
-    for dim in resolution.names:
-        lower, upper = extrapolation.valid_outer_faces(dim)
-        grids.append(cells.stagger(dim, lower, upper))
-    return geom.stack(grids, channel(staggered_direction=resolution.names))
-
-
-def expand_staggered(values: Tensor, resolution: Shape, extrapolation: Extrapolation):
-    """ Add missing spatial dimensions to `values` """
-    cells = GridCell(resolution, Box(math.const_vec(0, resolution), math.const_vec(1, resolution)))
-    components = values.vector.unstack(resolution.spatial_rank)
-    tensors = []
-    for dim, component in zip(resolution.spatial.names, components):
-        comp_cells = cells.stagger(dim, *extrapolation.valid_outer_faces(dim))
-        tensors.append(math.expand(component, comp_cells.resolution))
-    return math.stack(tensors, channel(vector=resolution.names))
-
-
-def resolution_from_staggered_tensor(values: Tensor, extrapolation: Extrapolation):
-    any_dim = values.shape.spatial.names[0]
-    x_shape = values.shape.after_gather({'vector': any_dim})
-    ext_lower, ext_upper = extrapolation.valid_outer_faces(any_dim)
-    delta = int(ext_lower) + int(ext_upper) - 1
-    resolution = x_shape.spatial._replace_single_size(any_dim, x_shape.get_size(any_dim) - delta)
-    return resolution
-
-
-def _sample_function(f, elements: Geometry):
-    from phi.math._functional import get_function_parameters
-    try:
-        params = get_function_parameters(f)
-        dims = elements.shape.get_size('vector')
-        names_match = tuple(params.keys())[:dims] == elements.shape.get_item_names('vector')
-        num_positional = 0
-        has_varargs = False
-        for n, p in params.items():
-            if p.default is p.empty:
-                num_positional += 1
-            if p.kind == 2:  # _ParameterKind.VAR_POSITIONAL
-                has_varargs = True
-        assert num_positional <= dims, f"Cannot sample {f.__name__}({', '.join(tuple(params))}) on physical space {elements.shape.get_item_names('vector')}"
-        pass_varargs = has_varargs or names_match or num_positional > 1 or num_positional == dims
-        if num_positional > 1 and not has_varargs:
-            assert names_match, f"Positional arguments of {f.__name__}({', '.join(tuple(params))}) should match physical space {elements.shape.get_item_names('vector')}"
-    except ValueError as err:  # signature not available for all functions
-        pass_varargs = False
-    if pass_varargs:
-        values = math.map_s2b(f)(*elements.center.vector)
-    else:
-        values = math.map_s2b(f)(elements.center)
-    assert isinstance(values, math.Tensor), f"values function must return a Tensor but returned {type(values)}"
-    return values
-
-
-def _get_bounds(bounds: Union[Box, float, None], resolution: Shape):
-    if bounds is None:
-        return Box(math.const_vec(0, resolution), math.wrap(resolution, channel(vector=resolution.names)))
-    if isinstance(bounds, Box):
-        assert set(bounds.vector.item_names) == set(resolution.names), f"bounds dimensions {bounds.vector.item_names} must match resolution {resolution}"
-        return bounds
-    if isinstance(bounds, (int, float)):
-        return Box(math.const_vec(0, resolution), math.const_vec(bounds, resolution))
-    raise ValueError(f"bounds must be a Box, float or None but got {type(bounds).__name__}")
-
-
-def _get_resolution(resolution: Shape, resolution_: dict, bounds: Box):
-    assert 'boundaries' not in resolution_, "'boundaries' is not a valid grid argument. Use 'extrapolation' instead, passing a value or math.extrapolation.Extrapolation object. See https://tum-pbs.github.io/PhiFlow/phi/math/extrapolation.html"
-    if isinstance(resolution, int):
-        assert not resolution_, "Cannot specify keyword resolution and integer resolution at the same time."
-        resolution = spatial(**{dim: resolution for dim in bounds.size.shape.get_item_names('vector')})
-    try:
-        resolution_ = spatial(**resolution_)
-    except AssertionError as err:
-        raise ValueError(f"Invalid grid resolution: {', '.join(f'{dim}={size}' for dim, size in resolution_.items())}. Pass an int for all sizes.") from err
-    return (resolution or math.EMPTY_SHAPE) & resolution_
-
-
-def _dyadic_interpolate(grid: Tensor, interpolation_dirs: List, padding: Extrapolation, order: int, implicit: Solve):
-    """
-    Samples a sub-grid from `grid` with an offset of half a grid cell in directions defined by `interpolation_dirs`.
-
-    Args:
-        grid: `Tensor` to be resampled.
-        interpolation_dirs: List which defines for every spatial dimension of `grid` if interpolation should be performed,
-            in positive direction `1` / negative direction `-1` / no interpolation`0`
-            len(interpolation_dirs) == len(grid.shape.spatial.names) is assumed
-            Example: With `grid.shape.spatial.names=['x', 'y']` and `interpolation_dirs: [1, -1]`
-                     grid will be interpolated half a grid cell in positive x direction and half a grid cell in negative y direction
-        padding: Extrapolation used for the needed out of Domain values
-        order: finite difference `Scheme` used for interpolation
-
-    Returns:
-      Sub-grid as `Tensor`
-    """
-    if implicit:
-        if order == 6:
-            values, needed_shifts = [1 / 20, 3 / 4, 3 / 4, 1 / 20], (-1, 0, 1, 2)
-            values_rhs, needed_shifts_rhs = [3 / 10, 1, 3 / 10], (-1, 0, 1)
-        else:
-            return NotImplemented
-    else:
-        return NotImplemented
-    result = grid
-    for dim, dir in zip(grid.shape.spatial.names, interpolation_dirs):
-        if dir == 0: continue
-        is_neg_dir = dir == -1
-        current_widths = [abs(min(needed_shifts)) + is_neg_dir, max(needed_shifts) - is_neg_dir]
-        padded = math.pad(result, {dim: tuple(current_widths)}, padding)
-        shifted = math.shift(padded, needed_shifts, [dim], padding=None, stack_dim=None)
-        result = sum([value * shift_ for value, shift_ in zip(values, shifted)])
-        if implicit:
-            implicit.x0 = result
-            result = math.solve_linear(dyadic_interpolate_lhs, result, implicit, values_rhs=values_rhs, needed_shifts_rhs=needed_shifts_rhs, dim=dim, padding=padding)
-    return result
-
-
-@math.jit_compile_linear(auxiliary_args="values_rhs, needed_shifts_rhs")
-def dyadic_interpolate_lhs(x, values_rhs, needed_shifts_rhs, dim, padding):
-    shifted = math.shift(x, needed_shifts_rhs, stack_dim=None, dims=[dim], padding=padding)
-    return sum([value * shift_ for value, shift_ in zip(values_rhs, shifted)])
+from typing import TypeVar, Any, Tuple, List, Union
+
+from phi.math import Solve
+
+from phi import math, geom
+from phi.geom import Box, Geometry, GridCell
+from ._embed import FieldEmbedding
+from ._field import SampledField, Field, sample, reduce_sample, as_extrapolation
+from ..geom._stack import GeometryStack
+from ..math import Shape, NUMPY
+from ..math._shape import spatial, channel, parse_dim_order
+from ..math._tensors import TensorStack, Tensor
+from ..math.extrapolation import Extrapolation
+from ..math.magic import slicing_dict
+
+
+class Grid(SampledField):
+    """
+    Base class for `CenteredGrid` and `StaggeredGrid`.
+    """
+
+    def __init__(self, elements: Geometry, values: Tensor, extrapolation: Union[float, Extrapolation], resolution: Union[Shape, int], bounds: Union[Box, float]):
+        assert isinstance(bounds, Box)
+        assert isinstance(resolution, Shape)
+        if bounds.size.vector.item_names is None:
+            with NUMPY:
+                bounds = bounds.shifted(math.zeros(channel(vector=spatial(values).names)))
+        SampledField.__init__(self, elements, values, extrapolation, bounds)
+        assert values.shape.spatial_rank == elements.spatial_rank, f"Spatial dimensions of values ({values.shape}) do not match elements {elements}"
+        assert values.shape.spatial_rank == bounds.spatial_rank, f"Spatial dimensions of values ({values.shape}) do not match elements {elements}"
+        assert values.shape.instance_rank == 0, f"Instance dimensions not supported for grids. Got values with shape {values.shape}"
+        assert set(resolution.names) == set(bounds.vector.item_names), f"Resolution does not match bounds"
+        self._resolution = resolution.only(bounds.vector.item_names, reorder=True)
+
+    def closest_values(self, points: Geometry):
+        """
+        Sample the closest grid point values of this field at the world-space locations (in physical units) given by `points`.
+        Points must have a single channel dimension named `vector`.
+        It may additionally contain any number of batch and spatial dimensions, all treated as batch dimensions.
+
+        Args:
+            points: world-space locations
+
+        Returns:
+            Closest grid point values as a `Tensor`.
+            For each dimension, the grid points immediately left and right of the sample points are evaluated.
+            For each point in `points`, a *2^d* cube of points is determined where *d* is the number of spatial dimensions of this field.
+            These values are stacked along the new dimensions `'closest_<dim>'` where `<dim>` refers to the name of a spatial dimension.
+        """
+        raise NotImplementedError(self)
+
+    def _sample(self, geometry: Geometry, **kwargs) -> math.Tensor:
+        raise NotImplementedError(self)
+
+    def with_values(self, values):
+        if isinstance(values, math.Tensor):
+            assert set(spatial(values).names) == set(self.bounds.vector.item_names), f"StaggeredGrid.with_values() only accepts tensor with same spatial dimensiosn but got {spatial(values)} for {self}"
+            return type(self)(values, extrapolation=self.extrapolation, bounds=self.bounds)
+        else:
+            return type(self)(values, extrapolation=self.extrapolation, bounds=self.bounds, resolution=self._resolution)
+
+    def with_extrapolation(self, extrapolation: Extrapolation):
+        return type(self)(self.values, extrapolation=extrapolation, bounds=self.bounds)
+
+    def with_bounds(self, bounds: Box):
+        return type(self)(self.values, extrapolation=self.extrapolation, bounds=bounds)
+
+    def __value_attrs__(self):
+        return '_values', '_extrapolation'
+
+    def __variable_attrs__(self):
+        return '_values',
+
+    def __expand__(self, dims: Shape, **kwargs) -> 'Grid':
+        return self.with_values(math.expand(self.values, dims, **kwargs))
+
+    def __replace_dims__(self, dims: Tuple[str, ...], new_dims: Shape, **kwargs) -> 'Grid':
+        for dim in dims:
+            if dim in self._resolution:
+                return NotImplemented
+        values = math.rename_dims(self.values, dims, new_dims)
+        extrapolation = math.rename_dims(self.extrapolation, dims, new_dims, **kwargs)
+        bounds = math.rename_dims(self.bounds, dims, new_dims, **kwargs)
+        return type(self)(values, extrapolation=extrapolation, bounds=bounds, resolution=self._resolution)
+
+
+    def __eq__(self, other):
+        if not type(self) == type(other):
+            return False
+        if not (self._bounds == other._bounds and self._resolution == other._resolution and self._extrapolation == other._extrapolation):
+            return False
+        if self.values is None:
+            return other.values is None
+        if other.values is None:
+            return False
+        if not math.all_available(self.values) or not math.all_available(other.values):  # tracers involved
+            if math.all_available(self.values) != math.all_available(other.values):
+                return False
+            else:  # both tracers
+                return self.values.shape == other.values.shape
+        if self.values.shape != other.values.shape:
+            return False
+        return bool((self.values == other.values).all)
+
+    def __getitem__(self, item) -> 'Grid':
+        raise NotImplementedError(self)
+
+    @property
+    def shape(self):
+        return self._resolution & self._values.shape.non_spatial
+
+    @property
+    def bounds(self) -> Box:
+        return self._bounds
+
+    @property
+    def box(self) -> Box:
+        return self._bounds
+
+    @property
+    def resolution(self) -> Shape:
+        return self._resolution
+
+    @property
+    def dx(self) -> Tensor:
+        return self.bounds.size / self.resolution
+
+    def __repr__(self):
+        if self._values is not None:
+            return f"{self.__class__.__name__}[{self.shape.non_spatial & self.resolution}, size={self.box.size}, extrapolation={self._extrapolation}]"
+        else:
+            return f"{self.__class__.__name__}[{self.resolution}, size={self.box.size}, extrapolation={self._extrapolation}]"
+
+    def uniform_values(self):
+        """
+        Returns a uniform tensor containing `values`.
+
+        For periodic grids, which always have a uniform value tensor, `values' is returned directly.
+        If `values` is not uniform, it is padded as in `StaggeredGrid.staggered_tensor()`.
+        """
+        return self.values
+
+
+GridType = TypeVar('GridType', bound=Grid)
+
+
+class CenteredGrid(Grid):
+    """
+    N-dimensional grid with values sampled at the cell centers.
+    A centered grid is defined through its `CenteredGrid.values` `phi.math.Tensor`, its `CenteredGrid.bounds` `phi.geom.Box` describing the physical size, and its `CenteredGrid.extrapolation` (`phi.math.extrapolation.Extrapolation`).
+    
+    Centered grids support batch, spatial and channel dimensions.
+
+    See Also:
+        `StaggeredGrid`,
+        `Grid`,
+        `SampledField`,
+        `Field`,
+        module documentation at https://tum-pbs.github.io/PhiFlow/Fields.html
+    """
+
+    def __init__(self,
+                 values: Any = 0.,
+                 extrapolation: Union[float, Extrapolation, dict, Field] = 0.,
+                 bounds: Union[Box, float] = None,
+                 resolution: Union[int, Shape] = None,
+                 **resolution_: Union[int, Tensor]):
+        """
+        Args:
+            values: Values to use for the grid.
+                Has to be one of the following:
+
+                * `phi.geom.Geometry`: sets inside values to 1, outside to 0
+                * `Field`: resamples the Field to the staggered sample points
+                * `Number`: uses the value for all sample points
+                * `tuple` or `list`: interprets the sequence as vector, used for all sample points
+                * `phi.math.Tensor` compatible with grid dims: uses tensor values as grid values
+                * Function `values(x)` where `x` is a `phi.math.Tensor` representing the physical location.
+                    The spatial dimensions of the grid will be passed as batch dimensions to the function.
+
+            extrapolation: The grid extrapolation determines the value outside the `values` tensor.
+                Allowed types: `float`, `phi.math.Tensor`, `phi.math.extrapolation.Extrapolation`.
+            bounds: Physical size and location of the grid as `phi.geom.Box`.
+                If the resolution is determined through `resolution` of `values`, a `float` can be passed for `bounds` to create a unit box.
+            resolution: Grid resolution as purely spatial `phi.math.Shape`.
+                If `bounds` is given as a `Box`, the resolution may be specified as an `int` to be equal along all axes.
+            **resolution_: Spatial dimensions as keyword arguments. Typically either `resolution` or `spatial_dims` are specified.
+        """
+        if resolution is None and not resolution_:
+            assert isinstance(values, math.Tensor), "Grid resolution must be specified when 'values' is not a Tensor."
+            resolution = values.shape.spatial
+            bounds = _get_bounds(bounds, resolution)
+            elements = GridCell(resolution, bounds)
+        else:
+            resolution = _get_resolution(resolution, resolution_, bounds)
+            bounds = _get_bounds(bounds, resolution)
+            elements = GridCell(resolution, bounds)
+            if isinstance(values, math.Tensor):
+                values = math.expand(values, resolution)
+            elif isinstance(values, Geometry):
+                values = reduce_sample(values, elements)
+            elif isinstance(values, Field):
+                values = reduce_sample(values, elements)
+            elif callable(values):
+                values = _sample_function(values, elements)
+            else:
+                if isinstance(values, (tuple, list)) and len(values) == resolution.rank:
+                    values = math.tensor(values, channel(vector=resolution.names))
+                values = math.expand(math.tensor(values), resolution)
+        if values.dtype.kind not in (float, complex):
+            values = math.to_float(values)
+        assert resolution.spatial_rank == bounds.spatial_rank, f"Resolution {resolution} does not match bounds {bounds}"
+        Grid.__init__(self, elements, values, extrapolation, values.shape.spatial, bounds)
+
+    def __getitem__(self, item):
+        item = slicing_dict(self, item)
+        if not item:
+            return self
+        values = self._values[item]
+        extrapolation = self._extrapolation[item]
+        keep_dims = [dim for dim in self.resolution.names if dim not in item or not isinstance(item[dim], int)]
+        bounds = self.elements[item].bounds[{'vector': keep_dims}]
+        return CenteredGrid(values, bounds=bounds, extrapolation=extrapolation)
+
+    def _sample(self, geometry: Geometry, **kwargs) -> Tensor:
+        if geometry == self.bounds:
+            return math.mean(self._values, self._resolution)
+        if isinstance(geometry, GeometryStack):
+            sampled = [self._sample(g, **kwargs) for g in geometry.geometries]
+            return math.stack(sampled, geometry.geometries.shape)
+        if isinstance(geometry, GridCell):
+            if self.elements == geometry:
+                return self.values
+            elif math.close(self.dx, geometry.size):
+                if all([math.close(offset, geometry.half_size) or math.close(offset, 0)
+                        for offset in math.abs(self.bounds.lower - geometry.bounds.lower)]):
+                    dyadic_interpolated = self._dyadic_interplate(geometry.resolution, geometry.bounds, **kwargs)
+                    if dyadic_interpolated is not NotImplemented:
+                        return dyadic_interpolated
+                if 'order' in kwargs and kwargs['order'] != 2:
+                    raise NotImplementedError(f"Only 6th-order implicit and 2nd-order resampling supported but got order={kwargs['order']}")
+                fast_resampled = self._shift_resample(geometry.resolution, geometry.bounds)
+                if fast_resampled is not NotImplemented:
+                    return fast_resampled
+        points = geometry.center
+        local_points = self.box.global_to_local(points) * self.resolution - 0.5
+        resampled_values = math.grid_sample(self.values, local_points, self.extrapolation, bounds=self.bounds)
+        if isinstance(self._extrapolation, FieldEmbedding):
+            if isinstance(geometry, GridCell) and ((geometry.bounds.upper <= self.bounds.upper).all or (geometry.bounds.lower >= self.bounds.lower).all):
+                # geometry is a subgrid of self
+                return resampled_values
+            else:  # otherwise we also sample the extrapolation Field
+                ext_values = self._extrapolation.field._sample(geometry, **kwargs)
+                inside = self.bounds.lies_inside(points)
+                return math.where(inside, resampled_values, ext_values)
+        return resampled_values
+
+    def _dyadic_interplate(self, resolution: Shape, bounds: Box, order=2, implicit: Solve = None):
+        offsets = bounds.lower - self.bounds.lower
+        interpolation_dirs = [0 if math.close(offset, 0) else int(math.sign(offset)) for offset in offsets]
+        return _dyadic_interpolate(self.values, interpolation_dirs, self.extrapolation, order, implicit)
+
+    def _shift_resample(self, resolution: Shape, bounds: Box, threshold=1e-5, max_padding=20):
+        assert math.all_available(bounds.lower, bounds.upper), "Shift resampling requires 'bounds' to be available."
+        lower = math.to_int32(math.ceil(math.maximum(0, self.box.lower - bounds.lower) / self.dx - threshold))
+        upper = math.to_int32(math.ceil(math.maximum(0, bounds.upper - self.box.upper) / self.dx - threshold))
+        total_padding = (math.sum(lower) + math.sum(upper)).numpy()
+        if total_padding > max_padding and self.extrapolation.native_grid_sample_mode:
+            return NotImplemented
+        elif total_padding > 0:
+            from phi.field import pad
+            padded = pad(self, {dim: (int(lower[i]), int(upper[i])) for i, dim in enumerate(self.shape.spatial.names)})
+            grid_box, grid_resolution, grid_values = padded.box, padded.resolution, padded.values
+        else:
+            grid_box, grid_resolution, grid_values = self.box, self.resolution, self.values
+        origin_in_local = grid_box.global_to_local(bounds.lower) * grid_resolution
+        data = math.sample_subgrid(grid_values, origin_in_local, resolution)
+        return data
+
+    def closest_values(self, points: Geometry):
+        local_points = self.box.global_to_local(points.center) * self.resolution - 0.5
+        return math.closest_grid_values(self.values, local_points, self.extrapolation)
+
+
+class StaggeredGrid(Grid):
+    """
+    N-dimensional grid whose vector components are sampled at the respective face centers.
+    A staggered grid is defined through its values tensor, its bounds describing the physical size, and its extrapolation.
+    
+    Staggered grids support batch and spatial dimensions but only one channel dimension for the staggered vector components.
+
+    See Also:
+        `CenteredGrid`,
+        `Grid`,
+        `SampledField`,
+        `Field`,
+        module documentation at https://tum-pbs.github.io/PhiFlow/Fields.html
+    """
+
+    def __init__(self,
+                 values: Any = 0.,
+                 extrapolation: Union[float, Extrapolation, dict, Field] = 0,
+                 bounds: Union[Box, float] = None,
+                 resolution: Union[Shape, int] = None,
+                 **resolution_: Union[int, Tensor]):
+        """
+        Args:
+            values: Values to use for the grid.
+                Has to be one of the following:
+
+                * `phi.geom.Geometry`: sets inside values to 1, outside to 0
+                * `Field`: resamples the Field to the staggered sample points
+                * `Number`: uses the value for all sample points
+                * `tuple` or `list`: interprets the sequence as vector, used for all sample points
+                * `phi.math.Tensor` with staggered shape: uses tensor values as grid values.
+                  Must contain a `vector` dimension with each slice consisting of one more element along the dimension they describe.
+                  Use `phi.math.stack()` to manually create this non-uniform tensor.
+                * Function `values(x)` where `x` is a `phi.math.Tensor` representing the physical location.
+                    The spatial dimensions of the grid will be passed as batch dimensions to the function.
+
+            extrapolation: The grid extrapolation determines the value outside the `values` tensor.
+                Allowed types: `float`, `phi.math.Tensor`, `phi.math.extrapolation.Extrapolation`.
+            bounds: Physical size and location of the grid as `phi.geom.Box`.
+                If the resolution is determined through `resolution` of `values`, a `float` can be passed for `bounds` to create a unit box.
+            resolution: Grid resolution as purely spatial `phi.math.Shape`.
+                If `bounds` is given as a `Box`, the resolution may be specified as an `int` to be equal along all axes.
+            **resolution_: Spatial dimensions as keyword arguments. Typically either `resolution` or `spatial_dims` are specified.
+        """
+        extrapolation = as_extrapolation(extrapolation)
+        if resolution is None and not resolution_:
+            assert isinstance(values, Tensor), "Grid resolution must be specified when 'values' is not a Tensor."
+            if not all(extrapolation.valid_outer_faces(d)[0] != extrapolation.valid_outer_faces(d)[1] for d in spatial(values).names):  # non-uniform values required
+                if values.shape.is_uniform:
+                    values = unstack_staggered_tensor(values, extrapolation)
+                resolution = resolution_from_staggered_tensor(values, extrapolation)
+            else:
+                resolution = spatial(values)
+            bounds = _get_bounds(bounds, resolution)
+            bounds = bounds or Box(math.const_vec(0, resolution), math.wrap(resolution, channel('vector')))
+            elements = staggered_elements(resolution, bounds, extrapolation)
+        else:
+            resolution = _get_resolution(resolution, resolution_, bounds)
+            bounds = _get_bounds(bounds, resolution)
+            elements = staggered_elements(resolution, bounds, extrapolation)
+            if isinstance(values, math.Tensor):
+                if not spatial(values):
+                    values = expand_staggered(values, resolution, extrapolation)
+                if not all(extrapolation.valid_outer_faces(d)[0] != extrapolation.valid_outer_faces(d)[1] for d in resolution.names):  # non-uniform values required
+                    if values.shape.is_uniform:
+                        values = unstack_staggered_tensor(values, extrapolation)
+                    else:  # Keep dim order from data and check it matches resolution
+                        assert set(resolution_from_staggered_tensor(values, extrapolation)) == set(resolution), f"Failed to create StaggeredGrid: values {values.shape} do not match given resolution {resolution} for extrapolation {extrapolation}. See https://tum-pbs.github.io/PhiFlow/Staggered_Grids.html"
+            elif isinstance(values, Geometry):
+                values = reduce_sample(values, elements)
+            elif isinstance(values, Field):
+                values = reduce_sample(values, elements)
+            elif callable(values):
+                values = _sample_function(values, elements)
+                if elements.shape.shape.rank > 1:  # Different number of X and Y faces
+                    assert isinstance(values, TensorStack), f"values function must return a staggered Tensor but returned {type(values)}"
+                assert 'staggered_direction' in values.shape
+                if 'vector' in values.shape:
+                    values = math.stack([values.staggered_direction[i].vector[i] for i in range(resolution.rank)], channel(vector=resolution))
+                else:
+                    values = values.staggered_direction.as_channel('vector')
+            else:
+                values = expand_staggered(math.tensor(values), resolution, extrapolation)
+        if values.dtype.kind not in (float, complex):
+            values = math.to_float(values)
+        assert resolution.spatial_rank == bounds.spatial_rank, f"Resolution {resolution} does not match bounds {bounds}"
+        Grid.__init__(self, elements, values, extrapolation, resolution, bounds)
+
+    @property
+    def cells(self):
+        return GridCell(self.resolution, self.bounds)
+
+    def with_extrapolation(self, extrapolation: Extrapolation):
+        extrapolation = as_extrapolation(extrapolation)
+        if all([extrapolation.valid_outer_faces(dim) == self.extrapolation.valid_outer_faces(dim) for dim in self.resolution.names]):
+            return StaggeredGrid(self.values, extrapolation=extrapolation, bounds=self.bounds)
+        else:
+            values = []
+            for dim, component in zip(self.shape.spatial.names, self.values.unstack('vector')):
+                old_lo, old_hi = [int(v) for v in self.extrapolation.valid_outer_faces(dim)]
+                new_lo, new_hi = [int(v) for v in extrapolation.valid_outer_faces(dim)]
+                widths = (new_lo - old_lo, new_hi - old_hi)
+                values.append(math.pad(component, {dim: widths}, self.extrapolation, bounds=self.bounds))
+            values = math.stack(values, channel(vector=self.resolution))
+            return StaggeredGrid(values, extrapolation, bounds=self.bounds)
+
+    def _sample(self, geometry: Geometry, **kwargs) -> Tensor:
+        channels = [sample(component, geometry, **kwargs) for component in self.vector.unstack()]
+        return math.stack(channels, geometry.shape['vector'])
+
+    def closest_values(self, points: Geometry):
+        if 'staggered_direction' in points.shape:
+            points_ = points.unstack('staggered_direction')
+            channels = [component.closest_values(p) for p, component in zip(points_, self.vector.unstack())]
+        else:
+            channels = [component.closest_values(points) for component in self.vector.unstack()]
+        return math.stack(channels, points.shape['vector'])
+
+    def at_centers(self) -> CenteredGrid:
+        """
+        Interpolates the staggered values to the cell centers.
+
+        Returns:
+            `CenteredGrid` sampled at cell centers.
+        """
+        return CenteredGrid(self, resolution=self.resolution, bounds=self.bounds, extrapolation=self.extrapolation)
+
+    def __getitem__(self, item):
+        item = slicing_dict(self, item)
+        if not item:
+            return self
+        if 'vector' in item:
+            selection = item['vector']
+            if isinstance(selection, int):
+                item['vector'] = self.resolution.names[selection]
+        values = self._values[{dim: sel for dim, sel in item.items() if dim not in self.shape.spatial}]
+        for dim, sel in item.items():
+            if dim in self.shape.spatial:
+                raise AssertionError("Cannot slice StaggeredGrid along spatial dimensions.")
+                # sel = slice(sel, sel + 1) if isinstance(sel, int) else sel
+                # values = []
+                # for vdim, val in zip(self.shape.spatial.names, self.values.unstack('vector')):
+                #     if vdim == dim:
+                #         values.append(val[{dim: slice(sel.start, sel.stop + 1)}])
+                #     else:
+                #         values.append(val[{dim: sel}])
+                # values = math.stack(values, channel('vector'))
+        extrapolation = self._extrapolation[item]
+        bounds = GridCell(self._resolution, self._bounds)[item].bounds
+        if 'vector' in item:
+            selection = item['vector']
+            if isinstance(selection, str) and ',' in selection:
+                selection = parse_dim_order(selection)
+            if isinstance(selection, str):  # single item name
+                item_names = self.shape.get_item_names('vector', fallback_spatial=True)
+                assert selection in item_names, f"Accessing field.vector['{selection}'] failed. Item names are {item_names}."
+                selection = item_names.index(selection)
+            if isinstance(selection, int):
+                dim = self.resolution.names[selection]
+                comp_cells = GridCell(self.resolution, bounds).stagger(dim, *self.extrapolation.valid_outer_faces(dim))
+                return CenteredGrid(values, bounds=comp_cells.bounds, extrapolation=extrapolation)
+            else:
+                assert isinstance(selection, slice) and not selection.start and not selection.stop
+        return StaggeredGrid(values, bounds=bounds, extrapolation=extrapolation)
+
+    def uniform_values(self):
+        if self.values.shape.is_uniform:
+            return self.values
+        else:
+            return self.staggered_tensor()
+
+    def staggered_tensor(self) -> Tensor:
+        """
+        Stacks all component grids into a single uniform `phi.math.Tensor`.
+        The individual components are padded to a common (larger) shape before being stacked.
+        The shape of the returned tensor is exactly one cell larger than the grid `resolution` in every spatial dimension.
+
+        Returns:
+            Uniform `phi.math.Tensor`.
+        """
+        padded = []
+        for dim, component in zip(self.resolution.names, math.unstack(self.values, 'vector')):
+            widths = {d: (0, 1) for d in self.resolution.names}
+            lo_valid, up_valid = self.extrapolation.valid_outer_faces(dim)
+            widths[dim] = (int(not lo_valid), int(not up_valid))
+            padded.append(math.pad(component, widths, self.extrapolation[{'vector': dim}], bounds=self.bounds))
+        result = math.stack(padded, channel(vector=self.resolution))
+        assert result.shape.is_uniform
+        return result
+
+    def _op2(self, other, operator):
+        if isinstance(other, StaggeredGrid) and self.bounds == other.bounds and self.shape.spatial == other.shape.spatial:
+            values = operator(self._values, other.values)
+            extrapolation_ = operator(self._extrapolation, other.extrapolation)
+            return StaggeredGrid(values=values, extrapolation=extrapolation_, bounds=self.bounds)
+        else:
+            return SampledField._op2(self, other, operator)
+
+
+def unstack_staggered_tensor(data: Tensor, extrapolation: Extrapolation) -> TensorStack:
+    sliced = []
+    for dim, component in zip(data.shape.spatial.names, data.unstack('vector')):
+        lo_valid, up_valid = extrapolation.valid_outer_faces(dim)
+        slices = {d: slice(0, -1) for d in data.shape.spatial.names}
+        slices[dim] = slice(int(not lo_valid), - int(not up_valid) or None)
+        sliced.append(component[slices])
+    return math.stack(sliced, channel(vector=spatial(data)))
+
+
+def staggered_elements(resolution: Shape, bounds: Box, extrapolation: Extrapolation):
+    cells = GridCell(resolution, bounds)
+    grids = {}
+    for dim in bounds.vector.item_names:
+        lower, upper = extrapolation.valid_outer_faces(dim)
+        grids[dim] = cells.stagger(dim, lower, upper)
+    return geom.stack(grids, channel('staggered_direction'))
+
+
+def expand_staggered(values: Tensor, resolution: Shape, extrapolation: Extrapolation):
+    """ Add missing spatial dimensions to `values` """
+    cells = GridCell(resolution, Box(math.const_vec(0, resolution), math.const_vec(1, resolution)))
+    components = values.vector.unstack(resolution.spatial_rank)
+    tensors = []
+    for dim, component in zip(resolution.spatial.names, components):
+        comp_cells = cells.stagger(dim, *extrapolation.valid_outer_faces(dim))
+        tensors.append(math.expand(component, comp_cells.resolution))
+    return math.stack(tensors, channel(vector=resolution.names))
+
+
+def resolution_from_staggered_tensor(values: Tensor, extrapolation: Extrapolation):
+    any_dim = values.shape.spatial.names[0]
+    x_shape = values.shape.after_gather({'vector': any_dim})
+    ext_lower, ext_upper = extrapolation.valid_outer_faces(any_dim)
+    delta = int(ext_lower) + int(ext_upper) - 1
+    resolution = x_shape.spatial._replace_single_size(any_dim, x_shape.get_size(any_dim) - delta)
+    return resolution
+
+
+def _sample_function(f, elements: Geometry):
+    from phi.math._functional import get_function_parameters
+    try:
+        params = get_function_parameters(f)
+        dims = elements.shape.get_size('vector')
+        names_match = tuple(params.keys())[:dims] == elements.shape.get_item_names('vector')
+        num_positional = 0
+        has_varargs = False
+        for n, p in params.items():
+            if p.default is p.empty:
+                num_positional += 1
+            if p.kind == 2:  # _ParameterKind.VAR_POSITIONAL
+                has_varargs = True
+        assert num_positional <= dims, f"Cannot sample {f.__name__}({', '.join(tuple(params))}) on physical space {elements.shape.get_item_names('vector')}"
+        pass_varargs = has_varargs or names_match or num_positional > 1 or num_positional == dims
+        if num_positional > 1 and not has_varargs:
+            assert names_match, f"Positional arguments of {f.__name__}({', '.join(tuple(params))}) should match physical space {elements.shape.get_item_names('vector')}"
+    except ValueError as err:  # signature not available for all functions
+        pass_varargs = False
+    if pass_varargs:
+        values = math.map_s2b(f)(*elements.center.vector)
+    else:
+        values = math.map_s2b(f)(elements.center)
+    assert isinstance(values, math.Tensor), f"values function must return a Tensor but returned {type(values)}"
+    return values
+
+
+def _get_bounds(bounds: Union[Box, float, None], resolution: Shape):
+    if bounds is None:
+        return Box(math.const_vec(0, resolution), math.wrap(resolution, channel(vector=resolution.names)))
+    if isinstance(bounds, Box):
+        assert set(bounds.vector.item_names) == set(resolution.names), f"bounds dimensions {bounds.vector.item_names} must match resolution {resolution}"
+        return bounds
+    if isinstance(bounds, (int, float)):
+        return Box(math.const_vec(0, resolution), math.const_vec(bounds, resolution))
+    raise ValueError(f"bounds must be a Box, float or None but got {type(bounds).__name__}")
+
+
+def _get_resolution(resolution: Shape, resolution_: dict, bounds: Box):
+    assert 'boundaries' not in resolution_, "'boundaries' is not a valid grid argument. Use 'extrapolation' instead, passing a value or math.extrapolation.Extrapolation object. See https://tum-pbs.github.io/PhiFlow/phi/math/extrapolation.html"
+    if isinstance(resolution, int):
+        assert not resolution_, "Cannot specify keyword resolution and integer resolution at the same time."
+        resolution = spatial(**{dim: resolution for dim in bounds.size.shape.get_item_names('vector')})
+    try:
+        resolution_ = spatial(**resolution_)
+    except AssertionError as err:
+        raise ValueError(f"Invalid grid resolution: {', '.join(f'{dim}={size}' for dim, size in resolution_.items())}. Pass an int for all sizes.") from err
+    return (resolution or math.EMPTY_SHAPE) & resolution_
+
+
+def _dyadic_interpolate(grid: Tensor, interpolation_dirs: List, padding: Extrapolation, order: int, implicit: Solve):
+    """
+    Samples a sub-grid from `grid` with an offset of half a grid cell in directions defined by `interpolation_dirs`.
+
+    Args:
+        grid: `Tensor` to be resampled.
+        interpolation_dirs: List which defines for every spatial dimension of `grid` if interpolation should be performed,
+            in positive direction `1` / negative direction `-1` / no interpolation`0`
+            len(interpolation_dirs) == len(grid.shape.spatial.names) is assumed
+            Example: With `grid.shape.spatial.names=['x', 'y']` and `interpolation_dirs: [1, -1]`
+                     grid will be interpolated half a grid cell in positive x direction and half a grid cell in negative y direction
+        padding: Extrapolation used for the needed out of Domain values
+        order: finite difference `Scheme` used for interpolation
+
+    Returns:
+      Sub-grid as `Tensor`
+    """
+    if implicit:
+        if order == 6:
+            values, needed_shifts = [1 / 20, 3 / 4, 3 / 4, 1 / 20], (-1, 0, 1, 2)
+            values_rhs, needed_shifts_rhs = [3 / 10, 1, 3 / 10], (-1, 0, 1)
+        else:
+            return NotImplemented
+    else:
+        return NotImplemented
+    result = grid
+    for dim, dir in zip(grid.shape.spatial.names, interpolation_dirs):
+        if dir == 0: continue
+        is_neg_dir = dir == -1
+        current_widths = [abs(min(needed_shifts)) + is_neg_dir, max(needed_shifts) - is_neg_dir]
+        padded = math.pad(result, {dim: tuple(current_widths)}, padding)
+        shifted = math.shift(padded, needed_shifts, [dim], padding=None, stack_dim=None)
+        result = sum([value * shift_ for value, shift_ in zip(values, shifted)])
+        if implicit:
+            implicit.x0 = result
+            result = math.solve_linear(dyadic_interpolate_lhs, result, implicit, values_rhs=values_rhs, needed_shifts_rhs=needed_shifts_rhs, dim=dim, padding=padding)
+    return result
+
+
+@math.jit_compile_linear(auxiliary_args="values_rhs, needed_shifts_rhs")
+def dyadic_interpolate_lhs(x, values_rhs, needed_shifts_rhs, dim, padding):
+    shifted = math.shift(x, needed_shifts_rhs, stack_dim=None, dims=[dim], padding=padding)
+    return sum([value * shift_ for value, shift_ in zip(values_rhs, shifted)])
```

### Comparing `phiflow-2.3.4/phi/field/_mask.py` & `phiflow-2.4.0/phi/field/_mask.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,44 +1,44 @@
-import warnings
-from typing import Union
-
-from phi import math
-from phi.geom import Geometry
-from ._field import Field
-from ..math import Tensor
-
-
-class HardGeometryMask(Field):
-    """
-    Deprecated since version 1.3. Use `phi.field.mask()` or `phi.field.resample()` instead.
-    """
-
-    def __init__(self, geometry: Geometry):
-        warnings.warn("HardGeometryMask and SoftGeometryMask are deprecated. Use field.mask or field.resample instead.", DeprecationWarning, stacklevel=2)
-        assert isinstance(geometry, Geometry)
-        self.geometry = geometry
-
-    @property
-    def shape(self):
-        return self.geometry.shape.non_channel
-
-    def _sample(self, geometry: Geometry, **kwargs) -> Tensor:
-        return math.to_float(self.geometry.lies_inside(geometry.center))
-
-    def __getitem__(self, item: dict):
-        return HardGeometryMask(self.geometry[item])
-
-
-class SoftGeometryMask(HardGeometryMask):
-    """
-    Deprecated since version 1.3. Use `phi.field.mask()` or `phi.field.resample()` instead.
-    """
-    def __init__(self, geometry: Geometry, balance: Union[Tensor, float] = 0.5):
-        warnings.warn("HardGeometryMask and SoftGeometryMask are deprecated. Use field.mask or field.resample instead.", DeprecationWarning, stacklevel=2)
-        super().__init__(geometry)
-        self.balance = balance
-
-    def _sample(self, geometry: Geometry, **kwargs) -> Tensor:
-        return self.geometry.approximate_fraction_inside(geometry, self.balance)
-
-    def __getitem__(self, item: dict):
-        return SoftGeometryMask(self.geometry[item], self.balance)
+import warnings
+from typing import Union
+
+from phi import math
+from phi.geom import Geometry
+from ._field import Field
+from ..math import Tensor
+
+
+class HardGeometryMask(Field):
+    """
+    Deprecated since version 1.3. Use `phi.field.mask()` or `phi.field.resample()` instead.
+    """
+
+    def __init__(self, geometry: Geometry):
+        warnings.warn("HardGeometryMask and SoftGeometryMask are deprecated. Use field.mask or field.resample instead.", DeprecationWarning, stacklevel=2)
+        assert isinstance(geometry, Geometry)
+        self.geometry = geometry
+
+    @property
+    def shape(self):
+        return self.geometry.shape.non_channel
+
+    def _sample(self, geometry: Geometry, **kwargs) -> Tensor:
+        return math.to_float(self.geometry.lies_inside(geometry.center))
+
+    def __getitem__(self, item: dict):
+        return HardGeometryMask(self.geometry[item])
+
+
+class SoftGeometryMask(HardGeometryMask):
+    """
+    Deprecated since version 1.3. Use `phi.field.mask()` or `phi.field.resample()` instead.
+    """
+    def __init__(self, geometry: Geometry, balance: Union[Tensor, float] = 0.5):
+        warnings.warn("HardGeometryMask and SoftGeometryMask are deprecated. Use field.mask or field.resample instead.", DeprecationWarning, stacklevel=2)
+        super().__init__(geometry)
+        self.balance = balance
+
+    def _sample(self, geometry: Geometry, **kwargs) -> Tensor:
+        return self.geometry.approximate_fraction_inside(geometry, self.balance)
+
+    def __getitem__(self, item: dict):
+        return SoftGeometryMask(self.geometry[item], self.balance)
```

### Comparing `phiflow-2.3.4/phi/field/_noise.py` & `phiflow-2.4.0/phi/field/_noise.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,64 +1,64 @@
-import warnings
-
-from phi import math
-from phi.geom import GridCell, Geometry
-from phi.math import random_normal, Tensor, channel
-from ._field import Field
-
-
-class Noise(Field):
-    """
-    Generates random noise fluctuations which can be configured in physical size and smoothness.
-    Each time values are sampled from a Noise field, a new noise field is generated.
-
-    Noise is typically used as an initializer for CenteredGrids or StaggeredGrids.
-    """
-
-    def __init__(self, *shape: math.Shape, scale=10., smoothness=1.0, **channel_dims):
-        """
-        Args:
-          shape: Batch and channel dimensions. Spatial dimensions will be added automatically once sampled on a grid.
-          scale: Size of noise fluctuations in physical units.
-          smoothness: Determines how quickly high frequencies die out.
-          **dims: Additional dimensions, added to `shape`.
-        """
-        self.scale = scale
-        self.smoothness = smoothness
-        self._shape = math.concat_shapes(*shape, channel(**channel_dims))
-
-    @property
-    def shape(self):
-        return self._shape
-
-    def _sample(self, geometry: Geometry, **kwargs) -> Tensor:
-        if isinstance(geometry, GridCell):
-            return self.grid_sample(geometry.resolution, geometry.grid_size)
-        raise NotImplementedError(f"{type(geometry)} not supported. Only GridCell allowed.")
-
-    def grid_sample(self, resolution: math.Shape, size, shape: math.Shape = None):
-        shape = (self._shape if shape is None else shape) & resolution
-        for dim in channel(self._shape):
-            if dim.item_names[0] is None:
-                warnings.warn(f"Please provide item names for Noise dim {dim} using {dim}='x,y,z'", FutureWarning)
-                shape &= channel(**{dim.name: resolution.names})
-        rndj = math.to_complex(random_normal(shape)) + 1j * math.to_complex(random_normal(shape))  # Note: there is no complex32
-        # --- Compute 1 / k^2 ---
-        k_vec = math.fftfreq(resolution) * resolution / math.tensor(size) * math.tensor(self.scale)  # in physical units
-        k2 = math.vec_squared(k_vec)
-        lowest_frequency = 0.1
-        weight_mask = math.to_float(k2 > lowest_frequency)
-        inv_k2 = math.divide_no_nan(1, k2)
-        # --- Compute result ---
-        fft = rndj * inv_k2 ** self.smoothness * weight_mask
-        array = math.real(math.ifft(fft))
-        array /= math.std(array, dim=array.shape.non_batch)
-        array -= math.mean(array, dim=array.shape.non_batch)
-        array = math.to_float(array)
-        return array
-
-    def __getitem__(self, item: dict):
-        new_shape = self.shape.after_gather(item)
-        return Noise(new_shape, scale=self.scale, smoothness=self.smoothness)
-
-    def __repr__(self):
-        return f"{self._shape}, scale={self.scale}, smoothness={self.smoothness}"
+import warnings
+
+from phi import math
+from phi.geom import GridCell, Geometry
+from phi.math import random_normal, Tensor, channel
+from ._field import Field
+
+
+class Noise(Field):
+    """
+    Generates random noise fluctuations which can be configured in physical size and smoothness.
+    Each time values are sampled from a Noise field, a new noise field is generated.
+
+    Noise is typically used as an initializer for CenteredGrids or StaggeredGrids.
+    """
+
+    def __init__(self, *shape: math.Shape, scale=10., smoothness=1.0, **channel_dims):
+        """
+        Args:
+          shape: Batch and channel dimensions. Spatial dimensions will be added automatically once sampled on a grid.
+          scale: Size of noise fluctuations in physical units.
+          smoothness: Determines how quickly high frequencies die out.
+          **dims: Additional dimensions, added to `shape`.
+        """
+        self.scale = scale
+        self.smoothness = smoothness
+        self._shape = math.concat_shapes(*shape, channel(**channel_dims))
+
+    @property
+    def shape(self):
+        return self._shape
+
+    def _sample(self, geometry: Geometry, **kwargs) -> Tensor:
+        if isinstance(geometry, GridCell):
+            return self.grid_sample(geometry.resolution, geometry.grid_size)
+        raise NotImplementedError(f"{type(geometry)} not supported. Only GridCell allowed.")
+
+    def grid_sample(self, resolution: math.Shape, size, shape: math.Shape = None):
+        shape = (self._shape if shape is None else shape) & resolution
+        for dim in channel(self._shape):
+            if dim.item_names[0] is None:
+                warnings.warn(f"Please provide item names for Noise dim {dim} using {dim}='x,y,z'", FutureWarning)
+                shape &= channel(**{dim.name: resolution.names})
+        rndj = math.to_complex(random_normal(shape)) + 1j * math.to_complex(random_normal(shape))  # Note: there is no complex32
+        # --- Compute 1 / k^2 ---
+        k_vec = math.fftfreq(resolution) * resolution / math.tensor(size) * math.tensor(self.scale)  # in physical units
+        k2 = math.vec_squared(k_vec)
+        lowest_frequency = 0.1
+        weight_mask = math.to_float(k2 > lowest_frequency)
+        inv_k2 = math.divide_no_nan(1, k2)
+        # --- Compute result ---
+        fft = rndj * inv_k2 ** self.smoothness * weight_mask
+        array = math.real(math.ifft(fft))
+        array /= math.std(array, dim=array.shape.non_batch)
+        array -= math.mean(array, dim=array.shape.non_batch)
+        array = math.to_float(array)
+        return array
+
+    def __getitem__(self, item: dict):
+        new_shape = self.shape.after_gather(item)
+        return Noise(new_shape, scale=self.scale, smoothness=self.smoothness)
+
+    def __repr__(self):
+        return f"{self._shape}, scale={self.scale}, smoothness={self.smoothness}"
```

### Comparing `phiflow-2.3.4/phi/field/_point_cloud.py` & `phiflow-2.4.0/phi/field/_point_cloud.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,250 +1,252 @@
-import warnings
-from typing import Any, Tuple, Union
-
-from phi.math import wrap, expand, non_batch, extrapolation, spatial
-
-from phi import math
-from phi.geom import Geometry, GridCell, Box, Point
-from ._field import SampledField, resample
-from ..geom._stack import GeometryStack
-from ..math import Tensor, instance, Shape
-from ..math._tensors import may_vary_along
-from ..math.extrapolation import Extrapolation, ConstantExtrapolation, PERIODIC
-from ..math.magic import slicing_dict
-
-
-class PointCloud(SampledField):
-    """
-    A `PointCloud` comprises:
-
-    * `elements`: a `Geometry` representing all points or volumes
-    * `values`: a `Tensor` representing the values corresponding to `elements`
-    * `extrapolation`: an `Extrapolation` defining the field value outside of `values`
-
-    The points / elements of the `PointCloud` are listed along *instance* or *spatial* dimensions of `elements`.
-    These dimensions are automatically added to `values` if not already present.
-
-    When sampling or resampling a `PointCloud`, the following keyword arguments can be specified.
-
-    * `soft`: default=False.
-      If `True`, interpolates smoothly from 1 to 0 between the inside and outside of elements.
-      If `False`, only the center position of the new representation elements is checked against the point cloud elements.
-    * `scatter`: default=False.
-      If `True`, scattering will be used to sample the point cloud onto grids. Then, each element of the point cloud can only affect a single cell. This is only recommended when the points are much smaller than the cells.
-    * `outside_handling`: default='discard'. One of `'discard'`, `'clamp'`, `'undefined'`.
-    * `balance`: default=0.5. Only used when `soft=True`.
-      See the description in `phi.geom.Geometry.approximate_fraction_inside()`.
-
-    See the `phi.field` module documentation at https://tum-pbs.github.io/PhiFlow/Fields.html
-    """
-
-    def __init__(self,
-                 elements: Union[Tensor, Geometry],
-                 values: Any = 1.,
-                 extrapolation: Union[Extrapolation, float] = 0.,
-                 add_overlapping=False,
-                 bounds: Box = None):
-        """
-        Args:
-          elements: `Tensor` or `Geometry` object specifying the sample points and sizes
-          values: values corresponding to elements
-          extrapolation: values outside elements
-          add_overlapping: True: values of overlapping geometries are summed. False: values between overlapping geometries are interpolated
-          bounds: (optional) size of the fixed domain in which the points should get visualized. None results in max and min coordinates of points.
-        """
-        SampledField.__init__(self, elements, expand(wrap(values), non_batch(elements).non_channel), extrapolation, bounds)
-        assert self._extrapolation is PERIODIC or isinstance(self._extrapolation, ConstantExtrapolation), f"Unsupported extrapolation for PointCloud: {self._extrapolation}"
-        self._add_overlapping = add_overlapping
-
-    @property
-    def shape(self):
-        return self._elements.shape.without('vector') & self._values.shape
-
-    def __getitem__(self, item):
-        item = slicing_dict(self, item)
-        if not item:
-            return self
-        elements = self.elements[{dim: selection for dim, selection in item.items() if dim != 'vector'}]
-        values = self._values[item]
-        extrapolation = self._extrapolation[item]
-        return PointCloud(elements, values, extrapolation, self._add_overlapping, self._bounds)
-
-    def with_elements(self, elements: Geometry):
-        return PointCloud(elements=elements, values=self.values, extrapolation=self.extrapolation, add_overlapping=self._add_overlapping, bounds=self._bounds)
-
-    def shifted(self, delta):
-        return self.with_elements(self.elements.shifted(delta))
-
-    def with_values(self, values):
-        return PointCloud(elements=self.elements, values=values, extrapolation=self.extrapolation, add_overlapping=self._add_overlapping, bounds=self._bounds)
-
-    def with_extrapolation(self, extrapolation: Extrapolation):
-        return PointCloud(elements=self.elements, values=self.values, extrapolation=extrapolation, add_overlapping=self._add_overlapping, bounds=self._bounds)
-
-    def with_bounds(self, bounds: Box):
-        return PointCloud(elements=self.elements, values=self.values, extrapolation=self.extrapolation, add_overlapping=self._add_overlapping, bounds=bounds)
-
-    def __value_attrs__(self):
-        return '_values', '_extrapolation'
-
-    def __variable_attrs__(self):
-        return '_values', '_elements'
-
-    def __expand__(self, dims: Shape, **kwargs) -> 'PointCloud':
-        return self.with_values(math.expand(self.values, dims, **kwargs))
-
-    def __replace_dims__(self, dims: Tuple[str, ...], new_dims: Shape, **kwargs) -> 'PointCloud':
-        elements = math.rename_dims(self.elements, dims, new_dims)
-        values = math.rename_dims(self.values, dims, new_dims)
-        extrapolation = math.rename_dims(self.extrapolation, dims, new_dims, **kwargs)
-        return PointCloud(elements, values, extrapolation, self._add_overlapping, self._bounds)
-
-    def __eq__(self, other):
-        if not type(self) == type(other):
-            return False
-        # Check everything but __variable_attrs__ (values): elements type, extrapolation, add_overlapping
-        if type(self.elements) is not type(other.elements):
-            return False
-        if self.extrapolation != other.extrapolation:
-            return False
-        if self._add_overlapping != other._add_overlapping:
-            return False
-        if self.values is None:
-            return other.values is None
-        if other.values is None:
-            return False
-        if not math.all_available(self.values) or not math.all_available(other.values):  # tracers involved
-            if math.all_available(self.values) != math.all_available(other.values):
-                return False
-            else:  # both tracers
-                return self.values.shape == other.values.shape
-        return bool((self.values == other.values).all)
-
-    @property
-    def bounds(self) -> Box:
-        if self._bounds is not None:
-            return self._bounds
-        else:
-            from phi.field._field_math import data_bounds
-            bounds = data_bounds(self.elements.center)
-            radius = math.max(self.elements.bounding_radius())
-            return Box(bounds.lower - radius, bounds.upper + radius)
-
-    def _sample(self, geometry: Geometry, soft=False, scatter=False, outside_handling='discard', balance=0.5) -> Tensor:
-        if geometry == self.elements:
-            return self.values
-        if isinstance(geometry, GeometryStack):
-            sampled = [self._sample(g, soft, scatter, outside_handling, balance) for g in geometry.geometries]
-            return math.stack(sampled, geometry.geometries.shape)
-        if self.extrapolation is extrapolation.PERIODIC:
-            raise NotImplementedError("Periodic PointClouds not yet supported")
-        if isinstance(geometry, GridCell) and scatter:
-            assert not soft, "Cannot soft-sample when scatter=True"
-            return self.grid_scatter(geometry.bounds, geometry.resolution, outside_handling)
-        else:
-            assert not isinstance(self._elements, Point), "Cannot sample Point-like elements with scatter=False"
-            if may_vary_along(self._values, instance(self._values) & spatial(self._values)):
-                raise NotImplementedError("Non-scatter resampling not yet supported for varying values")
-            idx0 = (instance(self._values) & spatial(self._values)).first_index()
-            outside = self._extrapolation.value if isinstance(self._extrapolation, ConstantExtrapolation) else 0
-            if soft:
-                frac_inside = self.elements.approximate_fraction_inside(geometry, balance)
-                return frac_inside * self._values[idx0] + (1 - frac_inside) * outside
-            else:
-                return math.where(self.elements.lies_inside(geometry.center), self._values[idx0], outside)
-
-    def grid_scatter(self, bounds: Box, resolution: math.Shape, outside_handling: str):
-        """
-        Approximately samples this field on a regular grid using math.scatter().
-
-        Args:
-            outside_handling: `str` passed to `phi.math.scatter()`.
-            bounds: physical dimensions of the grid
-            resolution: grid resolution
-
-        Returns:
-            `CenteredGrid`
-        """
-        closest_index = bounds.global_to_local(self.points) * resolution - 0.5
-        mode = 'add' if self._add_overlapping else 'mean'
-        base = math.zeros(resolution)
-        if isinstance(self._extrapolation, ConstantExtrapolation):
-            base += self._extrapolation.value
-        scattered = math.scatter(base, closest_index, self.values, mode=mode, outside_handling=outside_handling)
-        return scattered
-
-    def __repr__(self):
-        try:
-            return "PointCloud[%s]" % (self.shape,)
-        except:
-            return "PointCloud[invalid]"
-
-    def __and__(self, other):
-        assert isinstance(other, PointCloud)
-        assert instance(self).rank == instance(other).rank == 1, f"Can only use & on PointClouds that have a single instance dimension but got shapes {self.shape} & {other.shape}"
-        from ._field_math import concat
-        return concat([self, other], instance(self))
-
-
-def nonzero(field: SampledField):
-    indices = math.nonzero(field.values, list_dim=instance('points'))
-    elements = field.elements[indices]
-    return PointCloud(elements, values=math.tensor(1.), extrapolation=math.extrapolation.ZERO, add_overlapping=False, bounds=field.bounds)
-
-
-def distribute_points(geometries: Union[tuple, list, Geometry, float],
-                      dim: Shape = instance('points'),
-                      points_per_cell: int = 8,
-                      center: bool = False,
-                      radius: float = None,
-                      extrapolation: Union[float, Extrapolation] = math.NAN,
-                      **domain) -> PointCloud:
-    """
-    Transforms `Geometry` objects into a PointCloud.
-
-    Args:
-        geometries: Geometry objects marking the cells which should contain points
-        dim: Dimension along which the points are listed.
-        points_per_cell: Number of points for each cell of `geometries`
-        center: Set all points to the center of the grid cells.
-        radius: Sphere radius.
-        extrapolation: Extrapolation for the `PointCloud`, default `NaN` used for FLIP.
-
-    Returns:
-         PointCloud representation of `geometries`.
-    """
-    warnings.warn("distribute_points() is deprecated. Construct a PointCloud directly.", DeprecationWarning)
-    from phi.field import CenteredGrid
-    if isinstance(geometries, (tuple, list, Geometry)):
-        from phi.geom import union
-        geometries = union(geometries)
-    geometries = resample(geometries, CenteredGrid(0, extrapolation, **domain), scatter=False)
-    initial_points = _distribute_points(geometries.values, dim, points_per_cell, center=center)
-    if radius is None:
-        from phi.field._field_math import data_bounds
-        radius = math.mean(data_bounds(initial_points).size) * 0.005
-    from phi.geom import Sphere
-    return PointCloud(Sphere(initial_points, radius=radius), extrapolation=geometries.extrapolation, bounds=geometries.bounds)
-
-
-def _distribute_points(mask: math.Tensor, dim: Shape, points_per_cell: int = 1, center: bool = False) -> math.Tensor:
-    """
-    Generates points (either uniformly distributed or at the cell centers) according to the given tensor mask.
-
-    Args:
-        mask: Tensor with nonzero values at the indices where particles should get generated.
-        points_per_cell: Number of particles to generate at each marked index
-        center: Set points to cell centers. If False, points will be distributed using a uniform
-            distribution within each cell.
-
-    Returns:
-        A tensor containing the positions of the generated points.
-    """
-    indices = math.to_float(math.nonzero(mask, list_dim=dim))
-    temp = []
-    for _ in range(points_per_cell):
-        if center:
-            temp.append(indices + 0.5)
-        else:
-            temp.append(indices + (math.random_uniform(indices.shape)))
-    return math.concat(temp, dim=dim)
+import warnings
+from typing import Any, Tuple, Union
+
+from phi.math import wrap, expand, non_batch, extrapolation, spatial
+
+from phi import math
+from phi.geom import Geometry, GridCell, Box, Point
+from ._field import SampledField, resample
+from ..geom._stack import GeometryStack
+from ..math import Tensor, instance, Shape
+from ..math._tensors import may_vary_along
+from ..math.extrapolation import Extrapolation, ConstantExtrapolation, PERIODIC
+from ..math.magic import slicing_dict
+
+
+class PointCloud(SampledField):
+    """
+    A `PointCloud` comprises:
+
+    * `elements`: a `Geometry` representing all points or volumes
+    * `values`: a `Tensor` representing the values corresponding to `elements`
+    * `extrapolation`: an `Extrapolation` defining the field value outside of `values`
+
+    The points / elements of the `PointCloud` are listed along *instance* or *spatial* dimensions of `elements`.
+    These dimensions are automatically added to `values` if not already present.
+
+    When sampling or resampling a `PointCloud`, the following keyword arguments can be specified.
+
+    * `soft`: default=False.
+      If `True`, interpolates smoothly from 1 to 0 between the inside and outside of elements.
+      If `False`, only the center position of the new representation elements is checked against the point cloud elements.
+    * `scatter`: default=False.
+      If `True`, scattering will be used to sample the point cloud onto grids. Then, each element of the point cloud can only affect a single cell. This is only recommended when the points are much smaller than the cells.
+    * `outside_handling`: default='discard'. One of `'discard'`, `'clamp'`, `'undefined'`.
+    * `balance`: default=0.5. Only used when `soft=True`.
+      See the description in `phi.geom.Geometry.approximate_fraction_inside()`.
+
+    See the `phi.field` module documentation at https://tum-pbs.github.io/PhiFlow/Fields.html
+    """
+
+    def __init__(self,
+                 elements: Union[Tensor, Geometry],
+                 values: Any = 1.,
+                 extrapolation: Union[Extrapolation, float] = 0.,
+                 add_overlapping=False,
+                 bounds: Box = None):
+        """
+        Args:
+          elements: `Tensor` or `Geometry` object specifying the sample points and sizes
+          values: values corresponding to elements
+          extrapolation: values outside elements
+          add_overlapping: True: values of overlapping geometries are summed. False: values between overlapping geometries are interpolated
+          bounds: (optional) size of the fixed domain in which the points should get visualized. None results in max and min coordinates of points.
+        """
+        SampledField.__init__(self, elements, expand(wrap(values), non_batch(elements).non_channel), extrapolation, bounds)
+        assert self._extrapolation is PERIODIC or isinstance(self._extrapolation, ConstantExtrapolation), f"Unsupported extrapolation for PointCloud: {self._extrapolation}"
+        self._add_overlapping = add_overlapping
+
+    @property
+    def shape(self):
+        return self._elements.shape.without('vector') & self._values.shape
+
+    def __getitem__(self, item):
+        item = slicing_dict(self, item)
+        if not item:
+            return self
+        item_without_vec = {dim: selection for dim, selection in item.items() if dim != 'vector'}
+        elements = self.elements[item_without_vec]
+        values = self._values[item]
+        extrapolation = self._extrapolation[item]
+        bounds = self._bounds[item_without_vec] if self._bounds is not None else None
+        return PointCloud(elements, values, extrapolation, self._add_overlapping, bounds)
+
+    def with_elements(self, elements: Geometry):
+        return PointCloud(elements=elements, values=self.values, extrapolation=self.extrapolation, add_overlapping=self._add_overlapping, bounds=self._bounds)
+
+    def shifted(self, delta):
+        return self.with_elements(self.elements.shifted(delta))
+
+    def with_values(self, values):
+        return PointCloud(elements=self.elements, values=values, extrapolation=self.extrapolation, add_overlapping=self._add_overlapping, bounds=self._bounds)
+
+    def with_extrapolation(self, extrapolation: Extrapolation):
+        return PointCloud(elements=self.elements, values=self.values, extrapolation=extrapolation, add_overlapping=self._add_overlapping, bounds=self._bounds)
+
+    def with_bounds(self, bounds: Box):
+        return PointCloud(elements=self.elements, values=self.values, extrapolation=self.extrapolation, add_overlapping=self._add_overlapping, bounds=bounds)
+
+    def __value_attrs__(self):
+        return '_values', '_extrapolation'
+
+    def __variable_attrs__(self):
+        return '_values', '_elements'
+
+    def __expand__(self, dims: Shape, **kwargs) -> 'PointCloud':
+        return self.with_values(expand(self.values, dims, **kwargs))
+
+    def __replace_dims__(self, dims: Tuple[str, ...], new_dims: Shape, **kwargs) -> 'PointCloud':
+        elements = math.rename_dims(self.elements, dims, new_dims)
+        values = math.rename_dims(self.values, dims, new_dims)
+        extrapolation = math.rename_dims(self.extrapolation, dims, new_dims, **kwargs)
+        return PointCloud(elements, values, extrapolation, self._add_overlapping, self._bounds)
+
+    def __eq__(self, other):
+        if not type(self) == type(other):
+            return False
+        # Check everything but __variable_attrs__ (values): elements type, extrapolation, add_overlapping
+        if type(self.elements) is not type(other.elements):
+            return False
+        if self.extrapolation != other.extrapolation:
+            return False
+        if self._add_overlapping != other._add_overlapping:
+            return False
+        if self.values is None:
+            return other.values is None
+        if other.values is None:
+            return False
+        if not math.all_available(self.values) or not math.all_available(other.values):  # tracers involved
+            if math.all_available(self.values) != math.all_available(other.values):
+                return False
+            else:  # both tracers
+                return self.values.shape == other.values.shape
+        return bool((self.values == other.values).all)
+
+    @property
+    def bounds(self) -> Box:
+        if self._bounds is not None:
+            return self._bounds
+        else:
+            from phi.field._field_math import data_bounds
+            bounds = data_bounds(self.elements.center)
+            radius = math.max(self.elements.bounding_radius())
+            return Box(bounds.lower - radius, bounds.upper + radius)
+
+    def _sample(self, geometry: Geometry, soft=False, scatter=False, outside_handling='discard', balance=0.5) -> Tensor:
+        if geometry == self.elements:
+            return self.values
+        if isinstance(geometry, GeometryStack):
+            sampled = [self._sample(g, soft, scatter, outside_handling, balance) for g in geometry.geometries]
+            return math.stack(sampled, geometry.geometries.shape)
+        if self.extrapolation is extrapolation.PERIODIC:
+            raise NotImplementedError("Periodic PointClouds not yet supported")
+        if isinstance(geometry, GridCell) and scatter:
+            assert not soft, "Cannot soft-sample when scatter=True"
+            return self.grid_scatter(geometry.bounds, geometry.resolution, outside_handling)
+        else:
+            assert not isinstance(self._elements, Point), "Cannot sample Point-like elements with scatter=False"
+            if may_vary_along(self._values, instance(self._values) & spatial(self._values)):
+                raise NotImplementedError("Non-scatter resampling not yet supported for varying values")
+            idx0 = (instance(self._values) & spatial(self._values)).first_index()
+            outside = self._extrapolation.value if isinstance(self._extrapolation, ConstantExtrapolation) else 0
+            if soft:
+                frac_inside = self.elements.approximate_fraction_inside(geometry, balance)
+                return frac_inside * self._values[idx0] + (1 - frac_inside) * outside
+            else:
+                return math.where(self.elements.lies_inside(geometry.center), self._values[idx0], outside)
+
+    def grid_scatter(self, bounds: Box, resolution: math.Shape, outside_handling: str):
+        """
+        Approximately samples this field on a regular grid using math.scatter().
+
+        Args:
+            outside_handling: `str` passed to `phi.math.scatter()`.
+            bounds: physical dimensions of the grid
+            resolution: grid resolution
+
+        Returns:
+            `CenteredGrid`
+        """
+        closest_index = bounds.global_to_local(self.points) * resolution - 0.5
+        mode = 'add' if self._add_overlapping else 'mean'
+        base = math.zeros(resolution)
+        if isinstance(self._extrapolation, ConstantExtrapolation):
+            base += self._extrapolation.value
+        scattered = math.scatter(base, closest_index, self.values, mode=mode, outside_handling=outside_handling)
+        return scattered
+
+    def __repr__(self):
+        try:
+            return "PointCloud[%s]" % (self.shape,)
+        except:
+            return "PointCloud[invalid]"
+
+    def __and__(self, other):
+        assert isinstance(other, PointCloud)
+        assert instance(self).rank == instance(other).rank == 1, f"Can only use & on PointClouds that have a single instance dimension but got shapes {self.shape} & {other.shape}"
+        from ._field_math import concat
+        return concat([self, other], instance(self))
+
+
+def nonzero(field: SampledField):
+    indices = math.nonzero(field.values, list_dim=instance('points'))
+    elements = field.elements[indices]
+    return PointCloud(elements, values=math.tensor(1.), extrapolation=math.extrapolation.ZERO, add_overlapping=False, bounds=field.bounds)
+
+
+def distribute_points(geometries: Union[tuple, list, Geometry, float],
+                      dim: Shape = instance('points'),
+                      points_per_cell: int = 8,
+                      center: bool = False,
+                      radius: float = None,
+                      extrapolation: Union[float, Extrapolation] = math.NAN,
+                      **domain) -> PointCloud:
+    """
+    Transforms `Geometry` objects into a PointCloud.
+
+    Args:
+        geometries: Geometry objects marking the cells which should contain points
+        dim: Dimension along which the points are listed.
+        points_per_cell: Number of points for each cell of `geometries`
+        center: Set all points to the center of the grid cells.
+        radius: Sphere radius.
+        extrapolation: Extrapolation for the `PointCloud`, default `NaN` used for FLIP.
+
+    Returns:
+         PointCloud representation of `geometries`.
+    """
+    warnings.warn("distribute_points() is deprecated. Construct a PointCloud directly.", DeprecationWarning)
+    from phi.field import CenteredGrid
+    if isinstance(geometries, (tuple, list, Geometry)):
+        from phi.geom import union
+        geometries = union(geometries)
+    geometries = resample(geometries, CenteredGrid(0, extrapolation, **domain), scatter=False)
+    initial_points = _distribute_points(geometries.values, dim, points_per_cell, center=center)
+    if radius is None:
+        from phi.field._field_math import data_bounds
+        radius = math.mean(data_bounds(initial_points).size) * 0.005
+    from phi.geom import Sphere
+    return PointCloud(Sphere(initial_points, radius=radius), extrapolation=geometries.extrapolation, bounds=geometries.bounds)
+
+
+def _distribute_points(mask: math.Tensor, dim: Shape, points_per_cell: int = 1, center: bool = False) -> math.Tensor:
+    """
+    Generates points (either uniformly distributed or at the cell centers) according to the given tensor mask.
+
+    Args:
+        mask: Tensor with nonzero values at the indices where particles should get generated.
+        points_per_cell: Number of particles to generate at each marked index
+        center: Set points to cell centers. If False, points will be distributed using a uniform
+            distribution within each cell.
+
+    Returns:
+        A tensor containing the positions of the generated points.
+    """
+    indices = math.to_float(math.nonzero(mask, list_dim=dim))
+    temp = []
+    for _ in range(points_per_cell):
+        if center:
+            temp.append(indices + 0.5)
+        else:
+            temp.append(indices + (math.random_uniform(indices.shape)))
+    return math.concat(temp, dim=dim)
```

### Comparing `phiflow-2.3.4/phi/field/_scene.py` & `phiflow-2.4.0/phi/field/_scene.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,559 +1,574 @@
-import inspect
-import json
-import os
-import re
-import shutil
-import sys
-import warnings
-from os.path import join, isfile, isdir, abspath, expanduser, basename, split
-from typing import Tuple, Union
-
-import numpy as np
-
-from phi import math, __version__ as phi_version
-from ._field import SampledField
-from ._field_io import read, write
-from ..math import Shape, batch, stack, unpack_dim, wrap
-from ..math.magic import BoundDim
-
-
-typing_list = list
-
-
-def _filename(simpath, name, frame):
-    return join(simpath, f"{slugify(name)}_{frame:06d}.npz")
-
-
-def _str(bytes_or_str):  # on Linux, os.listdir returns bytes instead of strings
-    if isinstance(bytes_or_str, str):
-        return bytes_or_str
-    else:
-        return str(bytes_or_str, 'utf-8')
-
-
-def get_fieldnames(simpath) -> tuple:
-    fieldnames_set = {_str(f)[:-11] for f in os.listdir(simpath) if _str(f).endswith(".npz")}
-    return tuple(sorted(fieldnames_set))
-
-
-def get_frames(path: str, field_name: str = None, mode=set.intersection) -> tuple:
-    if field_name is not None:
-        all_frames = {int(f[-10:-4]) for f in os.listdir(path) if _str(f).startswith(field_name) and _str(f).endswith(".npz")}
-        return tuple(sorted(all_frames))
-    else:
-        fields = get_fieldnames(path)
-        if not fields:
-            return ()
-        frames_sets = [set(get_frames(path, field)) for field in fields]
-        frames = mode(*frames_sets)
-        return tuple(sorted(frames))
-
-
-class Scene:
-    """
-    Provides methods for reading and writing simulation data.
-
-    See the format documentation at https://tum-pbs.github.io/PhiFlow/Scene_Format_Specification.html .
-
-    All data of a `Scene` is located inside a single directory with name `sim_xxxxxx` where `xxxxxx` is the `id`.
-    The data of the scene is organized into NumPy files by *name* and *frame*.
-
-    To create a new scene, use `Scene.create()`.
-    To reference an existing scene, use `Scene.at()`.
-    To list all scenes within a directory, use `Scene.list()`.
-    """
-
-    def __init__(self, paths: Union[str, math.Tensor]):
-        self._paths = math.wrap(paths)
-        self._properties: Union[dict, None] = None
-
-    def __getitem__(self, item):
-        return Scene(self._paths[item])
-
-    def __getattr__(self, name: str) -> BoundDim:
-        return BoundDim(self, name)
-
-    def __variable_attrs__(self) -> Tuple[str, ...]:
-        return 'paths',
-
-    def __with_attrs__(self, **attrs):
-        if 'paths' in attrs:
-            return Scene(attrs['paths'])
-        else:
-            return Scene(self._paths)
-
-    @property
-    def shape(self):
-        return self._paths.shape
-
-    @property
-    def is_batch(self):
-        return self._paths.rank > 0
-
-    @property
-    def path(self) -> str:
-        """
-        Relative path of the scene directory.
-        This property only exists for single scenes, not scene batches.
-        """
-        assert not self.is_batch, "Scene.path is not defined for scene batches."
-        return self._paths.native()
-
-    @property
-    def paths(self) -> math.Tensor:
-        return self._paths
-
-    @staticmethod
-    def stack(*scenes: 'Scene', dim: Shape = batch('batch')) -> 'Scene':
-        return Scene(math.stack([s._paths for s in scenes], dim))
-
-    @staticmethod
-    def create(parent_directory: str,
-               shape: math.Shape = math.EMPTY_SHAPE,
-               name='sim',
-               copy_calling_script=True,
-               **dimensions) -> 'Scene':
-        """
-        Creates a new `Scene` or a batch of new scenes inside `parent_directory`.
-
-        See Also:
-            `Scene.at()`, `Scene.list()`.
-
-        Args:
-            parent_directory: Directory to hold the new `Scene`. If it doesn't exist, it will be created.
-            shape: Determines number of scenes to create. Multiple scenes will be represented by a `Scene` with `is_batch=True`.
-            name: Name of the directory (excluding index). Default is `'sim'`.
-            copy_calling_script: Whether to copy the Python file that invoked this method into the `src` folder of all created scenes.
-                See `Scene.copy_calling_script()`.
-            dimensions: Additional batch dimensions
-
-        Returns:
-            Single `Scene` object representing the new scene(s).
-        """
-        shape = shape & math.batch(**dimensions)
-        parent_directory = expanduser(parent_directory)
-        abs_dir = abspath(parent_directory)
-        if not isdir(abs_dir):
-            os.makedirs(abs_dir)
-            next_id = 0
-        else:
-            indices = [int(f[len(name)+1:]) for f in os.listdir(abs_dir) if f.startswith(f"{name}_")]
-            next_id = max([-1] + indices) + 1
-        ids = unpack_dim(wrap(tuple(range(next_id, next_id + shape.volume))), 'vector', shape)
-        paths = math.map(lambda id_: join(parent_directory, f"{name}_{id_:06d}"), ids)
-        scene = Scene(paths)
-        scene.mkdir()
-        if copy_calling_script:
-            try:
-                scene.copy_calling_script()
-            except IOError as err:
-                warnings.warn(f"Failed to copy calling script to scene during Scene.create(): {err}", RuntimeWarning)
-        return scene
-
-    @staticmethod
-    def list(parent_directory: str,
-             name='sim',
-             include_other: bool = False,
-             dim: Union[Shape, None] = None) -> Union['Scene', tuple]:
-        """
-        Lists all scenes inside the given directory.
-
-        See Also:
-            `Scene.at()`, `Scene.create()`.
-
-        Args:
-            parent_directory: Directory that contains scene folders.
-            name: Name of the directory (excluding index). Default is `'sim'`.
-            include_other: Whether folders that do not match the scene format should also be treated as scenes.
-            dim: Stack dimension. If None, returns tuple of `Scene` objects. Otherwise, returns a scene batch with this dimension.
-
-        Returns:
-            `tuple` of scenes.
-        """
-        parent_directory = expanduser(parent_directory)
-        abs_dir = abspath(parent_directory)
-        if not isdir(abs_dir):
-            return ()
-        names = [sim for sim in os.listdir(abs_dir) if sim.startswith(f"{name}_") or (include_other and isdir(join(abs_dir, sim)))]
-        if dim is None:
-            return tuple(Scene(join(parent_directory, n)) for n in names)
-        else:
-            paths = math.wrap([join(parent_directory, n) for n in names], dim)
-            return Scene(paths)
-
-    @staticmethod
-    def at(directory: Union[str, tuple, typing_list, math.Tensor, 'Scene'], id: Union[int, math.Tensor, None] = None) -> 'Scene':
-        """
-        Creates a `Scene` for an existing directory.
-
-        See Also:
-            `Scene.create()`, `Scene.list()`.
-
-        Args:
-            directory: Either directory containing scene folder if `id` is given, or scene path if `id=None`.
-            id: (Optional) Scene `id`, will be determined from `directory` if not specified.
-
-        Returns:
-            `Scene` object for existing scene.
-        """
-        if isinstance(directory, Scene):
-            assert id is None, f"Got id={id} but directory is already a Scene."
-            return directory
-        if isinstance(directory, (tuple, list)):
-            directory = math.wrap(directory, batch('scenes'))
-        directory = math.map(lambda d: expanduser(d), math.wrap(directory))
-        if id is None:
-            paths = directory
-        else:
-            id = math.wrap(id)
-            paths = math.map(lambda d, i: join(d, f"sim_{i:06d}"), directory, id)
-        # test all exist
-        for path in math.flatten(paths, flatten_batch=True):
-            if not isdir(path):
-                raise IOError(f"There is no scene at '{path}'")
-        return Scene(paths)
-
-    def subpath(self, name: str, create=False, create_parent=False) -> Union[str, tuple]:
-        """
-        Resolves the relative path `name` with this `Scene` as the root folder.
-
-        Args:
-            name: Relative path with this `Scene` as the root folder.
-            create: Whether to create a directory of that name.
-            create_parent: Whether to create the parent directory.
-
-        Returns:
-            Relative path including the path to this `Scene`.
-            In batch mode, returns a `tuple`, else a `str`.
-        """
-        def single_subpath(path):
-            path = join(path, name)
-            if create_parent and not isdir(os.path.dirname(path)):
-                os.makedirs(os.path.dirname(path))
-            if create and not isdir(path):
-                os.mkdir(path)
-            return path
-
-        result = math.map(single_subpath, self._paths)
-        if result.rank == 0:
-            return result.native()
-        else:
-            return result
-
-    def _init_properties(self):
-        if self._properties is not None:
-            return
-
-        def read_json(path: str) -> dict:
-            json_file = join(path, "description.json")
-            if isfile(json_file):
-                with open(json_file) as stream:
-                    props = json.load(stream)
-                if '__tensors__' in props:
-                    for key in props['__tensors__']:
-                        props[key] = math.from_dict(props[key])
-                return props
-            else:
-                return {}
-
-        if self._paths.shape.volume == 1:
-            self._properties = read_json(self._paths.native())
-        else:
-            self._properties = {}
-            dicts = [read_json(p) for p in self._paths]
-            keys = set(sum([tuple(d.keys()) for d in dicts], ()))
-            for key in keys:
-                assert all(key in d for d in dicts), f"Failed to create batched Scene because property '{key}' is present in some scenes but not all."
-                if all([math.all(d[key] == dicts[0][key]) for d in dicts]):
-                    self._properties[key] = dicts[0][key]
-                else:
-                    self._properties[key] = stack([d[key] for d in dicts], self._paths.shape)
-        if '__tensors__' in self._properties:
-            del self._properties['__tensors__']
-
-    def exist_properties(self):
-        """
-        Checks whether the file `description.json` exists or has existed.
-        """
-        if self._properties is not None:
-            return True  # must have been written or read
-        else:
-            json_file = join(next(iter(math.flatten(self._paths, flatten_batch=True))), "description.json")
-            return isfile(json_file)
-
-    def exists_config(self):
-        """ Tests if the configuration file *description.json* exists. In batch mode, tests if any configuration exists. """
-        if isinstance(self.path, str):
-            return isfile(join(self.path, "description.json"))
-        else:
-            return any(isfile(join(p, "description.json")) for p in self.path)
-
-    @property
-    def properties(self):
-        self._init_properties()
-        return self._properties
-
-    @properties.setter
-    def properties(self, dict):
-        self._properties = dict
-        with open(join(self.path, "description.json"), "w") as out:
-            json.dump(self._properties, out, indent=2)
-
-    def put_property(self, key, value):
-        """ See `Scene.put_properties()`. """
-        self._init_properties()
-        self._properties[key] = value
-        self._write_properties()
-
-    def put_properties(self, update: dict = None, **kw_updates):
-        """
-        Updates the properties dictionary and stores it in `description.json` of all scene folders.
-
-        Args:
-            update: new values, must be JSON serializable.
-            kw_updates: additional update as keyword arguments. This overrides `update`.
-        """
-        self._init_properties()
-        if update:
-            self._properties.update(update)
-        self._properties.update(kw_updates)
-        for key, value in self._properties.items():
-            if isinstance(value, (np.int64, np.int32)):
-                value = int(value)
-            elif isinstance(value, (np.float16, np.float32, np.float64, np.float16)) or (hasattr(np, 'float128') and isinstance(value, np.float128)):
-                value = float(value)
-            self._properties[key] = value
-        self._write_properties()
-
-    def _get_properties(self, index: dict):
-        result = dict(self._properties)
-        tensor_names = []
-        for key, value in self._properties.items():
-            if isinstance(value, math.Tensor):
-                value = value[index]
-                if value.rank == 0:
-                    value = value.dtype.kind(value)
-                else:
-                    value = math.to_dict(value)
-                    tensor_names.append(key)
-                result[key] = value
-        if tensor_names:
-            result['__tensors__'] = tuple(tensor_names)
-        return result
-
-    def _write_properties(self):
-        for instance in self.paths.shape.meshgrid():
-            path = self.paths[instance].native()
-            instance_properties = self._get_properties(instance)
-            with open(join(path, "description.json"), "w") as out:
-                json.dump(instance_properties, out, indent=2)
-
-    def write(self, data: dict = None, frame=0, **kw_data):
-        """
-        Writes fields to this scene.
-        One NumPy file will be created for each `phi.field.Field`
-
-        See Also:
-            `Scene.read()`.
-
-        Args:
-            data: `dict` mapping field names to `Field` objects that can be written using `phi.field.write()`.
-            kw_data: Additional data, overrides elements in `data`.
-            frame: Frame number.
-        """
-        data = dict(data) if data else {}
-        data.update(kw_data)
-        for name, field in data.items():
-            self.write_field(field, name, frame)
-
-    def write_field(self, field: SampledField, name: str, frame: int):
-        """
-        Write a `SampledField` to a file.
-        The filenames are created from the provided names and the frame index in accordance with the
-        scene format specification at https://tum-pbs.github.io/PhiFlow/Scene_Format_Specification.html .
-
-        Args:
-            field: single field or structure of Fields to save.
-            name: Base file name.
-            frame: Frame number as `int`, typically time step index.
-        """
-        if not isinstance(field, SampledField):
-            raise ValueError(f"Only SampledField instances can be saved but got {field}")
-        name = _slugify_filename(name)
-        files = math.map(lambda dir_: _filename(dir_, name, frame), self._paths)
-        write(field, files)
-
-    def read_field(self, name: str, frame: int, convert_to_backend=True) -> SampledField:
-        """
-        Reads a single `SampledField` from files contained in this `Scene` (batch).
-
-        Args:
-            name: Base file name.
-            frame: Frame number as `int`, typically time step index.
-            convert_to_backend: Whether to convert the read data to the data format of the default backend, e.g. TensorFlow tensors.
-
-        Returns:
-            `SampledField`
-        """
-        name = _slugify_filename(name)
-        files = math.map(lambda dir_: _filename(dir_, name, frame), self._paths)
-        return read(files, convert_to_backend=convert_to_backend)
-
-    read_array = read_field
-
-    def read(self, *names: str, frame=0, convert_to_backend=True):
-        """
-        Reads one or multiple fields from disc.
-
-        See Also:
-            `Scene.write()`.
-
-        Args:
-            names: Single field name or sequence of field names.
-            frame: Frame number.
-            convert_to_backend: Whether to convert the read data to the data format of the default backend, e.g. TensorFlow tensors.
-
-        Returns:
-            Single `phi.field.Field` or sequence of fields, depending on the type of `names`.
-        """
-        if len(names) == 1 and isinstance(names[0], (tuple, list)):
-            names = names[0]
-        result = [self.read_array(name, frame, convert_to_backend) for name in names]
-        return result[0] if len(names) == 1 else result
-
-    @property
-    def fieldnames(self) -> tuple:
-        """ Determines all field names present in this `Scene`, independent of frame. """
-        return get_fieldnames(self.path)
-
-    @property
-    def frames(self):
-        """ Determines all frame numbers present in this `Scene`, independent of field names. See `Scene.complete_frames`. """
-        return get_frames(self.path, mode=set.union)
-
-    @property
-    def complete_frames(self):
-        """
-        Determines all frame number for which all existing fields are available.
-        If there are multiple fields stored within this scene, a frame is considered complete only if an entry exists for all fields.
-
-        See Also:
-            `Scene.frames`
-        """
-        return get_frames(self.path, mode=set.intersection)
-
-    def __repr__(self):
-        return f"{self.paths:no-dtype}"
-
-    def __eq__(self, other):
-        return isinstance(other, Scene) and (other._paths == self._paths).all
-
-    def copy_calling_script(self, full_trace=False, include_context_information=True):
-        """
-        Copies the Python file that called this method into the `src` folder of this `Scene`.
-
-        In batch mode, the script is copied to all scenes.
-
-        Args:
-            full_trace: Whether to include scripts that indirectly called this method.
-            include_context_information: If True, writes the phiflow version and `sys.argv` into `context.json`.
-        """
-        script_paths = [frame.filename for frame in inspect.stack()]
-        script_paths = list(filter(lambda path: not _is_phi_file(path), script_paths))
-        script_paths = set(script_paths) if full_trace else [script_paths[0]]
-        self.subpath('src', create=True)
-        for script_path in script_paths:
-            if script_path.endswith('.py'):
-                self.copy_src(script_path, only_external=False)
-            elif 'ipython' in script_path:
-                from IPython import get_ipython
-                cells = get_ipython().user_ns['In']
-                blocks = [f"#%% In[{i}]\n{cell}" for i, cell in enumerate(cells)]
-                text = "\n\n".join(blocks)
-                self.copy_src_text('ipython.py', text)
-        if include_context_information:
-            for path in math.flatten(self._paths, flatten_batch=True):
-                with open(join(path, 'src', 'context.json'), 'w') as context_file:
-                    json.dump({
-                        'phi_version': phi_version,
-                        'argv': sys.argv
-                    }, context_file)
-
-    def copy_src(self, script_path, only_external=True):
-        for path in math.flatten(self._paths, flatten_batch=True):
-            if not only_external or not _is_phi_file(script_path):
-                shutil.copy(script_path, join(path, 'src', basename(script_path)))
-
-    def copy_src_text(self, filename, text):
-        for path in math.flatten(self._paths, flatten_batch=True):
-            target = join(path, 'src', filename)
-            with open(target, "w") as file:
-                file.writelines(text)
-
-    def mkdir(self):
-        for path in math.flatten(self._paths, flatten_batch=True):
-            isdir(path) or os.mkdir(path)
-
-    def remove(self):
-        """ Deletes the scene directory and all contained files. """
-        for p in math.flatten(self._paths, flatten_batch=True):
-            p = abspath(p)
-            if isdir(p):
-                shutil.rmtree(p)
-
-
-def _slugify_filename(struct_name):
-    struct_name = struct_name.replace('._', '.').replace('.', '_')
-    if struct_name.startswith('_'):
-        struct_name = struct_name[1:]
-    return struct_name
-
-
-def slugify(value):
-    """
-    Normalizes string, converts to lowercase, removes non-alpha characters,
-    and converts spaces to hyphens.
-    """
-    for greek_letter, name in greek.items():
-        value = value.replace(greek_letter, name)
-    value = re.sub('[^\\w\\s-]', '', value).strip().lower()
-    value = re.sub('[-\\s]+', '-', value)
-    return value
-
-
-greek = {
-    u'Α': 'Alpha',      u'α': 'alpha',
-    u'Β': 'Beta',       u'β': 'beta',
-    u'Γ': 'Gamma',      u'γ': 'gamma',
-    u'Δ': 'Delta',      u'δ': 'delta',
-    u'Ε': 'Epsilon',    u'ε': 'epsilon',
-    u'Ζ': 'Zeta',       u'ζ': 'zeta',
-    u'Η': 'Eta',        u'η': 'eta',
-    u'Θ': 'Theta',      u'θ': 'theta',
-    u'Ι': 'Iota',       u'ι': 'iota',
-    u'Κ': 'Kappa',      u'κ': 'kappa',
-    u'Λ': 'Lambda',     u'λ': 'lambda',
-    u'Μ': 'Mu',         u'μ': 'mu',
-    u'Ν': 'Nu',         u'ν': 'nu',
-    u'Ξ': 'Xi',         u'ξ': 'xi',
-    u'Ο': 'Omicron',    u'ο': 'omicron',
-    u'Π': 'Pi',         u'π': 'pi',
-    u'Ρ': 'Rho',        u'ρ': 'rho',
-    u'Σ': 'Sigma',      u'σ': 'sigma',
-    u'Τ': 'Tau',        u'τ': 'tau',
-    u'Υ': 'Upsilon',    u'υ': 'upsilon',
-    u'Φ': 'Phi',        u'φ': 'phi',
-    u'Χ': 'Chi',        u'χ': 'chi',
-    u'Ψ': 'Psi',        u'ψ': 'psi',
-    u'Ω': 'Omega',      u'ω': 'omega',
-}
-
-
-def _is_phi_file(path):
-    path, name = split(path)
-    if name == 'phi':
-        return True
-    elif path == '' or name == '':
-        return False
-    else:
-        return _is_phi_file(path)
+import inspect
+import json
+import os
+import re
+import shutil
+import sys
+import warnings
+from os.path import join, isfile, isdir, abspath, expanduser, basename, split
+from typing import Tuple, Union
+
+import numpy as np
+
+from phi import math, __version__ as phi_version
+from ._field import SampledField
+from ._field_io import read, write
+from ..math import Shape, batch, stack, unpack_dim, wrap
+from ..math.magic import BoundDim
+
+
+typing_list = list
+
+
+def _filename(simpath, name, frame):
+    return join(simpath, f"{slugify(name)}_{frame:06d}.npz")
+
+
+def _str(bytes_or_str):  # on Linux, os.listdir returns bytes instead of strings
+    if isinstance(bytes_or_str, str):
+        return bytes_or_str
+    else:
+        return str(bytes_or_str, 'utf-8')
+
+
+def get_fieldnames(simpath) -> tuple:
+    fieldnames_set = {_str(f)[:-11] for f in os.listdir(simpath) if _str(f).endswith(".npz")}
+    return tuple(sorted(fieldnames_set))
+
+
+def get_frames(path: str, field_name: str = None, mode=set.intersection) -> tuple:
+    if field_name is not None:
+        all_frames = {int(f[-10:-4]) for f in os.listdir(path) if _str(f).startswith(field_name) and _str(f).endswith(".npz")}
+        return tuple(sorted(all_frames))
+    else:
+        fields = get_fieldnames(path)
+        if not fields:
+            return ()
+        frames_sets = [set(get_frames(path, field)) for field in fields]
+        frames = mode(*frames_sets)
+        return tuple(sorted(frames))
+
+
+class Scene:
+    """
+    Provides methods for reading and writing simulation data.
+
+    See the format documentation at https://tum-pbs.github.io/PhiFlow/Scene_Format_Specification.html .
+
+    All data of a `Scene` is located inside a single directory with name `sim_xxxxxx` where `xxxxxx` is the `id`.
+    The data of the scene is organized into NumPy files by *name* and *frame*.
+
+    To create a new scene, use `Scene.create()`.
+    To reference an existing scene, use `Scene.at()`.
+    To list all scenes within a directory, use `Scene.list()`.
+    """
+
+    def __init__(self, paths: Union[str, math.Tensor]):
+        self._paths = math.wrap(paths)
+        self._properties: Union[dict, None] = None
+
+    def __getitem__(self, item):
+        return Scene(self._paths[item])
+
+    def __getattr__(self, name: str) -> BoundDim:
+        return BoundDim(self, name)
+
+    def __variable_attrs__(self) -> Tuple[str, ...]:
+        return 'paths',
+
+    def __with_attrs__(self, **attrs):
+        if 'paths' in attrs:
+            return Scene(attrs['paths'])
+        else:
+            return Scene(self._paths)
+
+    @property
+    def shape(self):
+        return self._paths.shape
+
+    @property
+    def is_batch(self):
+        return self._paths.rank > 0
+
+    @property
+    def path(self) -> str:
+        """
+        Relative path of the scene directory.
+        This property only exists for single scenes, not scene batches.
+        """
+        assert not self.is_batch, "Scene.path is not defined for scene batches."
+        return self._paths.native()
+
+    @property
+    def paths(self) -> math.Tensor:
+        return self._paths
+
+    @staticmethod
+    def stack(*scenes: 'Scene', dim: Shape = batch('batch')) -> 'Scene':
+        return Scene(math.stack([s._paths for s in scenes], dim))
+
+    @staticmethod
+    def create(parent_directory: str,
+               shape: math.Shape = math.EMPTY_SHAPE,
+               name='sim',
+               copy_calling_script=True,
+               **dimensions) -> 'Scene':
+        """
+        Creates a new `Scene` or a batch of new scenes inside `parent_directory`.
+
+        See Also:
+            `Scene.at()`, `Scene.list()`.
+
+        Args:
+            parent_directory: Directory to hold the new `Scene`. If it doesn't exist, it will be created.
+            shape: Determines number of scenes to create. Multiple scenes will be represented by a `Scene` with `is_batch=True`.
+            name: Name of the directory (excluding index). Default is `'sim'`.
+            copy_calling_script: Whether to copy the Python file that invoked this method into the `src` folder of all created scenes.
+                See `Scene.copy_calling_script()`.
+            dimensions: Additional batch dimensions
+
+        Returns:
+            Single `Scene` object representing the new scene(s).
+        """
+        shape = shape & math.batch(**dimensions)
+        parent_directory = expanduser(parent_directory)
+        abs_dir = abspath(parent_directory)
+        if not isdir(abs_dir):
+            os.makedirs(abs_dir)
+            next_id = 0
+        else:
+            indices = [int(f[len(name)+1:]) for f in os.listdir(abs_dir) if f.startswith(f"{name}_")]
+            next_id = max([-1] + indices) + 1
+        ids = unpack_dim(wrap(tuple(range(next_id, next_id + shape.volume))), 'vector', shape)
+        paths = math.map(lambda id_: join(parent_directory, f"{name}_{id_:06d}"), ids)
+        scene = Scene(paths)
+        scene.mkdir()
+        if copy_calling_script:
+            try:
+                scene.copy_calling_script()
+            except IOError as err:
+                warnings.warn(f"Failed to copy calling script to scene during Scene.create(): {err}", RuntimeWarning)
+        return scene
+
+    @staticmethod
+    def list(parent_directory: str,
+             name='sim',
+             include_other: bool = False,
+             dim: Union[Shape, None] = None) -> Union['Scene', tuple]:
+        """
+        Lists all scenes inside the given directory.
+
+        See Also:
+            `Scene.at()`, `Scene.create()`.
+
+        Args:
+            parent_directory: Directory that contains scene folders.
+            name: Name of the directory (excluding index). Default is `'sim'`.
+            include_other: Whether folders that do not match the scene format should also be treated as scenes.
+            dim: Stack dimension. If None, returns tuple of `Scene` objects. Otherwise, returns a scene batch with this dimension.
+
+        Returns:
+            `tuple` of scenes.
+        """
+        parent_directory = expanduser(parent_directory)
+        abs_dir = abspath(parent_directory)
+        if not isdir(abs_dir):
+            return ()
+        names = [sim for sim in os.listdir(abs_dir) if sim.startswith(f"{name}_") or (include_other and isdir(join(abs_dir, sim)))]
+        names = list(sorted(names))
+        if dim is None:
+            return tuple(Scene(join(parent_directory, n)) for n in names)
+        else:
+            paths = math.wrap([join(parent_directory, n) for n in names], dim)
+            return Scene(paths)
+
+    @staticmethod
+    def at(directory: Union[str, tuple, typing_list, math.Tensor, 'Scene'], id: Union[int, math.Tensor, None] = None) -> 'Scene':
+        """
+        Creates a `Scene` for an existing directory.
+
+        See Also:
+            `Scene.create()`, `Scene.list()`.
+
+        Args:
+            directory: Either directory containing scene folder if `id` is given, or scene path if `id=None`.
+            id: (Optional) Scene `id`, will be determined from `directory` if not specified.
+
+        Returns:
+            `Scene` object for existing scene.
+        """
+        if isinstance(directory, Scene):
+            assert id is None, f"Got id={id} but directory is already a Scene."
+            return directory
+        if isinstance(directory, (tuple, list)):
+            directory = math.wrap(directory, batch('scenes'))
+        directory = math.map(lambda d: expanduser(d), math.wrap(directory))
+        if isinstance(id, int) and id < 0:
+            assert directory.shape.volume == 1
+            scenes = Scene.list(directory.native())
+            assert len(scenes) >= -id, f"Failed to get scene {id} at {directory}. {len(scenes)} scenes available in that directory."
+            return scenes[id]
+        if id is None:
+            paths = directory
+        else:
+            id = math.wrap(id)
+            paths = math.map(lambda d, i: join(d, f"sim_{i:06d}"), directory, id)
+        # test all exist
+        for path in math.flatten(paths, flatten_batch=True):
+            if not isdir(path):
+                raise IOError(f"There is no scene at '{path}'")
+        return Scene(paths)
+
+    def subpath(self, name: str, create=False, create_parent=False) -> Union[str, tuple]:
+        """
+        Resolves the relative path `name` with this `Scene` as the root folder.
+
+        Args:
+            name: Relative path with this `Scene` as the root folder.
+            create: Whether to create a directory of that name.
+            create_parent: Whether to create the parent directory.
+
+        Returns:
+            Relative path including the path to this `Scene`.
+            In batch mode, returns a `tuple`, else a `str`.
+        """
+        def single_subpath(path):
+            path = join(path, name)
+            if create_parent and not isdir(os.path.dirname(path)):
+                os.makedirs(os.path.dirname(path))
+            if create and not isdir(path):
+                os.mkdir(path)
+            return path
+
+        result = math.map(single_subpath, self._paths)
+        if result.rank == 0:
+            return result.native()
+        else:
+            return result
+
+    def _init_properties(self):
+        if self._properties is not None:
+            return
+
+        def read_json(path: str) -> dict:
+            json_file = join(path, "description.json")
+            if isfile(json_file):
+                with open(json_file) as stream:
+                    props = json.load(stream)
+                if '__tensors__' in props:
+                    for key in props['__tensors__']:
+                        props[key] = math.from_dict(props[key])
+                return props
+            else:
+                return {}
+
+        if self._paths.shape.volume == 1:
+            self._properties = read_json(self._paths.native())
+        else:
+            self._properties = {}
+            dicts = [read_json(p) for p in self._paths]
+            keys = set(sum([tuple(d.keys()) for d in dicts], ()))
+            for key in keys:
+                assert all(key in d for d in dicts), f"Failed to create batched Scene because property '{key}' is present in some scenes but not all."
+                if all([math.all(d[key] == dicts[0][key]) for d in dicts]):
+                    self._properties[key] = dicts[0][key]
+                else:
+                    self._properties[key] = stack([d[key] for d in dicts], self._paths.shape)
+        if '__tensors__' in self._properties:
+            del self._properties['__tensors__']
+
+    def exist_properties(self):
+        """
+        Checks whether the file `description.json` exists or has existed.
+        """
+        if self._properties is not None:
+            return True  # must have been written or read
+        else:
+            json_file = join(next(iter(math.flatten(self._paths, flatten_batch=True))), "description.json")
+            return isfile(json_file)
+
+    def exists_config(self):
+        """ Tests if the configuration file *description.json* exists. In batch mode, tests if any configuration exists. """
+        if isinstance(self.path, str):
+            return isfile(join(self.path, "description.json"))
+        else:
+            return any(isfile(join(p, "description.json")) for p in self.path)
+
+    @property
+    def properties(self):
+        self._init_properties()
+        return self._properties
+
+    @properties.setter
+    def properties(self, dict):
+        self._properties = dict
+        with open(join(self.path, "description.json"), "w") as out:
+            json.dump(self._properties, out, indent=2)
+
+    def put_property(self, key, value):
+        """ See `Scene.put_properties()`. """
+        self._init_properties()
+        self._properties[key] = value
+        self._write_properties()
+
+    def put_properties(self, update: dict = None, **kw_updates):
+        """
+        Updates the properties dictionary and stores it in `description.json` of all scene folders.
+
+        Args:
+            update: new values, must be JSON serializable.
+            kw_updates: additional update as keyword arguments. This overrides `update`.
+        """
+        self._init_properties()
+        if update:
+            self._properties.update(update)
+        self._properties.update(kw_updates)
+        for key, value in self._properties.items():
+            if isinstance(value, (np.int64, np.int32)):
+                value = int(value)
+            elif isinstance(value, (np.float16, np.float32, np.float64, np.float16)) or (hasattr(np, 'float128') and isinstance(value, np.float128)):
+                value = float(value)
+            self._properties[key] = value
+        self._write_properties()
+
+    def _get_properties(self, index: dict):
+        result = dict(self._properties)
+        tensor_names = []
+        for key, value in self._properties.items():
+            if isinstance(value, math.Tensor):
+                value = value[index]
+                if value.rank == 0:
+                    value = value.dtype.kind(value)
+                else:
+                    value = math.to_dict(value)
+                    tensor_names.append(key)
+                result[key] = value
+        if tensor_names:
+            result['__tensors__'] = tuple(tensor_names)
+        return result
+
+    def _write_properties(self):
+        for instance in self.paths.shape.meshgrid():
+            path = self.paths[instance].native()
+            instance_properties = self._get_properties(instance)
+            with open(join(path, "description.json"), "w") as out:
+                json.dump(instance_properties, out, indent=2)
+
+    def write(self, data: dict = None, frame=0, **kw_data):
+        """
+        Writes fields to this scene.
+        One NumPy file will be created for each `phi.field.Field`
+
+        See Also:
+            `Scene.read()`.
+
+        Args:
+            data: `dict` mapping field names to `Field` objects that can be written using `phi.field.write()`.
+            kw_data: Additional data, overrides elements in `data`.
+            frame: Frame number.
+        """
+        data = dict(data) if data else {}
+        data.update(kw_data)
+        for name, field in data.items():
+            self.write_field(field, name, frame)
+
+    def write_field(self, field: SampledField, name: str, frame: int):
+        """
+        Write a `SampledField` to a file.
+        The filenames are created from the provided names and the frame index in accordance with the
+        scene format specification at https://tum-pbs.github.io/PhiFlow/Scene_Format_Specification.html .
+
+        Args:
+            field: single field or structure of Fields to save.
+            name: Base file name.
+            frame: Frame number as `int`, typically time step index.
+        """
+        if not isinstance(field, SampledField):
+            raise ValueError(f"Only SampledField instances can be saved but got {field}")
+        name = _slugify_filename(name)
+        files = math.map(lambda dir_: _filename(dir_, name, frame), self._paths)
+        write(field, files)
+
+    def read_field(self, name: str, frame: int, convert_to_backend=True) -> SampledField:
+        """
+        Reads a single `SampledField` from files contained in this `Scene` (batch).
+
+        Args:
+            name: Base file name.
+            frame: Frame number as `int`, typically time step index.
+            convert_to_backend: Whether to convert the read data to the data format of the default backend, e.g. TensorFlow tensors.
+
+        Returns:
+            `SampledField`
+        """
+        name = _slugify_filename(name)
+        files = math.map(lambda dir_: _filename(dir_, name, frame), self._paths)
+        return read(files, convert_to_backend=convert_to_backend)
+
+    read_array = read_field
+
+    def read(self, *names: str, frame=0, convert_to_backend=True):
+        """
+        Reads one or multiple fields from disc.
+
+        See Also:
+            `Scene.write()`.
+
+        Args:
+            names: Single field name or sequence of field names.
+            frame: Frame number.
+            convert_to_backend: Whether to convert the read data to the data format of the default backend, e.g. TensorFlow tensors.
+
+        Returns:
+            Single `phi.field.Field` or sequence of fields, depending on the type of `names`.
+        """
+        if len(names) == 1 and isinstance(names[0], (tuple, list)):
+            names = names[0]
+        result = [self.read_array(name, frame, convert_to_backend) for name in names]
+        return result[0] if len(names) == 1 else result
+
+    @property
+    def fieldnames(self) -> tuple:
+        """ Determines all field names present in this `Scene`, independent of frame. """
+        return get_fieldnames(self.path)
+
+    @property
+    def frames(self):
+        """ Determines all frame numbers present in this `Scene`, independent of field names. See `Scene.complete_frames`. """
+        return get_frames(self.path, mode=set.union)
+
+    @property
+    def complete_frames(self):
+        """
+        Determines all frame number for which all existing fields are available.
+        If there are multiple fields stored within this scene, a frame is considered complete only if an entry exists for all fields.
+
+        See Also:
+            `Scene.frames`
+        """
+        return get_frames(self.path, mode=set.intersection)
+
+    def __repr__(self):
+        return f"{self.paths:no-dtype}"
+
+    def __eq__(self, other):
+        return isinstance(other, Scene) and (other._paths == self._paths).all
+
+    def copy_calling_script(self, full_trace=False, include_context_information=True):
+        """
+        Copies the Python file that called this method into the `src` folder of this `Scene`.
+
+        In batch mode, the script is copied to all scenes.
+
+        Args:
+            full_trace: Whether to include scripts that indirectly called this method.
+            include_context_information: If True, writes the phiflow version and `sys.argv` into `context.json`.
+        """
+        script_paths = [frame.filename for frame in inspect.stack()]
+        script_paths = list(filter(lambda path: not _is_phi_file(path), script_paths))
+        script_paths = set(script_paths) if full_trace else [script_paths[0]]
+        self.subpath('src', create=True)
+        for script_path in script_paths:
+            if script_path.endswith('.py'):
+                self.copy_src(script_path, only_external=False)
+            elif 'ipython' in script_path:
+                from IPython import get_ipython
+                cells = get_ipython().user_ns['In']
+                blocks = [f"#%% In[{i}]\n{cell}" for i, cell in enumerate(cells)]
+                text = "\n\n".join(blocks)
+                self.copy_src_text('ipython.py', text)
+        if include_context_information:
+            for path in math.flatten(self._paths, flatten_batch=True):
+                with open(join(path, 'src', 'context.json'), 'w') as context_file:
+                    json.dump({
+                        'phi_version': phi_version,
+                        'argv': sys.argv
+                    }, context_file)
+
+    def copy_src(self, script_path, only_external=True):
+        for path in math.flatten(self._paths, flatten_batch=True):
+            if not only_external or not _is_phi_file(script_path):
+                shutil.copy(script_path, join(path, 'src', basename(script_path)))
+
+    def copy_src_text(self, filename, text):
+        for path in math.flatten(self._paths, flatten_batch=True):
+            target = join(path, 'src', filename)
+            with open(target, "w") as file:
+                file.writelines(text)
+
+    def mkdir(self):
+        for path in math.flatten(self._paths, flatten_batch=True):
+            isdir(path) or os.mkdir(path)
+
+    def remove(self):
+        """ Deletes the scene directory and all contained files. """
+        for p in math.flatten(self._paths, flatten_batch=True):
+            p = abspath(p)
+            if isdir(p):
+                shutil.rmtree(p)
+
+    def rename(self, name: str):
+        """ Deletes the scene directory and all contained files. """
+        for p in math.flatten(self._paths, flatten_batch=True):
+            p = abspath(p)
+            if isdir(p):
+                new_path = os.path.join(os.path.dirname(p), name)
+                print(f"Renaming {p} to {new_path}")
+                shutil.move(p, new_path)
+
+
+def _slugify_filename(struct_name):
+    struct_name = struct_name.replace('._', '.').replace('.', '_')
+    if struct_name.startswith('_'):
+        struct_name = struct_name[1:]
+    return struct_name
+
+
+def slugify(value):
+    """
+    Normalizes string, converts to lowercase, removes non-alpha characters,
+    and converts spaces to hyphens.
+    """
+    for greek_letter, name in greek.items():
+        value = value.replace(greek_letter, name)
+    value = re.sub('[^\\w\\s-]', '', value).strip().lower()
+    value = re.sub('[-\\s]+', '-', value)
+    return value
+
+
+greek = {
+    u'Α': 'Alpha',      u'α': 'alpha',
+    u'Β': 'Beta',       u'β': 'beta',
+    u'Γ': 'Gamma',      u'γ': 'gamma',
+    u'Δ': 'Delta',      u'δ': 'delta',
+    u'Ε': 'Epsilon',    u'ε': 'epsilon',
+    u'Ζ': 'Zeta',       u'ζ': 'zeta',
+    u'Η': 'Eta',        u'η': 'eta',
+    u'Θ': 'Theta',      u'θ': 'theta',
+    u'Ι': 'Iota',       u'ι': 'iota',
+    u'Κ': 'Kappa',      u'κ': 'kappa',
+    u'Λ': 'Lambda',     u'λ': 'lambda',
+    u'Μ': 'Mu',         u'μ': 'mu',
+    u'Ν': 'Nu',         u'ν': 'nu',
+    u'Ξ': 'Xi',         u'ξ': 'xi',
+    u'Ο': 'Omicron',    u'ο': 'omicron',
+    u'Π': 'Pi',         u'π': 'pi',
+    u'Ρ': 'Rho',        u'ρ': 'rho',
+    u'Σ': 'Sigma',      u'σ': 'sigma',
+    u'Τ': 'Tau',        u'τ': 'tau',
+    u'Υ': 'Upsilon',    u'υ': 'upsilon',
+    u'Φ': 'Phi',        u'φ': 'phi',
+    u'Χ': 'Chi',        u'χ': 'chi',
+    u'Ψ': 'Psi',        u'ψ': 'psi',
+    u'Ω': 'Omega',      u'ω': 'omega',
+}
+
+
+def _is_phi_file(path):
+    path, name = split(path)
+    if name == 'phi':
+        return True
+    elif path == '' or name == '':
+        return False
+    else:
+        return _is_phi_file(path)
```

### Comparing `phiflow-2.3.4/phi/flow.py` & `phiflow-2.4.0/phi/flow.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,43 +1,43 @@
-# pylint: disable-msg = unused-import
-"""
-*Main PhiFlow import:* `from phi.flow import *`
-
-Imports important functions and classes from
-`math`, `geom`, `field`, `physics` and `vis` (including sub-modules)
-as well as the modules and sub-modules themselves.
-
-See `phi.tf.flow`, `phi.torch.flow`, `phi.jax.flow`.
-"""
-
-# Modules
-import numpy
-import numpy as np
-import phi
-from . import math, geom, field, physics, vis
-from .math import extrapolation, backend
-from .physics import fluid, advect, diffuse
-
-# Classes
-from .math import Shape, Tensor, DType, Solve
-from .geom import Geometry, Sphere, Box, Cuboid
-from .field import Grid, CenteredGrid, StaggeredGrid, mask, Noise, PointCloud, Scene, resample, GeometryMask, SoftGeometryMask, HardGeometryMask
-from .vis import Viewer
-from .physics.fluid import Obstacle
-
-# Constants
-from .math import PI, INF, NAN
-from .math.extrapolation import PERIODIC, ZERO_GRADIENT
-
-# Functions
-from .math import (
-    wrap, tensor, vec,  # Tensor creation
-    shape, spatial, channel, batch, instance, dual,
-    non_spatial, non_channel, non_batch, non_instance, non_dual,  # Shape functions (magic)
-    unstack, stack, concat, expand, rename_dims, pack_dims, unpack_dim, flatten, cast,  # Magic Ops
-    jit_compile, jit_compile_linear, minimize, functional_gradient, solve_linear, solve_nonlinear, iterate, identity,  # jacobian, hessian, custom_gradient # Functional magic
-)
-from .geom import union
-from .vis import show, view, control, plot
-
-# Exceptions
-from .math import ConvergenceException, NotConverged, Diverged
+# pylint: disable-msg = unused-import
+"""
+*Main PhiFlow import:* `from phi.flow import *`
+
+Imports important functions and classes from
+`math`, `geom`, `field`, `physics` and `vis` (including sub-modules)
+as well as the modules and sub-modules themselves.
+
+See `phi.tf.flow`, `phi.torch.flow`, `phi.jax.flow`.
+"""
+
+# Modules
+import numpy
+import numpy as np
+import phi
+from . import math, geom, field, physics, vis
+from .math import extrapolation, backend
+from .physics import fluid, advect, diffuse
+
+# Classes
+from .math import Shape, Tensor, DType, Solve
+from .geom import Geometry, Sphere, Box, Cuboid
+from .field import Field, Grid, CenteredGrid, StaggeredGrid, mask, Noise, PointCloud, Scene, resample, GeometryMask, SoftGeometryMask, HardGeometryMask
+from .vis import Viewer
+from .physics.fluid import Obstacle
+
+# Constants
+from .math import PI, INF, NAN, f
+from .math.extrapolation import PERIODIC, ZERO_GRADIENT
+
+# Functions
+from .math import (
+    wrap, tensor, vec,  # Tensor creation
+    shape, spatial, channel, batch, instance, dual,
+    non_spatial, non_channel, non_batch, non_instance, non_dual,  # Shape functions (magic)
+    unstack, stack, concat, expand, rename_dims, pack_dims, unpack_dim, flatten, cast,  # Magic Ops
+    jit_compile, jit_compile_linear, minimize, functional_gradient, solve_linear, solve_nonlinear, iterate, identity,  # jacobian, hessian, custom_gradient # Functional magic
+)
+from .geom import union
+from .vis import show, view, control, plot
+
+# Exceptions
+from .math import ConvergenceException, NotConverged, Diverged
```

### Comparing `phiflow-2.3.4/phi/geom/_box.py` & `phiflow-2.4.0/phi/geom/_geom.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,544 +1,598 @@
-import warnings
-from typing import Dict, Tuple, Union
-
-import numpy as np
-
-from phi import math
-from ._geom import Geometry, _keep_vector
-from ..math import wrap, INF, Shape, channel, spatial, copy_with, Tensor
-from ..math._shape import parse_dim_order
-from ..math.magic import slicing_dict
-
-
-class BaseBox(Geometry):  # not a Subwoofer
-    """
-    Abstract base type for box-like geometries.
-    """
-
-    def __eq__(self, other):
-        raise NotImplementedError()
-
-    def __hash__(self):
-        raise NotImplementedError()
-
-    def __ne__(self, other):
-        return not self == other
-
-    @property
-    def shape(self):
-        raise NotImplementedError()
-
-    @property
-    def center(self) -> Tensor:
-        raise NotImplementedError()
-
-    def at(self, center: Tensor) -> 'BaseBox':
-        return Cuboid(center, self.half_size)
-
-    @property
-    def size(self) -> Tensor:
-        raise NotImplementedError(self)
-
-    @property
-    def half_size(self) -> Tensor:
-        raise NotImplementedError(self)
-
-    @property
-    def lower(self) -> Tensor:
-        raise NotImplementedError(self)
-
-    @property
-    def upper(self) -> Tensor:
-        raise NotImplementedError(self)
-
-    @property
-    def volume(self) -> Tensor:
-        return math.prod(self.size, 'vector')
-
-    @property
-    def shape_type(self) -> Tensor:
-        return math.tensor('B')
-
-    def bounding_radius(self):
-        return math.vec_length(self.half_size)
-
-    def bounding_half_extent(self):
-        return self.size * 0.5
-
-    def global_to_local(self, global_position: Tensor) -> Tensor:
-        if math.close(self.lower, 0):
-            return global_position / self.size
-        else:
-            return (global_position - self.lower) / self.size
-
-    def local_to_global(self, local_position):
-        return local_position * self.size + self.lower
-
-    def lies_inside(self, location):
-        bool_inside = (location >= self.lower) & (location <= self.upper)
-        bool_inside = math.all(bool_inside, 'vector')
-        bool_inside = math.any(bool_inside, self.shape.instance)  # union for instance dimensions
-        return bool_inside
-
-    def approximate_signed_distance(self, location: Union[Tensor, tuple]):
-        """
-        Computes the signed L-infinity norm (manhattan distance) from the location to the nearest side of the box.
-        For an outside location `l` with the closest surface point `s`, the distance is `max(abs(l - s))`.
-        For inside locations it is `-max(abs(l - s))`.
-
-        Args:
-          location: float tensor of shape (batch_size, ..., rank)
-
-        Returns:
-          float tensor of shape (*location.shape[:-1], 1).
-
-        """
-        center = 0.5 * (self.lower + self.upper)
-        extent = self.upper - self.lower
-        distance = math.abs(location - center) - extent * 0.5
-        distance = math.max(distance, 'vector')
-        distance = math.min(distance, self.shape.instance)  # union for instance dimensions
-        return distance
-
-    def push(self, positions: Tensor, outward: bool = True, shift_amount: float = 0) -> Tensor:
-        loc_to_center = positions - self.center
-        sgn_dist_from_surface = math.abs(loc_to_center) - self.half_size
-        if outward:
-            # --- get negative distances (particles are inside) towards the nearest boundary and add shift_amount ---
-            distances_of_interest = (sgn_dist_from_surface == math.max(sgn_dist_from_surface, 'vector')) & (sgn_dist_from_surface < 0)
-            shift = distances_of_interest * (sgn_dist_from_surface - shift_amount)
-        else:
-            shift = (sgn_dist_from_surface + shift_amount) * (sgn_dist_from_surface > 0)  # get positive distances (particles are outside) and add shift_amount
-            shift = math.where(math.abs(shift) > math.abs(loc_to_center), math.abs(loc_to_center), shift)  # ensure inward shift ends at center
-        return positions + math.where(loc_to_center < 0, 1, -1) * shift
-
-    def project(self, *dimensions: str):
-        """ Project this box into a lower-dimensional space. """
-        warnings.warn("Box.project(dims) is deprecated. Use Box.vector[dims] instead", DeprecationWarning, stacklevel=2)
-        return self.vector[dimensions]
-
-    def sample_uniform(self, *shape: math.Shape) -> Tensor:
-        uniform = math.random_uniform(self.shape.non_singleton, *shape, math.channel(vector=self.spatial_rank))
-        return self.lower + uniform * self.size
-
-    def corner_representation(self) -> 'Box':
-        return Box(self.lower, self.upper)
-
-    box = corner_representation
-
-    def center_representation(self) -> 'Cuboid':
-        return Cuboid(self.center, self.half_size)
-
-    def contains(self, other: 'BaseBox'):
-        """ Tests if the other box lies fully inside this box. """
-        return np.all(other.lower >= self.lower) and np.all(other.upper <= self.upper)
-
-    def rotated(self, angle) -> Geometry:
-        from ._transform import rotate
-        return rotate(self, angle)
-
-    def scaled(self, factor: Union[float, Tensor]) -> 'Geometry':
-        return Cuboid(self.center, self.half_size * factor)
-
-
-class BoxType(type):
-    """ Deprecated. Does not support item names. """
-
-    def __getitem__(self, item):
-        assert isinstance(item, tuple) and isinstance(item[0], str), "The Box constructor was updated in Φ-Flow version 2.2. Please add the dimension order as a comma-separated string as the first argument, e.g. Box['x,y', 0:1, 1:2] or use the kwargs constructor Box(x=1, y=(1, 2))"
-        assert len(item) <= 4, f"Box[...] can only be used for x, y, z but got {len(item)} elements"
-        dim_order = parse_dim_order(item[0])
-        assert len(dim_order) == len(item) - 1, f"Dimension order '{item[0]}' does not match number of slices, {len(item) - 1}"
-        lower = []
-        upper = []
-        for dim_name, dim in zip(dim_order, item[1:]):
-            assert isinstance(dim, slice)
-            assert dim.step is None or dim.step == 1, "Box: step must be 1 but is %s" % dim.step
-            lower.append(dim.start if dim.start is not None else -np.inf)
-            upper.append(dim.stop if dim.stop is not None else np.inf)
-        vec = math.channel(vector=dim_order)
-        lower = math.stack(lower, vec)
-        upper = math.stack(upper, vec)
-        return Box(lower, upper)
-
-
-class Box(BaseBox, metaclass=BoxType):
-    """
-    Simple cuboid defined by location of lower and upper corner in physical space.
-
-    Boxes can be constructed either from two positional vector arguments `(lower, upper)` or by specifying the limits by dimension name as `kwargs`.
-
-    Examples:
-        >>> Box(x=1, y=1)  # creates a two-dimensional unit box with `lower=(0, 0)` and `upper=(1, 1)`.
-        >>> Box(x=(None, 1), y=(0, None)  # creates a Box with `lower=(-inf, 0)` and `upper=(1, inf)`.
-
-        The slicing constructor was updated in version 2.2 and now requires the dimension order as the first argument.
-
-        >>> Box['x,y', 0:1, 0:1]  # creates a two-dimensional unit box with `lower=(0, 0)` and `upper=(1, 1)`.
-        >>> Box['x,y', :1, 0:]  # creates a Box with `lower=(-inf, 0)` and `upper=(1, inf)`.
-    """
-
-    def __init__(self, lower: Tensor = None, upper: Tensor = None, **size: Union[int, Tensor]):
-        """
-        Args:
-          lower: physical location of lower corner
-          upper: physical location of upper corner
-          **size: Specify size by dimension, either as `int` or `tuple` containing (lower, upper).
-        """
-        if lower is not None:
-            assert isinstance(lower, Tensor), f"lower must be a Tensor but got {type(lower)}"
-            assert 'vector' in lower.shape, "lower must have a vector dimension"
-            assert lower.vector.item_names is not None, "vector dimension of lower must list spatial dimension order"
-            self._lower = lower
-        if upper is not None:
-            assert isinstance(upper, Tensor), f"upper must be a Tensor but got {type(upper)}"
-            assert 'vector' in upper.shape, "lower must have a vector dimension"
-            assert upper.vector.item_names is not None, "vector dimension of lower must list spatial dimension order"
-            self._upper = upper
-        else:
-            lower = []
-            upper = []
-            for item in size.values():
-                if isinstance(item, (tuple, list)):
-                    assert len(item) == 2, f"Box kwargs must be either dim=upper or dim=(lower,upper) but got {item}"
-                    lo, up = item
-                    lower.append(lo)
-                    upper.append(up)
-                elif item is None:
-                    lower.append(-INF)
-                    upper.append(INF)
-                else:
-                    lower.append(0)
-                    upper.append(item)
-            lower = [-INF if l is None else l for l in lower]
-            upper = [INF if u is None else u for u in upper]
-            self._upper = math.wrap(upper, math.channel(vector=tuple(size.keys())))
-            self._lower = math.wrap(lower, math.channel(vector=tuple(size.keys())))
-        vector_shape = self._lower.shape & self._upper.shape
-        self._lower = math.expand(self._lower, vector_shape)
-        self._upper = math.expand(self._upper, vector_shape)
-        if self.size.vector.item_names is None:
-            warnings.warn("Creating a Box without item names prevents certain operations like project()", DeprecationWarning, stacklevel=2)
-
-    def __getitem__(self, item):
-        item = _keep_vector(slicing_dict(self, item))
-        return Box(self._lower[item], self._upper[item])
-
-    @staticmethod
-    def __stack__(values: tuple, dim: Shape, **kwargs) -> 'Geometry':
-        if all(isinstance(v, Box) for v in values):
-            return NotImplemented  # stack attributes
-        else:
-            return Geometry.__stack__(values, dim, **kwargs)
-
-    def __eq__(self, other):
-        if self._lower is None and self._upper is None:
-            return isinstance(other, Box)
-        return isinstance(other, BaseBox)\
-               and set(self.shape) == set(other.shape)\
-               and self.size.shape.get_size('vector') == other.size.shape.get_size('vector')\
-               and math.close(self._lower, other.lower)\
-               and math.close(self._upper, other.upper)
-
-    def without(self, dims: Tuple[str, ...]):
-        remaining = list(self.shape.get_item_names('vector'))
-        for dim in dims:
-            if dim in remaining:
-                remaining.remove(dim)
-        return self.vector[remaining]
-
-    def __hash__(self):
-        return hash(self._upper)
-
-    def __variable_attrs__(self):
-        return '_lower', '_upper'
-
-    @property
-    def shape(self):
-        if self._lower is None or self._upper is None:
-            return None
-        return self._lower.shape & self._upper.shape
-
-    @property
-    def lower(self):
-        return self._lower
-
-    @property
-    def upper(self):
-        return self._upper
-
-    @property
-    def size(self):
-        return self.upper - self.lower
-
-    @property
-    def center(self):
-        return 0.5 * (self.lower + self.upper)
-
-    @property
-    def half_size(self):
-        return self.size * 0.5
-
-    def shifted(self, delta, **delta_by_dim):
-        return Box(self.lower + delta, self.upper + delta)
-
-    def __mul__(self, other):
-        if not isinstance(other, Box):
-            return NotImplemented
-        lower = self._lower.vector.unstack(self.spatial_rank) + other._lower.vector.unstack(other.spatial_rank)
-        upper = self._upper.vector.unstack(self.spatial_rank) + other._upper.vector.unstack(other.spatial_rank)
-        names = self._upper.vector.item_names + other._upper.vector.item_names
-        lower = math.stack(lower, math.channel(vector=names))
-        upper = math.stack(upper, math.channel(vector=names))
-        return Box(lower, upper)
-
-    def __repr__(self):
-        if self.shape.non_channel.volume == 1:
-            item_names = self.size.vector.item_names
-            if item_names:
-                return f"Box({', '.join([f'{dim}=({lo}, {up})' for dim, lo, up in zip(item_names, self._lower, self._upper)])})"
-            else:  # deprecated
-                return 'Box[%s at %s]' % ('x'.join([str(x) for x in self.size.numpy().flatten()]), ','.join([str(x) for x in self.lower.numpy().flatten()]))
-        else:
-            return f'Box[shape={self.shape}]'
-
-
-class Cuboid(BaseBox):
-    """
-    Box specified by center position and half size.
-    """
-
-    def __init__(self,
-                 center: Tensor = 0,
-                 half_size: Union[float, Tensor] = None,
-                 **size: Union[float, Tensor]):
-        if half_size is not None:
-            assert isinstance(half_size, Tensor), "half_size must be a Tensor"
-            assert 'vector' in half_size.shape, f"Cuboid size must have a 'vector' dimension."
-            assert half_size.shape.get_item_names('vector') is not None, f"Vector dimension must list spatial dimensions as item names. Use the syntax Cuboid(x=x, y=y) to assign names."
-            self._half_size = half_size
-        else:
-            self._half_size = math.wrap(tuple(size.values()), math.channel(vector=tuple(size.keys()))) * 0.5
-        center = wrap(center)
-        if 'vector' not in center.shape or center.shape.get_item_names('vector') is None:
-            center = math.expand(center, channel(self._half_size))
-        self._center = center
-
-
-    def __eq__(self, other):
-        if self._center is None and self._half_size is None:
-            return isinstance(other, Cuboid)
-        return isinstance(other, BaseBox)\
-               and set(self.shape) == set(other.shape)\
-               and math.close(self._center, other.center)\
-               and math.close(self._half_size, other.half_size)
-
-    def __hash__(self):
-        return hash(self._center)
-
-    def __repr__(self):
-        return f"Cuboid(center={self._center}, half_size={self._half_size})"
-
-    def __getitem__(self, item):
-        item = _keep_vector(slicing_dict(self, item))
-        return Cuboid(self._center[item], self._half_size[item])
-
-    @staticmethod
-    def __stack__(values: tuple, dim: Shape, **kwargs) -> 'Geometry':
-        if all(isinstance(v, Cuboid) for v in values):
-            return Cuboid(math.stack([v.center for v in values], dim, **kwargs), math.stack([v.half_size for v in values], dim, **kwargs))
-        else:
-            return Geometry.__stack__(values, dim, **kwargs)
-
-    def __variable_attrs__(self):
-        return '_center', '_half_size'
-
-    @property
-    def center(self):
-        return self._center
-
-    @property
-    def half_size(self):
-        return self._half_size
-
-    @property
-    def shape(self):
-        if self._center is None or self._half_size is None:
-            return None
-        return self._center.shape & self._half_size.shape
-
-    @property
-    def size(self):
-        return 2 * self.half_size
-
-    @property
-    def lower(self):
-        return self.center - self.half_size
-
-    @property
-    def upper(self):
-        return self.center + self.half_size
-
-    def shifted(self, delta, **delta_by_dim) -> 'Cuboid':
-        return Cuboid(self._center + delta, self._half_size)
-
-
-def bounding_box(geometry):
-    center = geometry.center
-    extent = geometry.bounding_half_extent()
-    return Box(lower=center - extent, upper=center + extent)
-
-
-class GridCell(BaseBox):
-    """
-    An instance of GridCell represents all cells of a regular grid as a batch of boxes.
-    """
-
-    def __init__(self, resolution: math.Shape, bounds: BaseBox):
-        assert resolution.spatial_rank == resolution.rank, f"resolution must be purely spatial but got {resolution}"
-        assert resolution.spatial_rank == bounds.spatial_rank, f"bounds must match dimensions of resolution but got {bounds} for resolution {resolution}"
-        self._resolution = resolution
-        self._bounds = bounds
-        self._shape = resolution & bounds.shape.non_spatial
-
-    @property
-    def resolution(self):
-        return self._resolution
-
-    @property
-    def bounds(self):
-        return self._bounds
-
-    @property
-    def spatial_rank(self) -> int:
-        return self._resolution.spatial_rank
-
-    @property
-    def center(self):
-        local_coords = math.meshgrid(**{dim.name: math.linspace(0.5 / dim.size, 1 - 0.5 / dim.size, dim) for dim in self.resolution})
-        points = self.bounds.local_to_global(local_coords)
-        return points
-
-    @property
-    def grid_size(self):
-        return self._bounds.size
-
-    @property
-    def size(self):
-        return self.bounds.size / math.wrap(self.resolution.sizes)
-
-    @property
-    def dx(self):
-        return self.bounds.size / self.resolution
-
-    @property
-    def lower(self):
-        return self.center - self.half_size
-
-    @property
-    def upper(self):
-        return self.center + self.half_size
-
-    @property
-    def half_size(self):
-        return self.bounds.size / self.resolution.sizes / 2
-
-    def __getitem__(self, item):
-        item = slicing_dict(self, item)
-        bounds = self._bounds
-        dx = self.size
-        gather_dict = {}
-        for dim, selection in item.items():
-            if dim in self._resolution:
-                if isinstance(selection, int):
-                    start = selection
-                    stop = selection + 1
-                elif isinstance(selection, slice):
-                    start = selection.start or 0
-                    if start < 0:
-                        start += self.resolution.get_size(dim)
-                    stop = selection.stop or self.resolution.get_size(dim)
-                    if stop < 0:
-                        stop += self.resolution.get_size(dim)
-                    assert selection.step is None or selection.step == 1
-                else:
-                    raise ValueError(f"Illegal selection: {item}")
-                dim_mask = math.wrap(self.resolution.mask(dim))
-                lower = bounds.lower + start * dim_mask * dx
-                upper = bounds.upper + (stop - self.resolution.get_size(dim)) * dim_mask * dx
-                bounds = Box(lower, upper)
-                gather_dict[dim] = slice(start, stop)
-        resolution = self._resolution.after_gather(gather_dict)
-        return GridCell(resolution, bounds[{d: s for d, s in item.items() if d != 'vector'}])
-
-    def __pack_dims__(self, dims: Tuple[str, ...], packed_dim: Shape, pos: Union[int, None], **kwargs) -> 'Cuboid':
-        return math.pack_dims(self.center_representation(), dims, packed_dim, pos, **kwargs)
-
-    @staticmethod
-    def __stack__(values: tuple, dim: Shape, **kwargs) -> 'Geometry':
-        from ._stack import GeometryStack
-        return GeometryStack(math.layout(values, dim))
-
-    def list_cells(self, dim_name):
-        center = math.pack_dims(self.center, self._shape.spatial.names, dim_name)
-        return Cuboid(center, self.half_size)
-
-    def stagger(self, dim: str, lower: bool, upper: bool):
-        dim_mask = np.array(self.resolution.mask(dim))
-        unit = self.bounds.size / self.resolution * dim_mask
-        bounds = Box(self.bounds.lower + unit * (-0.5 if lower else 0.5), self.bounds.upper + unit * (0.5 if upper else -0.5))
-        ext_res = self.resolution.sizes + dim_mask * (int(lower) + int(upper) - 1)
-        return GridCell(self.resolution.with_sizes(ext_res), bounds)
-
-    def padded(self, widths: dict):
-        resolution, bounds = self.resolution, self.bounds
-        for dim, (lower, upper) in widths.items():
-            masked_dx = self.dx * math.dim_mask(self.resolution, dim)
-            resolution = resolution.with_dim_size(dim, self.resolution.get_size(dim) + lower + upper)
-            bounds = Box(bounds.lower - masked_dx * lower, bounds.upper + masked_dx * upper)
-        return GridCell(resolution, bounds)
-
-    # def face_centers(self, staggered_name='staggered'):
-    #     face_centers = [self.extend_symmetric(dim).center for dim in self.shape.spatial.names]
-    #     return math.channel_stack(face_centers, staggered_name)
-
-    @property
-    def shape(self):
-        return self._shape
-
-    def shifted(self, delta: Tensor, **delta_by_dim) -> BaseBox:
-        # delta += math.padded_stack()
-        if delta.shape.spatial_rank == 0:
-            return GridCell(self.resolution, self.bounds.shifted(delta))
-        else:
-            center = self.center + delta
-            return Cuboid(center, self.half_size)
-
-    def rotated(self, angle) -> Geometry:
-        raise NotImplementedError("Grids cannot be rotated. Use center_representation() to convert it to Cuboids first.")
-
-    def __eq__(self, other):
-        return isinstance(other, GridCell) and self._bounds == other._bounds and self._resolution == other._resolution
-
-    def shallow_equals(self, other):
-        return self == other
-
-    def __hash__(self):
-        return hash(self._resolution) + hash(self._bounds)
-
-    def __repr__(self):
-        return f"{self._resolution}, bounds={self._bounds}"
-
-    def __variable_attrs__(self):
-        return '_center', '_half_size'
-
-    def __with_attrs__(self, **attrs):
-        return copy_with(self.center_representation(), **attrs)
-
-    @property
-    def _center(self):
-        return self.center
-
-    @property
-    def _half_size(self):
-        return self.half_size
+from numbers import Number
+from typing import Union
+
+from phi import math
+from phi.math import Tensor, Shape, EMPTY_SHAPE, non_channel, wrap, shape
+from phi.math._magic_ops import variable_attributes, expand
+from phi.math.magic import BoundDim, slicing_dict
+
+
+class Geometry:
+    """
+    Abstract base class for N-dimensional shapes.
+
+    Main implementing classes:
+
+    * Sphere
+    * box family: box (generator), Box, Cuboid, BaseBox
+
+    All geometry objects support batching.
+    Thereby any parameter defining the geometry can be varied along arbitrary batch dims.
+    All batch dimensions are listed in Geometry.shape.
+    """
+
+    @property
+    def center(self) -> Tensor:
+        """
+        Center location in single channel dimension.
+        """
+        raise NotImplementedError(self)
+
+    @property
+    def shape(self) -> Shape:
+        """
+        The `shape` of a `Geometry` consists of the following dimensions:
+
+        * A single *channel* dimension called `'vector'` specifying the physical space
+        * Instance dimensions denote that this geometry consists of multiple copies in the same space
+        * Spatial dimensions denote a crystal (repeating structure) of this geometric primitive in space
+        * Batch dimensions indicate non-interacting versions of this geometry for parallelization only.
+        """
+        raise NotImplementedError()
+
+    @property
+    def volume(self) -> Tensor:
+        """
+        Volume of the geometry as `phi.math.Tensor`.
+        The result retains all batch dimensions while instance dimensions are summed over.
+        """
+        raise NotImplementedError()
+
+    @property
+    def shape_type(self) -> Tensor:
+        """
+        Returns the type (or types) of this geometry as a string `Tensor`
+        Boxes return `'B'`, spheres return `'S'`, points return `'P'`.
+        Returns `'?'` for unknown types, e.g. a union over multiple types.
+        Custom types can return their own identifiers.
+
+        Returns:
+            String `Tensor`
+        """
+        raise NotImplementedError()
+
+    def unstack(self, dimension: str) -> tuple:
+        """
+        Unstacks this Geometry along the given dimension.
+        The shapes of the returned geometries are reduced by `dimension`.
+
+        Args:
+            dimension: dimension along which to unstack
+
+        Returns:
+            geometries: tuple of length equal to `geometry.shape.get_size(dimension)`
+        """
+        return math.unstack(self, dimension)
+
+    @property
+    def spatial_rank(self) -> int:
+        """ Number of spatial dimensions of the geometry, 1 = 1D, 2 = 2D, 3 = 3D, etc. """
+        return self.shape.get_size('vector')
+
+    def lies_inside(self, location: Tensor) -> Tensor:
+        """
+        Tests whether the given location lies inside or outside of the geometry. Locations on the surface count as inside.
+
+        When dealing with unions or collections of geometries (instance dimensions), a point lies inside the geometry if it lies inside any instance.
+
+        Args:
+          location: float tensor of shape (batch_size, ..., rank)
+
+        Returns:
+          bool tensor of shape (*location.shape[:-1], 1).
+
+        """
+        raise NotImplementedError(self.__class__)
+
+    def approximate_signed_distance(self, location: Union[Tensor, tuple]) -> Tensor:
+        """
+        Computes the approximate distance from location to the surface of the geometry.
+        Locations outside return positive values, inside negative values and zero exactly at the boundary.
+
+        The exact distance metric used depends on the geometry.
+        The approximation holds close to the surface and the distance grows to infinity as the location is moved infinitely far from the geometry.
+        The distance metric is differentiable and its gradients are bounded at every point in space.
+
+        When dealing with unions or collections of geometries (instance dimensions), the shortest distance to any instance is returned.
+        This also holds for negative distances.
+
+        Args:
+          location: float tensor of shape (batch_size, ..., rank)
+          location: Tensor:
+
+        Returns:
+          float tensor of shape (*location.shape[:-1], 1).
+
+        """
+        raise NotImplementedError(self.__class__)
+
+    def approximate_fraction_inside(self, other_geometry: 'Geometry', balance: Union[Tensor, Number] = 0.5) -> Tensor:
+        """
+        Computes the approximate overlap between the geometry and a small other geometry.
+        Returns 1.0 if `other_geometry` is fully enclosed in this geometry and 0.0 if there is no overlap.
+        Close to the surface of this geometry, the fraction filled is differentiable w.r.t. the location and size of `other_geometry`.
+
+        To call this method on batches of geometries of same shape, pass a batched Geometry instance.
+        The result tensor will match the batch shape of `other_geometry`.
+
+        The result may only be accurate in special cases.
+        The given geometries may be approximated as spheres or boxes using `bounding_radius()` and `bounding_half_extent()`.
+
+        The default implementation of this method approximates other_geometry as a Sphere and computes the fraction using `approximate_signed_distance()`.
+
+        Args:
+            other_geometry: `Geometry` or geometry batch for which to compute the overlap with `self`.
+            balance: Mid-level between 0 and 1, default 0.5.
+                This value is returned when exactly half of `other_geometry` lies inside `self`.
+                `0.5 < balance <= 1` makes `self` seem larger while `0 <= balance < 0.5`makes `self` seem smaller.
+
+        Returns:
+          fraction of cell volume lying inside the geometry. float tensor of shape (other_geometry.batch_shape, 1).
+
+        """
+        assert isinstance(other_geometry, Geometry)
+        radius = other_geometry.bounding_radius()
+        location = other_geometry.center
+        distance = self.approximate_signed_distance(location)
+        inside_fraction = balance - distance / radius
+        inside_fraction = math.clip(inside_fraction, 0, 1)
+        return inside_fraction
+
+    def push(self, positions: Tensor, outward: bool = True, shift_amount: float = 0) -> Tensor:
+        """
+        Shifts positions either into or out of geometry.
+
+        Args:
+            positions: Tensor holding positions to shift
+            outward: Flag for indicating inward (False) or outward (True) shift
+            shift_amount: Minimum distance between positions and box boundaries after shifting
+
+        Returns:
+            Tensor holding shifted positions
+        """
+        raise NotImplementedError(self.__class__)
+
+    def sample_uniform(self, *shape: math.Shape) -> Tensor:
+        """
+        Samples uniformly distributed random points inside this volume.
+
+        Args:
+            *shape: How many points to sample per individual geometry.
+
+        Returns:
+            `Tensor` containing all dimensions from `Geometry.shape`, `shape` as well as a `channel` dimension `vector` matching the dimensionality of this `Geometry`.
+        """
+        raise NotImplementedError(self.__class__)
+
+    def bounding_radius(self) -> Tensor:
+        """
+        Returns the radius of a Sphere object that fully encloses this geometry.
+        The sphere is centered at the center of this geometry.
+
+        :return: radius of type float
+
+        Args:
+
+        Returns:
+
+        """
+        raise NotImplementedError(self.__class__)
+
+    def bounding_half_extent(self) -> Tensor:
+        """
+        The bounding half-extent sets a limit on the outer-most point for each coordinate axis.
+        Each component is non-negative.
+
+        Let the bounding half-extent have value `e` in dimension `d` (`extent[...,d] = e`).
+        Then, no point of the geometry lies further away from its center point than `e` along `d` (in both axis directions).
+
+        :return: float vector
+
+        Args:
+
+        Returns:
+
+        """
+        raise NotImplementedError(self.__class__)
+
+    def bounding_box(self) -> 'BaseBox':
+        """
+        Returns the approximately smallest axis-aligned box that contains this `Geometry`.
+        The center of the box may not be equal to `self.center`.
+
+        Returns:
+            `Box` or `Cuboid` that fully contains this `Geometry`.
+        """
+        from ._box import Cuboid
+        return Cuboid(self.center, half_size=self.bounding_half_extent())
+
+    def shifted(self, delta: Tensor) -> 'Geometry':
+        """
+        Returns a translated version of this geometry.
+
+        See Also:
+            `Geometry.at()`.
+
+        Args:
+          delta: direction vector
+          delta: Tensor:
+
+        Returns:
+          Geometry: shifted geometry
+
+        """
+        return self.at(self.center + delta)
+
+    def at(self, center: Tensor) -> 'Geometry':
+        """
+        Returns a copy of this `Geometry` with the center at `center`.
+        This is equal to calling `self @ center`.
+
+        See Also:
+            `Geometry.shifted()`.
+
+        Args:
+            center: New center as `Tensor`.
+
+        Returns:
+            `Geometry`.
+        """
+        raise NotImplementedError
+
+    def __matmul__(self, other):
+        return self.at(other)
+
+    def rotated(self, angle: Union[float, Tensor]) -> 'Geometry':
+        """
+        Returns a rotated version of this geometry.
+        The geometry is rotated about its center point.
+
+        Args:
+          angle: scalar (2d) or vector (3D+) representing delta angle
+
+        Returns:
+            Rotated `Geometry`
+        """
+        raise NotImplementedError(self.__class__)
+
+    def scaled(self, factor: Union[float, Tensor]) -> 'Geometry':
+        """
+        Scales each individual geometry by `factor`.
+        The individual `center` points act as pivots for the operation.
+
+        Args:
+            factor:
+
+        Returns:
+
+        """
+        raise NotImplementedError(self.__class__)
+
+    def __invert__(self):
+        return _InvertedGeometry(self)
+
+    def __eq__(self, other):
+        """
+        Slow equality check.
+        Unlike `==`, this method compares all tensor elements to check whether they are equal.
+        Use `==` for a faster check which only checks whether the referenced tensors are the same.
+
+        See Also:
+            `shallow_equals()`
+        """
+        if self is other:
+            return True
+        if not isinstance(other, type(self)):
+            return False
+        if self.shape != other.shape:
+            return False
+        c1 = {a: getattr(self, a) for a in variable_attributes(self)}
+        c2 = {a: getattr(other, a) for a in variable_attributes(self)}
+        for c in c1.keys():
+            if c1[c] is not c2[c] and math.any(c1[c] != c2[c]):
+                return False
+        return True
+
+    def shallow_equals(self, other):
+        """
+        Quick equality check.
+        May return `False` even if `other == self`.
+        However, if `True` is returned, the geometries are guaranteed to be equal.
+
+        The `shallow_equals()` check does not compare all tensor elements but merely checks whether the same tensors are referenced.
+        """
+        if self is other:
+            return True
+        if not isinstance(other, type(self)):
+            return False
+        if self.shape != other.shape:
+            return False
+        c1 = {a: getattr(self, a) for a in variable_attributes(self)}
+        c2 = {a: getattr(other, a) for a in variable_attributes(self)}
+        for c in c1.keys():
+            if c1[c] is not c2[c]:
+                return False
+        return True
+
+    @staticmethod
+    def __stack__(values: tuple, dim: Shape, **kwargs) -> 'Geometry':
+        if all(type(v) == type(values[0]) for v in values):
+            return NotImplemented  # let attributes be stacked
+        else:
+            from ._stack import GeometryStack
+            return GeometryStack(math.layout(values, dim))
+
+    def __flatten__(self, flat_dim: Shape, flatten_batch: bool, **kwargs) -> 'Geometry':
+        dims = self.shape.without('vector')
+        if not flatten_batch:
+            dims = dims.non_batch
+        return math.pack_dims(self, dims, flat_dim, **kwargs)
+
+    def __ne__(self, other):
+        return not self == other
+
+    def __hash__(self):
+        raise NotImplementedError(self.__class__)
+
+    def __repr__(self):
+        return f"{self.__class__.__name__}{self.shape}"
+
+    def __getitem__(self, item):
+        raise NotImplementedError
+        # assert isinstance(item, dict), "Index must be dict of type {dim: slice/int}."
+        # item = {dim: sel for dim, sel in item.items() if dim != 'vector'}
+        # attrs = {a: getattr(self, a)[item] for a in variable_attributes(self)}
+        # return copy_with(self, **attrs)
+
+    def __getattr__(self, name: str) -> BoundDim:
+        return BoundDim(self, name)
+
+
+class _InvertedGeometry(Geometry):
+
+    def __init__(self, geometry):
+        self.geometry = geometry
+
+    @property
+    def volume(self) -> Tensor:
+        return math.wrap(math.INF)
+
+    @property
+    def shape_type(self) -> Tensor:
+        raise NotImplementedError
+
+    def sample_uniform(self, *shape: math.Shape) -> Tensor:
+        raise NotImplementedError
+
+    def scaled(self, factor: Union[float, Tensor]) -> 'Geometry':
+        return _InvertedGeometry(self.geometry.scaled(factor))
+
+    def __getitem__(self, item: dict):
+        return _InvertedGeometry(self.geometry[item])
+
+    @property
+    def center(self):
+        raise NotImplementedError
+
+    @property
+    def shape(self):
+        return self.geometry.shape
+
+    def lies_inside(self, location: Tensor) -> Tensor:
+        return ~self.geometry.lies_inside(location)
+
+    def approximate_signed_distance(self, location: Tensor) -> Tensor:
+        return -self.geometry.approximate_signed_distance(location)
+
+    def approximate_fraction_inside(self, other_geometry: 'Geometry', balance: Union[Tensor, Number] = 0.5) -> Tensor:
+        return 1 - self.geometry.approximate_fraction_inside(other_geometry, 1 - balance)
+
+    def push(self, positions: Tensor, outward: bool = True, shift_amount: float = 0) -> Tensor:
+        return self.geometry.push(positions, outward=not outward, shift_amount=shift_amount)
+
+    def bounding_radius(self) -> Tensor:
+        raise NotImplementedError()
+
+    def bounding_half_extent(self) -> Tensor:
+        raise NotImplementedError()
+
+    def at(self, center: Tensor) -> 'Geometry':
+        return _InvertedGeometry(self.geometry.at(center))
+
+    def rotated(self, angle) -> Geometry:
+        return _InvertedGeometry(self.geometry.rotated(angle))
+
+    def unstack(self, dimension):
+        return [_InvertedGeometry(g) for g in self.geometry.unstack(dimension)]
+
+    def __eq__(self, other):
+        return isinstance(other, _InvertedGeometry) and self.geometry == other.geometry
+
+    def __hash__(self):
+        return -hash(self.geometry)
+
+
+def invert(geometry: Geometry):
+    """
+    Swaps inside and outside.
+
+    Args:
+        geometry: `phi.geom.Geometry` to swap
+
+    Returns:
+        New `phi.geom.Geometry` object with same surface but swapped normals
+    """
+    return ~geometry
+
+
+class _NoGeometry(Geometry):
+
+    @property
+    def shape_type(self) -> Tensor:
+        raise NotImplementedError
+
+    def push(self, positions: Tensor, outward: bool = True, shift_amount: float = 0) -> Tensor:
+        return positions
+
+    def sample_uniform(self, *shape: math.Shape) -> Tensor:
+        raise NotImplementedError
+
+    def scaled(self, factor: Union[float, Tensor]) -> 'Geometry':
+        return self
+
+    def __getitem__(self, item: dict):
+        return self
+
+    @property
+    def shape(self):
+        return EMPTY_SHAPE
+
+    @property
+    def volume(self) -> Tensor:
+        return wrap(0)
+
+    @property
+    def center(self) -> Tensor:
+        return wrap(0)
+
+    def bounding_radius(self) -> Tensor:
+        return wrap(0)
+
+    def bounding_half_extent(self) -> Tensor:
+        return wrap(0)
+
+    def approximate_signed_distance(self, location):
+        return math.expand(math.INF, non_channel(location))
+
+    def lies_inside(self, location):
+        return math.zeros(non_channel(location), dtype=bool)
+
+    def approximate_fraction_inside(self, other_geometry: 'Geometry', balance: Union[Tensor, Number] = 0.5) -> Tensor:
+        return math.zeros(other_geometry.shape)
+
+    def at(self, center: Tensor) -> 'Geometry':
+        return self
+
+    def rotated(self, angle):
+        return self
+
+    def unstack(self, dimension):
+        raise AssertionError('empty geometry cannot be unstacked')
+
+    def __eq__(self, other):
+        return isinstance(other, _NoGeometry)
+
+    def __hash__(self):
+        return 1
+
+
+NO_GEOMETRY = _NoGeometry()
+
+
+class Point(Geometry):
+    """
+    Points have zero volume and are determined by a single location.
+    An instance of `Point` represents a single n-dimensional point or a batch of points.
+    """
+
+    def __init__(self, location: math.Tensor):
+        assert 'vector' in location.shape, "location must have a vector dimension"
+        assert location.shape.get_item_names('vector') is not None, "Vector dimension needs to list spatial dimension as item names."
+        self._location = location
+
+    @property
+    def center(self) -> Tensor:
+        return self._location
+
+    @property
+    def shape(self) -> Shape:
+        return self._location.shape
+
+    def unstack(self, dimension: str) -> tuple:
+        return tuple(Point(loc) for loc in self._location.unstack(dimension))
+
+    def lies_inside(self, location: Tensor) -> Tensor:
+        return expand(math.wrap(False), shape(location).without('vector'))
+
+    def approximate_signed_distance(self, location: Union[Tensor, tuple]) -> Tensor:
+        return math.vec_abs(location - self._location)
+
+    def push(self, positions: Tensor, outward: bool = True, shift_amount: float = 0) -> Tensor:
+        return positions
+
+    def bounding_radius(self) -> Tensor:
+        return math.zeros()
+
+    def bounding_half_extent(self) -> Tensor:
+        return math.zeros()
+
+    def at(self, center: Tensor) -> 'Geometry':
+        return Point(center)
+
+    def rotated(self, angle) -> 'Geometry':
+        return self
+
+    def __hash__(self):
+        return hash(self._location)
+
+    def __variable_attrs__(self):
+        return '_location',
+
+    @property
+    def volume(self) -> Tensor:
+        return math.wrap(0)
+
+    @property
+    def shape_type(self) -> Tensor:
+        return math.tensor('P')
+
+    def sample_uniform(self, *shape: math.Shape) -> Tensor:
+        raise NotImplementedError
+
+    def scaled(self, factor: Union[float, Tensor]) -> 'Geometry':
+        return self
+
+    def __getitem__(self, item):
+        return Point(self._location[_keep_vector(slicing_dict(self, item))])
+
+
+def assert_same_rank(rank1, rank2, error_message):
+    """ Tests that two objects have the same spatial rank. Objects can be of types: `int`, `None` (no check), `Geometry`, `Shape`, `Tensor` """
+    rank1_, rank2_ = _rank(rank1), _rank(rank2)
+    if rank1_ is not None and rank2_ is not None:
+        assert rank1_ == rank2_, 'Ranks do not match: %s and %s. %s' % (rank1_, rank2_, error_message)
+
+
+def _rank(rank):
+    if rank is None:
+        return None
+    elif isinstance(rank, int):
+        pass
+    elif isinstance(rank, Geometry):
+        rank = rank.spatial_rank
+    elif isinstance(rank, Shape):
+        rank = rank.spatial.rank
+    elif isinstance(rank, Tensor):
+        rank = rank.shape.spatial_rank
+    else:
+        raise NotImplementedError(f"{type(rank)} now allowed. Allowed are (int, Geometry, Shape, Tensor).")
+    return None if rank == 0 else rank
+
+
+def _keep_vector(dim_selection: dict) -> dict:
+    if 'vector' not in dim_selection:
+        return dim_selection
+    item = dict(dim_selection)
+    if isinstance(item['vector'], int) or (isinstance(item['vector'], str) and ',' not in item['vector']):
+        item['vector'] = (item['vector'],)
+    return item
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `phiflow-2.3.4/phi/geom/_sphere.py` & `phiflow-2.4.0/phi/geom/_sphere.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,118 +1,118 @@
-from typing import Union
-
-from phi import math
-from ._geom import Geometry, _keep_vector
-from ..math import wrap, Tensor, expand
-from ..math.magic import slicing_dict
-
-
-class Sphere(Geometry):
-    """
-    N-dimensional sphere.
-    Defined through center position and radius.
-    """
-
-    def __init__(self,
-                 center: Tensor = None,
-                 radius: Union[float, Tensor] = None,
-                 **center_: Union[float, Tensor]):
-        """
-        Args:
-            center: Sphere center as `Tensor` with `vector` dimension.
-                The spatial dimension order should be specified in the `vector` dimension via item names.
-            radius: Sphere radius as `float` or `Tensor`
-            **center_: Specifies center when the `center` argument is not given. Center position by dimension, e.g. `x=0.5, y=0.2`.
-        """
-        if center is not None:
-            assert isinstance(center, Tensor), "center must be a Tensor"
-            assert 'vector' in center.shape, f"Sphere center must have a 'vector' dimension."
-            assert center.shape.get_item_names('vector') is not None, f"Vector dimension must list spatial dimensions as item names. Use the syntax Sphere(x=x, y=y) to assign names."
-            self._center = center
-        else:
-            self._center = wrap(tuple(center_.values()), math.channel(vector=tuple(center_.keys())))
-        assert radius is not None, "radius must be specified."
-        self._radius = wrap(radius)
-        assert 'vector' not in self._radius.shape, f"Sphere radius must not vary along vector but got {radius}"
-
-    @property
-    def shape(self):
-        if self._center is None or self._radius is None:
-            return None
-        return self._center.shape & self._radius.shape
-
-    @property
-    def radius(self):
-        return self._radius
-
-    @property
-    def center(self):
-        return self._center
-
-    @property
-    def volume(self) -> math.Tensor:
-        if self.spatial_rank == 1:
-            return 2 * self._radius
-        elif self.spatial_rank == 2:
-            return math.PI * self._radius ** 2
-        elif self.spatial_rank == 3:
-            return 4 / 3 * math.PI * self._radius ** 3
-        else:
-            raise NotImplementedError()
-            # n = self.spatial_rank
-            # return math.pi ** (n // 2) / math.faculty(math.ceil(n / 2)) * self._radius ** n
-
-    @property
-    def shape_type(self) -> Tensor:
-        return math.tensor('S')
-
-    def lies_inside(self, location):
-        distance_squared = math.sum((location - self.center) ** 2, dim='vector')
-        return math.any(distance_squared <= self.radius ** 2, self.shape.instance)  # union for instance dimensions
-
-    def approximate_signed_distance(self, location: Union[Tensor, tuple]):
-        """
-        Computes the exact distance from location to the closest point on the sphere.
-        Very close to the sphere center, the distance takes a constant value.
-
-        Args:
-          location: float tensor of shape (batch_size, ..., rank)
-
-        Returns:
-          float tensor of shape (*location.shape[:-1], 1).
-
-        """
-        distance_squared = math.vec_squared(location - self.center)
-        distance_squared = math.maximum(distance_squared, self.radius * 1e-2)  # Prevent infinite spatial_gradient at sphere center
-        distance = math.sqrt(distance_squared)
-        return math.min(distance - self.radius, self.shape.instance)  # union for instance dimensions
-
-    def sample_uniform(self, *shape: math.Shape):
-        raise NotImplementedError('Not yet implemented')  # ToDo
-
-    def bounding_radius(self):
-        return self.radius
-
-    def bounding_half_extent(self):
-        return expand(self.radius, self._center.shape.only('vector'))
-
-    def at(self, center: Tensor) -> 'Geometry':
-        return Sphere(center, self._radius)
-
-    def rotated(self, angle):
-        return self
-
-    def scaled(self, factor: Union[float, Tensor]) -> 'Geometry':
-        return Sphere(self.center, self.radius * factor)
-
-    def __variable_attrs__(self):
-        return '_center', '_radius'
-
-    def __getitem__(self, item):
-        item = slicing_dict(self, item)
-        return Sphere(self._center[_keep_vector(item)], self._radius[item])
-
-    def push(self, positions: Tensor, outward: bool = True, shift_amount: float = 0) -> Tensor:
-        raise NotImplementedError()
-
-    def __hash__(self):
-        return hash(self._center) + hash(self._radius)
+from typing import Union
+
+from phi import math
+from ._geom import Geometry, _keep_vector
+from ..math import wrap, Tensor, expand
+from ..math.magic import slicing_dict
+
+
+class Sphere(Geometry):
+    """
+    N-dimensional sphere.
+    Defined through center position and radius.
+    """
+
+    def __init__(self,
+                 center: Tensor = None,
+                 radius: Union[float, Tensor] = None,
+                 **center_: Union[float, Tensor]):
+        """
+        Args:
+            center: Sphere center as `Tensor` with `vector` dimension.
+                The spatial dimension order should be specified in the `vector` dimension via item names.
+            radius: Sphere radius as `float` or `Tensor`
+            **center_: Specifies center when the `center` argument is not given. Center position by dimension, e.g. `x=0.5, y=0.2`.
+        """
+        if center is not None:
+            assert isinstance(center, Tensor), "center must be a Tensor"
+            assert 'vector' in center.shape, f"Sphere center must have a 'vector' dimension."
+            assert center.shape.get_item_names('vector') is not None, f"Vector dimension must list spatial dimensions as item names. Use the syntax Sphere(x=x, y=y) to assign names."
+            self._center = center
+        else:
+            self._center = wrap(tuple(center_.values()), math.channel(vector=tuple(center_.keys())))
+        assert radius is not None, "radius must be specified."
+        self._radius = wrap(radius)
+        assert 'vector' not in self._radius.shape, f"Sphere radius must not vary along vector but got {radius}"
+
+    @property
+    def shape(self):
+        if self._center is None or self._radius is None:
+            return None
+        return self._center.shape & self._radius.shape
+
+    @property
+    def radius(self):
+        return self._radius
+
+    @property
+    def center(self):
+        return self._center
+
+    @property
+    def volume(self) -> math.Tensor:
+        if self.spatial_rank == 1:
+            return 2 * self._radius
+        elif self.spatial_rank == 2:
+            return math.PI * self._radius ** 2
+        elif self.spatial_rank == 3:
+            return 4 / 3 * math.PI * self._radius ** 3
+        else:
+            raise NotImplementedError()
+            # n = self.spatial_rank
+            # return math.pi ** (n // 2) / math.faculty(math.ceil(n / 2)) * self._radius ** n
+
+    @property
+    def shape_type(self) -> Tensor:
+        return math.tensor('S')
+
+    def lies_inside(self, location):
+        distance_squared = math.sum((location - self.center) ** 2, dim='vector')
+        return math.any(distance_squared <= self.radius ** 2, self.shape.instance)  # union for instance dimensions
+
+    def approximate_signed_distance(self, location: Union[Tensor, tuple]):
+        """
+        Computes the exact distance from location to the closest point on the sphere.
+        Very close to the sphere center, the distance takes a constant value.
+
+        Args:
+          location: float tensor of shape (batch_size, ..., rank)
+
+        Returns:
+          float tensor of shape (*location.shape[:-1], 1).
+
+        """
+        distance_squared = math.vec_squared(location - self.center)
+        distance_squared = math.maximum(distance_squared, self.radius * 1e-2)  # Prevent infinite spatial_gradient at sphere center
+        distance = math.sqrt(distance_squared)
+        return math.min(distance - self.radius, self.shape.instance)  # union for instance dimensions
+
+    def sample_uniform(self, *shape: math.Shape):
+        raise NotImplementedError('Not yet implemented')  # ToDo
+
+    def bounding_radius(self):
+        return self.radius
+
+    def bounding_half_extent(self):
+        return expand(self.radius, self._center.shape.only('vector'))
+
+    def at(self, center: Tensor) -> 'Geometry':
+        return Sphere(center, self._radius)
+
+    def rotated(self, angle):
+        return self
+
+    def scaled(self, factor: Union[float, Tensor]) -> 'Geometry':
+        return Sphere(self.center, self.radius * factor)
+
+    def __variable_attrs__(self):
+        return '_center', '_radius'
+
+    def __getitem__(self, item):
+        item = slicing_dict(self, item)
+        return Sphere(self._center[_keep_vector(item)], self._radius[item])
+
+    def push(self, positions: Tensor, outward: bool = True, shift_amount: float = 0) -> Tensor:
+        raise NotImplementedError()
+
+    def __hash__(self):
+        return hash(self._center) + hash(self._radius)
```

### Comparing `phiflow-2.3.4/phi/geom/_stack.py` & `phiflow-2.4.0/phi/geom/_stack.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,102 +1,102 @@
-from typing import List, Tuple
-
-from phi import math
-from . import GridCell
-from ._geom import Geometry
-from ..math import Tensor, expand
-from ..math._shape import shape_stack, Shape, INSTANCE_DIM, non_channel
-from ..math._magic_ops import variable_attributes, copy_with, unstack
-from ..math.magic import slicing_dict
-
-
-class GeometryStack(Geometry):
-
-    def __init__(self, geometries: Tensor):
-        self.geometries = geometries
-        self._shape = shape_stack(geometries.shape, *[g.shape for g in geometries])
-
-    def unstack(self, dimension) -> tuple:
-        if dimension == self.geometries.shape.name:
-            return tuple(self.geometries)
-        else:
-            # return GeometryStack([g.unstack(dimension) for g in self.geometries], self.geometries.shape)
-            raise NotImplementedError()
-
-    @property
-    def center(self):
-        centers = [g.center for g in self.geometries]
-        return math.stack(centers, self.geometries.shape)
-
-    @property
-    def spatial_rank(self) -> int:
-        return next(iter(self.geometries)).spatial_rank
-
-    @property
-    def shape(self) -> Shape:
-        return self._shape
-
-    @property
-    def volume(self) -> math.Tensor:
-        if self.geometries.shape.type == INSTANCE_DIM:
-            raise NotImplementedError("instance dimensions not yet supported")
-        return math.stack([g.volume for g in self.geometries], self.geometries.shape)
-
-    @property
-    def shape_type(self) -> Tensor:
-        types = [g.shape_type for g in self.geometries]
-        return math.stack(types, self.geometries.shape)
-
-    def lies_inside(self, location: math.Tensor):
-        if self.geometries.shape in location.shape:
-            location = location.unstack(self.geometries.shape.name)
-        else:
-            location = [location] * len(self.geometries)
-        inside = [g.lies_inside(loc) for g, loc in zip(self.geometries, location)]
-        return math.stack(inside, self.geometries.shape)
-
-    def approximate_signed_distance(self, location: math.Tensor):
-        raise NotImplementedError()
-
-    def bounding_radius(self):
-        radii = [expand(g.bounding_radius(), non_channel(g)) for g in self.geometries]
-        return math.stack(radii, self.geometries.shape)
-
-    def bounding_half_extent(self):
-        values = [expand(g.bounding_half_extent(), non_channel(g)) for g in self.geometries]
-        return math.stack(values, self.geometries.shape)
-
-    def at(self, center: Tensor) -> 'Geometry':
-        geometries = [self.geometries[idx].native().at(center[idx]) for idx in self.geometries.shape.meshgrid()]
-        return GeometryStack(math.layout(geometries, self.geometries.shape))
-
-    def rotated(self, angle):
-        geometries = [g.rotated(angle) for g in self.geometries]
-        return GeometryStack(math.layout(geometries, self.geometries.shape))
-
-    def push(self, positions: Tensor, outward: bool = True, shift_amount: float = 0) -> Tensor:
-        raise NotImplementedError('GeometryStack.push() is not yet implemented.')
-
-    def __eq__(self, other):
-        return isinstance(other, GeometryStack) \
-               and self._shape == other.shape \
-               and self.geometries.shape == other.stack_dim \
-               and self.geometries == other.geometries
-
-    def shallow_equals(self, other):
-        if self is other:
-            return True
-        if not isinstance(other, GeometryStack) or self._shape != other.shape:
-            return False
-        if self.geometries.shape != other.geometries.shape:
-            return False
-        return all(g1.shallow_equals(g2) for g1, g2 in zip(self.geometries, other.geometries))
-
-    def __hash__(self):
-        return hash(self.geometries)
-    
-    def __getitem__(self, item):
-        selected = self.geometries[slicing_dict(self, item)]
-        if selected.shape.volume > 1:
-            return GeometryStack(selected)
-        else:
-            return next(iter(selected))
+from typing import List, Tuple
+
+from phi import math
+from . import GridCell
+from ._geom import Geometry
+from ..math import Tensor, expand
+from ..math._shape import shape_stack, Shape, INSTANCE_DIM, non_channel
+from ..math._magic_ops import variable_attributes, copy_with, unstack
+from ..math.magic import slicing_dict
+
+
+class GeometryStack(Geometry):
+
+    def __init__(self, geometries: Tensor):
+        self.geometries = geometries
+        self._shape = shape_stack(geometries.shape, *[g.shape for g in geometries])
+
+    def unstack(self, dimension) -> tuple:
+        if dimension == self.geometries.shape.name:
+            return tuple(self.geometries)
+        else:
+            # return GeometryStack([g.unstack(dimension) for g in self.geometries], self.geometries.shape)
+            raise NotImplementedError()
+
+    @property
+    def center(self):
+        centers = [g.center for g in self.geometries]
+        return math.stack(centers, self.geometries.shape)
+
+    @property
+    def spatial_rank(self) -> int:
+        return next(iter(self.geometries)).spatial_rank
+
+    @property
+    def shape(self) -> Shape:
+        return self._shape
+
+    @property
+    def volume(self) -> math.Tensor:
+        if self.geometries.shape.type == INSTANCE_DIM:
+            raise NotImplementedError("instance dimensions not yet supported")
+        return math.stack([g.volume for g in self.geometries], self.geometries.shape)
+
+    @property
+    def shape_type(self) -> Tensor:
+        types = [g.shape_type for g in self.geometries]
+        return math.stack(types, self.geometries.shape)
+
+    def lies_inside(self, location: math.Tensor):
+        if self.geometries.shape in location.shape:
+            location = location.unstack(self.geometries.shape.name)
+        else:
+            location = [location] * len(self.geometries)
+        inside = [g.lies_inside(loc) for g, loc in zip(self.geometries, location)]
+        return math.stack(inside, self.geometries.shape)
+
+    def approximate_signed_distance(self, location: math.Tensor):
+        raise NotImplementedError()
+
+    def bounding_radius(self):
+        radii = [expand(g.bounding_radius(), non_channel(g)) for g in self.geometries]
+        return math.stack(radii, self.geometries.shape)
+
+    def bounding_half_extent(self):
+        values = [expand(g.bounding_half_extent(), non_channel(g)) for g in self.geometries]
+        return math.stack(values, self.geometries.shape)
+
+    def at(self, center: Tensor) -> 'Geometry':
+        geometries = [self.geometries[idx].native().at(center[idx]) for idx in self.geometries.shape.meshgrid()]
+        return GeometryStack(math.layout(geometries, self.geometries.shape))
+
+    def rotated(self, angle):
+        geometries = [g.rotated(angle) for g in self.geometries]
+        return GeometryStack(math.layout(geometries, self.geometries.shape))
+
+    def push(self, positions: Tensor, outward: bool = True, shift_amount: float = 0) -> Tensor:
+        raise NotImplementedError('GeometryStack.push() is not yet implemented.')
+
+    def __eq__(self, other):
+        return isinstance(other, GeometryStack) \
+               and self._shape == other.shape \
+               and self.geometries.shape == other.stack_dim \
+               and self.geometries == other.geometries
+
+    def shallow_equals(self, other):
+        if self is other:
+            return True
+        if not isinstance(other, GeometryStack) or self._shape != other.shape:
+            return False
+        if self.geometries.shape != other.geometries.shape:
+            return False
+        return all(g1.shallow_equals(g2) for g1, g2 in zip(self.geometries, other.geometries))
+
+    def __hash__(self):
+        return hash(self.geometries)
+    
+    def __getitem__(self, item):
+        selected = self.geometries[slicing_dict(self, item)]
+        if selected.shape.volume > 1:
+            return GeometryStack(selected)
+        else:
+            return next(iter(selected))
```

### Comparing `phiflow-2.3.4/phi/geom/_transform.py` & `phiflow-2.4.0/phi/geom/_transform.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,234 +1,234 @@
-from numbers import Number
-from typing import Tuple, Union
-
-from phi import math
-from phi.math import Tensor, Shape
-from . import BaseBox, Box
-from ._geom import Geometry
-from ._sphere import Sphere
-from ..math._shape import parse_dim_order
-
-
-class RotatedGeometry(Geometry):
-
-    def __init__(self, geometry: Geometry, angle: Union[float, math.Tensor]):
-        assert not isinstance(geometry, RotatedGeometry)
-        self._geometry = geometry
-        self._angle = math.wrap(angle)
-
-    @property
-    def shape(self):
-        return self._geometry.shape
-
-    def __variable_attrs__(self):
-        return '_geometry', '_angle'
-
-    @property
-    def geometry(self):
-        return self._geometry
-
-    @property
-    def angle(self):
-        return self._angle
-
-    @property
-    def center(self):
-        return self.geometry.center
-
-    @property
-    def volume(self) -> Tensor:
-        return self._geometry.volume
-
-    @property
-    def shape_type(self) -> Tensor:
-        return math.map(lambda s: f"rot{s}", self._geometry.shape_type)
-
-    def global_to_child(self, location):
-        """ Inverse transform. """
-        delta = location - self.center
-        if location.shape.get_size('vector') == 2:
-            rotated = math.rotate_vector(delta, self._angle)
-        elif location.shape.get_size('vector') == 3:
-            raise NotImplementedError('not yet implemented')  # ToDo apply angle
-        else:
-            raise NotImplementedError('Rotation only supported in 2D and 3D')
-        final = rotated + self.center
-        return final
-
-    def push(self, positions: Tensor, outward: bool = True, shift_amount: float = 0):
-        rotated = self.global_to_child(positions)
-        shifted_positions = self.geometry.push(rotated, outward=outward, shift_amount=shift_amount)
-        return positions + math.rotate_vector(shifted_positions - rotated, self._angle)
-
-    def lies_inside(self, location):
-        return self.geometry.lies_inside(self.global_to_child(location))
-
-    def approximate_signed_distance(self, location):
-        return self.geometry.approximate_signed_distance(self.global_to_child(location))
-
-    def bounding_radius(self):
-        return self.geometry.bounding_radius()
-
-    def bounding_half_extent(self):
-        bounding_sphere = Sphere(self.center, self.bounding_radius())
-        return bounding_sphere.bounding_half_extent()
-
-    @property
-    def rank(self):
-        return self.geometry.spatial_rank
-
-    def at(self, center: Tensor) -> 'Geometry':
-        return RotatedGeometry(self._geometry.at(center), self._angle)
-
-    def rotated(self, angle) -> Geometry:
-        return RotatedGeometry(self._geometry, self._angle + angle)
-
-    def scaled(self, factor: Union[float, Tensor]) -> 'Geometry':
-        return RotatedGeometry(self._geometry.scaled(factor), self._angle)
-
-    def unstack(self, dimension: str) -> tuple:
-        return tuple([RotatedGeometry(g, self._angle) for g in self._geometry.unstack(dimension)])
-
-    def sample_uniform(self, *shape: math.Shape) -> Tensor:
-        loc = self._geometry.sample_uniform(*shape)
-        return math.rotate_vector(loc, self._angle)
-
-    def __hash__(self):
-        return hash(self._angle) + hash(self._geometry)
-
-    def __repr__(self):
-        return f"rot({self._geometry}, angle={self._angle})"
-
-
-def rotate(geometry: Geometry, angle: Union[Number, Tensor]) -> Geometry:
-    """ Package-internal rotation function. Users should use Geometry.rotated() instead. """
-    assert isinstance(geometry, Geometry)
-    if isinstance(geometry, RotatedGeometry):
-        total_rotation = geometry.angle + angle  # ToDo concatenate rotations
-        return RotatedGeometry(geometry.geometry, total_rotation)
-    else:
-        return RotatedGeometry(geometry, angle)
-
-
-class _EmbeddedGeometry(Geometry):
-
-    def __init__(self, geometry, axes: Tuple[str]):
-        self.geometry = geometry
-        self.axes = axes  # spatial axis order
-
-    @property
-    def spatial_rank(self) -> int:
-        return len(self.axes)
-
-    @property
-    def center(self) -> Tensor:
-        raise NotImplementedError()
-        # c = self.geometry.center.vector.unstack()
-        # return math.stack()
-
-    @property
-    def shape(self) -> Shape:
-        return self.geometry.shape.with_dim_size('vector', self.axes)
-
-    @property
-    def volume(self) -> Tensor:
-        raise NotImplementedError()
-
-    @property
-    def shape_type(self) -> Tensor:
-        return math.wrap('?')
-
-    def unstack(self, dimension: str) -> tuple:
-        raise NotImplementedError()
-
-    def _down_project(self, location: Tensor):
-        item_names = list(location.shape.get_item_names('vector'))
-        for dim in self.axes:
-            if dim not in self.geometry.shape.get_item_names('vector'):
-                item_names.remove(dim)
-        projected_loc = location.vector[item_names]
-        return projected_loc
-
-    def lies_inside(self, location: Tensor) -> Tensor:
-        return self.geometry.lies_inside(self._down_project(location))
-
-    def approximate_signed_distance(self, location: Tensor) -> Tensor:
-        return self.geometry.approximate_signed_distance(self._down_project(location))
-
-    def push(self, positions: Tensor, outward: bool = True, shift_amount: float = 0) -> Tensor:
-        raise NotImplementedError()
-
-    def sample_uniform(self, *shape: math.Shape) -> Tensor:
-        raise NotImplementedError()
-
-    def bounding_radius(self) -> Tensor:
-        raise NotImplementedError()
-
-    def bounding_half_extent(self) -> Tensor:
-        raise NotImplementedError()
-
-    def shifted(self, delta: Tensor) -> 'Geometry':
-        raise NotImplementedError()
-
-    def at(self, center: Tensor) -> 'Geometry':
-        raise NotImplementedError()
-
-    def rotated(self, angle: Union[float, Tensor]) -> 'Geometry':
-        raise NotImplementedError()
-
-    def scaled(self, factor: Union[float, Tensor]) -> 'Geometry':
-        raise NotImplementedError()
-
-    def __hash__(self):
-        return hash(self.geometry) + hash(self.axes)
-
-
-def embed(geometry: Geometry, projected_dims: Union[math.Shape, str, tuple, list, None]) -> Geometry:
-    """
-    Adds fake spatial dimensions to a geometry.
-    The geometry value will be constant along the added dimensions, as if it had infinite length in these directions.
-
-    Dimensions that are already present with `geometry` are ignored.
-
-    Args:
-        geometry: `Geometry`
-        projected_dims: Additional dimensions
-
-    Returns:
-        `Geometry` with spatial rank `geometry.spatial_rank + projected_dims.rank`.
-    """
-    if projected_dims is None:
-        return geometry
-    axes = parse_dim_order(projected_dims)
-    embedded_axes = [a for a in axes if a not in geometry.shape.get_item_names('vector')]
-    if not embedded_axes:
-        return geometry
-    for name in reversed(geometry.shape.get_item_names('vector')):
-        if name not in projected_dims:
-            axes = (name,) + axes
-    if isinstance(geometry, BaseBox):
-        box = geometry.corner_representation()
-        return box * Box(**{dim: None for dim in embedded_axes})
-    return _EmbeddedGeometry(geometry, axes)
-
-
-def infinite_cylinder(center=None, radius=None, inf_dim: Union[str, Shape, tuple, list] = None, **center_) -> Geometry:
-    """
-    Creates an infinite cylinder.
-    This is equal to embedding an `n`-dimensional `Sphere` in `n+1` dimensions.
-
-    See Also:
-        `Sphere`, `embed`
-
-    Args:
-        center: Center coordinates without `inf_dim`. Alternatively use keyword arguments.
-        radius: Cylinder radius.
-        inf_dim: Dimension along which the cylinder is infinite.
-            Use `Geometry.rotated()` if the direction does not align with an axis.
-        **center_: Alternatively specify center coordinates without `inf_dim` as keyword arguments.
-
-    Returns:
-        `Geometry`
-    """
-    sphere = Sphere(center, radius, **center_)
-    return embed(sphere, inf_dim)
+from numbers import Number
+from typing import Tuple, Union
+
+from phi import math
+from phi.math import Tensor, Shape
+from . import BaseBox, Box
+from ._geom import Geometry
+from ._sphere import Sphere
+from ..math._shape import parse_dim_order
+
+
+class RotatedGeometry(Geometry):
+
+    def __init__(self, geometry: Geometry, angle: Union[float, math.Tensor]):
+        assert not isinstance(geometry, RotatedGeometry)
+        self._geometry = geometry
+        self._angle = math.wrap(angle)
+
+    @property
+    def shape(self):
+        return self._geometry.shape
+
+    def __variable_attrs__(self):
+        return '_geometry', '_angle'
+
+    @property
+    def geometry(self):
+        return self._geometry
+
+    @property
+    def angle(self):
+        return self._angle
+
+    @property
+    def center(self):
+        return self.geometry.center
+
+    @property
+    def volume(self) -> Tensor:
+        return self._geometry.volume
+
+    @property
+    def shape_type(self) -> Tensor:
+        return math.map(lambda s: f"rot{s}", self._geometry.shape_type)
+
+    def global_to_child(self, location):
+        """ Inverse transform. """
+        delta = location - self.center
+        if location.shape.get_size('vector') == 2:
+            rotated = math.rotate_vector(delta, self._angle)
+        elif location.shape.get_size('vector') == 3:
+            raise NotImplementedError('not yet implemented')  # ToDo apply angle
+        else:
+            raise NotImplementedError('Rotation only supported in 2D and 3D')
+        final = rotated + self.center
+        return final
+
+    def push(self, positions: Tensor, outward: bool = True, shift_amount: float = 0):
+        rotated = self.global_to_child(positions)
+        shifted_positions = self.geometry.push(rotated, outward=outward, shift_amount=shift_amount)
+        return positions + math.rotate_vector(shifted_positions - rotated, self._angle)
+
+    def lies_inside(self, location):
+        return self.geometry.lies_inside(self.global_to_child(location))
+
+    def approximate_signed_distance(self, location):
+        return self.geometry.approximate_signed_distance(self.global_to_child(location))
+
+    def bounding_radius(self):
+        return self.geometry.bounding_radius()
+
+    def bounding_half_extent(self):
+        bounding_sphere = Sphere(self.center, self.bounding_radius())
+        return bounding_sphere.bounding_half_extent()
+
+    @property
+    def rank(self):
+        return self.geometry.spatial_rank
+
+    def at(self, center: Tensor) -> 'Geometry':
+        return RotatedGeometry(self._geometry.at(center), self._angle)
+
+    def rotated(self, angle) -> Geometry:
+        return RotatedGeometry(self._geometry, self._angle + angle)
+
+    def scaled(self, factor: Union[float, Tensor]) -> 'Geometry':
+        return RotatedGeometry(self._geometry.scaled(factor), self._angle)
+
+    def unstack(self, dimension: str) -> tuple:
+        return tuple([RotatedGeometry(g, self._angle) for g in self._geometry.unstack(dimension)])
+
+    def sample_uniform(self, *shape: math.Shape) -> Tensor:
+        loc = self._geometry.sample_uniform(*shape)
+        return math.rotate_vector(loc, self._angle)
+
+    def __hash__(self):
+        return hash(self._angle) + hash(self._geometry)
+
+    def __repr__(self):
+        return f"rot({self._geometry}, angle={self._angle})"
+
+
+def rotate(geometry: Geometry, angle: Union[Number, Tensor]) -> Geometry:
+    """ Package-internal rotation function. Users should use Geometry.rotated() instead. """
+    assert isinstance(geometry, Geometry)
+    if isinstance(geometry, RotatedGeometry):
+        total_rotation = geometry.angle + angle  # ToDo concatenate rotations
+        return RotatedGeometry(geometry.geometry, total_rotation)
+    else:
+        return RotatedGeometry(geometry, angle)
+
+
+class _EmbeddedGeometry(Geometry):
+
+    def __init__(self, geometry, axes: Tuple[str]):
+        self.geometry = geometry
+        self.axes = axes  # spatial axis order
+
+    @property
+    def spatial_rank(self) -> int:
+        return len(self.axes)
+
+    @property
+    def center(self) -> Tensor:
+        raise NotImplementedError()
+        # c = self.geometry.center.vector.unstack()
+        # return math.stack()
+
+    @property
+    def shape(self) -> Shape:
+        return self.geometry.shape.with_dim_size('vector', self.axes)
+
+    @property
+    def volume(self) -> Tensor:
+        raise NotImplementedError()
+
+    @property
+    def shape_type(self) -> Tensor:
+        return math.wrap('?')
+
+    def unstack(self, dimension: str) -> tuple:
+        raise NotImplementedError()
+
+    def _down_project(self, location: Tensor):
+        item_names = list(location.shape.get_item_names('vector'))
+        for dim in self.axes:
+            if dim not in self.geometry.shape.get_item_names('vector'):
+                item_names.remove(dim)
+        projected_loc = location.vector[item_names]
+        return projected_loc
+
+    def lies_inside(self, location: Tensor) -> Tensor:
+        return self.geometry.lies_inside(self._down_project(location))
+
+    def approximate_signed_distance(self, location: Tensor) -> Tensor:
+        return self.geometry.approximate_signed_distance(self._down_project(location))
+
+    def push(self, positions: Tensor, outward: bool = True, shift_amount: float = 0) -> Tensor:
+        raise NotImplementedError()
+
+    def sample_uniform(self, *shape: math.Shape) -> Tensor:
+        raise NotImplementedError()
+
+    def bounding_radius(self) -> Tensor:
+        raise NotImplementedError()
+
+    def bounding_half_extent(self) -> Tensor:
+        raise NotImplementedError()
+
+    def shifted(self, delta: Tensor) -> 'Geometry':
+        raise NotImplementedError()
+
+    def at(self, center: Tensor) -> 'Geometry':
+        raise NotImplementedError()
+
+    def rotated(self, angle: Union[float, Tensor]) -> 'Geometry':
+        raise NotImplementedError()
+
+    def scaled(self, factor: Union[float, Tensor]) -> 'Geometry':
+        raise NotImplementedError()
+
+    def __hash__(self):
+        return hash(self.geometry) + hash(self.axes)
+
+
+def embed(geometry: Geometry, projected_dims: Union[math.Shape, str, tuple, list, None]) -> Geometry:
+    """
+    Adds fake spatial dimensions to a geometry.
+    The geometry value will be constant along the added dimensions, as if it had infinite length in these directions.
+
+    Dimensions that are already present with `geometry` are ignored.
+
+    Args:
+        geometry: `Geometry`
+        projected_dims: Additional dimensions
+
+    Returns:
+        `Geometry` with spatial rank `geometry.spatial_rank + projected_dims.rank`.
+    """
+    if projected_dims is None:
+        return geometry
+    axes = parse_dim_order(projected_dims)
+    embedded_axes = [a for a in axes if a not in geometry.shape.get_item_names('vector')]
+    if not embedded_axes:
+        return geometry
+    for name in reversed(geometry.shape.get_item_names('vector')):
+        if name not in projected_dims:
+            axes = (name,) + axes
+    if isinstance(geometry, BaseBox):
+        box = geometry.corner_representation()
+        return box * Box(**{dim: None for dim in embedded_axes})
+    return _EmbeddedGeometry(geometry, axes)
+
+
+def infinite_cylinder(center=None, radius=None, inf_dim: Union[str, Shape, tuple, list] = None, **center_) -> Geometry:
+    """
+    Creates an infinite cylinder.
+    This is equal to embedding an `n`-dimensional `Sphere` in `n+1` dimensions.
+
+    See Also:
+        `Sphere`, `embed`
+
+    Args:
+        center: Center coordinates without `inf_dim`. Alternatively use keyword arguments.
+        radius: Cylinder radius.
+        inf_dim: Dimension along which the cylinder is infinite.
+            Use `Geometry.rotated()` if the direction does not align with an axis.
+        **center_: Alternatively specify center coordinates without `inf_dim` as keyword arguments.
+
+    Returns:
+        `Geometry`
+    """
+    sphere = Sphere(center, radius, **center_)
+    return embed(sphere, inf_dim)
```

### Comparing `phiflow-2.3.4/phi/jax/_jax_backend.py` & `phiflow-2.4.0/phi/jax/_jax_backend.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,480 +1,556 @@
-import numbers
-import warnings
-from functools import wraps
-from typing import List, Callable, Tuple, Union
-from packaging import version
-
-import jax
-import jax.numpy as jnp
-import jax.scipy as scipy
-import numpy as np
-from jax import random
-from jax.core import Tracer
-from jax.interpreters.xla import DeviceArray
-
-if version.parse(jax.__version__) >= version.parse('0.2.20'):
-    from jax.experimental.sparse import BCOO, COO, CSR, CSC
-
-from phi.math import DType
-from phi.math.backend import Backend, ComputeDevice
-from phi.math.backend._backend import combined_dim, SolveResult, PHI_LOGGER, TensorType
-from ..math.backend._dtype import to_numpy_dtype, from_numpy_dtype
-
-
-from jax.config import config
-config.update("jax_enable_x64", True)
-
-
-class JaxBackend(Backend):
-
-    def __init__(self):
-        devices = []
-        for device_type in ['cpu', 'gpu', 'tpu']:
-            try:
-                for jax_dev in jax.devices(device_type):
-                    devices.append(ComputeDevice(self, device_type.upper(), jax_dev.platform.upper(), -1, -1, f"id={jax_dev.id}", jax_dev))
-            except RuntimeError as err:
-                pass  # this is just Jax not finding anything. jaxlib.xla_client._get_local_backends() could help but isn't currently available on GitHub actions
-        Backend.__init__(self, "Jax", devices, devices[-1])
-        try:
-            self.rnd_key = jax.random.PRNGKey(seed=0)
-        except RuntimeError as err:
-            warnings.warn(f"{err}", RuntimeWarning)
-            self.rnd_key = None
-
-    def prefers_channels_last(self) -> bool:
-        return True
-
-    def _check_float64(self):
-        if self.precision == 64:
-            if not jax.config.read('jax_enable_x64'):
-                jax.config.update('jax_enable_x64', True)
-            assert jax.config.read('jax_enable_x64'), "FP64 is disabled for Jax."
-
-    def seed(self, seed: int):
-        self.rnd_key = jax.random.PRNGKey(seed)
-
-    def as_tensor(self, x, convert_external=True):
-        self._check_float64()
-        if self.is_tensor(x, only_native=convert_external):
-            array = x
-        else:
-            array = jnp.array(x)
-        # --- Enforce Precision ---
-        if not isinstance(array, numbers.Number):
-            if self.dtype(array).kind == float:
-                array = self.to_float(array)
-            elif self.dtype(array).kind == complex:
-                array = self.to_complex(array)
-        return array
-
-    def is_module(self, obj):
-        return False
-
-    def is_tensor(self, x, only_native=False):
-        if isinstance(x, jnp.ndarray) and not isinstance(x, np.ndarray):  # NumPy arrays inherit from Jax arrays
-            return True
-        if isinstance(x, jnp.bool_) and not isinstance(x, np.bool_):
-            return True
-        if isinstance(x, (COO, BCOO, CSR, CSC)):
-            return True
-        # --- Above considered native ---
-        if only_native:
-            return False
-        # --- Non-native types ---
-        if isinstance(x, np.ndarray):
-            return True
-        if isinstance(x, np.bool_):
-            return True
-        if isinstance(x, (numbers.Number, bool)):
-            return True
-        if isinstance(x, (tuple, list)):
-            return all([self.is_tensor(item, False) for item in x])
-        return False
-
-    def is_available(self, tensor):
-        return not isinstance(tensor, Tracer)
-
-    def numpy(self, x):
-        return np.array(x)
-
-    def to_dlpack(self, tensor):
-        from jax import dlpack
-        return dlpack.to_dlpack(tensor)
-
-    def from_dlpack(self, capsule):
-        from jax import dlpack
-        return dlpack.from_dlpack(capsule)
-
-    def copy(self, tensor, only_mutable=False):
-        return jnp.array(tensor, copy=True)
-
-    def get_device(self, tensor: TensorType) -> ComputeDevice:
-        return self.get_device_by_ref(tensor.device())
-
-    def allocate_on_device(self, tensor: TensorType, device: ComputeDevice) -> TensorType:
-        return jax.device_put(tensor, device.ref)
-
-    sqrt = staticmethod(jnp.sqrt)
-    exp = staticmethod(jnp.exp)
-    sin = staticmethod(jnp.sin)
-    arcsin = staticmethod(jnp.arcsin)
-    cos = staticmethod(jnp.cos)
-    arccos = staticmethod(jnp.arccos)
-    tan = staticmethod(jnp.tan)
-    arctan = staticmethod(np.arctan)
-    arctan2 = staticmethod(np.arctan2)
-    sinh = staticmethod(np.sinh)
-    arcsinh = staticmethod(np.arcsinh)
-    cosh = staticmethod(np.cosh)
-    arccosh = staticmethod(np.arccosh)
-    tanh = staticmethod(np.tanh)
-    arctanh = staticmethod(np.arctanh)
-    log = staticmethod(jnp.log)
-    log2 = staticmethod(jnp.log2)
-    log10 = staticmethod(jnp.log10)
-    isfinite = staticmethod(jnp.isfinite)
-    abs = staticmethod(jnp.abs)
-    sign = staticmethod(jnp.sign)
-    round = staticmethod(jnp.round)
-    ceil = staticmethod(jnp.ceil)
-    floor = staticmethod(jnp.floor)
-    flip = staticmethod(jnp.flip)
-    stop_gradient = staticmethod(jax.lax.stop_gradient)
-    transpose = staticmethod(jnp.transpose)
-    equal = staticmethod(jnp.equal)
-    tile = staticmethod(jnp.tile)
-    stack = staticmethod(jnp.stack)
-    concat = staticmethod(jnp.concatenate)
-    maximum = staticmethod(jnp.maximum)
-    minimum = staticmethod(jnp.minimum)
-    clip = staticmethod(jnp.clip)
-    shape = staticmethod(jnp.shape)
-    staticshape = staticmethod(jnp.shape)
-    imag = staticmethod(jnp.imag)
-    real = staticmethod(jnp.real)
-    conj = staticmethod(jnp.conjugate)
-    einsum = staticmethod(jnp.einsum)
-    cumsum = staticmethod(jnp.cumsum)
-
-    def nonzero(self, values):
-        result = jnp.nonzero(values)
-        return jnp.stack(result, -1)
-
-    def jit_compile(self, f: Callable) -> Callable:
-        def run_jit_f(*args):
-            # print(jax.make_jaxpr(f)(*args))
-            PHI_LOGGER.debug(f"JaxBackend: running jit-compiled '{f.__name__}' with shapes {[self.shape(arg) for arg in args]} and dtypes {[self.dtype(arg) for arg in args]}")
-            return self.as_registered.call(jit_f, *args, name=f"run jit-compiled '{f.__name__}'")
-
-        run_jit_f.__name__ = f"Jax-Jit({f.__name__})"
-        jit_f = jax.jit(f, device=self._default_device.ref)
-        return run_jit_f
-
-    def block_until_ready(self, values):
-        if isinstance(values, DeviceArray):
-            values.block_until_ready()
-        if isinstance(values, (tuple, list)):
-            for v in values:
-                self.block_until_ready(v)
-
-    def jacobian(self, f, wrt: Union[tuple, list], get_output: bool, is_f_scalar: bool):
-        if get_output:
-            jax_grad_f = jax.value_and_grad(f, argnums=wrt, has_aux=True)
-            @wraps(f)
-            def unwrap_outputs(*args):
-                args = [self.to_float(arg) if self.dtype(arg).kind in (bool, int) else arg for arg in args]
-                (_, output_tuple), grads = jax_grad_f(*args)
-                return (*output_tuple, *grads)
-            return unwrap_outputs
-        else:
-            @wraps(f)
-            def nonaux_f(*args):
-                loss, output = f(*args)
-                return loss
-            return jax.grad(nonaux_f, argnums=wrt, has_aux=False)
-
-    def custom_gradient(self, f: Callable, gradient: Callable, get_external_cache: Callable = None, on_call_skipped: Callable = None) -> Callable:
-        jax_fun = jax.custom_vjp(f)  # custom vector-Jacobian product (reverse-mode differentiation)
-
-        def forward(*x):
-            y = f(*x)
-            return y, (x, y)
-
-        def backward(x_y, dy):
-            x, y = x_y
-            dx = gradient(x, y, dy)
-            return tuple(dx)
-
-        jax_fun.defvjp(forward, backward)
-        return jax_fun
-
-    def divide_no_nan(self, x, y):
-        return jnp.where(y == 0, 0, x / y)
-        # jnp.nan_to_num(x / y, copy=True, nan=0) covers up NaNs from before
-
-    def random_uniform(self, shape, low, high, dtype: Union[DType, None]):
-        self._check_float64()
-        self.rnd_key, subkey = jax.random.split(self.rnd_key)
-
-        dtype = dtype or self.float_type
-        jdt = to_numpy_dtype(dtype)
-        if dtype.kind == float:
-            tensor = random.uniform(subkey, shape, minval=low, maxval=high, dtype=jdt)
-        elif dtype.kind == complex:
-            real = random.uniform(subkey, shape, minval=low.real, maxval=high.real, dtype=to_numpy_dtype(DType(float, dtype.precision)))
-            imag = random.uniform(subkey, shape, minval=low.imag, maxval=high.imag, dtype=to_numpy_dtype(DType(float, dtype.precision)))
-            return real + 1j * imag
-        elif dtype.kind == int:
-            tensor = random.randint(subkey, shape, low, high, dtype=jdt)
-            if tensor.dtype != jdt:
-                warnings.warn(f"Jax failed to sample random integers with dtype {dtype}, returned {tensor.dtype} instead.", RuntimeWarning)
-        else:
-            raise ValueError(dtype)
-        return jax.device_put(tensor, self._default_device.ref)
-
-    def random_normal(self, shape, dtype: DType):
-        self._check_float64()
-        self.rnd_key, subkey = jax.random.split(self.rnd_key)
-        dtype = dtype or self.float_type
-        return jax.device_put(random.normal(subkey, shape, dtype=to_numpy_dtype(dtype)), self._default_device.ref)
-
-    def range(self, start, limit=None, delta=1, dtype: DType = DType(int, 32)):
-        if limit is None:
-            start, limit = 0, start
-        return jnp.arange(start, limit, delta, to_numpy_dtype(dtype))
-
-    def pad(self, value, pad_width, mode='constant', constant_values=0):
-        assert mode in ('constant', 'symmetric', 'periodic', 'reflect', 'boundary'), mode
-        if mode == 'constant':
-            constant_values = jnp.array(constant_values, dtype=value.dtype)
-            return jnp.pad(value, pad_width, 'constant', constant_values=constant_values)
-        else:
-            if mode in ('periodic', 'boundary'):
-                mode = {'periodic': 'wrap', 'boundary': 'edge'}[mode]
-            return jnp.pad(value, pad_width, mode)
-
-    def reshape(self, value, shape):
-        return jnp.reshape(value, shape)
-
-    def sum(self, value, axis=None, keepdims=False):
-        if isinstance(value, (tuple, list)):
-            assert axis == 0
-            return sum(value[1:], value[0])
-        return jnp.sum(value, axis=axis, keepdims=keepdims)
-
-    def prod(self, value, axis=None):
-        if not isinstance(value, jnp.ndarray):
-            value = jnp.array(value)
-        if value.dtype == bool:
-            return jnp.all(value, axis=axis)
-        return jnp.prod(value, axis=axis)
-
-    def where(self, condition, x=None, y=None):
-        if x is None or y is None:
-            return jnp.argwhere(condition)
-        return jnp.where(condition, x, y)
-
-    def zeros(self, shape, dtype: DType = None):
-        self._check_float64()
-        return jax.device_put(jnp.zeros(shape, dtype=to_numpy_dtype(dtype or self.float_type)), self._default_device.ref)
-
-    def zeros_like(self, tensor):
-        return jax.device_put(jnp.zeros_like(tensor), self._default_device.ref)
-
-    def ones(self, shape, dtype: DType = None):
-        self._check_float64()
-        return jax.device_put(jnp.ones(shape, dtype=to_numpy_dtype(dtype or self.float_type)), self._default_device.ref)
-
-    def ones_like(self, tensor):
-        return jax.device_put(jnp.ones_like(tensor), self._default_device.ref)
-
-    def meshgrid(self, *coordinates):
-        self._check_float64()
-        coordinates = [self.as_tensor(c) for c in coordinates]
-        return [jax.device_put(c, self._default_device.ref) for c in jnp.meshgrid(*coordinates, indexing='ij')]
-
-    def linspace(self, start, stop, number):
-        self._check_float64()
-        return jax.device_put(jnp.linspace(start, stop, number, dtype=to_numpy_dtype(self.float_type)), self._default_device.ref)
-
-    def mean(self, value, axis=None, keepdims=False):
-        return jnp.mean(value, axis, keepdims=keepdims)
-
-    def tensordot(self, a, a_axes: Union[tuple, list], b, b_axes: Union[tuple, list]):
-        return jnp.tensordot(a, b, (a_axes, b_axes))
-
-    def mul(self, a, b):
-        # if scipy.sparse.issparse(a):  # TODO sparse?
-        #     return a.multiply(b)
-        # elif scipy.sparse.issparse(b):
-        #     return b.multiply(a)
-        # else:
-            return Backend.mul(self, a, b)
-
-    def mul_matrix_batched_vector(self, A, b):
-        from jax.experimental.sparse import BCOO
-        if isinstance(A, BCOO):
-            return(A @ b.T).T
-        return jnp.stack([A.dot(b[i]) for i in range(b.shape[0])])
-
-    def get_diagonal(self, matrices, offset=0):
-        result = jnp.diagonal(matrices, offset=offset, axis1=1, axis2=2)
-        return jnp.transpose(result, [0, 2, 1])
-
-    def while_loop(self, loop: Callable, values: tuple, max_iter: Union[int, Tuple[int, ...], List[int]]):
-        if all(self.is_available(t) for t in values):
-            return self.stop_gradient_tree(Backend.while_loop(self, loop, values, max_iter))
-        if isinstance(max_iter, (tuple, list)):  # stack traced trajectory, unroll until max_iter
-            values = self.stop_gradient_tree(values)
-            trj = [values] if 0 in max_iter else []
-            for i in range(1, max(max_iter) + 1):
-                values = loop(*values)
-                if i in max_iter:
-                    trj.append(values)  # values are not mutable so no need to copy
-            return self.stop_gradient_tree(self.stack_leaves(trj))
-        else:
-            if max_iter is None:
-                cond = lambda vals: jnp.any(vals[0])
-                body = lambda vals: loop(*vals)
-                return jax.lax.while_loop(cond, body, values)
-            else:
-                cond = lambda vals: jnp.any(vals[1][0]) & (vals[0] < max_iter)
-                body = lambda vals: (vals[0] + 1, loop(*vals[1]))
-                return jax.lax.while_loop(cond, body, (self.as_tensor(0), values))[1]
-
-    def max(self, x, axis=None, keepdims=False):
-        return jnp.max(x, axis, keepdims=keepdims)
-
-    def min(self, x, axis=None, keepdims=False):
-        return jnp.min(x, axis, keepdims=keepdims)
-
-    def conv(self, value, kernel, zero_padding=True):
-        assert kernel.shape[0] in (1, value.shape[0])
-        assert value.shape[1] == kernel.shape[2], f"value has {value.shape[1]} channels but kernel has {kernel.shape[2]}"
-        assert value.ndim + 1 == kernel.ndim
-        # AutoDiff may require jax.lax.conv_general_dilated
-        result = []
-        for b in range(value.shape[0]):
-            b_kernel = kernel[min(b, kernel.shape[0] - 1)]
-            result_b = []
-            for o in range(kernel.shape[1]):
-                result_b.append(0)
-                for i in range(value.shape[1]):
-                    # result.at[b, o, ...].set(scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode='same' if zero_padding else 'valid'))
-                    result_b[-1] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode='same' if zero_padding else 'valid')
-            result.append(jnp.stack(result_b, 0))
-        return jnp.stack(result, 0)
-
-    def expand_dims(self, a, axis=0, number=1):
-        for _i in range(number):
-            a = jnp.expand_dims(a, axis)
-        return a
-
-    def cast(self, x, dtype: DType):
-        if self.is_tensor(x, only_native=True) and from_numpy_dtype(x.dtype) == dtype:
-            return x
-        else:
-            return jnp.array(x, to_numpy_dtype(dtype))
-
-    def gather(self, values, indices, axis: int):
-        slices = [indices if i == axis else slice(None) for i in range(self.ndims(values))]
-        return values[tuple(slices)]
-
-    def batched_gather_nd(self, values, indices):
-        values = self.as_tensor(values)
-        indices = self.as_tensor(indices)
-        assert indices.shape[-1] == self.ndims(values) - 2
-        batch_size = combined_dim(values.shape[0], indices.shape[0])
-        results = []
-        for b in range(batch_size):
-            b_values = values[min(b, values.shape[0] - 1)]
-            b_indices = self.unstack(indices[min(b, indices.shape[0] - 1)], -1)
-            results.append(b_values[b_indices])
-        return jnp.stack(results)
-
-    def repeat(self, x, repeats, axis: int):
-        return jnp.repeat(x, self.as_tensor(repeats), axis)
-
-    def std(self, x, axis=None, keepdims=False):
-        return jnp.std(x, axis, keepdims=keepdims)
-
-    def boolean_mask(self, x, mask, axis=0):
-        slices = [mask if i == axis else slice(None) for i in range(len(x.shape))]
-        return x[tuple(slices)]
-
-    def any(self, boolean_tensor, axis=None, keepdims=False):
-        if isinstance(boolean_tensor, (tuple, list)):
-            boolean_tensor = jnp.stack(boolean_tensor)
-        return jnp.any(boolean_tensor, axis=axis, keepdims=keepdims)
-
-    def all(self, boolean_tensor, axis=None, keepdims=False):
-        if isinstance(boolean_tensor, (tuple, list)):
-            boolean_tensor = jnp.stack(boolean_tensor)
-        return jnp.all(boolean_tensor, axis=axis, keepdims=keepdims)
-
-    def scatter(self, base_grid, indices, values, mode: str):
-        base_grid, values = self.auto_cast(base_grid, values)
-        batch_size = combined_dim(combined_dim(indices.shape[0], values.shape[0]), base_grid.shape[0])
-        spatial_dims = tuple(range(base_grid.ndim - 2))
-        dnums = jax.lax.ScatterDimensionNumbers(update_window_dims=(1,),  # channel dim of updates (batch dim removed)
-                                                inserted_window_dims=spatial_dims,  # no idea what this does but spatial_dims seems to work
-                                                scatter_dims_to_operand_dims=spatial_dims)  # spatial dims of base_grid (batch dim removed)
-        scatter = jax.lax.scatter_add if mode == 'add' else jax.lax.scatter
-        result = []
-        for b in range(batch_size):
-            b_grid = base_grid[b, ...]
-            b_indices = indices[min(b, indices.shape[0] - 1), ...]
-            b_values = values[min(b, values.shape[0] - 1), ...]
-            result.append(scatter(b_grid, b_indices, b_values, dnums))
-        return jnp.stack(result)
-
-    def quantile(self, x, quantiles):
-        return jnp.quantile(x, quantiles, axis=-1)
-
-    def fft(self, x, axes: Union[tuple, list]):
-        x = self.to_complex(x)
-        if not axes:
-            return x
-        if len(axes) == 1:
-            return jnp.fft.fft(x, axis=axes[0]).astype(x.dtype)
-        elif len(axes) == 2:
-            return jnp.fft.fft2(x, axes=axes).astype(x.dtype)
-        else:
-            return jnp.fft.fftn(x, axes=axes).astype(x.dtype)
-
-    def ifft(self, k, axes: Union[tuple, list]):
-        if not axes:
-            return k
-        if len(axes) == 1:
-            return jnp.fft.ifft(k, axis=axes[0]).astype(k.dtype)
-        elif len(axes) == 2:
-            return jnp.fft.ifft2(k, axes=axes).astype(k.dtype)
-        else:
-            return jnp.fft.ifftn(k, axes=axes).astype(k.dtype)
-
-    def dtype(self, array) -> DType:
-        if isinstance(array, int):
-            return DType(int, 32)
-        if isinstance(array, float):
-            return DType(float, 64)
-        if isinstance(array, complex):
-            return DType(complex, 128)
-        if not isinstance(array, jnp.ndarray):
-            array = jnp.array(array)
-        return from_numpy_dtype(array.dtype)
-
-    def matrix_solve_least_squares(self, matrix: TensorType, rhs: TensorType) -> Tuple[TensorType, TensorType, TensorType, TensorType]:
-        solution, residuals, rank, singular_values = lstsq_batched(matrix, rhs)
-        return solution, residuals, rank, singular_values
-
-    def solve_triangular_dense(self, matrix, rhs, lower: bool, unit_diagonal: bool):
-        matrix, rhs = self.auto_cast(matrix, rhs, int_to_float=True, bool_to_int=True)
-        x = jax.lax.linalg.triangular_solve(matrix, rhs, lower=lower, unit_diagonal=unit_diagonal, left_side=True)
-        return x
-
-    def sparse_coo_tensor(self, indices: Union[tuple, list], values, shape: tuple):
-        return BCOO((values, indices), shape=shape)
-
-
-lstsq_batched = jax.vmap(jnp.linalg.lstsq)  # map first dimension, required for JaxBackend.matrix_solve_least_squares()
+import numbers
+import warnings
+from functools import wraps, partial
+from typing import List, Callable, Tuple, Union
+from packaging import version
+
+import jax
+import jax.numpy as jnp
+import jax.scipy as scipy
+import numpy as np
+from jax import random
+from jax.core import Tracer
+
+if version.parse(jax.__version__) >= version.parse('0.2.20'):
+    from jax.experimental.sparse import BCOO, COO, CSR, CSC
+
+from phi.math import DType
+from phi.math.backend import Backend, ComputeDevice
+from phi.math.backend._backend import combined_dim, SolveResult, PHI_LOGGER, TensorType
+from ..math.backend._dtype import to_numpy_dtype, from_numpy_dtype
+
+
+from jax.config import config
+config.update("jax_enable_x64", True)
+
+
+class JaxBackend(Backend):
+
+    def __init__(self):
+        devices = []
+        for device_type in ['cpu', 'gpu', 'tpu']:
+            try:
+                for jax_dev in jax.devices(device_type):
+                    devices.append(ComputeDevice(self, device_type.upper(), jax_dev.platform.upper(), -1, -1, f"id={jax_dev.id}", jax_dev))
+            except RuntimeError as err:
+                pass  # this is just Jax not finding anything. jaxlib.xla_client._get_local_backends() could help but isn't currently available on GitHub actions
+        Backend.__init__(self, "Jax", devices, devices[-1])
+        try:
+            self.rnd_key = jax.random.PRNGKey(seed=0)
+        except RuntimeError as err:
+            warnings.warn(f"{err}", RuntimeWarning)
+            self.rnd_key = None
+
+    def prefers_channels_last(self) -> bool:
+        return True
+
+    def requires_fixed_shapes_when_tracing(self) -> bool:
+        return True
+
+    def _check_float64(self):
+        if self.precision == 64:
+            if not jax.config.read('jax_enable_x64'):
+                jax.config.update('jax_enable_x64', True)
+            assert jax.config.read('jax_enable_x64'), "FP64 is disabled for Jax."
+
+    def seed(self, seed: int):
+        self.rnd_key = jax.random.PRNGKey(seed)
+
+    def as_tensor(self, x, convert_external=True):
+        self._check_float64()
+        if self.is_tensor(x, only_native=convert_external):
+            array = x
+        else:
+            array = jnp.array(x)
+        # --- Enforce Precision ---
+        if not isinstance(array, numbers.Number):
+            if self.dtype(array).kind == float:
+                array = self.to_float(array)
+            elif self.dtype(array).kind == complex:
+                array = self.to_complex(array)
+        return array
+
+    def is_module(self, obj):
+        return False
+
+    def is_tensor(self, x, only_native=False):
+        if isinstance(x, jnp.ndarray) and not isinstance(x, np.ndarray):  # NumPy arrays inherit from Jax arrays
+            return True
+        if isinstance(x, jnp.bool_) and not isinstance(x, np.bool_):
+            return True
+        if self.is_sparse(x):
+            return True
+        # --- Above considered native ---
+        if only_native:
+            return False
+        # --- Non-native types ---
+        if isinstance(x, np.ndarray):
+            return True
+        if isinstance(x, np.bool_):
+            return True
+        if isinstance(x, (numbers.Number, bool)):
+            return True
+        if isinstance(x, (tuple, list)):
+            return all([self.is_tensor(item, False) for item in x])
+        return False
+
+    def is_sparse(self, x) -> bool:
+        return isinstance(x, (COO, BCOO, CSR, CSC))
+
+    def is_available(self, tensor):
+        return not isinstance(tensor, Tracer)
+
+    def numpy(self, tensor):
+        if isinstance(tensor, COO):
+            raise NotImplementedError
+        elif isinstance(tensor, BCOO):
+            indices = np.array(tensor.indices)
+            values = np.array(tensor.data)
+            indices = indices[..., 0], indices[..., 1]
+            assert values.ndim == 1, f"Cannot convert batched COO to NumPy"
+            from scipy.sparse import coo_matrix
+            return coo_matrix((values, indices), shape=self.staticshape(tensor))
+        elif isinstance(tensor, CSR):
+            raise NotImplementedError
+        elif isinstance(tensor, CSC):
+            raise NotImplementedError
+        else:
+            return np.array(tensor)
+
+    def to_dlpack(self, tensor):
+        from jax import dlpack
+        return dlpack.to_dlpack(tensor)
+
+    def from_dlpack(self, capsule):
+        from jax import dlpack
+        return dlpack.from_dlpack(capsule)
+
+    def copy(self, tensor, only_mutable=False):
+        return jnp.array(tensor, copy=True)
+
+    def get_device(self, tensor: TensorType) -> ComputeDevice:
+        return self.get_device_by_ref(tensor.device())
+
+    def allocate_on_device(self, tensor: TensorType, device: ComputeDevice) -> TensorType:
+        return jax.device_put(tensor, device.ref)
+
+    sqrt = staticmethod(jnp.sqrt)
+    exp = staticmethod(jnp.exp)
+    softplus = staticmethod(jax.nn.softplus)
+    sin = staticmethod(jnp.sin)
+    arcsin = staticmethod(jnp.arcsin)
+    cos = staticmethod(jnp.cos)
+    arccos = staticmethod(jnp.arccos)
+    tan = staticmethod(jnp.tan)
+    arctan = staticmethod(np.arctan)
+    arctan2 = staticmethod(np.arctan2)
+    sinh = staticmethod(np.sinh)
+    arcsinh = staticmethod(np.arcsinh)
+    cosh = staticmethod(np.cosh)
+    arccosh = staticmethod(np.arccosh)
+    tanh = staticmethod(np.tanh)
+    arctanh = staticmethod(np.arctanh)
+    log = staticmethod(jnp.log)
+    log2 = staticmethod(jnp.log2)
+    log10 = staticmethod(jnp.log10)
+    isfinite = staticmethod(jnp.isfinite)
+    isnan = staticmethod(jnp.isnan)
+    isinf = staticmethod(jnp.isinf)
+    abs = staticmethod(jnp.abs)
+    sign = staticmethod(jnp.sign)
+    round = staticmethod(jnp.round)
+    ceil = staticmethod(jnp.ceil)
+    floor = staticmethod(jnp.floor)
+    flip = staticmethod(jnp.flip)
+    stop_gradient = staticmethod(jax.lax.stop_gradient)
+    transpose = staticmethod(jnp.transpose)
+    equal = staticmethod(jnp.equal)
+    tile = staticmethod(jnp.tile)
+    stack = staticmethod(jnp.stack)
+    concat = staticmethod(jnp.concatenate)
+    maximum = staticmethod(jnp.maximum)
+    minimum = staticmethod(jnp.minimum)
+    clip = staticmethod(jnp.clip)
+    shape = staticmethod(jnp.shape)
+    staticshape = staticmethod(jnp.shape)
+    imag = staticmethod(jnp.imag)
+    real = staticmethod(jnp.real)
+    conj = staticmethod(jnp.conjugate)
+    einsum = staticmethod(jnp.einsum)
+    cumsum = staticmethod(jnp.cumsum)
+
+    def nonzero(self, values):
+        result = jnp.nonzero(values)
+        return jnp.stack(result, -1)
+
+    def vectorized_call(self, f, *args, output_dtypes=None, **aux_args):
+        batch_size = self.determine_size(args, 0)
+        args = [self.tile_to(t, 0, batch_size) for t in args]
+        def f_positional(*args):
+            return f(*args, **aux_args)
+        vec_f = jax.vmap(f_positional, 0, 0)
+        return vec_f(*args)
+
+    def jit_compile(self, f: Callable) -> Callable:
+        def run_jit_f(*args):
+            # print(jax.make_jaxpr(f)(*args))
+            PHI_LOGGER.debug(f"JaxBackend: running jit-compiled '{f.__name__}' with shapes {[self.shape(arg) for arg in args]} and dtypes {[self.dtype(arg) for arg in args]}")
+            return self.as_registered.call(jit_f, *args, name=f"run jit-compiled '{f.__name__}'")
+
+        run_jit_f.__name__ = f"Jax-Jit({f.__name__})"
+        jit_f = jax.jit(f, device=self._default_device.ref)
+        return run_jit_f
+
+    def block_until_ready(self, values):
+        if hasattr(values, 'block_until_ready'):
+            values.block_until_ready()
+        if isinstance(values, (tuple, list)):
+            for v in values:
+                self.block_until_ready(v)
+
+    def jacobian(self, f, wrt: Union[tuple, list], get_output: bool, is_f_scalar: bool):
+        if get_output:
+            jax_grad_f = jax.value_and_grad(f, argnums=wrt, has_aux=True)
+            @wraps(f)
+            def unwrap_outputs(*args):
+                args = [self.to_float(arg) if self.dtype(arg).kind in (bool, int) else arg for arg in args]
+                (_, output_tuple), grads = jax_grad_f(*args)
+                return (*output_tuple, *grads)
+            return unwrap_outputs
+        else:
+            @wraps(f)
+            def nonaux_f(*args):
+                loss, output = f(*args)
+                return loss
+            return jax.grad(nonaux_f, argnums=wrt, has_aux=False)
+
+    def custom_gradient(self, f: Callable, gradient: Callable, get_external_cache: Callable = None, on_call_skipped: Callable = None) -> Callable:
+        jax_fun = jax.custom_vjp(f)  # custom vector-Jacobian product (reverse-mode differentiation)
+
+        def forward(*x):
+            y = f(*x)
+            return y, (x, y)
+
+        def backward(x_y, dy):
+            x, y = x_y
+            dx = gradient(x, y, dy)
+            return tuple(dx)
+
+        jax_fun.defvjp(forward, backward)
+        return jax_fun
+
+    def divide_no_nan(self, x, y):
+        return jnp.where(y == 0, 0, x / y)
+        # jnp.nan_to_num(x / y, copy=True, nan=0) covers up NaNs from before
+
+    def random_uniform(self, shape, low, high, dtype: Union[DType, None]):
+        self._check_float64()
+        self.rnd_key, subkey = jax.random.split(self.rnd_key)
+
+        dtype = dtype or self.float_type
+        jdt = to_numpy_dtype(dtype)
+        if dtype.kind == float:
+            tensor = random.uniform(subkey, shape, minval=low, maxval=high, dtype=jdt)
+        elif dtype.kind == complex:
+            real = random.uniform(subkey, shape, minval=low.real, maxval=high.real, dtype=to_numpy_dtype(DType(float, dtype.precision)))
+            imag = random.uniform(subkey, shape, minval=low.imag, maxval=high.imag, dtype=to_numpy_dtype(DType(float, dtype.precision)))
+            return real + 1j * imag
+        elif dtype.kind == int:
+            tensor = random.randint(subkey, shape, low, high, dtype=jdt)
+            if tensor.dtype != jdt:
+                warnings.warn(f"Jax failed to sample random integers with dtype {dtype}, returned {tensor.dtype} instead.", RuntimeWarning)
+        else:
+            raise ValueError(dtype)
+        return jax.device_put(tensor, self._default_device.ref)
+
+    def random_normal(self, shape, dtype: DType):
+        self._check_float64()
+        self.rnd_key, subkey = jax.random.split(self.rnd_key)
+        dtype = dtype or self.float_type
+        return jax.device_put(random.normal(subkey, shape, dtype=to_numpy_dtype(dtype)), self._default_device.ref)
+
+    def range(self, start, limit=None, delta=1, dtype: DType = DType(int, 32)):
+        if limit is None:
+            start, limit = 0, start
+        return jnp.arange(start, limit, delta, to_numpy_dtype(dtype))
+
+    def pad(self, value, pad_width, mode='constant', constant_values=0):
+        assert mode in ('constant', 'symmetric', 'periodic', 'reflect', 'boundary'), mode
+        if mode == 'constant':
+            constant_values = jnp.array(constant_values, dtype=value.dtype)
+            return jnp.pad(value, pad_width, 'constant', constant_values=constant_values)
+        else:
+            if mode in ('periodic', 'boundary'):
+                mode = {'periodic': 'wrap', 'boundary': 'edge'}[mode]
+            return jnp.pad(value, pad_width, mode)
+
+    def reshape(self, value, shape):
+        return jnp.reshape(value, shape)
+
+    def sum(self, value, axis=None, keepdims=False):
+        if isinstance(value, (tuple, list)):
+            assert axis == 0
+            return sum(value[1:], value[0])
+        return jnp.sum(value, axis=axis, keepdims=keepdims)
+
+    def prod(self, value, axis=None):
+        if not isinstance(value, jnp.ndarray):
+            value = jnp.array(value)
+        if value.dtype == bool:
+            return jnp.all(value, axis=axis)
+        return jnp.prod(value, axis=axis)
+
+    def where(self, condition, x=None, y=None):
+        if x is None or y is None:
+            return jnp.argwhere(condition)
+        return jnp.where(condition, x, y)
+
+    def zeros(self, shape, dtype: DType = None):
+        self._check_float64()
+        return jax.device_put(jnp.zeros(shape, dtype=to_numpy_dtype(dtype or self.float_type)), self._default_device.ref)
+
+    def zeros_like(self, tensor):
+        return jax.device_put(jnp.zeros_like(tensor), self._default_device.ref)
+
+    def ones(self, shape, dtype: DType = None):
+        self._check_float64()
+        return jax.device_put(jnp.ones(shape, dtype=to_numpy_dtype(dtype or self.float_type)), self._default_device.ref)
+
+    def ones_like(self, tensor):
+        return jax.device_put(jnp.ones_like(tensor), self._default_device.ref)
+
+    def meshgrid(self, *coordinates):
+        self._check_float64()
+        coordinates = [self.as_tensor(c) for c in coordinates]
+        return [jax.device_put(c, self._default_device.ref) for c in jnp.meshgrid(*coordinates, indexing='ij')]
+
+    def linspace(self, start, stop, number):
+        self._check_float64()
+        return jax.device_put(jnp.linspace(start, stop, number, dtype=to_numpy_dtype(self.float_type)), self._default_device.ref)
+
+    def linspace_without_last(self, start, stop, number):
+        self._check_float64()
+        return jax.device_put(jnp.linspace(start, stop, number, endpoint=False, dtype=to_numpy_dtype(self.float_type)), self._default_device.ref)
+
+    def mean(self, value, axis=None, keepdims=False):
+        return jnp.mean(value, axis, keepdims=keepdims)
+
+    def log_gamma(self, x):
+        return jax.lax.lgamma(self.to_float(x))
+
+    def tensordot(self, a, a_axes: Union[tuple, list], b, b_axes: Union[tuple, list]):
+        return jnp.tensordot(a, b, (a_axes, b_axes))
+
+    def mul(self, a, b):
+        # if scipy.sparse.issparse(a):  # TODO sparse?
+        #     return a.multiply(b)
+        # elif scipy.sparse.issparse(b):
+        #     return b.multiply(a)
+        # else:
+            return Backend.mul(self, a, b)
+
+    def mul_matrix_batched_vector(self, A, b):
+        from jax.experimental.sparse import BCOO
+        if isinstance(A, BCOO):
+            return(A @ b.T).T
+        return jnp.stack([A.dot(b[i]) for i in range(b.shape[0])])
+
+    def get_diagonal(self, matrices, offset=0):
+        result = jnp.diagonal(matrices, offset=offset, axis1=1, axis2=2)
+        return jnp.transpose(result, [0, 2, 1])
+
+    def while_loop(self, loop: Callable, values: tuple, max_iter: Union[int, Tuple[int, ...], List[int]]):
+        if all(self.is_available(t) for t in values):
+            return self.stop_gradient_tree(Backend.while_loop(self, loop, values, max_iter))
+        if isinstance(max_iter, (tuple, list)):  # stack traced trajectory, unroll until max_iter
+            values = self.stop_gradient_tree(values)
+            trj = [values] if 0 in max_iter else []
+            for i in range(1, max(max_iter) + 1):
+                values = loop(*values)
+                if i in max_iter:
+                    trj.append(values)  # values are not mutable so no need to copy
+            return self.stop_gradient_tree(self.stack_leaves(trj))
+        else:
+            if max_iter is None:
+                cond = lambda vals: jnp.any(vals[0])
+                body = lambda vals: loop(*vals)
+                return jax.lax.while_loop(cond, body, values)
+            else:
+                cond = lambda vals: jnp.any(vals[1][0]) & (vals[0] < max_iter)
+                body = lambda vals: (vals[0] + 1, loop(*vals[1]))
+                return jax.lax.while_loop(cond, body, (self.as_tensor(0), values))[1]
+
+    def max(self, x, axis=None, keepdims=False):
+        return jnp.max(x, axis, keepdims=keepdims)
+
+    def min(self, x, axis=None, keepdims=False):
+        return jnp.min(x, axis, keepdims=keepdims)
+
+    def conv(self, value, kernel, zero_padding=True):
+        assert kernel.shape[0] in (1, value.shape[0])
+        assert value.shape[1] == kernel.shape[2], f"value has {value.shape[1]} channels but kernel has {kernel.shape[2]}"
+        assert value.ndim + 1 == kernel.ndim
+        # AutoDiff may require jax.lax.conv_general_dilated
+        result = []
+        for b in range(value.shape[0]):
+            b_kernel = kernel[min(b, kernel.shape[0] - 1)]
+            result_b = []
+            for o in range(kernel.shape[1]):
+                result_b.append(0)
+                for i in range(value.shape[1]):
+                    # result.at[b, o, ...].set(scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode='same' if zero_padding else 'valid'))
+                    result_b[-1] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode='same' if zero_padding else 'valid')
+            result.append(jnp.stack(result_b, 0))
+        return jnp.stack(result, 0)
+
+    def expand_dims(self, a, axis=0, number=1):
+        for _i in range(number):
+            a = jnp.expand_dims(a, axis)
+        return a
+
+    def cast(self, x, dtype: DType):
+        if self.is_tensor(x, only_native=True) and from_numpy_dtype(x.dtype) == dtype:
+            return x
+        else:
+            return jnp.array(x, to_numpy_dtype(dtype))
+
+    def unravel_index(self, flat_index, shape):
+        return jnp.stack(jnp.unravel_index(flat_index, shape), -1)
+
+    def ravel_multi_index(self, multi_index, shape, mode: Union[str, int] = 'undefined'):
+        if not self.is_available(shape):
+            return Backend.ravel_multi_index(self, multi_index, shape, mode)
+        mode = mode if isinstance(mode, int) else {'undefined': 'clip', 'periodic': 'wrap', 'clamp': 'clip'}[mode]
+        idx_first = jnp.transpose(multi_index, (self.ndims(multi_index)-1,) + tuple(range(self.ndims(multi_index)-1)))
+        result = jnp.ravel_multi_index(idx_first, shape, mode='wrap' if isinstance(mode, int) else mode)
+        if isinstance(mode, int):
+            outside = self.any((multi_index < 0) | (multi_index >= jnp.asarray(shape, dtype=multi_index.dtype)), -1)
+            result = self.where(outside, mode, result)
+        return result
+
+    def gather(self, values, indices, axis: int):
+        slices = [indices if i == axis else slice(None) for i in range(self.ndims(values))]
+        return values[tuple(slices)]
+
+    def batched_gather_nd(self, values, indices):
+        values = self.as_tensor(values)
+        indices = self.as_tensor(indices)
+        assert indices.shape[-1] == self.ndims(values) - 2
+        batch_size = combined_dim(values.shape[0], indices.shape[0])
+        results = []
+        for b in range(batch_size):
+            b_values = values[min(b, values.shape[0] - 1)]
+            b_indices = self.unstack(indices[min(b, indices.shape[0] - 1)], -1)
+            results.append(b_values[b_indices])
+        return jnp.stack(results)
+
+    def repeat(self, x, repeats, axis: int, new_length=None):
+        return jnp.repeat(x, self.as_tensor(repeats), axis, total_repeat_length=new_length)
+
+    def std(self, x, axis=None, keepdims=False):
+        return jnp.std(x, axis, keepdims=keepdims)
+
+    def boolean_mask(self, x, mask, axis=0, new_length=None, fill_value=0):
+        if new_length is None:
+            slices = [mask if i == axis else slice(None) for i in range(len(x.shape))]
+            return x[tuple(slices)]
+        else:
+            indices = jnp.argwhere(mask, size=new_length, fill_value=-1)[..., 0]
+            valid = indices >= 0
+            valid = valid[tuple([slice(None) if i == axis else None for i in range(len(x.shape))])]
+            result = self.gather(x, jnp.maximum(0, indices), axis)
+            return jnp.where(valid, result, fill_value)
+
+    def any(self, boolean_tensor, axis=None, keepdims=False):
+        if isinstance(boolean_tensor, (tuple, list)):
+            boolean_tensor = jnp.stack(boolean_tensor)
+        return jnp.any(boolean_tensor, axis=axis, keepdims=keepdims)
+
+    def all(self, boolean_tensor, axis=None, keepdims=False):
+        if isinstance(boolean_tensor, (tuple, list)):
+            boolean_tensor = jnp.stack(boolean_tensor)
+        return jnp.all(boolean_tensor, axis=axis, keepdims=keepdims)
+
+    def scatter(self, base_grid, indices, values, mode: str):
+        base_grid, values = self.auto_cast(base_grid, values)
+        batch_size = combined_dim(combined_dim(indices.shape[0], values.shape[0]), base_grid.shape[0])
+        spatial_dims = tuple(range(base_grid.ndim - 2))
+        dnums = jax.lax.ScatterDimensionNumbers(update_window_dims=(1,),  # channel dim of updates (batch dim removed)
+                                                inserted_window_dims=spatial_dims,  # no idea what this does but spatial_dims seems to work
+                                                scatter_dims_to_operand_dims=spatial_dims)  # spatial dims of base_grid (batch dim removed)
+        scatter = jax.lax.scatter_add if mode == 'add' else jax.lax.scatter
+        result = []
+        for b in range(batch_size):
+            b_grid = base_grid[b, ...]
+            b_indices = indices[min(b, indices.shape[0] - 1), ...]
+            b_values = values[min(b, values.shape[0] - 1), ...]
+            result.append(scatter(b_grid, b_indices, b_values, dnums))
+        return jnp.stack(result)
+
+    def histogram1d(self, values, weights, bin_edges):
+        def unbatched_hist(values, weights, bin_edges):
+            hist, _ = jnp.histogram(values, bin_edges, weights=weights)
+            return hist
+        return jax.vmap(unbatched_hist)(values, weights, bin_edges)
+
+    def bincount(self, x, weights, bins: int):
+        return jnp.bincount(x, weights=weights, minlength=bins, length=bins)
+
+    def quantile(self, x, quantiles):
+        return jnp.quantile(x, quantiles, axis=-1)
+
+    def argsort(self, x, axis=-1):
+        return jnp.argsort(x, axis)
+
+    def searchsorted(self, sorted_sequence, search_values, side: str, dtype=DType(int, 32)):
+        if self.ndims(sorted_sequence) == 1:
+            return jnp.searchsorted(sorted_sequence, search_values, side=side).astype(to_numpy_dtype(dtype))
+        else:
+            return jax.vmap(partial(self.searchsorted, side=side, dtype=dtype))(sorted_sequence, search_values)
+
+    def fft(self, x, axes: Union[tuple, list]):
+        x = self.to_complex(x)
+        if not axes:
+            return x
+        if len(axes) == 1:
+            return jnp.fft.fft(x, axis=axes[0]).astype(x.dtype)
+        elif len(axes) == 2:
+            return jnp.fft.fft2(x, axes=axes).astype(x.dtype)
+        else:
+            return jnp.fft.fftn(x, axes=axes).astype(x.dtype)
+
+    def ifft(self, k, axes: Union[tuple, list]):
+        if not axes:
+            return k
+        if len(axes) == 1:
+            return jnp.fft.ifft(k, axis=axes[0]).astype(k.dtype)
+        elif len(axes) == 2:
+            return jnp.fft.ifft2(k, axes=axes).astype(k.dtype)
+        else:
+            return jnp.fft.ifftn(k, axes=axes).astype(k.dtype)
+
+    def dtype(self, array) -> DType:
+        if isinstance(array, int):
+            return DType(int, 32)
+        if isinstance(array, float):
+            return DType(float, 64)
+        if isinstance(array, complex):
+            return DType(complex, 128)
+        if not isinstance(array, jnp.ndarray):
+            array = jnp.array(array)
+        return from_numpy_dtype(array.dtype)
+
+    def matrix_solve_least_squares(self, matrix: TensorType, rhs: TensorType) -> Tuple[TensorType, TensorType, TensorType, TensorType]:
+        solution, residuals, rank, singular_values = lstsq_batched(matrix, rhs)
+        return solution, residuals, rank, singular_values
+
+    def solve_triangular_dense(self, matrix, rhs, lower: bool, unit_diagonal: bool):
+        matrix, rhs = self.auto_cast(matrix, rhs, int_to_float=True, bool_to_int=True)
+        x = jax.lax.linalg.triangular_solve(matrix, rhs, lower=lower, unit_diagonal=unit_diagonal, left_side=True)
+        return x
+
+    def sparse_coo_tensor(self, indices: Union[tuple, list], values, shape: tuple):
+        return BCOO((values, indices), shape=shape)
+
+
+lstsq_batched = jax.vmap(jnp.linalg.lstsq)  # map first dimension, required for JaxBackend.matrix_solve_least_squares()
```

### Comparing `phiflow-2.3.4/phi/jax/flow.py` & `phiflow-2.4.0/phi/jax/flow.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,26 +1,26 @@
-# pylint: disable-msg = wildcard-import, unused-wildcard-import, unused-import
-"""
-Standard import for Jax mode without neural network support.
-
-Extends the import `from phi.flow import *` by Jax-related functions and modules.
-
-The following Jax modules are included: `jax`, `jax.numpy` as `jnp`, `jax.scipy` as `jsp`.
-
-Importing this module registers the Jax backend as the default backend unless called within a backend context.
-New tensors created via `phi.math` functions will be backed by Jax tensors.
-
-See `phi.flow`, `phi.torch.flow`, `phi.tf.flow`.
-"""
-
-from phi.flow import *
-from . import JAX
-import jax
-import jax.numpy as jnp
-import jax.scipy as jsp
-
-
-if not backend.context_backend():
-    backend.set_global_default_backend(JAX)
-else:
-    from ..math.backend import PHI_LOGGER as _LOGGER
-    _LOGGER.warning(f"Importing '{__name__}' within a backend context will not set the default backend.")
+# pylint: disable-msg = wildcard-import, unused-wildcard-import, unused-import
+"""
+Standard import for Jax mode without neural network support.
+
+Extends the import `from phi.flow import *` by Jax-related functions and modules.
+
+The following Jax modules are included: `jax`, `jax.numpy` as `jnp`, `jax.scipy` as `jsp`.
+
+Importing this module registers the Jax backend as the default backend unless called within a backend context.
+New tensors created via `phi.math` functions will be backed by Jax tensors.
+
+See `phi.flow`, `phi.torch.flow`, `phi.tf.flow`.
+"""
+
+from phi.flow import *
+from . import JAX
+import jax
+import jax.numpy as jnp
+import jax.scipy as jsp
+
+
+if not backend.context_backend():
+    backend.set_global_default_backend(JAX)
+else:
+    from ..math.backend import PHI_LOGGER as _LOGGER
+    _LOGGER.warning(f"Importing '{__name__}' within a backend context will not set the default backend.")
```

### Comparing `phiflow-2.3.4/phi/jax/stax/flow.py` & `phiflow-2.4.0/phi/jax/stax/flow.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,17 +1,17 @@
-# pylint: disable-msg = wildcard-import, unused-wildcard-import, unused-import
-"""
-Standard import for Jax + Stax mode.
-
-Extends the import `from phi.flow import *` by Jax-related functions and modules.
-
-The following Jax modules are included: `jax`, `jax.numpy` as `jnp`, `jax.scipy` as `jsp`.
-
-Importing this module registers the Jax backend as the default backend unless called within a backend context.
-New tensors created via `phi.math` functions will be backed by Jax tensors.
-
-See `phi.flow`, `phi.torch.flow`, `phi.tf.flow`.
-"""
-from ..flow import *
-
-from . import nets
-from .nets import parameter_count, get_parameters, save_state, load_state, dense_net, u_net, update_weights, adam, conv_net, res_net, adagrad, rmsprop, sgd, sgd as SGD, conv_classifier, coupling_layer, invertible_net
+# pylint: disable-msg = wildcard-import, unused-wildcard-import, unused-import
+"""
+Standard import for Jax + Stax mode.
+
+Extends the import `from phi.flow import *` by Jax-related functions and modules.
+
+The following Jax modules are included: `jax`, `jax.numpy` as `jnp`, `jax.scipy` as `jsp`.
+
+Importing this module registers the Jax backend as the default backend unless called within a backend context.
+New tensors created via `phi.math` functions will be backed by Jax tensors.
+
+See `phi.flow`, `phi.torch.flow`, `phi.tf.flow`.
+"""
+from ..flow import *
+
+from . import nets
+from .nets import parameter_count, get_parameters, save_state, load_state, dense_net, u_net, update_weights, adam, conv_net, res_net, adagrad, rmsprop, sgd, sgd as SGD, conv_classifier, coupling_layer, invertible_net
```

### Comparing `phiflow-2.3.4/phi/jax/stax/nets.py` & `phiflow-2.4.0/phi/jax/stax/nets.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,1101 +1,1101 @@
-"""
-Stax implementation of the unified machine learning API.
-Equivalent functions also exist for the other frameworks.
-
-For API documentation, see https://tum-pbs.github.io/PhiFlow/Network_API .
-"""
-import functools
-import warnings
-from typing import Callable, Union, Sequence
-
-import jax
-import jax.numpy as jnp
-import keras
-import numpy
-import numpy as np
-from jax import random
-from packaging import version
-
-if version.parse(jax.__version__) >= version.parse(
-        '0.2.25'):  # Stax and Optimizers were moved to jax.example_libraries on Oct 20, 2021
-    from jax.example_libraries import stax
-    import jax.example_libraries.optimizers as optim
-    from jax.example_libraries.optimizers import OptimizerState
-else:
-    from jax.experimental import stax
-    import jax.experimental.optimizers as optim
-    from jax.experimental.optimizers import OptimizerState
-
-    warnings.warn(f"Found Jax version {jax.__version__}. Using legacy imports.", FutureWarning)
-
-from phi import math
-from .. import JAX
-from ...math._functional import JitFunction
-
-
-class StaxNet:
-
-    def __init__(self, initialize: Callable, apply: Callable, input_shape: tuple):
-        self._initialize = initialize
-        self._apply = apply
-        self._input_shape = input_shape
-        self.parameters = None
-        self._tracers = None
-
-    def initialize(self):
-        rnd_key = JAX.rnd_key
-        JAX.rnd_key, init_key = random.split(rnd_key)
-        out_shape, params64 = self._initialize(init_key, input_shape=self._input_shape)
-        if math.get_precision() < 64:
-            self.parameters = _recursive_to_float32(params64)
-
-    def __call__(self, *args, **kwargs):
-        if self._tracers is not None:
-            return self._apply(self._tracers, *args)
-        else:
-            return self._apply(self.parameters, *args)
-
-
-class JaxOptimizer:
-
-    def __init__(self, initialize: Callable, update: Callable, get_params: Callable):
-        self._initialize, self._update, self._get_params = initialize, update, get_params  # Stax functions
-        self._state = None
-        self._step_i = 0
-        self._update_function_cache = {}
-
-    def initialize(self, net: tuple):
-        self._state = self._initialize(net)
-
-    def update_step(self, grads: tuple):
-        self._state = self._update(self._step_i, grads, self._state)
-        self._step_i += 1
-
-    def get_network_parameters(self):
-        return self._get_params(self._state)
-
-    def update(self, net: StaxNet, loss_function, wrt, loss_args, loss_kwargs):
-        if loss_function not in self._update_function_cache:
-            @functools.wraps(loss_function)
-            def update(packed_current_state, *loss_args, **loss_kwargs):
-                @functools.wraps(loss_function)
-                def loss_depending_on_net(params_tracer: tuple, *args, **kwargs):
-                    net._tracers = params_tracer
-                    loss_function_non_jit = loss_function.f if isinstance(loss_function, JitFunction) else loss_function
-                    result = loss_function_non_jit(*args, **kwargs)
-                    net._tracers = None
-                    return result
-
-                gradient_function = math.functional_gradient(loss_depending_on_net)
-                current_state = OptimizerState(packed_current_state, self._state.tree_def, self._state.subtree_defs)
-                current_params = self._get_params(current_state)
-                value, grads = gradient_function(current_params, *loss_args, **loss_kwargs)
-                next_state = self._update(self._step_i, grads[0], self._state)
-                return next_state.packed_state, value
-
-            if isinstance(loss_function, JitFunction):
-                update = math.jit_compile(update)
-            self._update_function_cache[loss_function] = update
-
-        next_packed_state, loss_output = self._update_function_cache[loss_function](self._state.packed_state,
-                                                                                    *loss_args, **loss_kwargs)
-        self._state = OptimizerState(next_packed_state, self._state.tree_def, self._state.subtree_defs)
-        return loss_output
-
-
-def parameter_count(model: StaxNet) -> int:
-    """
-    Counts the number of parameters in a model.
-
-    Args:
-        model: Stax model
-
-    Returns:
-        `int`
-    """
-    return int(_recursive_count_parameters(model.parameters))
-
-
-def _recursive_to_float32(obj):
-    if isinstance(obj, (tuple, list)):
-        return type(obj)([_recursive_to_float32(i) for i in obj])
-    elif isinstance(obj, dict):
-        return {k: _recursive_to_float32(v) for k, v in obj.items()}
-    else:
-        assert isinstance(obj, jax.numpy.ndarray)
-        return obj.astype(jax.numpy.float32)
-
-
-def _recursive_count_parameters(obj):
-    if isinstance(obj, (tuple, list)):
-        return sum([_recursive_count_parameters(item) for item in obj])
-    if isinstance(obj, dict):
-        return sum([_recursive_count_parameters(v) for v in obj.values()])
-    return numpy.prod(obj.shape)
-
-
-def get_parameters(model: StaxNet, wrap=True) -> dict:
-    result = {}
-    _recursive_add_parameters(model.parameters, wrap, (), result)
-    return result
-
-
-def _recursive_add_parameters(param, wrap: bool, prefix: tuple, result: dict):
-    if isinstance(param, dict):
-        for name, obj in param.items():
-            _recursive_add_parameters(obj, wrap, prefix + (str(name),), result)
-    elif isinstance(param, (tuple, list)):
-        for i, obj in enumerate(param):
-            _recursive_add_parameters(obj, wrap, prefix + (str(i),), result)
-    else:
-        rank = len(param.shape)
-        if prefix[-1] == 0 and rank == 2:
-            name = '.'.join(str(p) for p in prefix[:-1]) + '.weight'
-        elif prefix[-1] == 1 and rank == 1:
-            name = '.'.join(str(p) for p in prefix[:-1]) + '.bias'
-        else:
-            name = '.'.join(prefix)
-        if not wrap:
-            result[name] = param
-        else:
-            if rank == 1:
-                phi_tensor = math.wrap(param, math.channel('output'))
-            elif rank == 2:
-                phi_tensor = math.wrap(param, math.channel('input,output'))
-            elif rank == 3:
-                phi_tensor = math.wrap(param, math.channel('x,input,output'))
-            elif rank == 4:
-                phi_tensor = math.wrap(param, math.channel('x,y,input,output'))
-            elif rank == 5:
-                phi_tensor = math.wrap(param, math.channel('x,y,z,input,output'))
-            else:
-                raise NotImplementedError(rank)
-            result[name] = phi_tensor
-
-
-def save_state(obj: Union[StaxNet, JaxOptimizer], path: str):
-    """
-    Write the state of a module or optimizer to a file.
-
-    See Also:
-        `load_state()`
-
-    Args:
-        obj: `torch.nn.Module or torch.optim.Optimizer`
-        path: File path as `str`.
-    """
-    if not path.endswith('.npy'):
-        path += '.npy'
-    if isinstance(obj, StaxNet):
-        numpy.save(path, obj.parameters)
-    else:
-        raise NotImplementedError  # ToDo
-        # numpy.save(path, obj._state)
-
-
-def load_state(obj: Union[StaxNet, JaxOptimizer], path: str):
-    """
-    Read the state of a module or optimizer from a file.
-
-    See Also:
-        `save_state()`
-
-    Args:
-        obj: `torch.nn.Module or torch.optim.Optimizer`
-        path: File path as `str`.
-    """
-    if not path.endswith('.npy'):
-        path += '.npy'
-    if isinstance(obj, StaxNet):
-        state = numpy.load(path, allow_pickle=True)
-        obj.parameters = tuple([tuple(layer) for layer in state])
-    else:
-        raise NotImplementedError  # ToDo
-
-
-def update_weights(net: StaxNet, optimizer: JaxOptimizer, loss_function: Callable, *loss_args, **loss_kwargs):
-    """
-    Computes the gradients of `loss_function` w.r.t. the parameters of `net` and updates its weights using `optimizer`.
-
-    This is the Jax version. Analogue functions exist for other learning frameworks.
-
-    Args:
-        net: Learning model.
-        optimizer: Optimizer.
-        loss_function: Loss function, called as `loss_function(*loss_args, **loss_kwargs)`.
-        *loss_args: Arguments given to `loss_function`.
-        **loss_kwargs: Keyword arguments given to `loss_function`.
-
-    Returns:
-        Output of `loss_function`.
-    """
-    loss_output = optimizer.update(net, loss_function, net.parameters, loss_args, loss_kwargs)
-    net.parameters = optimizer.get_network_parameters()
-    return loss_output
-
-
-def adam(net: StaxNet, learning_rate: float = 1e-3, betas=(0.9, 0.999), epsilon=1e-07):
-    """
-    Creates an Adam optimizer for `net`, alias for [`jax.example_libraries.optimizers.adam`](https://jax.readthedocs.io/en/latest/jax.example_libraries.optimizers.html).
-    Analogous functions exist for other learning frameworks.
-    """
-    opt = JaxOptimizer(*optim.adam(learning_rate, betas[0], betas[1], epsilon))
-    opt.initialize(net.parameters)
-    return opt
-
-
-def sgd(net: StaxNet, learning_rate: float = 1e-3, momentum=0, dampening=0, weight_decay=0, nesterov=False):
-    """
-    Creates an SGD optimizer for `net`, alias for [`jax.example_libraries.optimizers.SGD`](https://jax.readthedocs.io/en/latest/jax.example_libraries.optimizers.html).
-    Analogous functions exist for other learning frameworks.
-    """
-    if momentum == 0:
-        opt = JaxOptimizer(*optim.sgd(learning_rate))
-    else:
-        opt = JaxOptimizer(*optim.momentum(learning_rate, momentum))
-    opt.initialize(net.parameters)
-    return opt
-
-
-def adagrad(net: StaxNet, learning_rate: float = 1e-3, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10):
-    """
-    Creates an Adagrad optimizer for `net`, alias for [`jax.example_libraries.optimizers.adagrad`](https://jax.readthedocs.io/en/latest/jax.example_libraries.optimizers.html).
-    Analogue functions exist for other learning frameworks.
-    """
-    opt = JaxOptimizer(*optim.adagrad(learning_rate))
-    opt.initialize(net.parameters)
-    return opt
-
-
-def rmsprop(net: StaxNet, learning_rate: float = 1e-3, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False):
-    """
-    Creates an RMSprop optimizer for `net`, alias for [`jax.example_libraries.optimizers.rmsprop`](https://jax.readthedocs.io/en/latest/jax.example_libraries.optimizers.html).
-    Analogue functions exist for other learning frameworks.
-    """
-    if momentum == 0:
-        opt = JaxOptimizer(*optim.rmsprop(learning_rate, alpha, eps))
-    else:
-        opt = JaxOptimizer(*optim.rmsprop_momentum(learning_rate, alpha, eps, momentum))
-    opt.initialize(net.parameters)
-    return opt
-
-
-def dense_net(in_channels: int,
-              out_channels: int,
-              layers: Sequence[int],
-              batch_norm=False,
-              activation='ReLU',
-              softmax=False) -> StaxNet:
-    """
-    Fully-connected neural networks are available in ΦFlow via dense_net().
-    Arguments:
-        in_channels : size of input layer, int
-        out_channels = size of output layer, int
-        layers : tuple of linear layers between input and output neurons, list or tuple
-        activation : activation function used within the layers, string
-        batch_norm : use of batch norm after each linear layer, bool
-
-    Returns:
-        Dense net model as specified by input arguments
-    """
-    activation = {'ReLU': stax.Relu, 'Sigmoid': stax.Sigmoid, 'tanh': stax.Tanh}[activation]
-    stax_layers = []
-    for neuron_count in layers:
-        stax_layers.append(stax.Dense(neuron_count))
-        stax_layers.append(activation)
-        if batch_norm:
-            stax_layers.append(stax.BatchNorm(axis=(0,)))
-    stax_layers.append(stax.Dense(out_channels))
-    if softmax:
-        stax_layers.append(stax.elementwise(stax.softmax, axis=-1))
-    net_init, net_apply = stax.serial(*stax_layers)
-    net = StaxNet(net_init, net_apply, (-1, in_channels))
-    net.initialize()
-    return net
-
-
-def u_net(in_channels: int,
-          out_channels: int,
-          levels: int = 4,
-          filters: Union[int, tuple, list] = 16,
-          batch_norm: bool = True,
-          activation='ReLU',
-          in_spatial: Union[tuple, int] = 2,
-          periodic=False,
-          use_res_blocks: bool = False) -> StaxNet:
-    """
-     ΦFlow provides a built-in U-net architecture, classically popular for Semantic Segmentation in Computer Vision, composed of downsampling and upsampling layers.
-
-     Arguments:
-
-         in_channels: input channels of the feature map, dtype : int
-         out_channels : output channels of the feature map, dtype : int
-         levels : number of levels of down-sampling and upsampling, dtype : int
-         filters : filter sizes at each down/up sampling convolutional layer, if the input is integer all conv layers have the same filter size,
-         dtype : int or tuple
-         activation : activation function used within the layers, dtype : string
-         batch_norm : use of batchnorm after each conv layer, dtype : bool
-         in_spatial : spatial dimensions of the input feature map, dtype : int
-         use_res_blocks : use convolutional blocks with skip connections instead of regular convolutional blocks, dtype : bool
-
-     Returns:
-
-         U-net model as specified by input arguments
-     """
-    if isinstance(filters, (tuple, list)):
-        assert len(filters) == levels, f"List of filters has length {len(filters)} but u-net has {levels} levels."
-    else:
-        filters = (filters,) * levels
-    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
-    if isinstance(in_spatial, int):
-        d = in_spatial
-        in_spatial = (1,) * d
-    else:
-        assert isinstance(in_spatial, tuple)
-        d = len(in_spatial)
-    # Create layers
-    if use_res_blocks:
-        inc_init, inc_apply = resnet_block(in_channels, filters[0], periodic, batch_norm, activation, d)
-    else:
-        inc_init, inc_apply = create_double_conv(d, filters[0], filters[0], batch_norm, activation, periodic)
-    init_functions, apply_functions = {}, {}
-    for i in range(1, levels):
-        if use_res_blocks:
-            init_functions[f'down{i}'], apply_functions[f'down{i}'] = resnet_block(filters[i - 1], filters[i], periodic, batch_norm, activation, d)
-            init_functions[f'up{i}'], apply_functions[f'up{i}'] = resnet_block(filters[i] + filters[i - 1], filters[i - 1], periodic, batch_norm, activation, d)
-        else:
-            init_functions[f'down{i}'], apply_functions[f'down{i}'] = create_double_conv(d, filters[i], filters[i], batch_norm, activation, periodic)
-            init_functions[f'up{i}'], apply_functions[f'up{i}'] = create_double_conv(d, filters[i - 1], filters[i - 1], batch_norm, activation, periodic)
-    outc_init, outc_apply = CONV[d](out_channels, (1,) * d, padding='same')
-    max_pool_init, max_pool_apply = stax.MaxPool((2,) * d, padding='same', strides=(2,) * d)
-    _, up_apply = create_upsample()
-
-    def net_init(rng, input_shape):
-        params = {}
-        rngs = random.split(rng, 2)
-        shape = input_shape
-        # Layers
-        shape, params['inc'] = inc_init(rngs[0], shape)
-        shapes = [shape]
-        for i in range(1, levels):
-            shape, _ = max_pool_init(None, shape)
-            shape, params[f'down{i}'] = init_functions[f'down{i}'](rngs[i], shape)
-            shapes.insert(0, shape)
-        for i in range(1, levels):
-            shape = shapes[i][:-1] + (shapes[i][-1] + shape[-1],)
-            shape, params[f'up{i}'] = init_functions[f'up{i}'](rngs[levels + i], shape)
-        shape, params['outc'] = outc_init(rngs[-1], shape)
-        return shape, params
-
-    # no @jax.jit needed here since the user can jit this in the loss_function
-    def net_apply(params, inputs, **kwargs):
-        x = inputs
-        x = inc_apply(params['inc'], x, **kwargs)
-        xs = [x]
-        for i in range(1, levels):
-            x = max_pool_apply(None, x, **kwargs)
-            x = apply_functions[f'down{i}'](params[f'down{i}'], x, **kwargs)
-            xs.insert(0, x)
-        for i in range(1, levels):
-            x = up_apply(None, x, **kwargs)
-            x = jnp.concatenate([x, xs[i]], axis=-1)
-            x = apply_functions[f'up{i}'](params[f'up{i}'], x, **kwargs)
-        x = outc_apply(params['outc'], x, **kwargs)
-        return x
-
-    net = StaxNet(net_init, net_apply, (1,) + in_spatial + (in_channels,))
-    net.initialize()
-    return net
-
-
-ACTIVATIONS = {'ReLU': stax.Relu, 'Sigmoid': stax.Sigmoid, 'tanh': stax.Tanh, 'SiLU': stax.Selu}
-CONV = [None,
-        functools.partial(stax.GeneralConv, ('NWC', 'WIO', 'NWC')),
-        functools.partial(stax.GeneralConv, ('NWHC', 'WHIO', 'NWHC')),
-        functools.partial(stax.GeneralConv, ('NWHDC', 'WHDIO', 'NWHDC')), ]
-
-'''
-def create_double_conv(d: int, out_channels: int, mid_channels: int, batch_norm: bool, activation: Callable):
-    
-    return stax.serial(
-        CONV[d](out_channels, (3,) * d, padding='same'),
-        stax.BatchNorm(axis=tuple(range(d + 1))) if batch_norm else stax.Identity,
-        activation,
-        CONV[d](out_channels, (3,) * d, padding='same'),
-        stax.BatchNorm(axis=tuple(range(d + 1))) if batch_norm else stax.Identity,
-        activation)
-'''
-
-
-# Periodic Implementation
-def create_double_conv(d: int, out_channels: int, mid_channels: int, batch_norm: bool, activation: Callable, periodic: bool):
-    init_fn, apply_fn = {}, {}
-    init_fn['conv1'], apply_fn['conv1'] = stax.serial(CONV[d](mid_channels, (3,) * d, padding='valid'), stax.BatchNorm(axis=tuple(range(d + 1))) if batch_norm else stax.Identity, activation)
-    init_fn['conv2'], apply_fn['conv2'] = stax.serial(CONV[d](mid_channels, (3,) * d, padding='valid'), stax.BatchNorm(axis=tuple(range(d + 1))) if batch_norm else stax.Identity, activation)
-
-    def net_init(rng, input_shape):
-        params = {}
-        rngs = random.split(rng, 2)
-        shape, params['conv1'] = init_fn['conv1'](rngs[0], input_shape)
-        shape, params['conv2'] = init_fn['conv2'](rngs[1], shape)
-
-        return shape, params
-
-    def net_apply(params, inputs):
-        x = inputs
-        pad_tuple = [[0, 0]] + [[1, 1] for i in range(d)] + [[0, 0]]
-        out = jnp.pad(x, pad_width=pad_tuple, mode='wrap' if periodic else 'constant')
-        out = apply_fn['conv1'](params['conv1'], out)
-        out = jnp.pad(out, pad_width=pad_tuple, mode='wrap' if periodic else 'constant')
-        out = apply_fn['conv2'](params['conv2'], out)
-        return out
-
-    return net_init, net_apply
-
-
-def create_upsample():
-    # def upsample_init(rng, input_shape):
-    #     return shape, []
-    def upsample_apply(params, inputs, **kwargs):
-        x = math.wrap(inputs, math.batch('batch'), *[math.spatial(f'{i}') for i in range(len(inputs.shape) - 2)],
-                      math.channel('vector'))
-        x = math.upsample2x(x)
-        return x.native(x.shape)
-
-    return NotImplemented, upsample_apply
-
-
-def conv_classifier(in_features: int,
-                    in_spatial: Union[tuple, list],
-                    num_classes: int,
-                    blocks=(64, 128, 256, 256, 512, 512),
-                    dense_layers=(4096, 4096, 100),
-                    batch_norm=True,
-                    activation='ReLU',
-                    softmax=True,
-                    periodic=False):
-    """
-    Based on VGG16.
-    """
-    if isinstance(in_spatial, int):
-        d = in_spatial
-        in_spatial = (1,) * d
-    else:
-        assert isinstance(in_spatial, tuple)
-        d = len(in_spatial)
-    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
-    stax_dense_layers = []
-    init_fn, apply_fn = {}, {}
-
-    net_list = []
-    for i, (prev, next) in enumerate(zip((in_features,) + tuple(blocks[:-1]), blocks)):
-        if i in (0, 1):
-            net_list.append(f'conv{i+1}')
-            init_fn[net_list[-1]], apply_fn[net_list[-1]] = create_double_conv(d, next, next, batch_norm, activation, periodic)
-        else:
-            net_list.append(f'conv{i+1}_1')
-            init_fn[net_list[-1]], apply_fn[net_list[-1]] = create_double_conv(d, 256, 256, batch_norm, activation, periodic)
-            net_list.append(f'conv{i+1}_2')
-            init_fn[net_list[-1]], apply_fn[net_list[-1]] = stax.serial(CONV[d](256, (3,) * d, padding='valid'),
-                                                                        stax.BatchNorm(axis=tuple(range(d + 1))) if batch_norm else stax.Identity,
-                                                                        activation)
-        net_list.append(f'max_pool{i+1}')
-        init_fn[net_list[-1]], apply_fn[net_list[-1]] = stax.MaxPool((2,) * d, padding='valid', strides=(2,) * d)
-    init_fn['flatten'], apply_fn['flatten'] = stax.Flatten
-    for i, neuron_count in enumerate(dense_layers):
-        stax_dense_layers.append(stax.Dense(neuron_count))
-        stax_dense_layers.append(activation)
-        if batch_norm:
-            stax_dense_layers.append(stax.BatchNorm(axis=(0,)))
-    stax_dense_layers.append(stax.Dense(num_classes))
-    if softmax:
-        stax_dense_layers.append(stax.elementwise(stax.softmax, axis=-1))
-    dense_init, dense_apply = stax.serial(*stax_dense_layers)
-
-    def net_init(rng, input_shape):
-        params = {}
-        rngs = random.split(rng, 2)
-        shape = input_shape
-        N = len(net_list)
-        for i in range(N):
-            shape, params[f'{net_list[i]}'] = init_fn[f'{net_list[i]}'](rngs[i], shape)
-        shape, params['flatten'] = init_fn['flatten'](rngs[N], shape)
-        flat_size = int(np.prod(in_spatial) * blocks[-1] / (2**d) ** len(blocks))
-        shape, params['dense'] = dense_init(rngs[N + 1], (1,) + (flat_size,))
-        return shape, params
-
-    def net_apply(params, inputs, **kwargs):
-        x = inputs
-        pad_tuple = [[0, 0]] + [[1, 1]] * d + [[0, 0]]
-        for i in range(len(net_list)):
-            if net_list[i] in ['conv3_2', 'conv4_2', 'conv5_2']:
-                x = jnp.pad(x, pad_width=pad_tuple, mode='wrap' if periodic else 'constant')
-            x = apply_fn[f'{net_list[i]}'](params[f'{net_list[i]}'], x)
-        x = apply_fn['flatten'](params['flatten'], x)
-        out = dense_apply(params['dense'], x, **kwargs)
-        return out
-
-    net = StaxNet(net_init, net_apply, (1,) + in_spatial + (in_features,))
-    net.initialize()
-    return net
-
-
-def conv_net(in_channels: int,
-             out_channels: int,
-             layers: Sequence[int],
-             batch_norm: bool = False,
-             activation: Union[str, Callable] = 'ReLU',
-             periodic=False,
-             in_spatial: Union[int, tuple] = 2) -> StaxNet:
-    """
-    Built in Conv-Nets are also provided. Contrary to the classical convolutional neural networks, the feature map spatial size remains the same throughout the layers. Each layer of the network is essentially a convolutional block comprising of two conv layers. A filter size of 3 is used in the convolutional layers.
-    Arguments:
-
-        in_channels : input channels of the feature map, dtype : int
-        out_channels : output channels of the feature map, dtype : int
-        layers : list or tuple of output channels for each intermediate layer between the input and final output channels, dtype : list or tuple
-        activation : activation function used within the layers, dtype : string
-        batch_norm : use of batchnorm after each conv layer, dtype : bool
-        in_spatial : spatial dimensions of the input feature map, dtype : int
-
-    Returns:
-
-        Conv-net model as specified by input arguments
-    """
-    if isinstance(in_spatial, int):
-        d = in_spatial
-        in_spatial = (1,) * d
-    else:
-        assert isinstance(in_spatial, tuple)
-        d = len(in_spatial)
-    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
-    init_fn, apply_fn = {}, {}
-    if len(layers) < 1:
-        layers.append(out_channels)
-    init_fn['conv_in'], apply_fn['conv_in'] = stax.serial(
-        CONV[d](layers[0], (3,) * d, padding='valid'),
-        stax.BatchNorm(axis=tuple(range(d + 1))) if batch_norm else stax.Identity,
-        activation)
-    for i in range(1, len(layers)):
-        init_fn[f'conv{i}'], apply_fn[f'conv{i}'] = stax.serial(
-            CONV[d](layers[i], (3,) * d, padding='valid'),
-            stax.BatchNorm(axis=tuple(range(d + 1))) if batch_norm else stax.Identity,
-            activation)
-    init_fn['conv_out'], apply_fn['conv_out'] = CONV[d](out_channels, (1,) * d)
-
-    def net_init(rng, input_shape):
-        params = {}
-        rngs = random.split(rng, 2)
-        shape, params['conv_in'] = init_fn['conv_in'](rngs[0], input_shape)
-        for i in range(1, len(layers)):
-            shape, params[f'conv{i + 1}'] = init_fn[f'conv{i + 1}'](rngs[i], shape)
-        shape, params['conv_out'] = init_fn['conv_out'](rngs[len(layers)], shape)
-        return shape, params
-
-    def net_apply(params, inputs):
-        x = inputs
-        pad_tuple = [(0, 0)]
-        for i in range(d):
-            pad_tuple.append((1, 1))
-        pad_tuple.append((0, 0))
-        out = jnp.pad(x, pad_width=pad_tuple, mode='wrap' if periodic else 'constant')
-        out = apply_fn['conv_in'](params['conv_in'], out)
-        for i in range(1, len(layers)):
-            out = jnp.pad(out, pad_width=pad_tuple, mode='wrap' if periodic else 'constant')
-            out = apply_fn[f'conv{i + 1}'](params[f'conv{i + 1}'], out)
-        out = apply_fn['conv_out'](params['conv_out'], out)
-        return out
-
-    net = StaxNet(net_init, net_apply, (1,) + in_spatial + (in_channels,))
-    net.initialize()
-    return net
-
-
-def res_net(in_channels: int,
-            out_channels: int,
-            layers: Sequence[int],
-            batch_norm: bool = False,
-            activation: Union[str, Callable] = 'ReLU',
-            periodic=False,
-            in_spatial: Union[int, tuple] = 2) -> StaxNet:
-    """
-    Built in Res-Nets are provided in the ΦFlow framework. Similar to the conv-net, the feature map spatial size remains the same throughout the layers.
-    These networks use residual blocks composed of two conv layers with a skip connection added from the input to the output feature map.
-    A default filter size of 3 is used in the convolutional layers.
-
-    Arguments:
-
-        in_channels : input channels of the feature map, dtype : int
-        out_channels : output channels of the feature map, dtype : int
-        layers : list or tuple of output channels for each intermediate layer between the input and final output channels, dtype : list or tuple
-        activation : activation function used within the layers, dtype : string
-        batch_norm : use of batchnorm after each conv layer, dtype : bool
-        in_spatial : spatial dimensions of the input feature map, dtype : int
-
-    Returns:
-
-        Res-net model as specified by input arguments
-    """
-    if isinstance(in_spatial, int):
-        d = in_spatial
-        in_spatial = (1,) * d
-    else:
-        assert isinstance(in_spatial, tuple)
-        d = len(in_spatial)
-    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
-    stax_layers = []
-    if len(layers) > 0:
-        stax_layers.append(resnet_block(in_channels, layers[0], periodic, batch_norm, activation, d))
-
-        for i in range(1, len(layers)):
-            stax_layers.append(resnet_block(layers[i - 1], layers[i], periodic, batch_norm, activation, d))
-
-        stax_layers.append(resnet_block(layers[len(layers) - 1], out_channels, periodic, batch_norm, activation, d))
-    else:
-        stax_layers.append(resnet_block(in_channels, out_channels, periodic, batch_norm, activation, d))
-    net_init, net_apply = stax.serial(*stax_layers)
-    net = StaxNet(net_init, net_apply, (1,) + in_spatial + (in_channels,))
-    net.initialize()
-    return net
-
-
-def resnet_block(in_channels: int,
-                 out_channels: int,
-                 periodic: bool,
-                 batch_norm: bool,
-                 activation: Union[str, Callable] = 'ReLU',
-                 d: Union[int, tuple] = 2):
-    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
-    init_fn, apply_fn = {}, {}
-    init_fn['conv1'], apply_fn['conv1'] = stax.serial(
-        CONV[d](out_channels, (3,) * d, padding='valid'),
-        stax.BatchNorm(axis=tuple(range(d + 1))) if batch_norm else stax.Identity,
-        activation)
-    init_fn['conv2'], apply_fn['conv2'] = stax.serial(
-        CONV[d](out_channels, (3,) * d, padding='valid'),
-        stax.BatchNorm(axis=tuple(range(d + 1))) if batch_norm else stax.Identity,
-        activation)
-
-    init_activation, apply_activation = activation
-    if in_channels != out_channels:
-        init_fn['sample_conv'], apply_fn['sample_conv'] = stax.serial(
-            CONV[d](out_channels, (1,) * d, padding='VALID'),
-            stax.BatchNorm(axis=tuple(range(d + 1))) if batch_norm else stax.Identity)
-    else:
-        init_fn['sample_conv'], apply_fn['sample_conv'] = stax.Identity
-
-    def net_init(rng, input_shape):
-        params = {}
-        rngs = random.split(rng, 2)
-
-        # Preparing a list of shapes and dictionary of parameters to return
-        shape, params['conv1'] = init_fn['conv1'](rngs[0], input_shape)
-        shape, params['conv2'] = init_fn['conv2'](rngs[1], shape)
-        shape, params['sample_conv'] = init_fn['sample_conv'](rngs[2], input_shape)
-        shape, params['activation'] = init_activation(rngs[3], shape)
-        return shape, params
-
-    def net_apply(params, inputs, **kwargs):
-        x = inputs
-
-        pad_tuple = [[0, 0]] + [[1, 1] for i in range(d)] + [[0, 0]]
-
-        out = jnp.pad(x, pad_width=pad_tuple, mode='wrap' if periodic else 'constant')
-        out = apply_fn['conv1'](params['conv1'], out)
-        out = jnp.pad(out, pad_width=pad_tuple, mode='wrap' if periodic else 'constant')
-        out = apply_fn['conv2'](params['conv2'], out)
-        skip_x = apply_fn['sample_conv'](params['sample_conv'], x, **kwargs)
-        out = jnp.add(out, skip_x)
-        # out = apply_activation(params['activation'], out)
-        return out
-
-    return net_init, net_apply
-
-
-def get_mask(inputs, reverse_mask, data_format='NHWC'):
-    """ Compute mask for slicing input feature map for Invertible Nets """
-    shape = inputs.shape
-    if len(shape) == 2:
-        N = shape[-1]
-        range_n = jnp.arange(0, N)
-        even_ind = range_n % 2
-        checker = jnp.reshape(even_ind, (-1, N))
-    elif len(shape) == 4:
-        H = shape[2] if data_format == 'NCHW' else shape[1]
-        W = shape[3] if data_format == 'NCHW' else shape[2]
-
-        range_h = jnp.arange(0, H) % 2
-        range_w = jnp.arange(0, W) % 2
-
-        even_ind_h = range_h.astype(bool)
-        even_ind_w = range_w.astype(bool)
-
-        ind_h = jnp.tile(jnp.expand_dims(even_ind_h, -1), [1, W])
-        ind_w = jnp.tile(jnp.expand_dims(even_ind_w, 0), [H, 1])
-        # ind_h = even_ind_h.unsqueeze(-1).repeat(1, W)
-        # ind_w = even_ind_w.unsqueeze( 0).repeat(H, 1)
-
-        checker = jnp.logical_xor(ind_h, ind_w)
-
-        reshape = [-1, 1, H, W] if data_format == 'NCHW' else [-1, H, W, 1]
-        checker = jnp.reshape(checker, reshape)
-        checker = checker.astype(jnp.float32)
-
-    else:
-        raise ValueError('Invalid tensor shape. Dimension of the tensor shape must be '
-                         '2 (NxD) or 4 (NxCxHxW or NxHxWxC), got {}.'.format(inputs.get_shape().as_list()))
-
-    if reverse_mask:
-        checker = 1 - checker
-
-    return checker
-
-
-def Dense_resnet_block(in_channels: int,
-                       mid_channels: int,
-                       batch_norm: bool = False,
-                       activation: Union[str, Callable] = 'ReLU'):
-    inputs = keras.Input(shape=(in_channels,))
-    x_1 = inputs
-
-    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
-    init_fn, apply_fn = {}, {}
-    init_fn['dense1'], apply_fn['dense1'] = stax.serial(stax.Dense(mid_channels),
-                                                        stax.BatchNorm(axis=(0,)),
-                                                        activation)
-    init_fn['dense2'], apply_fn['dense2'] = stax.serial(stax.Dense(in_channels),
-                                                        stax.BatchNorm(axis=(0,)),
-                                                        activation)
-    init_activation, apply_activation = activation
-
-    def net_init(rng, input_shape):
-        params = {}
-        rngs = random.split(rng, 2)
-
-        shape, params['dense1'] = init_fn['dense1'](rngs[0], input_shape)
-        shape, params['dense2'] = init_fn['dense2'](rngs[1], shape)
-        shape, params['activation'] = init_activation(rngs[2], shape)
-        return shape, params
-
-    def net_apply(params, inputs, **kwargs):
-        x = inputs
-
-        out = apply_fn['dense1'](params['dense1'], x)
-        out = apply_fn['dense2'](params['dense2'], out)
-
-        out = jnp.add(out, x)
-
-        return out
-
-    return net_init, net_apply
-
-
-def conv_net_unit(in_channels: int,
-                  out_channels: int,
-                  layers: Sequence[int],
-                  periodic: bool = False,
-                  batch_norm: bool = False,
-                  activation: Union[str, Callable] = 'ReLU',
-                  in_spatial: Union[int, tuple] = 2, **kwargs):
-    """ Conv-net unit for Invertible Nets"""
-    if isinstance(in_spatial, int):
-        d = in_spatial
-    else:
-        assert isinstance(in_spatial, tuple)
-        d = len(in_spatial)
-    if isinstance(activation, str):
-        activation = ACTIVATIONS[activation]
-
-    init_fn, apply_fn = {}, {}
-    if len(layers) < 1:
-        layers.append(out_channels)
-    init_fn['conv_in'], apply_fn['conv_in'] = stax.serial(
-        CONV[d](layers[0], (3,) * d, padding='valid'),
-        stax.BatchNorm(axis=tuple(range(d + 1))) if batch_norm else stax.Identity,
-        activation)
-    for i in range(1, len(layers)):
-        init_fn[f'conv{i}'], apply_fn[f'conv{i}'] = stax.serial(
-            CONV[d](layers[i], (3,) * d, padding='valid'),
-            stax.BatchNorm(axis=tuple(range(d + 1))) if batch_norm else stax.Identity,
-            activation)
-
-    init_fn['conv_out'], apply_fn['conv_out'] = CONV[d](out_channels, (1,) * d)
-
-    def net_init(rng, input_shape):
-        params = {}
-        rngs = random.split(rng, 2)
-
-        shape, params['conv_in'] = init_fn['conv_in'](rngs[0], input_shape)
-
-        for i in range(1, len(layers)):
-            shape, params[f'conv{i + 1}'] = init_fn[f'conv{i + 1}'](rngs[i], shape)
-
-        shape, params['conv_out'] = init_fn['conv_out'](rngs[len(layers)], shape)
-
-        return shape, params
-
-    def net_apply(params, inputs):
-        x = inputs
-
-        pad_tuple = [(0, 0)]
-        for i in range(d):
-            pad_tuple.append((1, 1))
-        pad_tuple.append((0, 0))
-
-        out = jnp.pad(x, pad_width=pad_tuple, mode='wrap' if periodic else 'constant')
-
-        out = apply_fn['conv_in'](params['conv_in'], out)
-
-        for i in range(1, len(layers)):
-            out = jnp.pad(out, pad_width=pad_tuple, mode='wrap' if periodic else 'constant')
-            out = apply_fn[f'conv{i + 1}'](params[f'conv{i + 1}'], out)
-
-        out = apply_fn['conv_out'](params['conv_out'], out)
-
-        return out
-
-    return net_init, net_apply
-
-
-def u_net_unit(in_channels: int,
-               out_channels: int,
-               levels: int = 4,
-               filters: Union[int, tuple, list] = 16,
-               batch_norm: bool = True,
-               activation='ReLU',
-               periodic=False,
-               in_spatial: Union[tuple, int] = 2,
-               use_res_blocks: bool = False, **kwargs):
-    """ U-net unit for Invertible Nets"""
-    if isinstance(filters, (tuple, list)):
-        assert len(filters) == levels, f"List of filters has length {len(filters)} but u-net has {levels} levels."
-    else:
-        filters = (filters,) * levels
-    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
-    if isinstance(in_spatial, int):
-        d = in_spatial
-        in_spatial = (1,) * d
-    else:
-        assert isinstance(in_spatial, tuple)
-        d = len(in_spatial)
-    # Create layers
-    if use_res_blocks:
-        inc_init, inc_apply = resnet_block(in_channels, filters[0], periodic, batch_norm, activation, d)
-    else:
-        inc_init, inc_apply = create_double_conv(d, filters[0], filters[0], batch_norm, activation, periodic)
-    init_functions, apply_functions = {}, {}
-    for i in range(1, levels):
-        if use_res_blocks:
-            init_functions[f'down{i}'], apply_functions[f'down{i}'] = resnet_block(filters[i - 1], filters[i], periodic, batch_norm, activation, d)
-            init_functions[f'up{i}'], apply_functions[f'up{i}'] = resnet_block(filters[i] + filters[i - 1], filters[i - 1], periodic, batch_norm, activation, d)
-        else:
-            init_functions[f'down{i}'], apply_functions[f'down{i}'] = create_double_conv(d, filters[i], filters[i], batch_norm, activation, periodic)
-            init_functions[f'up{i}'], apply_functions[f'up{i}'] = create_double_conv(d, filters[i - 1], filters[i - 1], batch_norm, activation, periodic)
-    outc_init, outc_apply = CONV[d](out_channels, (1,) * d, padding='same')
-    max_pool_init, max_pool_apply = stax.MaxPool((2,) * d, padding='same', strides=(2,) * d)
-    _, up_apply = create_upsample()
-
-    def net_init(rng, input_shape):
-        params = {}
-        rngs = random.split(rng, 2)
-        shape = input_shape
-        # Layers
-        shape, params['inc'] = inc_init(rngs[0], shape)
-        shapes = [shape]
-        for i in range(1, levels):
-            shape, _ = max_pool_init(None, shape)
-            shape, params[f'down{i}'] = init_functions[f'down{i}'](rngs[i], shape)
-            shapes.insert(0, shape)
-        for i in range(1, levels):
-            shape = shapes[i][:-1] + (shapes[i][-1] + shape[-1],)
-            shape, params[f'up{i}'] = init_functions[f'up{i}'](rngs[levels + i], shape)
-        shape, params['outc'] = outc_init(rngs[-1], shape)
-        return shape, params
-
-    # no @jax.jit needed here since the user can jit this in the loss_function
-    def net_apply(params, inputs, **kwargs):
-        x = inputs
-        x = inc_apply(params['inc'], x, **kwargs)
-        xs = [x]
-        for i in range(1, levels):
-            x = max_pool_apply(None, x, **kwargs)
-            x = apply_functions[f'down{i}'](params[f'down{i}'], x, **kwargs)
-            xs.insert(0, x)
-        for i in range(1, levels):
-            x = up_apply(None, x, **kwargs)
-            x = jnp.concatenate([x, xs[i]], axis=-1)
-            x = apply_functions[f'up{i}'](params[f'up{i}'], x, **kwargs)
-        x = outc_apply(params['outc'], x, **kwargs)
-        return x
-
-    return net_init, net_apply
-
-
-def res_net_unit(in_channels: int,
-                 out_channels: int,
-                 layers: Sequence[int],
-                 batch_norm: bool = False,
-                 activation: Union[str, Callable] = 'ReLU',
-                 periodic=False,
-                 in_spatial: Union[int, tuple] = 2, **kwargs):
-    """ Res-net unit for Invertible Nets"""
-    if isinstance(in_spatial, int):
-        d = in_spatial
-    else:
-        assert isinstance(in_spatial, tuple)
-        d = len(in_spatial)
-    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
-    stax_layers = []
-    if len(layers) < 1:
-        layers.append(out_channels)
-    stax_layers.append(resnet_block(in_channels, layers[0], periodic, batch_norm, activation, d))
-    for i in range(1, len(layers)):
-        stax_layers.append(resnet_block(layers[i - 1], layers[i], periodic, batch_norm, activation, d))
-    stax_layers.append(CONV[d](out_channels, (1,) * d))
-    return stax.serial(*stax_layers)
-
-
-NET = {'u_net': u_net_unit, 'res_net': res_net_unit, 'conv_net': conv_net_unit}
-
-
-def coupling_layer(in_channels: int,
-                   activation: Union[str, Callable] = 'ReLU',
-                   batch_norm: bool = False,
-                   in_spatial: Union[int, tuple] = 2,
-                   net: str = 'u_net',
-                   reverse_mask: bool = False,
-                   **kwargs):
-    if isinstance(in_spatial, int):
-        d = in_spatial
-    else:
-        assert isinstance(in_spatial, tuple)
-        d = len(in_spatial)
-    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
-    init_fn, apply_fn = {}, {}
-    if d == 0:
-        init_fn['s1'], apply_fn['s1'] = stax.serial(
-            Dense_resnet_block(in_channels, in_channels, batch_norm, activation),
-            stax.Tanh)
-        init_fn['t1'], apply_fn['t1'] = Dense_resnet_block(in_channels, in_channels, batch_norm, activation)
-
-        init_fn['s2'], apply_fn['s2'] = stax.serial(
-            Dense_resnet_block(in_channels, in_channels, batch_norm, activation),
-            stax.Tanh)
-        init_fn['t2'], apply_fn['t2'] = Dense_resnet_block(in_channels, in_channels, batch_norm, activation)
-    else:
-        init_fn['s1'], apply_fn['s1'] = NET[net](in_channels=in_channels, out_channels=in_channels, layers=[], batch_norm=batch_norm, activation=activation, in_spatial=in_spatial, **kwargs)
-        init_fn['t1'], apply_fn['t1'] = NET[net](in_channels=in_channels, out_channels=in_channels, layers=[], batch_norm=batch_norm, activation=activation, in_spatial=in_spatial, **kwargs)
-        init_fn['s2'], apply_fn['s2'] = NET[net](in_channels=in_channels, out_channels=in_channels, layers=[], batch_norm=batch_norm, activation=activation, in_spatial=in_spatial, **kwargs)
-        init_fn['t2'], apply_fn['t2'] = NET[net](in_channels=in_channels, out_channels=in_channels, layers=[], batch_norm=batch_norm, activation=activation, in_spatial=in_spatial, **kwargs)
-
-    def net_init(rng, input_shape):
-        params = {}
-        rngs = random.split(rng, 2)
-
-        shape, params['s1'] = init_fn['s1'](rngs[0], input_shape)
-        shape, params['t1'] = init_fn['t1'](rngs[1], input_shape)
-        shape, params['s2'] = init_fn['s2'](rngs[2], input_shape)
-        shape, params['t2'] = init_fn['t2'](rngs[3], input_shape)
-
-        return shape, params
-
-    def net_apply(params, inputs, invert=False):
-        x = inputs
-
-        mask = get_mask(x, reverse_mask, 'NCHW')
-
-        if invert:
-            v1 = x * mask
-            v2 = x * (1 - mask)
-
-            s1 = apply_fn['s1'](params['s1'], v1)
-            t1 = apply_fn['t1'](params['t1'], v1)
-
-            u2 = (1 - mask) * (v2 - t1) * jnp.exp(-jnp.tanh(s1))
-
-            s2 = apply_fn['s2'](params['s2'], u2)
-            t2 = apply_fn['t2'](params['t2'], u2)
-
-            u1 = mask * (v1 - t2) * jnp.exp(-jnp.tanh(s2))
-
-            return u1 + u2
-        else:
-            u1 = x * mask
-            u2 = x * (1 - mask)
-
-            s2 = apply_fn['s2'](params['s2'], u2)
-            t2 = apply_fn['t2'](params['t2'], u2)
-
-            v1 = mask * (u1 * jnp.exp(jnp.tanh(s2)) + t2)
-
-            s1 = apply_fn['s1'](params['s1'], v1)
-            t1 = apply_fn['t1'](params['t1'], v1)
-
-            v2 = (1 - mask) * (u2 * jnp.exp(jnp.tanh(s1)) + t1)
-
-            return v1 + v2
-
-    return net_init, net_apply
-
-
-def invertible_net(in_channels: int,
-                   num_blocks: int,
-                   batch_norm: bool = False,
-                   net: str = 'u_net',
-                   activation: Union[str, type] = 'ReLU',
-                   in_spatial: Union[tuple, int] = 2, **kwargs):
-    """
-    ΦFlow also provides invertible neural networks that are capable of inverting the output tensor back to the input tensor initially passed.\ These networks have far reaching applications in predicting input parameters of a problem given its observations.\ Invertible nets are composed of multiple concatenated coupling blocks wherein each such block consists of arbitrary neural networks.
-
-    Currently, these arbitrary neural networks could be set to u_net(default), conv_net, res_net or dense_net blocks with in_channels = out_channels.
-    The architecture used is popularized by ["Real NVP"](https://arxiv.org/abs/1605.08803).
-
-    Arguments:
-
-        in_channels : input channels of the feature map, dtype : int
-        num_blocks : number of coupling blocks inside the invertible net, dtype : int
-        activation : activation function used within the layers, dtype : string
-        batch_norm : use of batchnorm after each layer, dtype : bool
-        in_spatial : spatial dimensions of the input feature map, dtype : int
-        net : type of neural network blocks used in coupling layers, dtype : str
-        **kwargs : placeholder for arguments not supported by the function
-
-    Returns:
-
-        Invertible Net model as specified by input arguments
-
-    Note: Currently supported values for net are 'u_net'(default), 'conv_net' and 'res_net'.
-    For choosing 'dense_net' as the network block in coupling layers in_spatial must be set to zero.
-    """
-    if isinstance(in_spatial, int):
-        d = in_spatial
-    else:
-        assert isinstance(in_spatial, tuple)
-        d = len(in_spatial)
-    init_fn, apply_fn = {}, {}
-    for i in range(num_blocks):
-        init_fn[f'CouplingLayer{i + 1}'], apply_fn[f'CouplingLayer{i + 1}'] = coupling_layer(in_channels, activation, batch_norm, d, net, (i % 2 == 0), **kwargs)
-
-    def net_init(rng, input_shape):
-        params = {}
-        rngs = random.split(rng, 2)
-        for i in range(num_blocks):
-            shape, params[f'CouplingLayer{i + 1}'] = init_fn[f'CouplingLayer{i + 1}'](rngs[i], input_shape)
-        return shape, params
-
-    def net_apply(params, inputs, invert=False):
-        out = inputs
-        if invert:
-            for i in range(num_blocks, 0, -1):
-                out = apply_fn[f'CouplingLayer{i}'](params[f'CouplingLayer{i}'], out, invert)
-        else:
-            for i in range(1, num_blocks + 1):
-                out = apply_fn[f'CouplingLayer{i}'](params[f'CouplingLayer{i}'], out)
-        return out
-
-    if d == 0:
-        net = StaxNet(net_init, net_apply, (1,) + (in_channels,))
-    else:
-        net = StaxNet(net_init, net_apply, (1,) + (1,) * d + (in_channels,))
-    net.initialize()
-    return net
+"""
+Stax implementation of the unified machine learning API.
+Equivalent functions also exist for the other frameworks.
+
+For API documentation, see https://tum-pbs.github.io/PhiFlow/Network_API .
+"""
+import functools
+import warnings
+from typing import Callable, Union, Sequence
+
+import jax
+import jax.numpy as jnp
+import keras
+import numpy
+import numpy as np
+from jax import random
+from packaging import version
+
+if version.parse(jax.__version__) >= version.parse(
+        '0.2.25'):  # Stax and Optimizers were moved to jax.example_libraries on Oct 20, 2021
+    from jax.example_libraries import stax
+    import jax.example_libraries.optimizers as optim
+    from jax.example_libraries.optimizers import OptimizerState
+else:
+    from jax.experimental import stax
+    import jax.experimental.optimizers as optim
+    from jax.experimental.optimizers import OptimizerState
+
+    warnings.warn(f"Found Jax version {jax.__version__}. Using legacy imports.", FutureWarning)
+
+from phi import math
+from .. import JAX
+from ...math._functional import JitFunction
+
+
+class StaxNet:
+
+    def __init__(self, initialize: Callable, apply: Callable, input_shape: tuple):
+        self._initialize = initialize
+        self._apply = apply
+        self._input_shape = input_shape
+        self.parameters = None
+        self._tracers = None
+
+    def initialize(self):
+        rnd_key = JAX.rnd_key
+        JAX.rnd_key, init_key = random.split(rnd_key)
+        out_shape, params64 = self._initialize(init_key, input_shape=self._input_shape)
+        if math.get_precision() < 64:
+            self.parameters = _recursive_to_float32(params64)
+
+    def __call__(self, *args, **kwargs):
+        if self._tracers is not None:
+            return self._apply(self._tracers, *args)
+        else:
+            return self._apply(self.parameters, *args)
+
+
+class JaxOptimizer:
+
+    def __init__(self, initialize: Callable, update: Callable, get_params: Callable):
+        self._initialize, self._update, self._get_params = initialize, update, get_params  # Stax functions
+        self._state = None
+        self._step_i = 0
+        self._update_function_cache = {}
+
+    def initialize(self, net: tuple):
+        self._state = self._initialize(net)
+
+    def update_step(self, grads: tuple):
+        self._state = self._update(self._step_i, grads, self._state)
+        self._step_i += 1
+
+    def get_network_parameters(self):
+        return self._get_params(self._state)
+
+    def update(self, net: StaxNet, loss_function, wrt, loss_args, loss_kwargs):
+        if loss_function not in self._update_function_cache:
+            @functools.wraps(loss_function)
+            def update(packed_current_state, *loss_args, **loss_kwargs):
+                @functools.wraps(loss_function)
+                def loss_depending_on_net(params_tracer: tuple, *args, **kwargs):
+                    net._tracers = params_tracer
+                    loss_function_non_jit = loss_function.f if isinstance(loss_function, JitFunction) else loss_function
+                    result = loss_function_non_jit(*args, **kwargs)
+                    net._tracers = None
+                    return result
+
+                gradient_function = math.functional_gradient(loss_depending_on_net)
+                current_state = OptimizerState(packed_current_state, self._state.tree_def, self._state.subtree_defs)
+                current_params = self._get_params(current_state)
+                value, grads = gradient_function(current_params, *loss_args, **loss_kwargs)
+                next_state = self._update(self._step_i, grads[0], self._state)
+                return next_state.packed_state, value
+
+            if isinstance(loss_function, JitFunction):
+                update = math.jit_compile(update)
+            self._update_function_cache[loss_function] = update
+
+        next_packed_state, loss_output = self._update_function_cache[loss_function](self._state.packed_state,
+                                                                                    *loss_args, **loss_kwargs)
+        self._state = OptimizerState(next_packed_state, self._state.tree_def, self._state.subtree_defs)
+        return loss_output
+
+
+def parameter_count(model: StaxNet) -> int:
+    """
+    Counts the number of parameters in a model.
+
+    Args:
+        model: Stax model
+
+    Returns:
+        `int`
+    """
+    return int(_recursive_count_parameters(model.parameters))
+
+
+def _recursive_to_float32(obj):
+    if isinstance(obj, (tuple, list)):
+        return type(obj)([_recursive_to_float32(i) for i in obj])
+    elif isinstance(obj, dict):
+        return {k: _recursive_to_float32(v) for k, v in obj.items()}
+    else:
+        assert isinstance(obj, jax.numpy.ndarray)
+        return obj.astype(jax.numpy.float32)
+
+
+def _recursive_count_parameters(obj):
+    if isinstance(obj, (tuple, list)):
+        return sum([_recursive_count_parameters(item) for item in obj])
+    if isinstance(obj, dict):
+        return sum([_recursive_count_parameters(v) for v in obj.values()])
+    return numpy.prod(obj.shape)
+
+
+def get_parameters(model: StaxNet, wrap=True) -> dict:
+    result = {}
+    _recursive_add_parameters(model.parameters, wrap, (), result)
+    return result
+
+
+def _recursive_add_parameters(param, wrap: bool, prefix: tuple, result: dict):
+    if isinstance(param, dict):
+        for name, obj in param.items():
+            _recursive_add_parameters(obj, wrap, prefix + (str(name),), result)
+    elif isinstance(param, (tuple, list)):
+        for i, obj in enumerate(param):
+            _recursive_add_parameters(obj, wrap, prefix + (str(i),), result)
+    else:
+        rank = len(param.shape)
+        if prefix[-1] == 0 and rank == 2:
+            name = '.'.join(str(p) for p in prefix[:-1]) + '.weight'
+        elif prefix[-1] == 1 and rank == 1:
+            name = '.'.join(str(p) for p in prefix[:-1]) + '.bias'
+        else:
+            name = '.'.join(prefix)
+        if not wrap:
+            result[name] = param
+        else:
+            if rank == 1:
+                phi_tensor = math.wrap(param, math.channel('output'))
+            elif rank == 2:
+                phi_tensor = math.wrap(param, math.channel('input,output'))
+            elif rank == 3:
+                phi_tensor = math.wrap(param, math.channel('x,input,output'))
+            elif rank == 4:
+                phi_tensor = math.wrap(param, math.channel('x,y,input,output'))
+            elif rank == 5:
+                phi_tensor = math.wrap(param, math.channel('x,y,z,input,output'))
+            else:
+                raise NotImplementedError(rank)
+            result[name] = phi_tensor
+
+
+def save_state(obj: Union[StaxNet, JaxOptimizer], path: str):
+    """
+    Write the state of a module or optimizer to a file.
+
+    See Also:
+        `load_state()`
+
+    Args:
+        obj: `torch.nn.Module or torch.optim.Optimizer`
+        path: File path as `str`.
+    """
+    if not path.endswith('.npy'):
+        path += '.npy'
+    if isinstance(obj, StaxNet):
+        numpy.save(path, obj.parameters)
+    else:
+        raise NotImplementedError  # ToDo
+        # numpy.save(path, obj._state)
+
+
+def load_state(obj: Union[StaxNet, JaxOptimizer], path: str):
+    """
+    Read the state of a module or optimizer from a file.
+
+    See Also:
+        `save_state()`
+
+    Args:
+        obj: `torch.nn.Module or torch.optim.Optimizer`
+        path: File path as `str`.
+    """
+    if not path.endswith('.npy'):
+        path += '.npy'
+    if isinstance(obj, StaxNet):
+        state = numpy.load(path, allow_pickle=True)
+        obj.parameters = tuple([tuple(layer) for layer in state])
+    else:
+        raise NotImplementedError  # ToDo
+
+
+def update_weights(net: StaxNet, optimizer: JaxOptimizer, loss_function: Callable, *loss_args, **loss_kwargs):
+    """
+    Computes the gradients of `loss_function` w.r.t. the parameters of `net` and updates its weights using `optimizer`.
+
+    This is the Jax version. Analogue functions exist for other learning frameworks.
+
+    Args:
+        net: Learning model.
+        optimizer: Optimizer.
+        loss_function: Loss function, called as `loss_function(*loss_args, **loss_kwargs)`.
+        *loss_args: Arguments given to `loss_function`.
+        **loss_kwargs: Keyword arguments given to `loss_function`.
+
+    Returns:
+        Output of `loss_function`.
+    """
+    loss_output = optimizer.update(net, loss_function, net.parameters, loss_args, loss_kwargs)
+    net.parameters = optimizer.get_network_parameters()
+    return loss_output
+
+
+def adam(net: StaxNet, learning_rate: float = 1e-3, betas=(0.9, 0.999), epsilon=1e-07):
+    """
+    Creates an Adam optimizer for `net`, alias for [`jax.example_libraries.optimizers.adam`](https://jax.readthedocs.io/en/latest/jax.example_libraries.optimizers.html).
+    Analogous functions exist for other learning frameworks.
+    """
+    opt = JaxOptimizer(*optim.adam(learning_rate, betas[0], betas[1], epsilon))
+    opt.initialize(net.parameters)
+    return opt
+
+
+def sgd(net: StaxNet, learning_rate: float = 1e-3, momentum=0, dampening=0, weight_decay=0, nesterov=False):
+    """
+    Creates an SGD optimizer for `net`, alias for [`jax.example_libraries.optimizers.SGD`](https://jax.readthedocs.io/en/latest/jax.example_libraries.optimizers.html).
+    Analogous functions exist for other learning frameworks.
+    """
+    if momentum == 0:
+        opt = JaxOptimizer(*optim.sgd(learning_rate))
+    else:
+        opt = JaxOptimizer(*optim.momentum(learning_rate, momentum))
+    opt.initialize(net.parameters)
+    return opt
+
+
+def adagrad(net: StaxNet, learning_rate: float = 1e-3, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10):
+    """
+    Creates an Adagrad optimizer for `net`, alias for [`jax.example_libraries.optimizers.adagrad`](https://jax.readthedocs.io/en/latest/jax.example_libraries.optimizers.html).
+    Analogue functions exist for other learning frameworks.
+    """
+    opt = JaxOptimizer(*optim.adagrad(learning_rate))
+    opt.initialize(net.parameters)
+    return opt
+
+
+def rmsprop(net: StaxNet, learning_rate: float = 1e-3, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False):
+    """
+    Creates an RMSprop optimizer for `net`, alias for [`jax.example_libraries.optimizers.rmsprop`](https://jax.readthedocs.io/en/latest/jax.example_libraries.optimizers.html).
+    Analogue functions exist for other learning frameworks.
+    """
+    if momentum == 0:
+        opt = JaxOptimizer(*optim.rmsprop(learning_rate, alpha, eps))
+    else:
+        opt = JaxOptimizer(*optim.rmsprop_momentum(learning_rate, alpha, eps, momentum))
+    opt.initialize(net.parameters)
+    return opt
+
+
+def dense_net(in_channels: int,
+              out_channels: int,
+              layers: Sequence[int],
+              batch_norm=False,
+              activation='ReLU',
+              softmax=False) -> StaxNet:
+    """
+    Fully-connected neural networks are available in ΦFlow via dense_net().
+    Arguments:
+        in_channels : size of input layer, int
+        out_channels = size of output layer, int
+        layers : tuple of linear layers between input and output neurons, list or tuple
+        activation : activation function used within the layers, string
+        batch_norm : use of batch norm after each linear layer, bool
+
+    Returns:
+        Dense net model as specified by input arguments
+    """
+    activation = {'ReLU': stax.Relu, 'Sigmoid': stax.Sigmoid, 'tanh': stax.Tanh}[activation]
+    stax_layers = []
+    for neuron_count in layers:
+        stax_layers.append(stax.Dense(neuron_count))
+        stax_layers.append(activation)
+        if batch_norm:
+            stax_layers.append(stax.BatchNorm(axis=(0,)))
+    stax_layers.append(stax.Dense(out_channels))
+    if softmax:
+        stax_layers.append(stax.elementwise(stax.softmax, axis=-1))
+    net_init, net_apply = stax.serial(*stax_layers)
+    net = StaxNet(net_init, net_apply, (-1, in_channels))
+    net.initialize()
+    return net
+
+
+def u_net(in_channels: int,
+          out_channels: int,
+          levels: int = 4,
+          filters: Union[int, tuple, list] = 16,
+          batch_norm: bool = True,
+          activation='ReLU',
+          in_spatial: Union[tuple, int] = 2,
+          periodic=False,
+          use_res_blocks: bool = False) -> StaxNet:
+    """
+     ΦFlow provides a built-in U-net architecture, classically popular for Semantic Segmentation in Computer Vision, composed of downsampling and upsampling layers.
+
+     Arguments:
+
+         in_channels: input channels of the feature map, dtype : int
+         out_channels : output channels of the feature map, dtype : int
+         levels : number of levels of down-sampling and upsampling, dtype : int
+         filters : filter sizes at each down/up sampling convolutional layer, if the input is integer all conv layers have the same filter size,
+         dtype : int or tuple
+         activation : activation function used within the layers, dtype : string
+         batch_norm : use of batchnorm after each conv layer, dtype : bool
+         in_spatial : spatial dimensions of the input feature map, dtype : int
+         use_res_blocks : use convolutional blocks with skip connections instead of regular convolutional blocks, dtype : bool
+
+     Returns:
+
+         U-net model as specified by input arguments
+     """
+    if isinstance(filters, (tuple, list)):
+        assert len(filters) == levels, f"List of filters has length {len(filters)} but u-net has {levels} levels."
+    else:
+        filters = (filters,) * levels
+    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
+    if isinstance(in_spatial, int):
+        d = in_spatial
+        in_spatial = (1,) * d
+    else:
+        assert isinstance(in_spatial, tuple)
+        d = len(in_spatial)
+    # Create layers
+    if use_res_blocks:
+        inc_init, inc_apply = resnet_block(in_channels, filters[0], periodic, batch_norm, activation, d)
+    else:
+        inc_init, inc_apply = create_double_conv(d, filters[0], filters[0], batch_norm, activation, periodic)
+    init_functions, apply_functions = {}, {}
+    for i in range(1, levels):
+        if use_res_blocks:
+            init_functions[f'down{i}'], apply_functions[f'down{i}'] = resnet_block(filters[i - 1], filters[i], periodic, batch_norm, activation, d)
+            init_functions[f'up{i}'], apply_functions[f'up{i}'] = resnet_block(filters[i] + filters[i - 1], filters[i - 1], periodic, batch_norm, activation, d)
+        else:
+            init_functions[f'down{i}'], apply_functions[f'down{i}'] = create_double_conv(d, filters[i], filters[i], batch_norm, activation, periodic)
+            init_functions[f'up{i}'], apply_functions[f'up{i}'] = create_double_conv(d, filters[i - 1], filters[i - 1], batch_norm, activation, periodic)
+    outc_init, outc_apply = CONV[d](out_channels, (1,) * d, padding='same')
+    max_pool_init, max_pool_apply = stax.MaxPool((2,) * d, padding='same', strides=(2,) * d)
+    _, up_apply = create_upsample()
+
+    def net_init(rng, input_shape):
+        params = {}
+        rngs = random.split(rng, 2)
+        shape = input_shape
+        # Layers
+        shape, params['inc'] = inc_init(rngs[0], shape)
+        shapes = [shape]
+        for i in range(1, levels):
+            shape, _ = max_pool_init(None, shape)
+            shape, params[f'down{i}'] = init_functions[f'down{i}'](rngs[i], shape)
+            shapes.insert(0, shape)
+        for i in range(1, levels):
+            shape = shapes[i][:-1] + (shapes[i][-1] + shape[-1],)
+            shape, params[f'up{i}'] = init_functions[f'up{i}'](rngs[levels + i], shape)
+        shape, params['outc'] = outc_init(rngs[-1], shape)
+        return shape, params
+
+    # no @jax.jit needed here since the user can jit this in the loss_function
+    def net_apply(params, inputs, **kwargs):
+        x = inputs
+        x = inc_apply(params['inc'], x, **kwargs)
+        xs = [x]
+        for i in range(1, levels):
+            x = max_pool_apply(None, x, **kwargs)
+            x = apply_functions[f'down{i}'](params[f'down{i}'], x, **kwargs)
+            xs.insert(0, x)
+        for i in range(1, levels):
+            x = up_apply(None, x, **kwargs)
+            x = jnp.concatenate([x, xs[i]], axis=-1)
+            x = apply_functions[f'up{i}'](params[f'up{i}'], x, **kwargs)
+        x = outc_apply(params['outc'], x, **kwargs)
+        return x
+
+    net = StaxNet(net_init, net_apply, (1,) + in_spatial + (in_channels,))
+    net.initialize()
+    return net
+
+
+ACTIVATIONS = {'ReLU': stax.Relu, 'Sigmoid': stax.Sigmoid, 'tanh': stax.Tanh, 'SiLU': stax.Selu}
+CONV = [None,
+        functools.partial(stax.GeneralConv, ('NWC', 'WIO', 'NWC')),
+        functools.partial(stax.GeneralConv, ('NWHC', 'WHIO', 'NWHC')),
+        functools.partial(stax.GeneralConv, ('NWHDC', 'WHDIO', 'NWHDC')), ]
+
+'''
+def create_double_conv(d: int, out_channels: int, mid_channels: int, batch_norm: bool, activation: Callable):
+    
+    return stax.serial(
+        CONV[d](out_channels, (3,) * d, padding='same'),
+        stax.BatchNorm(axis=tuple(range(d + 1))) if batch_norm else stax.Identity,
+        activation,
+        CONV[d](out_channels, (3,) * d, padding='same'),
+        stax.BatchNorm(axis=tuple(range(d + 1))) if batch_norm else stax.Identity,
+        activation)
+'''
+
+
+# Periodic Implementation
+def create_double_conv(d: int, out_channels: int, mid_channels: int, batch_norm: bool, activation: Callable, periodic: bool):
+    init_fn, apply_fn = {}, {}
+    init_fn['conv1'], apply_fn['conv1'] = stax.serial(CONV[d](mid_channels, (3,) * d, padding='valid'), stax.BatchNorm(axis=tuple(range(d + 1))) if batch_norm else stax.Identity, activation)
+    init_fn['conv2'], apply_fn['conv2'] = stax.serial(CONV[d](mid_channels, (3,) * d, padding='valid'), stax.BatchNorm(axis=tuple(range(d + 1))) if batch_norm else stax.Identity, activation)
+
+    def net_init(rng, input_shape):
+        params = {}
+        rngs = random.split(rng, 2)
+        shape, params['conv1'] = init_fn['conv1'](rngs[0], input_shape)
+        shape, params['conv2'] = init_fn['conv2'](rngs[1], shape)
+
+        return shape, params
+
+    def net_apply(params, inputs):
+        x = inputs
+        pad_tuple = [[0, 0]] + [[1, 1] for i in range(d)] + [[0, 0]]
+        out = jnp.pad(x, pad_width=pad_tuple, mode='wrap' if periodic else 'constant')
+        out = apply_fn['conv1'](params['conv1'], out)
+        out = jnp.pad(out, pad_width=pad_tuple, mode='wrap' if periodic else 'constant')
+        out = apply_fn['conv2'](params['conv2'], out)
+        return out
+
+    return net_init, net_apply
+
+
+def create_upsample():
+    # def upsample_init(rng, input_shape):
+    #     return shape, []
+    def upsample_apply(params, inputs, **kwargs):
+        x = math.wrap(inputs, math.batch('batch'), *[math.spatial(f'{i}') for i in range(len(inputs.shape) - 2)],
+                      math.channel('vector'))
+        x = math.upsample2x(x)
+        return x.native(x.shape)
+
+    return NotImplemented, upsample_apply
+
+
+def conv_classifier(in_features: int,
+                    in_spatial: Union[tuple, list],
+                    num_classes: int,
+                    blocks=(64, 128, 256, 256, 512, 512),
+                    dense_layers=(4096, 4096, 100),
+                    batch_norm=True,
+                    activation='ReLU',
+                    softmax=True,
+                    periodic=False):
+    """
+    Based on VGG16.
+    """
+    if isinstance(in_spatial, int):
+        d = in_spatial
+        in_spatial = (1,) * d
+    else:
+        assert isinstance(in_spatial, tuple)
+        d = len(in_spatial)
+    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
+    stax_dense_layers = []
+    init_fn, apply_fn = {}, {}
+
+    net_list = []
+    for i, (prev, next) in enumerate(zip((in_features,) + tuple(blocks[:-1]), blocks)):
+        if i in (0, 1):
+            net_list.append(f'conv{i+1}')
+            init_fn[net_list[-1]], apply_fn[net_list[-1]] = create_double_conv(d, next, next, batch_norm, activation, periodic)
+        else:
+            net_list.append(f'conv{i+1}_1')
+            init_fn[net_list[-1]], apply_fn[net_list[-1]] = create_double_conv(d, 256, 256, batch_norm, activation, periodic)
+            net_list.append(f'conv{i+1}_2')
+            init_fn[net_list[-1]], apply_fn[net_list[-1]] = stax.serial(CONV[d](256, (3,) * d, padding='valid'),
+                                                                        stax.BatchNorm(axis=tuple(range(d + 1))) if batch_norm else stax.Identity,
+                                                                        activation)
+        net_list.append(f'max_pool{i+1}')
+        init_fn[net_list[-1]], apply_fn[net_list[-1]] = stax.MaxPool((2,) * d, padding='valid', strides=(2,) * d)
+    init_fn['flatten'], apply_fn['flatten'] = stax.Flatten
+    for i, neuron_count in enumerate(dense_layers):
+        stax_dense_layers.append(stax.Dense(neuron_count))
+        stax_dense_layers.append(activation)
+        if batch_norm:
+            stax_dense_layers.append(stax.BatchNorm(axis=(0,)))
+    stax_dense_layers.append(stax.Dense(num_classes))
+    if softmax:
+        stax_dense_layers.append(stax.elementwise(stax.softmax, axis=-1))
+    dense_init, dense_apply = stax.serial(*stax_dense_layers)
+
+    def net_init(rng, input_shape):
+        params = {}
+        rngs = random.split(rng, 2)
+        shape = input_shape
+        N = len(net_list)
+        for i in range(N):
+            shape, params[f'{net_list[i]}'] = init_fn[f'{net_list[i]}'](rngs[i], shape)
+        shape, params['flatten'] = init_fn['flatten'](rngs[N], shape)
+        flat_size = int(np.prod(in_spatial) * blocks[-1] / (2**d) ** len(blocks))
+        shape, params['dense'] = dense_init(rngs[N + 1], (1,) + (flat_size,))
+        return shape, params
+
+    def net_apply(params, inputs, **kwargs):
+        x = inputs
+        pad_tuple = [[0, 0]] + [[1, 1]] * d + [[0, 0]]
+        for i in range(len(net_list)):
+            if net_list[i] in ['conv3_2', 'conv4_2', 'conv5_2']:
+                x = jnp.pad(x, pad_width=pad_tuple, mode='wrap' if periodic else 'constant')
+            x = apply_fn[f'{net_list[i]}'](params[f'{net_list[i]}'], x)
+        x = apply_fn['flatten'](params['flatten'], x)
+        out = dense_apply(params['dense'], x, **kwargs)
+        return out
+
+    net = StaxNet(net_init, net_apply, (1,) + in_spatial + (in_features,))
+    net.initialize()
+    return net
+
+
+def conv_net(in_channels: int,
+             out_channels: int,
+             layers: Sequence[int],
+             batch_norm: bool = False,
+             activation: Union[str, Callable] = 'ReLU',
+             periodic=False,
+             in_spatial: Union[int, tuple] = 2) -> StaxNet:
+    """
+    Built in Conv-Nets are also provided. Contrary to the classical convolutional neural networks, the feature map spatial size remains the same throughout the layers. Each layer of the network is essentially a convolutional block comprising of two conv layers. A filter size of 3 is used in the convolutional layers.
+    Arguments:
+
+        in_channels : input channels of the feature map, dtype : int
+        out_channels : output channels of the feature map, dtype : int
+        layers : list or tuple of output channels for each intermediate layer between the input and final output channels, dtype : list or tuple
+        activation : activation function used within the layers, dtype : string
+        batch_norm : use of batchnorm after each conv layer, dtype : bool
+        in_spatial : spatial dimensions of the input feature map, dtype : int
+
+    Returns:
+
+        Conv-net model as specified by input arguments
+    """
+    if isinstance(in_spatial, int):
+        d = in_spatial
+        in_spatial = (1,) * d
+    else:
+        assert isinstance(in_spatial, tuple)
+        d = len(in_spatial)
+    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
+    init_fn, apply_fn = {}, {}
+    if len(layers) < 1:
+        layers.append(out_channels)
+    init_fn['conv_in'], apply_fn['conv_in'] = stax.serial(
+        CONV[d](layers[0], (3,) * d, padding='valid'),
+        stax.BatchNorm(axis=tuple(range(d + 1))) if batch_norm else stax.Identity,
+        activation)
+    for i in range(1, len(layers)):
+        init_fn[f'conv{i}'], apply_fn[f'conv{i}'] = stax.serial(
+            CONV[d](layers[i], (3,) * d, padding='valid'),
+            stax.BatchNorm(axis=tuple(range(d + 1))) if batch_norm else stax.Identity,
+            activation)
+    init_fn['conv_out'], apply_fn['conv_out'] = CONV[d](out_channels, (1,) * d)
+
+    def net_init(rng, input_shape):
+        params = {}
+        rngs = random.split(rng, 2)
+        shape, params['conv_in'] = init_fn['conv_in'](rngs[0], input_shape)
+        for i in range(1, len(layers)):
+            shape, params[f'conv{i + 1}'] = init_fn[f'conv{i + 1}'](rngs[i], shape)
+        shape, params['conv_out'] = init_fn['conv_out'](rngs[len(layers)], shape)
+        return shape, params
+
+    def net_apply(params, inputs):
+        x = inputs
+        pad_tuple = [(0, 0)]
+        for i in range(d):
+            pad_tuple.append((1, 1))
+        pad_tuple.append((0, 0))
+        out = jnp.pad(x, pad_width=pad_tuple, mode='wrap' if periodic else 'constant')
+        out = apply_fn['conv_in'](params['conv_in'], out)
+        for i in range(1, len(layers)):
+            out = jnp.pad(out, pad_width=pad_tuple, mode='wrap' if periodic else 'constant')
+            out = apply_fn[f'conv{i + 1}'](params[f'conv{i + 1}'], out)
+        out = apply_fn['conv_out'](params['conv_out'], out)
+        return out
+
+    net = StaxNet(net_init, net_apply, (1,) + in_spatial + (in_channels,))
+    net.initialize()
+    return net
+
+
+def res_net(in_channels: int,
+            out_channels: int,
+            layers: Sequence[int],
+            batch_norm: bool = False,
+            activation: Union[str, Callable] = 'ReLU',
+            periodic=False,
+            in_spatial: Union[int, tuple] = 2) -> StaxNet:
+    """
+    Built in Res-Nets are provided in the ΦFlow framework. Similar to the conv-net, the feature map spatial size remains the same throughout the layers.
+    These networks use residual blocks composed of two conv layers with a skip connection added from the input to the output feature map.
+    A default filter size of 3 is used in the convolutional layers.
+
+    Arguments:
+
+        in_channels : input channels of the feature map, dtype : int
+        out_channels : output channels of the feature map, dtype : int
+        layers : list or tuple of output channels for each intermediate layer between the input and final output channels, dtype : list or tuple
+        activation : activation function used within the layers, dtype : string
+        batch_norm : use of batchnorm after each conv layer, dtype : bool
+        in_spatial : spatial dimensions of the input feature map, dtype : int
+
+    Returns:
+
+        Res-net model as specified by input arguments
+    """
+    if isinstance(in_spatial, int):
+        d = in_spatial
+        in_spatial = (1,) * d
+    else:
+        assert isinstance(in_spatial, tuple)
+        d = len(in_spatial)
+    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
+    stax_layers = []
+    if len(layers) > 0:
+        stax_layers.append(resnet_block(in_channels, layers[0], periodic, batch_norm, activation, d))
+
+        for i in range(1, len(layers)):
+            stax_layers.append(resnet_block(layers[i - 1], layers[i], periodic, batch_norm, activation, d))
+
+        stax_layers.append(resnet_block(layers[len(layers) - 1], out_channels, periodic, batch_norm, activation, d))
+    else:
+        stax_layers.append(resnet_block(in_channels, out_channels, periodic, batch_norm, activation, d))
+    net_init, net_apply = stax.serial(*stax_layers)
+    net = StaxNet(net_init, net_apply, (1,) + in_spatial + (in_channels,))
+    net.initialize()
+    return net
+
+
+def resnet_block(in_channels: int,
+                 out_channels: int,
+                 periodic: bool,
+                 batch_norm: bool,
+                 activation: Union[str, Callable] = 'ReLU',
+                 d: Union[int, tuple] = 2):
+    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
+    init_fn, apply_fn = {}, {}
+    init_fn['conv1'], apply_fn['conv1'] = stax.serial(
+        CONV[d](out_channels, (3,) * d, padding='valid'),
+        stax.BatchNorm(axis=tuple(range(d + 1))) if batch_norm else stax.Identity,
+        activation)
+    init_fn['conv2'], apply_fn['conv2'] = stax.serial(
+        CONV[d](out_channels, (3,) * d, padding='valid'),
+        stax.BatchNorm(axis=tuple(range(d + 1))) if batch_norm else stax.Identity,
+        activation)
+
+    init_activation, apply_activation = activation
+    if in_channels != out_channels:
+        init_fn['sample_conv'], apply_fn['sample_conv'] = stax.serial(
+            CONV[d](out_channels, (1,) * d, padding='VALID'),
+            stax.BatchNorm(axis=tuple(range(d + 1))) if batch_norm else stax.Identity)
+    else:
+        init_fn['sample_conv'], apply_fn['sample_conv'] = stax.Identity
+
+    def net_init(rng, input_shape):
+        params = {}
+        rngs = random.split(rng, 2)
+
+        # Preparing a list of shapes and dictionary of parameters to return
+        shape, params['conv1'] = init_fn['conv1'](rngs[0], input_shape)
+        shape, params['conv2'] = init_fn['conv2'](rngs[1], shape)
+        shape, params['sample_conv'] = init_fn['sample_conv'](rngs[2], input_shape)
+        shape, params['activation'] = init_activation(rngs[3], shape)
+        return shape, params
+
+    def net_apply(params, inputs, **kwargs):
+        x = inputs
+
+        pad_tuple = [[0, 0]] + [[1, 1] for i in range(d)] + [[0, 0]]
+
+        out = jnp.pad(x, pad_width=pad_tuple, mode='wrap' if periodic else 'constant')
+        out = apply_fn['conv1'](params['conv1'], out)
+        out = jnp.pad(out, pad_width=pad_tuple, mode='wrap' if periodic else 'constant')
+        out = apply_fn['conv2'](params['conv2'], out)
+        skip_x = apply_fn['sample_conv'](params['sample_conv'], x, **kwargs)
+        out = jnp.add(out, skip_x)
+        # out = apply_activation(params['activation'], out)
+        return out
+
+    return net_init, net_apply
+
+
+def get_mask(inputs, reverse_mask, data_format='NHWC'):
+    """ Compute mask for slicing input feature map for Invertible Nets """
+    shape = inputs.shape
+    if len(shape) == 2:
+        N = shape[-1]
+        range_n = jnp.arange(0, N)
+        even_ind = range_n % 2
+        checker = jnp.reshape(even_ind, (-1, N))
+    elif len(shape) == 4:
+        H = shape[2] if data_format == 'NCHW' else shape[1]
+        W = shape[3] if data_format == 'NCHW' else shape[2]
+
+        range_h = jnp.arange(0, H) % 2
+        range_w = jnp.arange(0, W) % 2
+
+        even_ind_h = range_h.astype(bool)
+        even_ind_w = range_w.astype(bool)
+
+        ind_h = jnp.tile(jnp.expand_dims(even_ind_h, -1), [1, W])
+        ind_w = jnp.tile(jnp.expand_dims(even_ind_w, 0), [H, 1])
+        # ind_h = even_ind_h.unsqueeze(-1).repeat(1, W)
+        # ind_w = even_ind_w.unsqueeze( 0).repeat(H, 1)
+
+        checker = jnp.logical_xor(ind_h, ind_w)
+
+        reshape = [-1, 1, H, W] if data_format == 'NCHW' else [-1, H, W, 1]
+        checker = jnp.reshape(checker, reshape)
+        checker = checker.astype(jnp.float32)
+
+    else:
+        raise ValueError('Invalid tensor shape. Dimension of the tensor shape must be '
+                         '2 (NxD) or 4 (NxCxHxW or NxHxWxC), got {}.'.format(inputs.get_shape().as_list()))
+
+    if reverse_mask:
+        checker = 1 - checker
+
+    return checker
+
+
+def Dense_resnet_block(in_channels: int,
+                       mid_channels: int,
+                       batch_norm: bool = False,
+                       activation: Union[str, Callable] = 'ReLU'):
+    inputs = keras.Input(shape=(in_channels,))
+    x_1 = inputs
+
+    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
+    init_fn, apply_fn = {}, {}
+    init_fn['dense1'], apply_fn['dense1'] = stax.serial(stax.Dense(mid_channels),
+                                                        stax.BatchNorm(axis=(0,)),
+                                                        activation)
+    init_fn['dense2'], apply_fn['dense2'] = stax.serial(stax.Dense(in_channels),
+                                                        stax.BatchNorm(axis=(0,)),
+                                                        activation)
+    init_activation, apply_activation = activation
+
+    def net_init(rng, input_shape):
+        params = {}
+        rngs = random.split(rng, 2)
+
+        shape, params['dense1'] = init_fn['dense1'](rngs[0], input_shape)
+        shape, params['dense2'] = init_fn['dense2'](rngs[1], shape)
+        shape, params['activation'] = init_activation(rngs[2], shape)
+        return shape, params
+
+    def net_apply(params, inputs, **kwargs):
+        x = inputs
+
+        out = apply_fn['dense1'](params['dense1'], x)
+        out = apply_fn['dense2'](params['dense2'], out)
+
+        out = jnp.add(out, x)
+
+        return out
+
+    return net_init, net_apply
+
+
+def conv_net_unit(in_channels: int,
+                  out_channels: int,
+                  layers: Sequence[int],
+                  periodic: bool = False,
+                  batch_norm: bool = False,
+                  activation: Union[str, Callable] = 'ReLU',
+                  in_spatial: Union[int, tuple] = 2, **kwargs):
+    """ Conv-net unit for Invertible Nets"""
+    if isinstance(in_spatial, int):
+        d = in_spatial
+    else:
+        assert isinstance(in_spatial, tuple)
+        d = len(in_spatial)
+    if isinstance(activation, str):
+        activation = ACTIVATIONS[activation]
+
+    init_fn, apply_fn = {}, {}
+    if len(layers) < 1:
+        layers.append(out_channels)
+    init_fn['conv_in'], apply_fn['conv_in'] = stax.serial(
+        CONV[d](layers[0], (3,) * d, padding='valid'),
+        stax.BatchNorm(axis=tuple(range(d + 1))) if batch_norm else stax.Identity,
+        activation)
+    for i in range(1, len(layers)):
+        init_fn[f'conv{i}'], apply_fn[f'conv{i}'] = stax.serial(
+            CONV[d](layers[i], (3,) * d, padding='valid'),
+            stax.BatchNorm(axis=tuple(range(d + 1))) if batch_norm else stax.Identity,
+            activation)
+
+    init_fn['conv_out'], apply_fn['conv_out'] = CONV[d](out_channels, (1,) * d)
+
+    def net_init(rng, input_shape):
+        params = {}
+        rngs = random.split(rng, 2)
+
+        shape, params['conv_in'] = init_fn['conv_in'](rngs[0], input_shape)
+
+        for i in range(1, len(layers)):
+            shape, params[f'conv{i + 1}'] = init_fn[f'conv{i + 1}'](rngs[i], shape)
+
+        shape, params['conv_out'] = init_fn['conv_out'](rngs[len(layers)], shape)
+
+        return shape, params
+
+    def net_apply(params, inputs):
+        x = inputs
+
+        pad_tuple = [(0, 0)]
+        for i in range(d):
+            pad_tuple.append((1, 1))
+        pad_tuple.append((0, 0))
+
+        out = jnp.pad(x, pad_width=pad_tuple, mode='wrap' if periodic else 'constant')
+
+        out = apply_fn['conv_in'](params['conv_in'], out)
+
+        for i in range(1, len(layers)):
+            out = jnp.pad(out, pad_width=pad_tuple, mode='wrap' if periodic else 'constant')
+            out = apply_fn[f'conv{i + 1}'](params[f'conv{i + 1}'], out)
+
+        out = apply_fn['conv_out'](params['conv_out'], out)
+
+        return out
+
+    return net_init, net_apply
+
+
+def u_net_unit(in_channels: int,
+               out_channels: int,
+               levels: int = 4,
+               filters: Union[int, tuple, list] = 16,
+               batch_norm: bool = True,
+               activation='ReLU',
+               periodic=False,
+               in_spatial: Union[tuple, int] = 2,
+               use_res_blocks: bool = False, **kwargs):
+    """ U-net unit for Invertible Nets"""
+    if isinstance(filters, (tuple, list)):
+        assert len(filters) == levels, f"List of filters has length {len(filters)} but u-net has {levels} levels."
+    else:
+        filters = (filters,) * levels
+    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
+    if isinstance(in_spatial, int):
+        d = in_spatial
+        in_spatial = (1,) * d
+    else:
+        assert isinstance(in_spatial, tuple)
+        d = len(in_spatial)
+    # Create layers
+    if use_res_blocks:
+        inc_init, inc_apply = resnet_block(in_channels, filters[0], periodic, batch_norm, activation, d)
+    else:
+        inc_init, inc_apply = create_double_conv(d, filters[0], filters[0], batch_norm, activation, periodic)
+    init_functions, apply_functions = {}, {}
+    for i in range(1, levels):
+        if use_res_blocks:
+            init_functions[f'down{i}'], apply_functions[f'down{i}'] = resnet_block(filters[i - 1], filters[i], periodic, batch_norm, activation, d)
+            init_functions[f'up{i}'], apply_functions[f'up{i}'] = resnet_block(filters[i] + filters[i - 1], filters[i - 1], periodic, batch_norm, activation, d)
+        else:
+            init_functions[f'down{i}'], apply_functions[f'down{i}'] = create_double_conv(d, filters[i], filters[i], batch_norm, activation, periodic)
+            init_functions[f'up{i}'], apply_functions[f'up{i}'] = create_double_conv(d, filters[i - 1], filters[i - 1], batch_norm, activation, periodic)
+    outc_init, outc_apply = CONV[d](out_channels, (1,) * d, padding='same')
+    max_pool_init, max_pool_apply = stax.MaxPool((2,) * d, padding='same', strides=(2,) * d)
+    _, up_apply = create_upsample()
+
+    def net_init(rng, input_shape):
+        params = {}
+        rngs = random.split(rng, 2)
+        shape = input_shape
+        # Layers
+        shape, params['inc'] = inc_init(rngs[0], shape)
+        shapes = [shape]
+        for i in range(1, levels):
+            shape, _ = max_pool_init(None, shape)
+            shape, params[f'down{i}'] = init_functions[f'down{i}'](rngs[i], shape)
+            shapes.insert(0, shape)
+        for i in range(1, levels):
+            shape = shapes[i][:-1] + (shapes[i][-1] + shape[-1],)
+            shape, params[f'up{i}'] = init_functions[f'up{i}'](rngs[levels + i], shape)
+        shape, params['outc'] = outc_init(rngs[-1], shape)
+        return shape, params
+
+    # no @jax.jit needed here since the user can jit this in the loss_function
+    def net_apply(params, inputs, **kwargs):
+        x = inputs
+        x = inc_apply(params['inc'], x, **kwargs)
+        xs = [x]
+        for i in range(1, levels):
+            x = max_pool_apply(None, x, **kwargs)
+            x = apply_functions[f'down{i}'](params[f'down{i}'], x, **kwargs)
+            xs.insert(0, x)
+        for i in range(1, levels):
+            x = up_apply(None, x, **kwargs)
+            x = jnp.concatenate([x, xs[i]], axis=-1)
+            x = apply_functions[f'up{i}'](params[f'up{i}'], x, **kwargs)
+        x = outc_apply(params['outc'], x, **kwargs)
+        return x
+
+    return net_init, net_apply
+
+
+def res_net_unit(in_channels: int,
+                 out_channels: int,
+                 layers: Sequence[int],
+                 batch_norm: bool = False,
+                 activation: Union[str, Callable] = 'ReLU',
+                 periodic=False,
+                 in_spatial: Union[int, tuple] = 2, **kwargs):
+    """ Res-net unit for Invertible Nets"""
+    if isinstance(in_spatial, int):
+        d = in_spatial
+    else:
+        assert isinstance(in_spatial, tuple)
+        d = len(in_spatial)
+    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
+    stax_layers = []
+    if len(layers) < 1:
+        layers.append(out_channels)
+    stax_layers.append(resnet_block(in_channels, layers[0], periodic, batch_norm, activation, d))
+    for i in range(1, len(layers)):
+        stax_layers.append(resnet_block(layers[i - 1], layers[i], periodic, batch_norm, activation, d))
+    stax_layers.append(CONV[d](out_channels, (1,) * d))
+    return stax.serial(*stax_layers)
+
+
+NET = {'u_net': u_net_unit, 'res_net': res_net_unit, 'conv_net': conv_net_unit}
+
+
+def coupling_layer(in_channels: int,
+                   activation: Union[str, Callable] = 'ReLU',
+                   batch_norm: bool = False,
+                   in_spatial: Union[int, tuple] = 2,
+                   net: str = 'u_net',
+                   reverse_mask: bool = False,
+                   **kwargs):
+    if isinstance(in_spatial, int):
+        d = in_spatial
+    else:
+        assert isinstance(in_spatial, tuple)
+        d = len(in_spatial)
+    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
+    init_fn, apply_fn = {}, {}
+    if d == 0:
+        init_fn['s1'], apply_fn['s1'] = stax.serial(
+            Dense_resnet_block(in_channels, in_channels, batch_norm, activation),
+            stax.Tanh)
+        init_fn['t1'], apply_fn['t1'] = Dense_resnet_block(in_channels, in_channels, batch_norm, activation)
+
+        init_fn['s2'], apply_fn['s2'] = stax.serial(
+            Dense_resnet_block(in_channels, in_channels, batch_norm, activation),
+            stax.Tanh)
+        init_fn['t2'], apply_fn['t2'] = Dense_resnet_block(in_channels, in_channels, batch_norm, activation)
+    else:
+        init_fn['s1'], apply_fn['s1'] = NET[net](in_channels=in_channels, out_channels=in_channels, layers=[], batch_norm=batch_norm, activation=activation, in_spatial=in_spatial, **kwargs)
+        init_fn['t1'], apply_fn['t1'] = NET[net](in_channels=in_channels, out_channels=in_channels, layers=[], batch_norm=batch_norm, activation=activation, in_spatial=in_spatial, **kwargs)
+        init_fn['s2'], apply_fn['s2'] = NET[net](in_channels=in_channels, out_channels=in_channels, layers=[], batch_norm=batch_norm, activation=activation, in_spatial=in_spatial, **kwargs)
+        init_fn['t2'], apply_fn['t2'] = NET[net](in_channels=in_channels, out_channels=in_channels, layers=[], batch_norm=batch_norm, activation=activation, in_spatial=in_spatial, **kwargs)
+
+    def net_init(rng, input_shape):
+        params = {}
+        rngs = random.split(rng, 2)
+
+        shape, params['s1'] = init_fn['s1'](rngs[0], input_shape)
+        shape, params['t1'] = init_fn['t1'](rngs[1], input_shape)
+        shape, params['s2'] = init_fn['s2'](rngs[2], input_shape)
+        shape, params['t2'] = init_fn['t2'](rngs[3], input_shape)
+
+        return shape, params
+
+    def net_apply(params, inputs, invert=False):
+        x = inputs
+
+        mask = get_mask(x, reverse_mask, 'NCHW')
+
+        if invert:
+            v1 = x * mask
+            v2 = x * (1 - mask)
+
+            s1 = apply_fn['s1'](params['s1'], v1)
+            t1 = apply_fn['t1'](params['t1'], v1)
+
+            u2 = (1 - mask) * (v2 - t1) * jnp.exp(-jnp.tanh(s1))
+
+            s2 = apply_fn['s2'](params['s2'], u2)
+            t2 = apply_fn['t2'](params['t2'], u2)
+
+            u1 = mask * (v1 - t2) * jnp.exp(-jnp.tanh(s2))
+
+            return u1 + u2
+        else:
+            u1 = x * mask
+            u2 = x * (1 - mask)
+
+            s2 = apply_fn['s2'](params['s2'], u2)
+            t2 = apply_fn['t2'](params['t2'], u2)
+
+            v1 = mask * (u1 * jnp.exp(jnp.tanh(s2)) + t2)
+
+            s1 = apply_fn['s1'](params['s1'], v1)
+            t1 = apply_fn['t1'](params['t1'], v1)
+
+            v2 = (1 - mask) * (u2 * jnp.exp(jnp.tanh(s1)) + t1)
+
+            return v1 + v2
+
+    return net_init, net_apply
+
+
+def invertible_net(in_channels: int,
+                   num_blocks: int,
+                   batch_norm: bool = False,
+                   net: str = 'u_net',
+                   activation: Union[str, type] = 'ReLU',
+                   in_spatial: Union[tuple, int] = 2, **kwargs):
+    """
+    ΦFlow also provides invertible neural networks that are capable of inverting the output tensor back to the input tensor initially passed.\ These networks have far reaching applications in predicting input parameters of a problem given its observations.\ Invertible nets are composed of multiple concatenated coupling blocks wherein each such block consists of arbitrary neural networks.
+
+    Currently, these arbitrary neural networks could be set to u_net(default), conv_net, res_net or dense_net blocks with in_channels = out_channels.
+    The architecture used is popularized by ["Real NVP"](https://arxiv.org/abs/1605.08803).
+
+    Arguments:
+
+        in_channels : input channels of the feature map, dtype : int
+        num_blocks : number of coupling blocks inside the invertible net, dtype : int
+        activation : activation function used within the layers, dtype : string
+        batch_norm : use of batchnorm after each layer, dtype : bool
+        in_spatial : spatial dimensions of the input feature map, dtype : int
+        net : type of neural network blocks used in coupling layers, dtype : str
+        **kwargs : placeholder for arguments not supported by the function
+
+    Returns:
+
+        Invertible Net model as specified by input arguments
+
+    Note: Currently supported values for net are 'u_net'(default), 'conv_net' and 'res_net'.
+    For choosing 'dense_net' as the network block in coupling layers in_spatial must be set to zero.
+    """
+    if isinstance(in_spatial, int):
+        d = in_spatial
+    else:
+        assert isinstance(in_spatial, tuple)
+        d = len(in_spatial)
+    init_fn, apply_fn = {}, {}
+    for i in range(num_blocks):
+        init_fn[f'CouplingLayer{i + 1}'], apply_fn[f'CouplingLayer{i + 1}'] = coupling_layer(in_channels, activation, batch_norm, d, net, (i % 2 == 0), **kwargs)
+
+    def net_init(rng, input_shape):
+        params = {}
+        rngs = random.split(rng, 2)
+        for i in range(num_blocks):
+            shape, params[f'CouplingLayer{i + 1}'] = init_fn[f'CouplingLayer{i + 1}'](rngs[i], input_shape)
+        return shape, params
+
+    def net_apply(params, inputs, invert=False):
+        out = inputs
+        if invert:
+            for i in range(num_blocks, 0, -1):
+                out = apply_fn[f'CouplingLayer{i}'](params[f'CouplingLayer{i}'], out, invert)
+        else:
+            for i in range(1, num_blocks + 1):
+                out = apply_fn[f'CouplingLayer{i}'](params[f'CouplingLayer{i}'], out)
+        return out
+
+    if d == 0:
+        net = StaxNet(net_init, net_apply, (1,) + (in_channels,))
+    else:
+        net = StaxNet(net_init, net_apply, (1,) + (1,) * d + (in_channels,))
+    net.initialize()
+    return net
```

### Comparing `phiflow-2.3.4/phi/math/__init__.py` & `phiflow-2.4.0/phi/math/__init__.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,116 +1,135 @@
-"""
-Vectorized operations, tensors with named dimensions.
-
-This package provides a common interface for tensor operations.
-Is internally uses NumPy, TensorFlow or PyTorch.
-
-Main classes: `Tensor`, `Shape`, `DType`, `Extrapolation`.
-
-The provided operations are not implemented directly.
-Instead, they delegate the actual computation to either NumPy, TensorFlow or PyTorch, depending on the configuration.
-This allows the user to write simulation code once and have it run with various computation backends.
-
-See the documentation at https://tum-pbs.github.io/PhiFlow/Math.html
-"""
-
-from .backend._dtype import DType
-from .backend import NUMPY, precision, set_global_precision, get_precision
-
-from ._shape import (
-    shape, Shape, EMPTY_SHAPE, DimFilter,
-    spatial, channel, batch, instance, dual,
-    non_batch, non_spatial, non_instance, non_channel, non_dual,
-    merge_shapes, concat_shapes, IncompatibleShapes,
-    enable_debug_checks,
-)
-
-from ._magic_ops import slice_ as slice, unstack, stack, concat, expand, rename_dims, pack_dims, unpack_dim, flatten, copy_with, replace
-
-from ._tensors import wrap, tensor, layout, Tensor, Dict, to_dict, from_dict, is_scalar
-
-from ._sparse import dense, get_sparsity, factor_ilu
-
-from .extrapolation import Extrapolation
-
-from ._ops import (
-    choose_backend_t as choose_backend, all_available, convert, seed,
-    native, numpy, reshaped_native, reshaped_tensor, reshaped_numpy, copy, native_call,
-    print_ as print,
-    map_ as map,
-    zeros, ones, fftfreq, random_normal, random_uniform, meshgrid, linspace, arange as range, range_tensor,  # creation operators (use default backend)
-    zeros_like, ones_like,
-    pad,
-    transpose,  # reshape operations
-    divide_no_nan,
-    where, nonzero,
-    sum_ as sum, finite_sum, mean, finite_mean, std, prod, max_ as max, finite_max, min_ as min, finite_min, any_ as any, all_ as all, quantile, median,  # reduce
-    dot,
-    abs_ as abs, sign,
-    round_ as round, ceil, floor,
-    maximum, minimum, clip,
-    sqrt, exp, log, log2, log10, sigmoid,
-    sin, cos, tan, sinh, cosh, tanh, arcsin, arccos, arctan, arcsinh, arccosh, arctanh,
-    to_float, to_int32, to_int64, to_complex, imag, real, conjugate,
-    degrees,
-    boolean_mask,
-    is_finite, is_finite as isfinite,
-    closest_grid_values, grid_sample, scatter, gather,
-    fft, ifft, convolve, cumulative_sum,
-    dtype, cast,
-    close, assert_close,
-    stop_gradient,
-    pairwise_distances,
-)
-
-from ._trace import matrix_from_function
-
-from ._functional import (
-    LinearFunction, jit_compile_linear, jit_compile,
-    jacobian, jacobian as gradient, functional_gradient, custom_gradient, print_gradient,
-    map_types, map_s2b, map_i2b,
-    iterate,
-    identity,
-    trace_check,
-)
-
-from ._optimize import solve_linear, solve_nonlinear, minimize, Solve, SolveInfo, ConvergenceException, NotConverged, Diverged, SolveTape
-
-from ._nd import (
-    shift,
-    vec, const_vec, vec_abs, vec_abs as vec_length, vec_squared, vec_normalize, cross_product, rotate_vector, dim_mask,
-    normalize_to,
-    l1_loss, l2_loss, frequency_loss,
-    spatial_gradient, laplace,
-    fourier_laplace, fourier_poisson, abs_square,
-    downsample2x, upsample2x, sample_subgrid,
-    masked_fill, finite_fill
-)
-
-PI = 3.14159265358979323846
-"""Value of π to double precision """
-pi = PI  # intentionally undocumented, use PI instead. Exists only as an anlog to numpy.pi
-
-INF = float("inf")
-""" Floating-point representation of positive infinity. """
-inf = INF  # intentionally undocumented, use INF instead. Exists only as an anlog to numpy.inf
-
-
-NAN = float("nan")
-""" Floating-point representation of NaN (not a number). """
-nan = NAN  # intentionally undocumented, use NAN instead. Exists only as an anlog to numpy.nan
-
-NUMPY = NUMPY  # to show up in pdoc
-"""Default backend for NumPy arrays and SciPy objects."""
-
-__all__ = [key for key in globals().keys() if not key.startswith('_')]
-
-__pdoc__ = {
-    'Extrapolation': False,
-    'Shape.__init__': False,
-    'SolveInfo.__init__': False,
-    'TensorDim.__init__': False,
-    'ConvergenceException.__init__': False,
-    'Diverged.__init__': False,
-    'NotConverged.__init__': False,
-    'LinearFunction.__init__': False,
-}
+"""
+Vectorized operations, tensors with named dimensions.
+
+This package provides a common interface for tensor operations.
+Is internally uses NumPy, TensorFlow or PyTorch.
+
+Main classes: `Tensor`, `Shape`, `DType`, `Extrapolation`.
+
+The provided operations are not implemented directly.
+Instead, they delegate the actual computation to either NumPy, TensorFlow or PyTorch, depending on the configuration.
+This allows the user to write simulation code once and have it run with various computation backends.
+
+See the documentation at https://tum-pbs.github.io/PhiFlow/Math.html
+"""
+
+from .backend._dtype import DType
+from .backend import NUMPY, precision, set_global_precision, get_precision
+
+from ._shape import (
+    shape, Shape, EMPTY_SHAPE, DimFilter,
+    spatial, channel, batch, instance, dual,
+    non_batch, non_spatial, non_instance, non_channel, non_dual, non_primal, primal,
+    merge_shapes, concat_shapes, IncompatibleShapes,
+    enable_debug_checks,
+)
+
+from ._magic_ops import (
+    slice_ as slice, unstack,
+    stack, concat, expand,
+    rename_dims, rename_dims as replace_dims, pack_dims, unpack_dim, flatten,
+    b2i, c2b, i2b, s2b, si2d,
+    copy_with, replace
+)
+
+from ._tensors import wrap, tensor, layout, Tensor, Dict, to_dict, from_dict, is_scalar, BROADCAST_FORMATTER as f
+
+from ._sparse import dense, get_sparsity, get_format, sparse_tensor, stored_indices, stored_values, tensor_like
+
+from .extrapolation import Extrapolation
+
+from ._ops import (
+    choose_backend_t as choose_backend, all_available, convert, seed, to_device,
+    native, numpy, reshaped_native, reshaped_tensor, reshaped_numpy, copy, native_call,
+    print_ as print,
+    map_ as map,
+    zeros, ones, fftfreq, random_normal, random_uniform, meshgrid, linspace, arange as range, range_tensor,  # creation operators (use default backend)
+    zeros_like, ones_like,
+    pad,
+    transpose,  # reshape operations
+    safe_div, safe_div as divide_no_nan,
+    where, nonzero,
+    sum_ as sum, finite_sum, mean, finite_mean, std, prod, max_ as max, finite_max, min_ as min, finite_min, any_ as any, all_ as all, quantile, median,  # reduce
+    dot,
+    abs_ as abs, sign,
+    round_ as round, ceil, floor,
+    maximum, minimum, clip,
+    sqrt, exp, log, log2, log10, sigmoid, soft_plus,
+    sin, cos, tan, sinh, cosh, tanh, arcsin, arccos, arctan, arcsinh, arccosh, arctanh, log_gamma, factorial,
+    to_float, to_int32, to_int64, to_complex, imag, real, conjugate,
+    degrees,
+    boolean_mask,
+    is_finite, is_finite as isfinite, is_nan, is_inf,
+    closest_grid_values, grid_sample, scatter, gather,
+    histogram,
+    fft, ifft, convolve, cumulative_sum,
+    dtype, cast,
+    close, assert_close,
+    stop_gradient,
+    pairwise_distances, map_pairs,
+)
+
+from ._nd import (
+    shift,
+    vec, const_vec, vec_abs, vec_abs as vec_length, vec_squared, vec_normalize, cross_product, rotate_vector, dim_mask,
+    normalize_to,
+    l1_loss, l2_loss, frequency_loss,
+    spatial_gradient, laplace,
+    fourier_laplace, fourier_poisson, abs_square,
+    downsample2x, upsample2x, sample_subgrid,
+    masked_fill, finite_fill
+)
+
+from ._trace import matrix_from_function
+
+from ._functional import (
+    LinearFunction, jit_compile_linear, jit_compile,
+    jacobian, jacobian as gradient, functional_gradient, custom_gradient, print_gradient,
+    map_types, map_s2b, map_i2b, map_c2b,
+    broadcast,
+    iterate,
+    identity,
+    trace_check,
+)
+
+from ._optimize import solve_linear, solve_nonlinear, minimize, Solve, SolveInfo, ConvergenceException, NotConverged, Diverged, SolveTape, factor_ilu
+
+PI = 3.14159265358979323846
+"""Value of π to double precision """
+pi = PI  # intentionally undocumented, use PI instead. Exists only as an anlog to numpy.pi
+
+INF = float("inf")
+""" Floating-point representation of positive infinity. """
+inf = INF  # intentionally undocumented, use INF instead. Exists only as an anlog to numpy.inf
+
+
+NAN = float("nan")
+""" Floating-point representation of NaN (not a number). """
+nan = NAN  # intentionally undocumented, use NAN instead. Exists only as an anlog to numpy.nan
+
+NUMPY = NUMPY  # to show up in pdoc
+"""Default backend for NumPy arrays and SciPy objects."""
+
+f = f
+"""
+Automatic mapper for broadcast string formatting of tensors, resulting in tensors of strings.
+Used with the special `-f-` syntax.
+
+Examples:
+    >>> from phi.math import f
+    >>> -f-f'String containing {tensor1} and {tensor2:.1f}'
+    # Result is a str tensor containing all dims of tensor1 and tensor2
+"""
+
+__all__ = [key for key in globals().keys() if not key.startswith('_')]
+
+__pdoc__ = {
+    'Extrapolation': False,
+    'Shape.__init__': False,
+    'SolveInfo.__init__': False,
+    'TensorDim.__init__': False,
+    'ConvergenceException.__init__': False,
+    'Diverged.__init__': False,
+    'NotConverged.__init__': False,
+    'LinearFunction.__init__': False,
+}
```

### Comparing `phiflow-2.3.4/phi/math/_fit.py` & `phiflow-2.4.0/phi/math/_fit.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,73 +1,73 @@
-from phi.math import EMPTY_SHAPE
-from ._magic_ops import concat
-from ._ops import mean, ones, reshaped_tensor, reshaped_native
-from ._shape import DimFilter, instance, shape, channel
-from ._tensors import Tensor
-
-
-def fit_line_2d(x: Tensor, y: Tensor, point_dim: DimFilter = instance, weights: Tensor = 1.):
-    """
-    Fits a line of the form *slope · x + offset* to pass through the data points defined by their coordinates `x` and `y`.
-
-    Args:
-        x: X coordinate of the points.
-        y: Y coordinate of the points.
-        point_dim: Dimension listing the points the line should pass through. This dimension will be reduced in the operation.
-            By default, all instance dimensions
-        weights: (Optional) Tensor assigning a weight to each point in `x` and `y` to determine the relative influence of that point in the overall fit.
-
-    Returns:
-        slope: Line slope in units y/x as `Tensor`
-        offset: Line value for x=0.
-    """
-    assert shape(x).only(point_dim) or shape(y).only(point_dim), f"Either x or y need to have a dimension corresponding to point_dim but got x: {shape(x)}, y: {shape(y)}"
-    if not shape(weights):  # unweighted fit
-        mean_x = mean(x, point_dim)
-        x_rel = x - mean_x
-        var_x = mean(x_rel ** 2, point_dim)
-        slope = mean(x_rel * y, point_dim) / var_x
-        offset = mean(y, point_dim) - slope * mean_x
-    else:  # weighted fit
-        mean_w = mean(weights, point_dim)
-        mean_x = mean(weights * x, point_dim) / mean_w
-        x_rel = x - mean_x
-        var_wx = mean(weights * x_rel ** 2, point_dim)
-        slope = mean(weights * x_rel * y, point_dim) / var_wx
-        offset = mean(weights * y, point_dim) / mean_w - slope * mean_x
-    return slope, offset
-
-
-def fit_hyperplane(x: Tensor, y: Tensor, point_dim: DimFilter = instance, weights: Tensor = 1.):
-    """
-    Fits an n-dimensional plane through the points (*x, y).
-
-    Args:
-        x: `Tensor` containing `point_dim` and a channel dimensions for the vector components.
-        y: `Tensor` containing `point_dim`
-        point_dim: Dimension listing the points the hyperplane should pass through. This dimension will be reduced in the operation.
-            By default, all instance dimensions
-        weights: (Optional) Tensor assigning a weight to each point in `x` and `y` to determine the relative influence of that point in the overall fit.
-
-    Returns:
-        slope: Plane slope in units y/x as `Tensor`
-        offset: Plane value for x=0.
-    """
-    point_dim = shape(x).only(point_dim)
-    assert point_dim.rank == 1
-    vec_dim = channel(x).without(point_dim)
-    assert vec_dim.rank == 1, f"x must have a channel dimension for to encode vectors but has shape {shape(x)}"
-    assert vec_dim not in shape(y)
-    batch_dims = (shape(x).without(vec_dim) & shape(y)).without(point_dim)
-    assert point_dim not in shape(weights), f"Weights may not contain the vector/features dimension {point_dim}."
-    assert vec_dim.rank == 1, f"x must have exactly 1 channel dimension (excluding point_dim) to act as the vector dimension listing the components/features but got shape {shape(x)}"
-    mat = concat([x, ones(shape(x).without(vec_dim), channel(**{vec_dim.name: 'y'}))], vec_dim) * weights
-    y *= weights
-    # Least Squares fit
-    np_mat = reshaped_native(mat, [batch_dims, point_dim, vec_dim.name], force_expand=True)
-    np_rhs = reshaped_native(y, [batch_dims, point_dim, '_batch_per_matrix'], force_expand=True)
-    from phi.math.backend import choose_backend
-    backend = choose_backend(np_mat, np_rhs)
-    solution, *_ = backend.matrix_solve_least_squares(np_mat, np_rhs)
-    slope = reshaped_tensor(solution[..., :-1, :], [batch_dims, vec_dim, EMPTY_SHAPE])
-    offset = reshaped_tensor(solution[..., -1, :], [batch_dims, EMPTY_SHAPE])
-    return slope, offset
+from phi.math import EMPTY_SHAPE
+from ._magic_ops import concat
+from ._ops import mean, ones, reshaped_tensor, reshaped_native
+from ._shape import DimFilter, instance, shape, channel
+from ._tensors import Tensor
+
+
+def fit_line_2d(x: Tensor, y: Tensor, point_dim: DimFilter = instance, weights: Tensor = 1.):
+    """
+    Fits a line of the form *slope · x + offset* to pass through the data points defined by their coordinates `x` and `y`.
+
+    Args:
+        x: X coordinate of the points.
+        y: Y coordinate of the points.
+        point_dim: Dimension listing the points the line should pass through. This dimension will be reduced in the operation.
+            By default, all instance dimensions
+        weights: (Optional) Tensor assigning a weight to each point in `x` and `y` to determine the relative influence of that point in the overall fit.
+
+    Returns:
+        slope: Line slope in units y/x as `Tensor`
+        offset: Line value for x=0.
+    """
+    assert shape(x).only(point_dim) or shape(y).only(point_dim), f"Either x or y need to have a dimension corresponding to point_dim but got x: {shape(x)}, y: {shape(y)}"
+    if not shape(weights):  # unweighted fit
+        mean_x = mean(x, point_dim)
+        x_rel = x - mean_x
+        var_x = mean(x_rel ** 2, point_dim)
+        slope = mean(x_rel * y, point_dim) / var_x
+        offset = mean(y, point_dim) - slope * mean_x
+    else:  # weighted fit
+        mean_w = mean(weights, point_dim)
+        mean_x = mean(weights * x, point_dim) / mean_w
+        x_rel = x - mean_x
+        var_wx = mean(weights * x_rel ** 2, point_dim)
+        slope = mean(weights * x_rel * y, point_dim) / var_wx
+        offset = mean(weights * y, point_dim) / mean_w - slope * mean_x
+    return slope, offset
+
+
+def fit_hyperplane(x: Tensor, y: Tensor, point_dim: DimFilter = instance, weights: Tensor = 1.):
+    """
+    Fits an n-dimensional plane through the points (*x, y).
+
+    Args:
+        x: `Tensor` containing `point_dim` and a channel dimensions for the vector components.
+        y: `Tensor` containing `point_dim`
+        point_dim: Dimension listing the points the hyperplane should pass through. This dimension will be reduced in the operation.
+            By default, all instance dimensions
+        weights: (Optional) Tensor assigning a weight to each point in `x` and `y` to determine the relative influence of that point in the overall fit.
+
+    Returns:
+        slope: Plane slope in units y/x as `Tensor`
+        offset: Plane value for x=0.
+    """
+    point_dim = shape(x).only(point_dim)
+    assert point_dim.rank == 1
+    vec_dim = channel(x).without(point_dim)
+    assert vec_dim.rank == 1, f"x must have a channel dimension for to encode vectors but has shape {shape(x)}"
+    assert vec_dim not in shape(y)
+    batch_dims = (shape(x).without(vec_dim) & shape(y)).without(point_dim)
+    assert point_dim not in shape(weights), f"Weights may not contain the vector/features dimension {point_dim}."
+    assert vec_dim.rank == 1, f"x must have exactly 1 channel dimension (excluding point_dim) to act as the vector dimension listing the components/features but got shape {shape(x)}"
+    mat = concat([x, ones(shape(x).without(vec_dim), channel(**{vec_dim.name: 'y'}))], vec_dim) * weights
+    y *= weights
+    # Least Squares fit
+    np_mat = reshaped_native(mat, [batch_dims, point_dim, vec_dim.name])
+    np_rhs = reshaped_native(y, [batch_dims, point_dim, '_batch_per_matrix'])
+    from phi.math.backend import choose_backend
+    backend = choose_backend(np_mat, np_rhs)
+    solution, *_ = backend.matrix_solve_least_squares(np_mat, np_rhs)
+    slope = reshaped_tensor(solution[..., :-1, :], [batch_dims, vec_dim, EMPTY_SHAPE])
+    offset = reshaped_tensor(solution[..., -1, :], [batch_dims, EMPTY_SHAPE])
+    return slope, offset
```

### Comparing `phiflow-2.3.4/phi/math/_functional.py` & `phiflow-2.4.0/phi/math/_functional.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,1097 +1,1115 @@
-import inspect
-import types
-import warnings
-from functools import wraps, partial
-from typing import Tuple, Callable, Dict, Generic, List, TypeVar, Any, Set, Union
-
-import numpy as np
-
-from ._sparse import SparseCoordinateTensor, CompressedSparseMatrix
-from ._trace import ShiftLinTracer, matrix_from_function, LinearTraceInProgress
-from .backend import Backend, NUMPY
-from .backend._backend import get_spatial_derivative_order, functional_derivative_evaluation, PHI_LOGGER
-from ._shape import EMPTY_SHAPE, Shape, vector_add, merge_shapes, spatial, instance, batch
-from .magic import PhiTreeNode
-from ._magic_ops import stack, unpack_dim
-from ._tensors import Tensor, disassemble_tree, assemble_tree, disassemble_tensors, assemble_tensors, variable_attributes, wrap
-from . import _ops as math
-
-X = TypeVar('X')
-Y = TypeVar('Y')
-
-
-class SignatureKey:
-
-    def __init__(self,
-                 source_function: Union[Callable, None],
-                 tree: Dict[str, Any],
-                 shapes: Union[Shape, Tuple[Shape]],
-                 specs: Union[Tuple[Shape], None],
-                 backend: Backend,
-                 tracing: bool,
-                 condition: Any = None):
-        if source_function is None:  # this is an input signature
-            assert isinstance(shapes, tuple)
-        self.source_function = source_function
-        self.tree: Dict[str, Any] = tree
-        self.shapes = shapes
-        self.backend = backend
-        self.tracing = tracing
-        self.specs = specs
-        self.auxiliary_kwargs = condition
-        self.spatial_derivative_order = get_spatial_derivative_order()
-
-    def __repr__(self):
-        return f"{self.tree} with shapes {self.shapes}"
-
-    def __eq__(self, other: 'SignatureKey'):
-        assert isinstance(other, SignatureKey)
-        cond_equal = self.auxiliary_kwargs == other.auxiliary_kwargs
-        if isinstance(cond_equal, Tensor):
-            cond_equal = cond_equal.all
-        # shapes need not be compared because they are included in specs
-        return self.tree == other.tree and self.specs == other.specs and self.backend == other.backend and self.spatial_derivative_order == other.spatial_derivative_order and cond_equal
-
-    def __hash__(self):
-        return hash(self.shapes) + hash(self.backend)
-
-    def matches_structure_and_names(self, other: 'SignatureKey'):
-        assert isinstance(other, SignatureKey)
-        cond_equal = self.auxiliary_kwargs == other.auxiliary_kwargs
-        if isinstance(cond_equal, Tensor):
-            cond_equal = cond_equal.all
-        return self.tree == other.tree and all(s1.names == s2.names for s1, s2 in zip(self.shapes, other.shapes)) and self.backend == other.backend and cond_equal
-
-    def extrapolate(self, rec_in: 'SignatureKey', new_in: 'SignatureKey') -> 'SignatureKey':
-        assert self.source_function is not None, "extrapolate() must be called on output keys"
-        shapes = [self._extrapolate_shape(s, rec_in, new_in) for s in self.shapes]
-        return SignatureKey(self.source_function, self.tree, shapes, self.specs, self.backend, self.tracing, self.auxiliary_kwargs)
-
-    @staticmethod
-    def _extrapolate_shape(shape_: Shape, rec_in: 'SignatureKey', new_in: 'SignatureKey') -> Shape:
-        sizes = []
-        for dim, size in shape_._named_sizes:
-            for p_in, n_in in zip(rec_in.shapes, new_in.shapes):
-                if dim in p_in and size == p_in.get_size(dim):
-                    sizes.append(n_in.get_size(dim))
-                    break
-            else:
-                raise ValueError(shape_, rec_in, new_in)
-        return shape_.with_sizes(sizes)
-
-
-def match_output_signature(new_in: SignatureKey, recorded_mappings: Dict[SignatureKey, SignatureKey], source) -> SignatureKey:
-    for rec_in, rec_out in recorded_mappings.items():
-        if rec_in == new_in:  # exact match
-            return rec_out
-    for rec_in, rec_out in recorded_mappings.items():
-        if rec_in.matches_structure_and_names(new_in):
-            return rec_out.extrapolate(rec_in, new_in)
-    transforms_str = ''.join([f'\n* {i} -> {o}' for i, o in recorded_mappings.items()])
-    raise RuntimeError(f"{source}: no output shape found for input shapes {new_in}.\n"
-                       f"Maybe the backend extrapolated the concrete function from another trace?\n"
-                       f"Registered transforms:\n{transforms_str}")  # KeyError does not support \n
-
-
-def key_from_args(args: tuple, kwargs: Dict[str, Any], parameters: Tuple[str, ...], cache=False, aux: Set[str] = ()) -> Tuple[SignatureKey, List[Tensor], tuple, Dict[str, Any]]:
-    kwargs = {**kwargs, **{parameters[i]: v for i, v in enumerate(args)}}
-    aux_kwargs = {}
-    if aux:
-        for param in aux:
-            if param in kwargs:
-                aux_kwargs[param] = kwargs[param]
-                del kwargs[param]
-    tree, tensors = disassemble_tree(kwargs)
-    tracing = not math.all_available(*tensors)
-    backend = math.choose_backend_t(*tensors)
-    natives, shapes, specs = disassemble_tensors(tensors, expand=cache)
-    key = SignatureKey(None, tree, shapes, specs, backend, tracing, aux_kwargs)
-    return key, tensors, natives, kwargs
-
-
-# def key_from_args_pack_batch(args, kwargs, parameters: Tuple[str, ...], cache=False) -> Tuple[SignatureKey, List[Tensor], list, Dict[str, Any], Shape]:
-#     kwargs = {**kwargs, **{parameters[i]: v for i, v in enumerate(args)}}
-#     tree, tensors = disassemble_tree(kwargs)
-#     tracing = not math.all_available(*tensors)
-#     backend = math.choose_backend_t(*tensors)
-#     # if tracing and cache:
-#     #     cache = False
-#     #     warnings.warn("Cannot cache a tensor while tracing.", RuntimeWarning)
-#     batch_shape = merge_shapes(*[t.shape.batch for t in tensors])
-#     # tensors = [math.pack_dims(t, batch_shape, batch('batch'), pos=0) for t in tensors]
-#     natives = [math.reshaped_native(t, [batch_shape, *t.shape.non_batch], force_expand=True) for t in tensors]
-#     natives, shapes, specs = disassemble_tensors(tensors, expand=cache)
-#     shapes = tuple([math.concat_shapes(batch(batch=batch_shape.volume), *t.shape.non_batch) for t in tensors])
-#     key = SignatureKey(None, tree, shapes, specs, backend, tracing, {})
-#     return key, tensors, natives, kwargs, batch_shape
-
-
-def function_parameters(f) -> Tuple[str]:
-    return tuple(get_function_parameters(f).keys())
-
-
-def get_function_parameters(f) -> Dict[str, inspect.Parameter]:
-    if isinstance(f, (JitFunction, GradientFunction, HessianFunction, CustomGradientFunction, LinearFunction)):
-        return get_function_parameters(f.f)
-    elif hasattr(f, '__wrapped__') and f.__wrapped__ is not None:
-        inner_params = get_function_parameters(f.__wrapped__)
-        outer_parameters = dict(inspect.signature(f, follow_wrapped=False).parameters)
-        args_param = [name for name, param in outer_parameters.items() if param.kind == inspect.Parameter.VAR_POSITIONAL]
-        assert args_param, f"Wrapping function {f.__name__} must have a varargs parameter"
-        kwargs_param = [name for name, param in outer_parameters.items() if param.kind == inspect.Parameter.VAR_KEYWORD]
-        outer_names: List[str] = list(outer_parameters.keys())
-        if kwargs_param:
-            outer_names.remove(kwargs_param[0])
-        index = outer_names.index(args_param[0])
-        return dict(**{n: outer_parameters[n] for n in outer_names[:index]}, **inner_params, **{n: outer_parameters[n] for n in outer_names[index + 1:]})
-    else:
-        params = dict(inspect.signature(f).parameters)
-        assert 'args' not in params, f"Failed to determine signature of {f}. If it wraps another function, decorate it with @functools.wraps(func_with_signature)"
-        return params
-
-
-def f_name(f):
-    assert callable(f), f
-    if hasattr(f, '__name__'):
-        return f.__name__
-    if isinstance(f, partial):
-        return f"partial({f.func})"
-    else:
-        return "unknown"
-
-
-class JitFunction:
-
-    def __init__(self, f: Callable, auxiliary_args: Set[str], forget_traces: bool):
-        self.f = f
-        self.f_params = function_parameters(f)
-        self.auxiliary_args = auxiliary_args
-        self.forget_traces = forget_traces
-        self.traces: Dict[SignatureKey, Callable] = {}
-        self.recorded_mappings: Dict[SignatureKey, SignatureKey] = {}
-        self.grad_jit = GradientFunction(f.f, self.f_params, f.wrt, f.get_output, f.is_f_scalar, jit=True) if isinstance(f, GradientFunction) else None
-
-    def _jit_compile(self, in_key: SignatureKey):
-        PHI_LOGGER.debug(f"Φ-jit: '{f_name(self.f)}' called with new key. shapes={[s.volume for s in in_key.shapes]}, args={in_key.tree}")
-
-        def jit_f_native(*natives):
-            PHI_LOGGER.debug(f"Φ-jit: Tracing '{f_name(self.f)}'")
-            in_tensors = assemble_tensors(natives, in_key.specs)
-            kwargs = assemble_tree(in_key.tree, in_tensors)
-            result = self.f(**kwargs, **in_key.auxiliary_kwargs)  # Tensor or tuple/list of Tensors
-            tree, out_tensors = disassemble_tree(result)
-            result_natives, result_shapes, specs = disassemble_tensors(out_tensors, expand=True)
-            self.recorded_mappings[in_key] = SignatureKey(jit_f_native, tree, result_shapes, specs, in_key.backend, in_key.tracing)
-            return result_natives
-
-        jit_f_native.__name__ = f"native({f_name(self.f) if isinstance(self.f, types.FunctionType) else str(self.f)})"
-        return in_key.backend.jit_compile(jit_f_native)
-
-    def __call__(self, *args, **kwargs):
-        try:
-            key, _, natives, _ = key_from_args(args, kwargs, self.f_params, cache=True, aux=self.auxiliary_args)
-        except LinearTraceInProgress:
-            return self.f(*args, **kwargs)
-        if isinstance(self.f, GradientFunction) and key.backend.supports(Backend.jit_compile_grad):
-            return self.grad_jit(*args, **kwargs)
-        if not key.backend.supports(Backend.jit_compile):
-            warnings.warn(f"jit_copmile() not supported by {key.backend}. Running function '{f_name(self.f)}' as-is.", RuntimeWarning)
-            return self.f(*args, **kwargs)
-        if key not in self.traces:
-            if self.forget_traces:
-                self.traces.clear()
-                self.recorded_mappings.clear()
-            self.traces[key] = self._jit_compile(key)
-            if len(self.traces) >= 10:
-                warnings.warn(f"""Φ-lin: The jit-compiled function '{f_name(self.f)}' was traced {len(self.traces)} times.
-Performing many traces may be slow and cause memory leaks.
-Re-tracing occurs when the number or types of arguments vary, tensor shapes vary between calls or different auxiliary arguments are given (compared by reference).
-Set forget_traces=True to avoid memory leaks when many traces are required.""", RuntimeWarning)
-        native_result = self.traces[key](*natives)
-        output_key = match_output_signature(key, self.recorded_mappings, self)
-        output_tensors = assemble_tensors(native_result, output_key.specs)
-        return assemble_tree(output_key.tree, output_tensors)
-
-    def __repr__(self):
-        return f"jit({f_name(self.f)})"
-
-    @property
-    def __name__(self):
-        return f_name(self.f)
-
-
-def jit_compile(f: Callable = None, auxiliary_args: str = '', forget_traces: bool = None) -> Callable:
-    """
-    Compiles a graph based on the function `f`.
-    The graph compilation is performed just-in-time (jit), e.g. when the returned function is called for the first time.
-
-    The traced function will compute the same result as `f` but may run much faster.
-    Some checks may be disabled in the compiled function.
-
-    Can be used as a decorator:
-    ```python
-    @math.jit_compile
-    def my_function(x: math.Tensor) -> math.Tensor:
-    ```
-
-    Invoking the returned function may invoke re-tracing / re-compiling `f` after the first call if either
-
-    * it is called with a different number of arguments,
-    * the tensor arguments have different dimension names or types (the dimension order also counts),
-    * any `Tensor` arguments require a different backend than previous invocations,
-    * `phi.math.magic.PhiTreeNode` positional arguments do not match in non-variable properties.
-
-    Compilation is implemented for the following backends:
-
-    * PyTorch: [`torch.jit.trace`](https://pytorch.org/docs/stable/jit.html)
-    * TensorFlow: [`tf.function`](https://www.tensorflow.org/guide/function)
-    * Jax: [`jax.jit`](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#using-jit-to-speed-up-functions)
-
-    Jit-compilations cannot be nested, i.e. you cannot call `jit_compile()` while another function is being compiled.
-    An exception to this is `jit_compile_linear()` which can be called from within a jit-compiled function.
-
-    See Also:
-        `jit_compile_linear()`
-
-    Args:
-        f: Function to be traced.
-            All positional arguments must be of type `Tensor` or `phi.math.magic.PhiTreeNode` returning a single `Tensor` or `phi.math.magic.PhiTreeNode`.
-        auxiliary_args: Comma-separated parameter names of arguments that are not relevant to backpropagation.
-        forget_traces: If `True`, only remembers the most recent compiled instance of this function.
-            Upon tracing with new instance (due to changed shapes or auxiliary args), deletes the previous traces.
-
-    Returns:
-        Function with similar signature and return values as `f`.
-    """
-    if f is None:
-        kwargs = {k: v for k, v in locals().items() if v is not None}
-        return partial(jit_compile, **kwargs)
-    auxiliary_args = set(s.strip() for s in auxiliary_args.split(',') if s.strip())
-    return f if isinstance(f, (JitFunction, LinearFunction)) and f.auxiliary_args == auxiliary_args else JitFunction(f, auxiliary_args, forget_traces or False)
-
-
-class LinearFunction(Generic[X, Y], Callable[[X], Y]):
-    """
-    Just-in-time compiled linear function of `Tensor` arguments and return values.
-
-    Use `jit_compile_linear()` to create a linear function representation.
-    """
-
-    def __init__(self, f, auxiliary_args: Set[str], forget_traces: bool):
-        self.f = f
-        self.f_params = function_parameters(f)
-        self.auxiliary_args = auxiliary_args
-        self.forget_traces = forget_traces
-        self.matrices_and_biases: Dict[SignatureKey, Tuple[SparseCoordinateTensor, Tensor]] = {}
-        self.nl_jit = JitFunction(f, self.auxiliary_args, forget_traces)  # for backends that do not support sparse matrices
-
-    def _trace(self, in_key: SignatureKey, prefer_numpy: bool) -> 'ShiftLinTracer':
-        assert in_key.shapes[0].is_uniform, f"math.jit_compile_linear() only supports uniform tensors for function input and output but input shape was {in_key.shapes[0]}"
-        with NUMPY if prefer_numpy else in_key.backend:
-            x = math.ones(in_key.shapes[0])
-            tracer = ShiftLinTracer(x, {EMPTY_SHAPE: math.ones()}, x.shape, math.zeros(x.shape))
-        x_kwargs = assemble_tree(in_key.tree, [tracer])
-        result = self.f(**x_kwargs, **in_key.auxiliary_kwargs)
-        _, result_tensors = disassemble_tree(result)
-        assert len(result_tensors) == 1, f"Linear function must return a single Tensor or tensor-like but got {result}"
-        result_tensor = result_tensors[0]
-        assert isinstance(result_tensor, ShiftLinTracer), f"Tracing linear function '{f_name(self.f)}' failed. Make sure only linear operations are used."
-        return result_tensor
-
-    def _get_or_trace(self, key: SignatureKey, args: tuple, f_kwargs: dict):
-        if not key.tracing and key in self.matrices_and_biases:
-            return self.matrices_and_biases[key]
-        else:
-            if self.forget_traces:
-                self.matrices_and_biases.clear()
-            matrix, bias = matrix_from_function(self.f, *args, **f_kwargs, auto_compress=True)
-            if not key.tracing:
-                self.matrices_and_biases[key] = matrix, bias
-                if len(self.matrices_and_biases) >= 4:
-                    warnings.warn(f"""Φ-lin: The compiled linear function '{f_name(self.f)}' was traced {len(self.matrices_and_biases)} times.
-Performing many traces may be slow and cause memory leaks.
-Tensors in auxiliary arguments (all except the first parameter unless specified otherwise) are compared by reference, not by tensor values.
-Auxiliary arguments: {key.auxiliary_kwargs}
-Multiple linear traces can be avoided by jit-compiling the code that calls the linear function or setting forget_traces=True.""", RuntimeWarning, stacklevel=3)
-            return matrix, bias
-
-    def __call__(self, *args: X, **kwargs) -> Y:
-        try:
-            key, tensors, natives, x = key_from_args(args, kwargs, self.f_params, cache=False, aux=self.auxiliary_args)
-        except LinearTraceInProgress:
-            return self.f(*args, **kwargs)
-        assert tensors, "Linear function requires at least one argument"
-        if any(isinstance(t, ShiftLinTracer) for t in tensors):
-            # TODO: if t is identity, use cached ShiftLinTracer, otherwise multiply two ShiftLinTracers
-            return self.f(*args, **kwargs)
-        if not key.backend.supports(Backend.sparse_coo_tensor):  # This might be called inside a Jax linear solve
-            # warnings.warn(f"Sparse matrices are not supported by {backend}. Falling back to regular jit compilation.", RuntimeWarning)
-            if not math.all_available(*tensors):  # avoid nested tracing, Typical case jax.scipy.sparse.cg(LinearFunction). Nested traces cannot be reused which results in lots of traces per cg.
-                PHI_LOGGER.debug(f"Φ-lin: Running '{f_name(self.f)}' as-is with {key.backend} because it is being traced.")
-                return self.f(*args, **kwargs)
-            else:
-                return self.nl_jit(*args, **kwargs)
-        matrix, bias = self._get_or_trace(key, args, kwargs)
-        return matrix @ tensors[0] + bias
-
-    def sparse_matrix(self, *args, **kwargs):
-        """
-        Create an explicit representation of this linear function as a sparse matrix.
-
-        See Also:
-            `sparse_matrix_and_bias()`.
-
-        Args:
-            *args: Function arguments. This determines the size of the matrix.
-            **kwargs: Additional keyword arguments for the linear function.
-
-        Returns:
-            Sparse matrix representation with `values` property and `native()` method.
-        """
-        key, *_ = key_from_args(args, kwargs, self.f_params, cache=False, aux=self.auxiliary_args)
-        matrix, bias = self._get_or_trace(key, args, kwargs)
-        assert math.close(bias, 0), "This is an affine function and cannot be represented by a single matrix. Use sparse_matrix_and_bias() instead."
-        return matrix
-
-    def sparse_matrix_and_bias(self, *args, **kwargs):
-        """
-        Create an explicit representation of this affine function as a sparse matrix and a bias vector.
-
-        Args:
-            *args: Positional arguments to the linear function.
-                This determines the size of the matrix.
-            **kwargs: Additional keyword arguments for the linear function.
-
-        Returns:
-            matrix: Sparse matrix representation with `values` property and `native()` method.
-            bias: `Tensor`
-        """
-        key, *_ = key_from_args(args, kwargs, self.f_params, cache=False, aux=self.auxiliary_args)
-        return self._get_or_trace(key, args, kwargs)
-
-    def __repr__(self):
-        return f"lin({f_name(self.f)})"
-
-
-def jit_compile_linear(f: Callable[[X], Y] = None, auxiliary_args: str = None, forget_traces: bool = None) -> 'LinearFunction[X, Y]':
-    """
-    Compile an optimized representation of the linear function `f`.
-    For backends that support sparse tensors, a sparse matrix will be constructed for `f`.
-
-    Can be used as a decorator:
-    ```python
-    @math.jit_compile_linear
-    def my_linear_function(x: math.Tensor) -> math.Tensor:
-    ```
-
-    Unlike `jit_compile()`, `jit_compile_linear()` can be called during a regular jit compilation.
-
-    See Also:
-        `jit_compile()`
-
-    Args:
-        f: Function that is linear in its positional arguments.
-            All positional arguments must be of type `Tensor` and `f` must return a `Tensor`.
-        auxiliary_args: Which parameters `f` is not linear in. These arguments are treated as conditioning arguments and will cause re-tracing on change.
-        forget_traces: If `True`, only remembers the most recent compiled instance of this function.
-            Upon tracing with new instance (due to changed shapes or auxiliary args), deletes the previous traces.
-
-    Returns:
-        `LinearFunction` with similar signature and return values as `f`.
-    """
-    if f is None:
-        kwargs = {k: v for k, v in locals().items() if v is not None}
-        return partial(jit_compile_linear, **kwargs)
-    if isinstance(f, JitFunction):
-        f = f.f  # cannot trace linear function from jitted version
-    if isinstance(auxiliary_args, str):
-        auxiliary_args = set(s.strip() for s in auxiliary_args.split(',') if s.strip())
-    else:
-        assert auxiliary_args is None
-        f_params = function_parameters(f)
-        auxiliary_args = f_params[1:]
-    return f if isinstance(f, LinearFunction) and f.auxiliary_args == auxiliary_args else LinearFunction(f, auxiliary_args, forget_traces or False)
-
-
-def simplify_wrt(f, wrt: Union[str, int, tuple, list]):
-    f_params = function_parameters(f)
-    if wrt is None:  # Old default
-        wrt = f_params[0],
-    elif isinstance(wrt, (tuple, list)) and all(isinstance(i, str) for i in wrt):
-        wrt = tuple(wrt)
-    elif isinstance(wrt, str) and ',' in wrt:
-        wrt = tuple(i.strip() for i in wrt.split(',') if i.strip())
-    elif isinstance(wrt, str):
-        wrt = wrt
-    else:  # int or tuple or list
-        if isinstance(wrt, int):
-            wrt = f_params[wrt]
-        elif isinstance(wrt, (tuple, list)) and all(isinstance(i, int) for i in wrt):
-            wrt = tuple(f_params[i] for i in wrt)
-        else:
-            raise ValueError(f"Invalid value given as wrt: {wrt}. Please pass a comma-separated string of parameter names.")
-        warnings.warn("Specifying wrt by position is deprecated in phi.math.funcitonal_gradient() and phi.math.jacobian(). Please pass a list or comma-separated string of parameter names.",
-                      SyntaxWarning, stacklevel=4)
-    return f_params, wrt
-
-
-class GradientFunction:
-    """ Jacobian or Gradient of a function. """
-
-    def __init__(self, f: Callable, f_params, wrt: Union[str, Tuple[str, ...]], get_output: bool, is_f_scalar: bool, jit=False):
-        self.f = f
-        self.f_params = f_params
-        self.wrt = wrt
-        self._wrt_tuple = wrt if isinstance(wrt, tuple) else (wrt,)
-        self.get_output = get_output
-        self.is_f_scalar = is_f_scalar
-        self.traces: Dict[SignatureKey, Callable] = {}
-        self.recorded_mappings: Dict[SignatureKey, SignatureKey] = {}
-        self.jit = jit
-
-    def _trace_grad(self, in_key: SignatureKey, wrt_natives):
-        def f_native(*natives):
-            PHI_LOGGER.debug(f"Φ-grad: Evaluating gradient of {f_name(self.f)}")
-            in_tensors = assemble_tensors(natives, in_key.specs)
-            kwargs = assemble_tree(in_key.tree, in_tensors)
-            with functional_derivative_evaluation(order=1):
-                result = self.f(**kwargs)  # Tensor or tuple/list of Tensors
-            loss = result[0] if isinstance(result, (tuple, list)) else result
-            if isinstance(loss, Tensor):
-                loss_reduced = math.sum_(loss, batch)
-                loss_native = loss_reduced.native(loss_reduced.shape.names)
-            else:
-                loss_native = loss
-                loss_shape = in_key.backend.staticshape(loss_native)
-                assert len(
-                    loss_shape) == 0, f"Only scalar losses are allowed when returning a native tensor but {f_name(self.f)} returned {type(loss_native).__name__} of shape {loss_shape}. For higher-dimensional values, use Φ-Tensors instead."
-            nest, out_tensors = disassemble_tree(result)
-            result_natives, result_shapes, specs = disassemble_tensors(out_tensors, expand=True)
-            self.recorded_mappings[in_key] = SignatureKey(f_native, nest, result_shapes, specs, in_key.backend, in_key.tracing)
-            return loss_native, result_natives
-
-        if self.jit:
-            return in_key.backend.jit_compile_grad(f_native, wrt=wrt_natives, get_output=self.get_output, is_f_scalar=self.is_f_scalar)
-        else:
-            return in_key.backend.jacobian(f_native, wrt=wrt_natives, get_output=self.get_output, is_f_scalar=self.is_f_scalar)
-
-    def __call__(self, *args, **kwargs):
-        key, tensors, natives, kwargs = key_from_args(args, kwargs, self.f_params, cache=True)
-        if not key.backend.supports(Backend.jacobian):
-            if math.default_backend().supports(Backend.jacobian):
-                warnings.warn(f"Using {math.default_backend()} for gradient computation because {key.backend} does not support jacobian()", RuntimeWarning)
-                key.backend = math.default_backend()
-            else:
-                raise AssertionError(f"jacobian() not supported by {key.backend}.")
-        wrt_tensors = self._track_wrt(kwargs)
-        wrt_natives = self._track_wrt_natives(wrt_tensors, disassemble_tree(kwargs)[1])
-        if key not in self.traces:
-            self.traces[key] = self._trace_grad(key, wrt_natives)
-        native_result = self.traces[key](*natives)
-        output_key = match_output_signature(key, self.recorded_mappings, self)
-        jac_shape = output_key.shapes[0].non_batch  # ToDo prepend this to all wrt shapes
-        wrt_specs = [key.specs[i] for i in wrt_tensors]
-        if self.get_output:
-            output_tensors = assemble_tensors(native_result, list(output_key.specs) + wrt_specs)
-            output_structure, grad_tuple = assemble_tree((output_key.tree, [key.tree[i] for i in self._wrt_tuple]), output_tensors)
-            return output_structure, grad_tuple if isinstance(self.wrt, tuple) else grad_tuple[0]
-        else:
-            output_tensors = assemble_tensors(native_result, wrt_specs)
-            grad_tuple = assemble_tree([key.tree[i] for i in self._wrt_tuple], output_tensors)
-            return grad_tuple if isinstance(self.wrt, tuple) else grad_tuple[0]
-
-    def __repr__(self):
-        return f"grad({f_name(self.f)})"
-
-    @property
-    def __name__(self):
-        return f_name(self.f)
-
-    def _track_wrt(self, kwargs: dict):
-        wrt_tensors = []
-        for name, arg in kwargs.items():
-            _, tensors = disassemble_tree(arg)
-            wrt_tensors.extend([name] * len(tensors))
-        return [t_i for t_i, name in enumerate(wrt_tensors) if name in self._wrt_tuple]
-
-    @staticmethod
-    def _track_wrt_natives(wrt_tensors, values):
-        wrt_natives = []
-        for i, value in enumerate(values):
-            wrt_natives.extend([i] * len(value._natives()))
-        return [n_i for n_i, t_i in enumerate(wrt_natives) if t_i in wrt_tensors]
-
-
-def jacobian(f: Callable, wrt: str = None, get_output=True) -> Callable:
-    """
-    Creates a function which computes the Jacobian matrix of `f`.
-    For scalar functions, consider using `functional_gradient()` instead.
-
-    Example:
-    ```python
-    def f(x, y):
-        prediction = f(x)
-        loss = math.l2_loss(prediction - y)
-        return loss, prediction
-
-    dx = jacobian(loss_function, wrt='x', get_output=False)(x, y)
-
-    (loss, prediction), (dx, dy) = jacobian(loss_function,
-                                        wrt='x,y', get_output=True)(x, y)
-    ```
-
-    Functional gradients are implemented for the following backends:
-
-    * PyTorch: [`torch.autograd.grad`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad) / [`torch.autograd.backward`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward)
-    * TensorFlow: [`tf.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape)
-    * Jax: [`jax.grad`](https://jax.readthedocs.io/en/latest/jax.html#jax.grad)
-
-    When the gradient function is invoked, `f` is called with tensors that track the gradient.
-    For PyTorch, `arg.requires_grad = True` for all positional arguments of `f`.
-
-    Args:
-        f: Function to be differentiated.
-            `f` must return a floating point `Tensor` with rank zero.
-            It can return additional tensors which are treated as auxiliary data and will be returned by the gradient function if `return_values=True`.
-            All arguments for which the gradient is computed must be of dtype float or complex.
-        get_output: Whether the gradient function should also return the return values of `f`.
-        wrt: Comma-separated parameter names of `f` with respect to which the gradient should be computed.
-            If not specified, the gradient will be computed w.r.t. the first positional argument (highly discouraged).
-
-    Returns:
-        Function with the same arguments as `f` that returns the value of `f`, auxiliary data and Jacobian of `f` if `get_output=True`, else just the Jacobian of `f`.
-    """
-    f_params, wrt = simplify_wrt(f, wrt)
-    return GradientFunction(f, f_params, wrt, get_output, is_f_scalar=False)
-
-
-def functional_gradient(f: Callable, wrt: str = None, get_output=True) -> Callable:
-    """
-    Creates a function which computes the gradient of `f`.
-
-    Example:
-    ```python
-    def loss_function(x, y):
-        prediction = f(x)
-        loss = math.l2_loss(prediction - y)
-        return loss, prediction
-
-    dx = functional_gradient(loss_function, 'x', get_output=False)(x, y)
-
-    (loss, prediction), (dx, dy) = functional_gradient(loss_function,
-                                            'x,y', get_output=True)(x, y)
-    ```
-
-    Functional gradients are implemented for the following backends:
-
-    * PyTorch: [`torch.autograd.grad`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad) / [`torch.autograd.backward`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward)
-    * TensorFlow: [`tf.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape)
-    * Jax: [`jax.grad`](https://jax.readthedocs.io/en/latest/jax.html#jax.grad)
-
-    When the gradient function is invoked, `f` is called with tensors that track the gradient.
-    For PyTorch, `arg.requires_grad = True` for all positional arguments of `f`.
-
-    Args:
-        f: Function to be differentiated.
-            `f` must return a floating point `Tensor` with rank zero.
-            It can return additional tensors which are treated as auxiliary data and will be returned by the gradient function if `return_values=True`.
-            All arguments for which the gradient is computed must be of dtype float or complex.
-        get_output: Whether the gradient function should also return the return values of `f`.
-        wrt: Comma-separated parameter names of `f` with respect to which the gradient should be computed.
-            If not specified, the gradient will be computed w.r.t. the first positional argument (highly discouraged).
-
-    Returns:
-        Function with the same arguments as `f` that returns the value of `f`, auxiliary data and gradient of `f` if `get_output=True`, else just the gradient of `f`.
-    """
-    f_params, wrt = simplify_wrt(f, wrt)
-    return GradientFunction(f, f_params, wrt, get_output, is_f_scalar=True)
-
-
-class HessianFunction:
-
-    def __init__(self, f: Callable, f_params, wrt: tuple, get_output: bool, get_gradient: bool, dim_suffixes: tuple, jit=False):
-        assert isinstance(dim_suffixes, tuple) and len(dim_suffixes) == 2
-        self.f = f
-        self.f_params = f_params
-        self.wrt = wrt
-        self._wrt_tuple = wrt if isinstance(wrt, tuple) else (wrt,)
-        self.get_output = get_output
-        self.get_gradient = get_gradient
-        self.dim_suffixes = dim_suffixes
-        self.traces: Dict[SignatureKey, Callable] = {}
-        self.recorded_mappings: Dict[SignatureKey, SignatureKey] = {}
-        self.jit = jit
-#
-#     def _trace_hessian(self, in_key: SignatureKey, wrt_natives):
-#         def f_native(*natives):
-#             PHI_LOGGER.debug(f"Φ-grad: Evaluating gradient of {f_name(self.f)}")
-#             in_tensors = assemble_tensors(natives, in_key.specs)
-#             kwargs = assemble_tree(in_key.tree, in_tensors)
-#             with functional_derivative_evaluation(order=2):
-#                 result = self.f(**kwargs)
-#             nest, out_tensors = disassemble_tree(result)
-#             result_natives, result_shapes, specs = disassemble_tensors(out_tensors, expand=True)
-#             self.recorded_mappings[in_key] = SignatureKey(f_native, nest, result_shapes, specs, in_key.backend, in_key.tracing)
-#             return result_natives
-#
-#         hessian_generator = in_key.backend.jit_compile_hessian if self.jit else in_key.backend.hessian
-#         return hessian_generator(f_native, wrt=wrt_natives, get_output=self.get_output, get_gradient=self.get_gradient)
-#
-#     def __call__(self, *args, **kwargs):
-#         key, tensors, natives, kwargs, batch_shape = key_from_args_pack_batch(args, kwargs, self.f_params, cache=True)
-#         if not key.backend.supports(Backend.jacobian):
-#             if math.default_backend().supports(Backend.jacobian):
-#                 warnings.warn(f"Using {math.default_backend()} for gradient computation because {key.backend} does not support jacobian()", RuntimeWarning)
-#                 key.backend = math.default_backend()
-#             else:
-#                 raise AssertionError(f"jacobian() not supported by {key.backend}.")
-#         wrt_tensors: List[int] = self._track_wrt(kwargs)
-#         wrt_natives: List[int] = self._track_wrt_natives(wrt_tensors, disassemble_tree(kwargs)[1])
-#         if key not in self.traces:
-#             self.traces[key] = self._trace_hessian(key, wrt_natives)
-#         native_result = self.traces[key](*natives)
-#         assert len(native_result) == 1 + int(self.get_output) + int(self.get_gradient)
-#         output_key = match_output_signature(key, self.recorded_mappings, self)
-#         result = ()
-#         if self.get_output:
-#             output_tensors = assemble_tensors(native_result[0], output_key.specs)
-#             output_tensors = [unpack_dim(t, 'batch', batch_shape) for t in output_tensors]
-#             # output_tensors = [math.reshaped_tensor(n, [batch_shape, *shape.non_batch]) for n, shape in zip(native_result[0], output_key.shapes)]
-#             result += assemble_tree(output_key.tree, output_tensors),
-#         if self.get_gradient:
-#             grad_tensors = assemble_tensors(native_result[int(self.get_output)], [key.specs[i] for i in wrt_tensors])
-#             grad_tensors = [unpack_dim(t, 'batch', batch_shape) for t in grad_tensors]
-#             grads = assemble_tree([key.tree[i] for i in self._wrt_tuple], grad_tensors)
-#             if not isinstance(self.wrt, tuple):
-#                 grads = grads[0]
-#             result += grads,
-#         if len(wrt_natives) == 1:
-#             native_hessian = native_result[-1][0][0]
-#             hessian_tensor = math.reshaped_tensor(native_hessian, [batch_shape, *self.shape_with_suffixes(key.shapes[0].non_batch, self.dim_suffixes[0]),
-#                                                                    *self.shape_with_suffixes(key.shapes[0].non_batch, self.dim_suffixes[1])], check_sizes=True)
-#             hessian_tree = assemble_tree(key.tree[self.wrt[0] if isinstance(self.wrt, tuple) else self.wrt], [hessian_tensor])
-#             result += [hessian_tree] if isinstance(self.wrt, tuple) else hessian_tree,
-#         else:
-#             assert all([t is None for t in key.tree]), "When computing the Hessian w.r.t. multiple tensors, all inputs must be Tensors."
-#             raise NotImplementedError()
-#             hessian_tree = [[] for _ in self.wrt]
-#             for i in range(len(self.wrt)):
-#                 for j in range(len(self.wrt)):
-#                     native_hessian_ij = native_result[-1][i][j]
-#                     hessian_tensor_ij = math.reshaped_tensor(native_hessian_ij, [batch_shape, *key.shapes[i].non_batch, *self.dupli_shape(key.shapes[j].non_batch)], check_sizes=True)
-#                     hessian_tree[i].append(hessian_tensor_ij)
-#             result += tuple([tuple(col) for col in hessian_tree]),
-#         return result
-#
-#     def shape_with_suffixes(self, shape: Shape, suffix: str):
-#         return shape._with_names([n + suffix for n in shape.names])
-#
-#     def __repr__(self):
-#         return f"grad({f_name(self.f)})"
-#
-#     @property
-#     def __name__(self):
-#         return f_name(self.f)
-#
-#     def _track_wrt(self, kwargs: dict):
-#         wrt_tensors = []
-#         for name, arg in kwargs.items():
-#             _, tensors = disassemble_tree(arg)
-#             wrt_tensors.extend([name] * len(tensors))
-#         return [t_i for t_i, name in enumerate(wrt_tensors) if name in self._wrt_tuple]
-#
-#     @staticmethod
-#     def _track_wrt_natives(wrt_tensors, values):
-#         wrt_natives = []
-#         for i, value in enumerate(values):
-#             wrt_natives.extend([i] * len(value._natives()))
-#         return [n_i for n_i, t_i in enumerate(wrt_natives) if t_i in wrt_tensors]
-#
-#
-# def hessian(f: Callable, wrt: str, get_output=True, get_gradient=True, dim_suffixes=('', '_')) -> Callable:
-#     """
-#     *Experimental. This function currently only supports PyTorch and the Hessian can only be computed w.r.t. one argument.*
-#
-#     Creates a function which computes the Hessian (second derivative) of `f`.
-#
-#     Example:
-#     ```python
-#     def loss_function(x, y):
-#         prediction = f(x)
-#         loss = math.l2_loss(prediction - y)
-#         return loss, prediction
-#
-#     hess, = hessian(loss_function, 'x', get_output=False, get_gradient=False)(x, y)
-#
-#     (loss, prediction), (dx, dy), ((dx_dx, dx_dy), (dy_dx, dy_dy)) = hessian(loss_function,
-#                                         wrt='x,y', get_output=True)(x, y)
-#     ```
-#
-#     When the gradient function is invoked, `f` is called with tensors that track the gradient.
-#     For PyTorch, `arg.requires_grad = True` for all positional arguments of `f`.
-#
-#     Args:
-#         f: Function to be differentiated.
-#             `f` must return a floating point `Tensor` with rank zero.
-#             It can return additional tensors which are treated as auxiliary data and will be returned by the gradient function if `return_values=True`.
-#             All arguments for which the gradient is computed must be of dtype float or complex.
-#         wrt: Comma-separated parameter names of `f` with respect to which the gradient should be computed.
-#             If not specified, the gradient will be computed w.r.t. the first positional argument (highly discouraged).
-#         get_output: Whether the Hessian function should also return the return values of `f`.
-#         get_gradient: Whether the Hessian function should also return the gradient of `f`.
-#         dim_suffixes: `tuple` containing two strings.
-#             All Non-batch dimensions of the parameters occur twice in the corresponding Hessian.
-#             To avoid duplicate names, suffixes are added to non-batch dimensions.
-#             The dimensions from the first derivative computation are appended with `dim_suffixes[0]` and the second ones with `dim_suffixes[1]`.
-#             This argument has no effect on the dimension names of the gradient if `get_gradient=True`.
-#
-#     Returns:
-#         Function with the same arguments as `f` that returns `(f(x), g(x), H(x))` or less depending on `get_output` and `get_gradient`.
-#     """
-#     f_params, wrt = simplify_wrt(f, wrt)
-#     return HessianFunction(f, f_params, wrt, get_output, get_gradient, dim_suffixes)
-
-
-class CustomGradientFunction:
-
-    def __init__(self, f: Callable, gradient: Callable, auxiliary_args: Set[str]):
-        self.f = f
-        self.f_params = function_parameters(f)
-        self.gradient = gradient
-        self.auxiliary_args = auxiliary_args
-        self.traces: Dict[SignatureKey, Callable] = {}
-        self.recorded_mappings: Dict[SignatureKey, SignatureKey] = {}
-
-    def _trace(self, in_key: SignatureKey):
-        def forward_native(*natives):
-            in_tensors = assemble_tensors(natives, in_key.specs)
-            kwargs = assemble_tree(in_key.tree, in_tensors)
-            PHI_LOGGER.debug(f"Running forward pass of custom op {forward_native.__name__} given args {tuple(kwargs.keys())} containing {len(natives)} native tensors")
-            result = self.f(**kwargs, **in_key.auxiliary_kwargs)  # Tensor or tuple/list of Tensors
-            nest, out_tensors = disassemble_tree(result)
-            result_natives, result_shapes, specs = disassemble_tensors(out_tensors, expand=True)
-            self.recorded_mappings[in_key] = SignatureKey(forward_native, nest, result_shapes, specs, in_key.backend, in_key.tracing)
-            return result_natives
-
-        def backward_native(x_natives, y_natives, dy_natives):
-            PHI_LOGGER.debug(f"Running backward pass of custom op {backward_native.__name__}")
-            out_key = self.recorded_mappings[in_key]
-            # del self.recorded_mappings[in_key]  # this may be required multiple times
-            x_tensors = assemble_tensors(x_natives, in_key.specs)
-            y_tensors = assemble_tensors(y_natives, out_key.specs)
-            dy_tensors = assemble_tensors(dy_natives, out_key.specs)
-            kwargs = assemble_tree(in_key.tree, x_tensors)
-            if in_key.auxiliary_kwargs:
-                kwargs = {**kwargs, **in_key.auxiliary_kwargs}
-            y = assemble_tree(out_key.tree, y_tensors)
-            dy = assemble_tree(out_key.tree, dy_tensors)
-            result = self.gradient(kwargs, y, dy)
-            assert isinstance(result, dict) and all(key in kwargs for key in result.keys()), f"gradient function must return a dict containing only parameter names of the forward function. Forward '{f_name(self.f)}' has arguments {kwargs}."
-            full_result = tuple(result.get(name, None) for name in in_key.tree.keys())
-            result_natives = self.incomplete_tree_to_natives(full_result, tuple(in_key.tree.values()), list(in_key.shapes))
-            PHI_LOGGER.debug(f"Backward pass of custom op {backward_native.__name__} returned gradients for {tuple(result.keys())} out of {tuple(in_key.tree.keys())} containing {len(result_natives)} native tensors")
-            return result_natives
-
-        forward_native.__name__ = f"forward '{f_name(self.f) if isinstance(self.f, types.FunctionType) else str(self.f)}'"
-        backward_native.__name__ = f"{self.gradient.__name__ if isinstance(self.gradient, types.FunctionType) else str(self.gradient)} (of '{f_name(self.f) if isinstance(self.f, types.FunctionType) else str(self.f)}')"
-
-        return in_key.backend.custom_gradient(forward_native, backward_native, get_external_cache=lambda: self.recorded_mappings[in_key], on_call_skipped=partial(self.recorded_mappings.__setitem__, in_key))
-
-    def __call__(self, *args, **kwargs):
-        key, _, natives, _ = key_from_args(args, kwargs, self.f_params, cache=False, aux=self.auxiliary_args)
-        if not key.backend.supports(Backend.jacobian) and not key.backend.supports(Backend.jacobian):
-            return self.f(*args, **kwargs)  # no need to use custom gradient if gradients aren't supported anyway
-        elif not key.backend.supports(Backend.custom_gradient):
-            warnings.warn(f"custom_gradient() not supported by {key.backend}. Running function '{f_name(self.f)}' as-is.", RuntimeWarning)
-            return self.f(*args, **kwargs)
-        if key not in self.traces:
-            self.traces[key] = self._trace(key)
-            if len(self.traces) >= 8:
-                warnings.warn(f"""{self.__name__} has been traced {len(self.traces)} times.
-To avoid memory leaks, call {f_name(self.f)}.traces.clear(), {f_name(self.f)}.recorded_mappings.clear().
-Traces can be avoided by jit-compiling the code that calls custom gradient functions.
-""", RuntimeWarning, stacklevel=2)
-        native_result = self.traces[key](*natives)  # With PyTorch + jit, this does not call forward_native every time
-        output_key = match_output_signature(key, self.recorded_mappings, self)
-        output_tensors = assemble_tensors(native_result, output_key.specs)
-        return assemble_tree(output_key.tree, output_tensors)
-
-    def __repr__(self):
-        return f"custom_gradient(forward={f_name(self.f)}, backward={self.gradient.__name__}, id={id(self)})"
-
-    @property
-    def __name__(self):
-        return f"custom_grad({f_name(self.f)})"
-
-    @staticmethod
-    def incomplete_tree_to_natives(incomplete, tree, complete_shapes: List[Shape]) -> list:
-        """ None in nest means there is a tensor. """
-        if tree is None:
-            c_shape = complete_shapes.pop(0)
-            if incomplete is None:
-                return [None] * c_shape.shape.without('dims').volume
-            else:
-                assert isinstance(incomplete, Tensor)
-                return list(incomplete._natives())
-        elif isinstance(tree, (tuple, list)):
-            if incomplete is None:
-                return sum([CustomGradientFunction.incomplete_tree_to_natives(None, item, complete_shapes) for item in tree], [])
-            else:
-                assert type(tree) == type(incomplete) and len(tree) == len(incomplete)
-                return sum([CustomGradientFunction.incomplete_tree_to_natives(i_item, c_item, complete_shapes) for i_item, c_item in zip(incomplete, tree)], [])
-        elif isinstance(tree, dict):
-            if incomplete is None:
-                return sum([CustomGradientFunction.incomplete_tree_to_natives(None, item, complete_shapes) for item in tree.values()], [])
-            else:
-                assert type(tree) == type(incomplete) and len(tree) == len(incomplete) and set(tree.keys()) == set(incomplete.keys())
-                return sum([CustomGradientFunction.incomplete_tree_to_natives(incomplete[key], c_item, complete_shapes) for key, c_item in tree.items()], [])
-        elif isinstance(tree, PhiTreeNode):
-            attributes = variable_attributes(tree)
-            natives = []
-            for attr in attributes:
-                n_val = getattr(tree, attr)
-                i_val = getattr(incomplete, attr) if incomplete is not None else None
-                natives_item = CustomGradientFunction.incomplete_tree_to_natives(i_val, n_val, complete_shapes)
-                natives.extend(natives_item)
-            return natives
-        else:
-            assert incomplete is None
-            return []
-
-
-def custom_gradient(f: Callable, gradient: Callable, auxiliary_args: str = ''):
-    """
-    Creates a function based on `f` that uses a custom gradient for the backpropagation pass.
-
-    *Warning* This method can lead to memory leaks if the gradient function is not called.
-    Make sure to pass tensors without gradients if the gradient is not required, see `stop_gradient()`.
-
-    Args:
-        f: Forward function mapping `Tensor` arguments `x` to a single `Tensor` output or sequence of tensors `y`.
-        gradient: Function to compute the vector-Jacobian product for backpropagation.
-            Will be called as `gradient(input_dict, *y, *dy) -> output_dict` where `input_dict` contains all named arguments passed to the forward function
-            and `output_dict` contains only those parameters for which a gradient is defined.
-        auxiliary_args: Comma-separated parameter names of arguments that are not relevant to backpropagation.
-
-    Returns:
-        Function with similar signature and return values as `f`. However, the returned function does not support keyword arguments.
-    """
-    auxiliary_args = set(s.strip() for s in auxiliary_args.split(',') if s.strip())
-    return CustomGradientFunction(f, gradient, auxiliary_args)
-
-
-def print_gradient(value: Tensor, name="", detailed=False) -> Tensor:
-    """
-    Prints the gradient vector of `value` when computed.
-    The gradient at `value` is the vector-Jacobian product of all operations between the output of this function and the loss value.
-
-    The gradient is not printed in jit mode, see `jit_compile()`.
-
-    Example:
-        ```python
-        def f(x):
-            x = math.print_gradient(x, 'dx')
-            return math.l1_loss(x)
-
-        math.jacobian(f)(math.ones(x=6))
-        ```
-
-    Args:
-        value: `Tensor` for which the gradient may be computed later.
-        name: (Optional) Name to print along with the gradient values
-        detailed: If `False`, prints a short summary of the gradient tensor.
-
-    Returns:
-        `identity(value)` which when differentiated, prints the gradient vector.
-    """
-
-    def print_grad(params: dict, _y, dx):
-        param_name, x = next(iter(params.items()))
-        if math.all_available(x, dx):
-            if detailed:
-                math.print_(dx, name=name)
-            else:
-                print(f"{name}:  \t{dx}")
-        else:
-            print(f"Cannot print gradient for {param_name}, data not available.")
-        return {param_name: dx}
-
-    identity = custom_gradient(lambda x: x, print_grad)
-    return identity(value)
-
-
-def trace_check(f, *args, **kwargs):
-    """
-    Tests if `f(*args, **kwargs)` has already been traced.
-    If true, jit-compiled functions are very fast since the Python function is not actually called anymore.
-
-    Args:
-        f: Transformed Function, e.g. jit-compiled or linear function.
-        *args: Hypothetical arguments to be passed to `f`
-        **kwargs: Hypothetical keyword arugments to be passed to `f`
-
-    Returns:
-        result: `True` if there is an existing trace that can be used, `False` if `f` would have to be re-traced.
-        reason: Message giving hints as to why `f` needs to be re-traced given `args` and `kwargs`.
-    """
-    if isinstance(f, (JitFunction, GradientFunction, HessianFunction, CustomGradientFunction)):
-        keys = f.traces.keys()
-    elif isinstance(f, LinearFunction):
-        keys = f.matrices_and_biases.keys()
-    else:
-        raise ValueError(f"{f_name(f)} is not a traceable function. Only supports jit_compile, jit_compile_linear, functional_gradient, custom_gradient, jacobian, hessian")
-    key, *_ = key_from_args(args, kwargs, f.f_params, aux=f.auxiliary_args)
-    if not keys:
-        return False, "Function has not yet been traced"
-    if key in keys:
-        return True, ""
-    traced_key = next(iter(keys))  # ToDo compare against all
-    cond_equal = key.auxiliary_kwargs == traced_key.auxiliary_kwargs
-    if isinstance(cond_equal, Tensor):
-        cond_equal = cond_equal.all
-    if not cond_equal:
-        return False, "Auxiliary arguments do not match"
-    # shapes need not be compared because they are included in specs
-    if traced_key.tree.keys() != key.tree.keys():
-        return False, f"Different primary arguments passed: {set(traced_key.tree.keys())} vs {set(key.tree.keys())}"
-    for name in traced_key.tree.keys():
-        if traced_key.tree[name] != key.tree[name]:
-            return False, f"Primary argument '{name}' differs in non-traced variables: {traced_key.tree[name]} vs {key.tree[name]}. Make sure the corresponding class overrides __eq__()."
-    if traced_key.specs != key.specs:
-        return False, "Traced variables differ in shape"
-    if traced_key.backend != key.backend:
-        return False, f"Function was not traced with backend {key.backend}"
-    if traced_key.spatial_derivative_order != key.spatial_derivative_order:
-        return False, f"Different in spatial_derivative_order. This is likely an internal problem."
-    return True
-
-
-def map_types(f: Callable, dims: Union[Shape, tuple, list, str, Callable], dim_type: Union[Callable, str]) -> Callable:
-    """
-    Wraps a function to change the dimension types of its `Tensor` and `phi.math.magic.PhiTreeNode` arguments.
-
-    Args:
-        f: Function to wrap.
-        dims: Concrete dimensions or dimension type, such as `spatial` or `batch`.
-            These dimensions will be mapped to `dim_type` for all positional function arguments.
-        dim_type: Dimension type, such as `spatial` or `batch`.
-            `f` will be called with dimensions remapped to this type.
-
-    Returns:
-        Function with signature matching `f`.
-    """
-
-    def forward_retype(obj, input_types: Shape):
-        tree, tensors = disassemble_tree(obj)
-        retyped = []
-        for t in tensors:
-            for dim in t.shape.only(dims):
-                t = t.dimension(dim).as_type(dim_type)
-                input_types = math.merge_shapes(input_types, dim.with_size(None))
-            retyped.append(t)
-        return assemble_tree(tree, retyped), input_types
-
-    def reverse_retype(obj, input_types: Shape):
-        tree, tensors = disassemble_tree(obj)
-        retyped = []
-        for t in tensors:
-            for dim in t.shape.only(input_types.names):
-                t = t.dimension(dim).as_type(input_types.get_type(dim))
-            retyped.append(t)
-        return assemble_tree(tree, retyped)
-
-    @wraps(f)
-    def retyped_f(*args, **kwargs):
-        input_types = EMPTY_SHAPE
-        retyped_args = []
-        for arg in args:
-            retyped_arg, input_types = forward_retype(arg, input_types)
-            retyped_args.append(retyped_arg)
-        output = f(*retyped_args, **kwargs)
-        restored_output = reverse_retype(output, input_types)
-        return restored_output
-
-    return retyped_f
-
-
-def map_s2b(f: Callable) -> Callable:
-    """ Map spatial dimensions to batch dimensions. Short for `map_types(f, spatial, batch)`. """
-    return map_types(f, spatial, batch)
-
-
-def map_i2b(f: Callable) -> Callable:
-    """ Map instance dimensions to batch dimensions. Short for `map_types(f, instance, batch)`. """
-    return map_types(f, instance, batch)
-
-
-def iterate(f: Callable,
-            iterations: Union[int, Shape],
-            *x0,
-            f_kwargs: dict = None,
-            range: Callable = range,
-            measure: Callable = None,
-            **f_kwargs_):
-    """
-    Repeatedly call `function`, passing the previous output as the next input.
-
-    Args:
-        f: Function to call. Must be callable as `f(x0, **f_kwargs)` and `f(f(x0, **f_kwargs), **f_kwargs)`.
-        iterations: Number of iterations as `int` or single-dimension `Shape`.
-            If `int`, returns the final output of `f`.
-            If `Shape`, returns the trajectory (`x0` and all outputs of `f`), stacking the values along this dimension.
-        x0: Initial positional arguments for `f`.
-        range: Range function. Can be used to generate tqdm output by passing `trange`.
-        measure: Function without arguments to call at the start and end (and in between if `isinstance(iterations, Shape)`) calls to `f`.
-            The measure of each call to `f` is `measure()` after minus `measure()` before the call.
-        f_kwargs: Additional keyword arguments to be passed to `f`.
-            These arguments can be of any type.
-        f_kwargs_: More keyword arguments.
-
-    Returns:
-        trajectory: Trajectory of final output of `f`, depending on `iterations`.
-        measured: Only if `measure` was specified, returns the measured value or trajectory tensor.
-    """
-    if f_kwargs is None:
-        f_kwargs = {}
-    f_kwargs.update(f_kwargs_)
-    x = x0
-    if isinstance(iterations, int):
-        start_time = measure() if measure else None
-        for _ in range(iterations):
-            x = f(*x, **f_kwargs)
-            if not isinstance(x, tuple):
-                x = (x,)
-            assert len(x) == len(x0), f"Function to iterate must return {len(x0)} outputs to match input but got {x}"
-        result = x[0] if len(x0) == 1 else x
-        return (result, measure() - start_time) if measure else result
-    elif isinstance(iterations, Shape):
-        xs = [x0]
-        ts = [measure()] if measure else None
-        for _ in range(iterations.size):
-            x = f(*x, **f_kwargs)
-            if not isinstance(x, tuple):
-                x = (x,)
-            assert len(x) == len(x0), f"Function to iterate must return {len(x0)} outputs to match input but got {x}"
-            xs.append(x)
-            if measure:
-                ts.append(measure())
-        xs = [stack(item, iterations.with_size(None)) for item in zip(*xs)]
-        result = xs[0] if len(x0) == 1 else xs
-        ts = np.asarray(ts)
-        return (result, wrap(ts[1:] - ts[:-1], iterations.with_size(None))) if measure else result
-    else:
-        raise ValueError(f"iterations must be an int or Shape but got {type(iterations)}")
-
-
-def identity(x):
-    """
-    Identity function for one argument.
-    Vararg functions cannot be transformed as the argument names are unknown.
-
-    Args:
-        x: Positional argument.
-
-    Returns:
-        `x`
-    """
-    return x
+import inspect
+import types
+import warnings
+from functools import wraps, partial
+from typing import Tuple, Callable, Dict, Generic, List, TypeVar, Any, Set, Union
+
+import numpy as np
+
+from . import _ops as math
+from ._magic_ops import stack
+from ._shape import EMPTY_SHAPE, Shape, spatial, instance, batch, channel
+from ._sparse import SparseCoordinateTensor
+from ._tensors import Tensor, disassemble_tree, assemble_tree, disassemble_tensors, assemble_tensors, variable_attributes, wrap, specs_equal
+from ._trace import ShiftLinTracer, matrix_from_function, LinearTraceInProgress
+from .backend import Backend, NUMPY
+from .backend._backend import get_spatial_derivative_order, functional_derivative_evaluation, PHI_LOGGER
+from .magic import PhiTreeNode
+
+X = TypeVar('X')
+Y = TypeVar('Y')
+
+
+class SignatureKey:
+
+    def __init__(self,
+                 source_function: Union[Callable, None],
+                 tree: Dict[str, Any],
+                 shapes: Union[Shape, Tuple[Shape]],
+                 specs: Union[Tuple[Shape], None],
+                 backend: Backend,
+                 tracing: bool,
+                 condition: Any = None):
+        if source_function is None:  # this is an input signature
+            assert isinstance(shapes, tuple)
+        self.source_function = source_function
+        self.tree: Dict[str, Any] = tree
+        self.shapes = shapes
+        self.backend = backend
+        self.tracing = tracing
+        self.specs = specs
+        self.auxiliary_kwargs = condition
+        self.spatial_derivative_order = get_spatial_derivative_order()
+
+    def __repr__(self):
+        return f"{self.tree} with shapes {self.shapes}"
+
+    def __eq__(self, other: 'SignatureKey'):
+        assert isinstance(other, SignatureKey)
+        cond_equal = self.auxiliary_kwargs == other.auxiliary_kwargs
+        if isinstance(cond_equal, Tensor):
+            cond_equal = cond_equal.all
+        # shapes need not be compared because they are included in specs
+        specs = specs_equal(self.specs, other.specs)
+        return self.tree == other.tree and specs and self.backend == other.backend and self.spatial_derivative_order == other.spatial_derivative_order and cond_equal
+
+    def __hash__(self):
+        return hash(self.shapes) + hash(self.backend)
+
+    def matches_structure_and_names(self, other: 'SignatureKey'):
+        assert isinstance(other, SignatureKey)
+        cond_equal = self.auxiliary_kwargs == other.auxiliary_kwargs
+        if isinstance(cond_equal, Tensor):
+            cond_equal = cond_equal.all
+        return self.tree == other.tree and all(s1.names == s2.names for s1, s2 in zip(self.shapes, other.shapes)) and self.backend == other.backend and cond_equal
+
+    def extrapolate(self, rec_in: 'SignatureKey', new_in: 'SignatureKey') -> 'SignatureKey':
+        assert self.source_function is not None, "extrapolate() must be called on output keys"
+        shapes = [self._extrapolate_shape(s, rec_in, new_in) for s in self.shapes]
+        return SignatureKey(self.source_function, self.tree, shapes, self.specs, self.backend, self.tracing, self.auxiliary_kwargs)
+
+    @staticmethod
+    def _extrapolate_shape(shape_: Shape, rec_in: 'SignatureKey', new_in: 'SignatureKey') -> Shape:
+        sizes = []
+        for dim, size in shape_._named_sizes:
+            for p_in, n_in in zip(rec_in.shapes, new_in.shapes):
+                if dim in p_in and size == p_in.get_size(dim):
+                    sizes.append(n_in.get_size(dim))
+                    break
+            else:
+                raise ValueError(shape_, rec_in, new_in)
+        return shape_.with_sizes(sizes)
+
+
+def match_output_signature(new_in: SignatureKey, recorded_mappings: Dict[SignatureKey, SignatureKey], source) -> SignatureKey:
+    for rec_in, rec_out in recorded_mappings.items():
+        if rec_in == new_in:  # exact match
+            return rec_out
+    for rec_in, rec_out in recorded_mappings.items():
+        if rec_in.matches_structure_and_names(new_in):
+            return rec_out.extrapolate(rec_in, new_in)
+    transforms_str = ''.join([f'\n* {i} -> {o}' for i, o in recorded_mappings.items()])
+    raise RuntimeError(f"{source}: no output shape found for input shapes {new_in}.\n"
+                       f"Maybe the backend extrapolated the concrete function from another trace?\n"
+                       f"Registered transforms:\n{transforms_str}")  # KeyError does not support \n
+
+
+def key_from_args(args: tuple, kwargs: Dict[str, Any], parameters: Tuple[str, ...], cache=False, aux: Set[str] = ()) -> Tuple[SignatureKey, List[Tensor], tuple, Dict[str, Any]]:
+    kwargs = {**kwargs, **{parameters[i]: v for i, v in enumerate(args)}}
+    aux_kwargs = {}
+    if aux:
+        for param in aux:
+            if param in kwargs:
+                aux_kwargs[param] = kwargs[param]
+                del kwargs[param]
+    tree, tensors = disassemble_tree(kwargs)
+    tracing = not math.all_available(*tensors)
+    backend = math.choose_backend_t(*tensors)
+    natives, shapes, specs = disassemble_tensors(tensors, expand=cache)
+    key = SignatureKey(None, tree, shapes, specs, backend, tracing, aux_kwargs)
+    return key, tensors, natives, kwargs
+
+
+def function_parameters(f) -> Tuple[str]:
+    return tuple(get_function_parameters(f).keys())
+
+
+def get_function_parameters(f) -> Dict[str, inspect.Parameter]:
+    if isinstance(f, (JitFunction, GradientFunction, HessianFunction, CustomGradientFunction, LinearFunction)):
+        return get_function_parameters(f.f)
+    elif hasattr(f, '__wrapped__') and f.__wrapped__ is not None:
+        inner_params = get_function_parameters(f.__wrapped__)
+        outer_parameters = dict(inspect.signature(f, follow_wrapped=False).parameters)
+        args_param = [name for name, param in outer_parameters.items() if param.kind == inspect.Parameter.VAR_POSITIONAL]
+        assert args_param, f"Wrapping function {f.__name__} must have a varargs parameter"
+        kwargs_param = [name for name, param in outer_parameters.items() if param.kind == inspect.Parameter.VAR_KEYWORD]
+        outer_names: List[str] = list(outer_parameters.keys())
+        if kwargs_param:
+            outer_names.remove(kwargs_param[0])
+        index = outer_names.index(args_param[0])
+        return dict(**{n: outer_parameters[n] for n in outer_names[:index]}, **inner_params, **{n: outer_parameters[n] for n in outer_names[index + 1:]})
+    else:
+        params = dict(inspect.signature(f).parameters)
+        assert 'args' not in params, f"Failed to determine signature of {f}. If it wraps another function, decorate it with @functools.wraps(func_with_signature)"
+        return params
+
+
+def f_name(f):
+    assert callable(f), f
+    if hasattr(f, '__name__'):
+        return f.__name__
+    if isinstance(f, partial):
+        return f"partial({f.func})"
+    else:
+        return "unknown"
+
+
+class JitFunction:
+
+    def __init__(self, f: Callable, auxiliary_args: Set[str], forget_traces: bool):
+        self.f = f
+        self.f_params = function_parameters(f)
+        self.auxiliary_args = auxiliary_args
+        self.forget_traces = forget_traces
+        self.traces: Dict[SignatureKey, Callable] = {}
+        self.recorded_mappings: Dict[SignatureKey, SignatureKey] = {}
+        self.grad_jit = GradientFunction(f.f, self.f_params, f.wrt, f.get_output, f.is_f_scalar, jit=True) if isinstance(f, GradientFunction) else None
+
+    def _jit_compile(self, in_key: SignatureKey):
+        PHI_LOGGER.debug(f"Φ-jit: '{f_name(self.f)}' called with new key. shapes={[s.volume for s in in_key.shapes]}, args={in_key.tree}")
+
+        def jit_f_native(*natives):
+            PHI_LOGGER.debug(f"Φ-jit: Tracing '{f_name(self.f)}'")
+            _TRACING_JIT.append(self)
+            in_tensors = assemble_tensors(natives, in_key.specs)
+            kwargs = assemble_tree(in_key.tree, in_tensors)
+            result = self.f(**kwargs, **in_key.auxiliary_kwargs)  # Tensor or tuple/list of Tensors
+            tree, out_tensors = disassemble_tree(result)
+            result_natives, result_shapes, specs = disassemble_tensors(out_tensors, expand=True)
+            self.recorded_mappings[in_key] = SignatureKey(jit_f_native, tree, result_shapes, specs, in_key.backend, in_key.tracing)
+            assert _TRACING_JIT.pop(-1) is self
+            return result_natives
+
+        jit_f_native.__name__ = f"native({f_name(self.f) if isinstance(self.f, types.FunctionType) else str(self.f)})"
+        return in_key.backend.jit_compile(jit_f_native)
+
+    def __call__(self, *args, **kwargs):
+        try:
+            key, _, natives, _ = key_from_args(args, kwargs, self.f_params, cache=True, aux=self.auxiliary_args)
+        except LinearTraceInProgress:
+            return self.f(*args, **kwargs)
+        if isinstance(self.f, GradientFunction) and key.backend.supports(Backend.jit_compile_grad):
+            return self.grad_jit(*args, **kwargs)
+        if not key.backend.supports(Backend.jit_compile):
+            warnings.warn(f"jit_copmile() not supported by {key.backend}. Running function '{f_name(self.f)}' as-is.", RuntimeWarning)
+            return self.f(*args, **kwargs)
+        if key not in self.traces:
+            if self.forget_traces:
+                self.traces.clear()
+                self.recorded_mappings.clear()
+            self.traces[key] = self._jit_compile(key)
+            if len(self.traces) >= 10:
+                warnings.warn(f"""Φ-lin: The jit-compiled function '{f_name(self.f)}' was traced {len(self.traces)} times.
+Performing many traces may be slow and cause memory leaks.
+Re-tracing occurs when the number or types of arguments vary, tensor shapes vary between calls or different auxiliary arguments are given (compared by reference).
+Set forget_traces=True to avoid memory leaks when many traces are required.""", RuntimeWarning)
+        native_result = self.traces[key](*natives)
+        output_key = match_output_signature(key, self.recorded_mappings, self)
+        output_tensors = assemble_tensors(native_result, output_key.specs)
+        return assemble_tree(output_key.tree, output_tensors)
+
+    def __repr__(self):
+        return f"jit({f_name(self.f)})"
+
+    @property
+    def __name__(self):
+        return f_name(self.f)
+
+
+_TRACING_JIT: List[JitFunction] = []
+
+
+def jit_compile(f: Callable = None, auxiliary_args: str = '', forget_traces: bool = None) -> Callable:
+    """
+    Compiles a graph based on the function `f`.
+    The graph compilation is performed just-in-time (jit), e.g. when the returned function is called for the first time.
+
+    The traced function will compute the same result as `f` but may run much faster.
+    Some checks may be disabled in the compiled function.
+
+    Can be used as a decorator:
+    ```python
+    @math.jit_compile
+    def my_function(x: math.Tensor) -> math.Tensor:
+    ```
+
+    Invoking the returned function may invoke re-tracing / re-compiling `f` after the first call if either
+
+    * it is called with a different number of arguments,
+    * the tensor arguments have different dimension names or types (the dimension order also counts),
+    * any `Tensor` arguments require a different backend than previous invocations,
+    * `phi.math.magic.PhiTreeNode` positional arguments do not match in non-variable properties.
+
+    Compilation is implemented for the following backends:
+
+    * PyTorch: [`torch.jit.trace`](https://pytorch.org/docs/stable/jit.html)
+    * TensorFlow: [`tf.function`](https://www.tensorflow.org/guide/function)
+    * Jax: [`jax.jit`](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#using-jit-to-speed-up-functions)
+
+    Jit-compilations cannot be nested, i.e. you cannot call `jit_compile()` while another function is being compiled.
+    An exception to this is `jit_compile_linear()` which can be called from within a jit-compiled function.
+
+    See Also:
+        `jit_compile_linear()`
+
+    Args:
+        f: Function to be traced.
+            All positional arguments must be of type `Tensor` or `phi.math.magic.PhiTreeNode` returning a single `Tensor` or `phi.math.magic.PhiTreeNode`.
+        auxiliary_args: Comma-separated parameter names of arguments that are not relevant to backpropagation.
+        forget_traces: If `True`, only remembers the most recent compiled instance of this function.
+            Upon tracing with new instance (due to changed shapes or auxiliary args), deletes the previous traces.
+
+    Returns:
+        Function with similar signature and return values as `f`.
+    """
+    if f is None:
+        kwargs = {k: v for k, v in locals().items() if v is not None}
+        return partial(jit_compile, **kwargs)
+    auxiliary_args = set(s.strip() for s in auxiliary_args.split(',') if s.strip())
+    return f if isinstance(f, (JitFunction, LinearFunction)) and f.auxiliary_args == auxiliary_args else JitFunction(f, auxiliary_args, forget_traces or False)
+
+
+class LinearFunction(Generic[X, Y], Callable[[X], Y]):
+    """
+    Just-in-time compiled linear function of `Tensor` arguments and return values.
+
+    Use `jit_compile_linear()` to create a linear function representation.
+    """
+
+    def __init__(self, f, auxiliary_args: Set[str], forget_traces: bool):
+        self.f = f
+        self.f_params = function_parameters(f)
+        self.auxiliary_args = auxiliary_args
+        self.forget_traces = forget_traces
+        self.matrices_and_biases: Dict[SignatureKey, Tuple[SparseCoordinateTensor, Tensor]] = {}
+        self.nl_jit = JitFunction(f, self.auxiliary_args, forget_traces)  # for backends that do not support sparse matrices
+
+    # def _trace(self, in_key: SignatureKey, prefer_numpy: bool) -> 'ShiftLinTracer':
+    #     assert in_key.shapes[0].is_uniform, f"math.jit_compile_linear() only supports uniform tensors for function input and output but input shape was {in_key.shapes[0]}"
+    #     with NUMPY if prefer_numpy else in_key.backend:
+    #         x = math.ones(in_key.shapes[0])
+    #         tracer = ShiftLinTracer(x, {EMPTY_SHAPE: math.ones()}, x.shape, math.zeros(x.shape))
+    #     _TRACING_JIT.append(self)
+    #     x_kwargs = assemble_tree(in_key.tree, [tracer])
+    #     result = self.f(**x_kwargs, **in_key.auxiliary_kwargs)
+    #     _, result_tensors = disassemble_tree(result)
+    #     assert len(result_tensors) == 1, f"Linear function must return a single Tensor or tensor-like but got {result}"
+    #     result_tensor = result_tensors[0]
+    #     assert isinstance(result_tensor, ShiftLinTracer), f"Tracing linear function '{f_name(self.f)}' failed. Make sure only linear operations are used."
+    #     assert _TRACING_JIT.pop(-1) is self
+    #     return result_tensor
+
+    def _get_or_trace(self, key: SignatureKey, args: tuple, f_kwargs: dict):
+        if not key.tracing and key in self.matrices_and_biases:
+            return self.matrices_and_biases[key]
+        else:
+            if self.forget_traces:
+                self.matrices_and_biases.clear()
+            matrix, bias = matrix_from_function(self.f, *args, **f_kwargs, auto_compress=True)
+            if not key.tracing:
+                self.matrices_and_biases[key] = matrix, bias
+                if len(self.matrices_and_biases) >= 4:
+                    warnings.warn(f"""Φ-lin: The compiled linear function '{f_name(self.f)}' was traced {len(self.matrices_and_biases)} times.
+Performing many traces may be slow and cause memory leaks.
+Tensors in auxiliary arguments (all except the first parameter unless specified otherwise) are compared by reference, not by tensor values.
+Auxiliary arguments: {key.auxiliary_kwargs}
+Multiple linear traces can be avoided by jit-compiling the code that calls the linear function or setting forget_traces=True.""", RuntimeWarning, stacklevel=3)
+            return matrix, bias
+
+    def __call__(self, *args: X, **kwargs) -> Y:
+        try:
+            key, tensors, natives, x = key_from_args(args, kwargs, self.f_params, cache=False, aux=self.auxiliary_args)
+        except LinearTraceInProgress:
+            return self.f(*args, **kwargs)
+        assert tensors, "Linear function requires at least one argument"
+        if any(isinstance(t, ShiftLinTracer) for t in tensors):
+            # TODO: if t is identity, use cached ShiftLinTracer, otherwise multiply two ShiftLinTracers
+            return self.f(*args, **kwargs)
+        if not key.backend.supports(Backend.sparse_coo_tensor):  # This might be called inside a Jax linear solve
+            # warnings.warn(f"Sparse matrices are not supported by {backend}. Falling back to regular jit compilation.", RuntimeWarning)
+            if not math.all_available(*tensors):  # avoid nested tracing, Typical case jax.scipy.sparse.cg(LinearFunction). Nested traces cannot be reused which results in lots of traces per cg.
+                PHI_LOGGER.debug(f"Φ-lin: Running '{f_name(self.f)}' as-is with {key.backend} because it is being traced.")
+                return self.f(*args, **kwargs)
+            else:
+                return self.nl_jit(*args, **kwargs)
+        matrix, bias = self._get_or_trace(key, args, kwargs)
+        return matrix @ tensors[0] + bias
+
+    def sparse_matrix(self, *args, **kwargs):
+        """
+        Create an explicit representation of this linear function as a sparse matrix.
+
+        See Also:
+            `sparse_matrix_and_bias()`.
+
+        Args:
+            *args: Function arguments. This determines the size of the matrix.
+            **kwargs: Additional keyword arguments for the linear function.
+
+        Returns:
+            Sparse matrix representation with `values` property and `native()` method.
+        """
+        key, *_ = key_from_args(args, kwargs, self.f_params, cache=False, aux=self.auxiliary_args)
+        matrix, bias = self._get_or_trace(key, args, kwargs)
+        assert math.close(bias, 0), "This is an affine function and cannot be represented by a single matrix. Use sparse_matrix_and_bias() instead."
+        return matrix
+
+    def sparse_matrix_and_bias(self, *args, **kwargs):
+        """
+        Create an explicit representation of this affine function as a sparse matrix and a bias vector.
+
+        Args:
+            *args: Positional arguments to the linear function.
+                This determines the size of the matrix.
+            **kwargs: Additional keyword arguments for the linear function.
+
+        Returns:
+            matrix: Sparse matrix representation with `values` property and `native()` method.
+            bias: `Tensor`
+        """
+        key, *_ = key_from_args(args, kwargs, self.f_params, cache=False, aux=self.auxiliary_args)
+        return self._get_or_trace(key, args, kwargs)
+
+    def __repr__(self):
+        return f"lin({f_name(self.f)})"
+
+
+def jit_compile_linear(f: Callable[[X], Y] = None, auxiliary_args: str = None, forget_traces: bool = None) -> 'LinearFunction[X, Y]':
+    """
+    Compile an optimized representation of the linear function `f`.
+    For backends that support sparse tensors, a sparse matrix will be constructed for `f`.
+
+    Can be used as a decorator:
+    ```python
+    @math.jit_compile_linear
+    def my_linear_function(x: math.Tensor) -> math.Tensor:
+    ```
+
+    Unlike `jit_compile()`, `jit_compile_linear()` can be called during a regular jit compilation.
+
+    See Also:
+        `jit_compile()`
+
+    Args:
+        f: Function that is linear in its positional arguments.
+            All positional arguments must be of type `Tensor` and `f` must return a `Tensor`.
+        auxiliary_args: Which parameters `f` is not linear in. These arguments are treated as conditioning arguments and will cause re-tracing on change.
+        forget_traces: If `True`, only remembers the most recent compiled instance of this function.
+            Upon tracing with new instance (due to changed shapes or auxiliary args), deletes the previous traces.
+
+    Returns:
+        `LinearFunction` with similar signature and return values as `f`.
+    """
+    if f is None:
+        kwargs = {k: v for k, v in locals().items() if v is not None}
+        return partial(jit_compile_linear, **kwargs)
+    if isinstance(f, JitFunction):
+        f = f.f  # cannot trace linear function from jitted version
+    if isinstance(auxiliary_args, str):
+        auxiliary_args = set(s.strip() for s in auxiliary_args.split(',') if s.strip())
+    else:
+        assert auxiliary_args is None
+        f_params = function_parameters(f)
+        auxiliary_args = f_params[1:]
+    return f if isinstance(f, LinearFunction) and f.auxiliary_args == auxiliary_args else LinearFunction(f, auxiliary_args, forget_traces or False)
+
+
+def simplify_wrt(f, wrt: Union[str, int, tuple, list]):
+    f_params = function_parameters(f)
+    if wrt is None:  # Old default
+        wrt = f_params[0],
+    elif isinstance(wrt, (tuple, list)) and all(isinstance(i, str) for i in wrt):
+        wrt = tuple(wrt)
+    elif isinstance(wrt, str) and ',' in wrt:
+        wrt = tuple(i.strip() for i in wrt.split(',') if i.strip())
+    elif isinstance(wrt, str):
+        wrt = wrt
+    else:  # int or tuple or list
+        if isinstance(wrt, int):
+            wrt = f_params[wrt]
+        elif isinstance(wrt, (tuple, list)) and all(isinstance(i, int) for i in wrt):
+            wrt = tuple(f_params[i] for i in wrt)
+        else:
+            raise ValueError(f"Invalid value given as wrt: {wrt}. Please pass a comma-separated string of parameter names.")
+        warnings.warn("Specifying wrt by position is deprecated in phi.math.funcitonal_gradient() and phi.math.jacobian(). Please pass a list or comma-separated string of parameter names.",
+                      SyntaxWarning, stacklevel=4)
+    return f_params, wrt
+
+
+class GradientFunction:
+    """ Jacobian or Gradient of a function. """
+
+    def __init__(self, f: Callable, f_params, wrt: Union[str, Tuple[str, ...]], get_output: bool, is_f_scalar: bool, jit=False):
+        self.f = f
+        self.f_params = f_params
+        self.wrt = wrt
+        self._wrt_tuple = wrt if isinstance(wrt, tuple) else (wrt,)
+        self.get_output = get_output
+        self.is_f_scalar = is_f_scalar
+        self.traces: Dict[SignatureKey, Callable] = {}
+        self.recorded_mappings: Dict[SignatureKey, SignatureKey] = {}
+        self.jit = jit
+
+    def _trace_grad(self, in_key: SignatureKey, wrt_natives):
+        def f_native(*natives):
+            PHI_LOGGER.debug(f"Φ-grad: Evaluating gradient of {f_name(self.f)}")
+            in_tensors = assemble_tensors(natives, in_key.specs)
+            kwargs = assemble_tree(in_key.tree, in_tensors)
+            with functional_derivative_evaluation(order=1):
+                result = self.f(**kwargs)  # Tensor or tuple/list of Tensors
+            loss = result[0] if isinstance(result, (tuple, list)) else result
+            if isinstance(loss, Tensor):
+                loss_reduced = math.sum_(loss, batch)
+                loss_native = loss_reduced.native(loss_reduced.shape.names)
+            else:
+                loss_native = loss
+                loss_shape = in_key.backend.staticshape(loss_native)
+                assert len(
+                    loss_shape) == 0, f"Only scalar losses are allowed when returning a native tensor but {f_name(self.f)} returned {type(loss_native).__name__} of shape {loss_shape}. For higher-dimensional values, use Φ-Tensors instead."
+            nest, out_tensors = disassemble_tree(result)
+            result_natives, result_shapes, specs = disassemble_tensors(out_tensors, expand=True)
+            self.recorded_mappings[in_key] = SignatureKey(f_native, nest, result_shapes, specs, in_key.backend, in_key.tracing)
+            return loss_native, result_natives
+
+        if self.jit:
+            return in_key.backend.jit_compile_grad(f_native, wrt=wrt_natives, get_output=self.get_output, is_f_scalar=self.is_f_scalar)
+        else:
+            return in_key.backend.jacobian(f_native, wrt=wrt_natives, get_output=self.get_output, is_f_scalar=self.is_f_scalar)
+
+    def __call__(self, *args, **kwargs):
+        key, tensors, natives, kwargs = key_from_args(args, kwargs, self.f_params, cache=True)
+        if not key.backend.supports(Backend.jacobian):
+            if math.default_backend().supports(Backend.jacobian):
+                warnings.warn(f"Using {math.default_backend()} for gradient computation because {key.backend} does not support jacobian()", RuntimeWarning)
+                key.backend = math.default_backend()
+            else:
+                raise AssertionError(f"jacobian() not supported by {key.backend}.")
+        wrt_tensors = self._track_wrt(kwargs)
+        wrt_natives = self._track_wrt_natives(wrt_tensors, disassemble_tree(kwargs)[1])
+        if key not in self.traces:
+            self.traces[key] = self._trace_grad(key, wrt_natives)
+        native_result = self.traces[key](*natives)
+        output_key = match_output_signature(key, self.recorded_mappings, self)
+        jac_shape = output_key.shapes[0].non_batch  # ToDo prepend this to all wrt shapes
+        wrt_specs = [key.specs[i] for i in wrt_tensors]
+        if self.get_output:
+            output_tensors = assemble_tensors(native_result, list(output_key.specs) + wrt_specs)
+            output_structure, grad_tuple = assemble_tree((output_key.tree, [key.tree[i] for i in self._wrt_tuple]), output_tensors)
+            return output_structure, grad_tuple if isinstance(self.wrt, tuple) else grad_tuple[0]
+        else:
+            output_tensors = assemble_tensors(native_result, wrt_specs)
+            grad_tuple = assemble_tree([key.tree[i] for i in self._wrt_tuple], output_tensors)
+            return grad_tuple if isinstance(self.wrt, tuple) else grad_tuple[0]
+
+    def __repr__(self):
+        return f"grad({f_name(self.f)})"
+
+    @property
+    def __name__(self):
+        return f_name(self.f)
+
+    def _track_wrt(self, kwargs: dict):
+        wrt_tensors = []
+        for name, arg in kwargs.items():
+            _, tensors = disassemble_tree(arg)
+            wrt_tensors.extend([name] * len(tensors))
+        return [t_i for t_i, name in enumerate(wrt_tensors) if name in self._wrt_tuple]
+
+    @staticmethod
+    def _track_wrt_natives(wrt_tensors, values):
+        wrt_natives = []
+        for i, value in enumerate(values):
+            wrt_natives.extend([i] * len(value._natives()))
+        return [n_i for n_i, t_i in enumerate(wrt_natives) if t_i in wrt_tensors]
+
+
+def jacobian(f: Callable, wrt: str = None, get_output=True) -> Callable:
+    """
+    Creates a function which computes the Jacobian matrix of `f`.
+    For scalar functions, consider using `functional_gradient()` instead.
+
+    Example:
+    ```python
+    def f(x, y):
+        prediction = f(x)
+        loss = math.l2_loss(prediction - y)
+        return loss, prediction
+
+    dx = jacobian(loss_function, wrt='x', get_output=False)(x, y)
+
+    (loss, prediction), (dx, dy) = jacobian(loss_function,
+                                        wrt='x,y', get_output=True)(x, y)
+    ```
+
+    Functional gradients are implemented for the following backends:
+
+    * PyTorch: [`torch.autograd.grad`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad) / [`torch.autograd.backward`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward)
+    * TensorFlow: [`tf.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape)
+    * Jax: [`jax.grad`](https://jax.readthedocs.io/en/latest/jax.html#jax.grad)
+
+    When the gradient function is invoked, `f` is called with tensors that track the gradient.
+    For PyTorch, `arg.requires_grad = True` for all positional arguments of `f`.
+
+    Args:
+        f: Function to be differentiated.
+            `f` must return a floating point `Tensor` with rank zero.
+            It can return additional tensors which are treated as auxiliary data and will be returned by the gradient function if `return_values=True`.
+            All arguments for which the gradient is computed must be of dtype float or complex.
+        get_output: Whether the gradient function should also return the return values of `f`.
+        wrt: Comma-separated parameter names of `f` with respect to which the gradient should be computed.
+            If not specified, the gradient will be computed w.r.t. the first positional argument (highly discouraged).
+
+    Returns:
+        Function with the same arguments as `f` that returns the value of `f`, auxiliary data and Jacobian of `f` if `get_output=True`, else just the Jacobian of `f`.
+    """
+    f_params, wrt = simplify_wrt(f, wrt)
+    return GradientFunction(f, f_params, wrt, get_output, is_f_scalar=False)
+
+
+def functional_gradient(f: Callable, wrt: str = None, get_output=True) -> Callable:
+    """
+    Creates a function which computes the gradient of `f`.
+
+    Example:
+    ```python
+    def loss_function(x, y):
+        prediction = f(x)
+        loss = math.l2_loss(prediction - y)
+        return loss, prediction
+
+    dx = functional_gradient(loss_function, 'x', get_output=False)(x, y)
+
+    (loss, prediction), (dx, dy) = functional_gradient(loss_function,
+                                            'x,y', get_output=True)(x, y)
+    ```
+
+    Functional gradients are implemented for the following backends:
+
+    * PyTorch: [`torch.autograd.grad`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad) / [`torch.autograd.backward`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward)
+    * TensorFlow: [`tf.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape)
+    * Jax: [`jax.grad`](https://jax.readthedocs.io/en/latest/jax.html#jax.grad)
+
+    When the gradient function is invoked, `f` is called with tensors that track the gradient.
+    For PyTorch, `arg.requires_grad = True` for all positional arguments of `f`.
+
+    Args:
+        f: Function to be differentiated.
+            `f` must return a floating point `Tensor` with rank zero.
+            It can return additional tensors which are treated as auxiliary data and will be returned by the gradient function if `return_values=True`.
+            All arguments for which the gradient is computed must be of dtype float or complex.
+        get_output: Whether the gradient function should also return the return values of `f`.
+        wrt: Comma-separated parameter names of `f` with respect to which the gradient should be computed.
+            If not specified, the gradient will be computed w.r.t. the first positional argument (highly discouraged).
+
+    Returns:
+        Function with the same arguments as `f` that returns the value of `f`, auxiliary data and gradient of `f` if `get_output=True`, else just the gradient of `f`.
+    """
+    f_params, wrt = simplify_wrt(f, wrt)
+    return GradientFunction(f, f_params, wrt, get_output, is_f_scalar=True)
+
+
+class HessianFunction:
+
+    def __init__(self, f: Callable, f_params, wrt: tuple, get_output: bool, get_gradient: bool, dim_suffixes: tuple, jit=False):
+        assert isinstance(dim_suffixes, tuple) and len(dim_suffixes) == 2
+        self.f = f
+        self.f_params = f_params
+        self.wrt = wrt
+        self._wrt_tuple = wrt if isinstance(wrt, tuple) else (wrt,)
+        self.get_output = get_output
+        self.get_gradient = get_gradient
+        self.dim_suffixes = dim_suffixes
+        self.traces: Dict[SignatureKey, Callable] = {}
+        self.recorded_mappings: Dict[SignatureKey, SignatureKey] = {}
+        self.jit = jit
+#
+#     def _trace_hessian(self, in_key: SignatureKey, wrt_natives):
+#         def f_native(*natives):
+#             PHI_LOGGER.debug(f"Φ-grad: Evaluating gradient of {f_name(self.f)}")
+#             in_tensors = assemble_tensors(natives, in_key.specs)
+#             kwargs = assemble_tree(in_key.tree, in_tensors)
+#             with functional_derivative_evaluation(order=2):
+#                 result = self.f(**kwargs)
+#             nest, out_tensors = disassemble_tree(result)
+#             result_natives, result_shapes, specs = disassemble_tensors(out_tensors, expand=True)
+#             self.recorded_mappings[in_key] = SignatureKey(f_native, nest, result_shapes, specs, in_key.backend, in_key.tracing)
+#             return result_natives
+#
+#         hessian_generator = in_key.backend.jit_compile_hessian if self.jit else in_key.backend.hessian
+#         return hessian_generator(f_native, wrt=wrt_natives, get_output=self.get_output, get_gradient=self.get_gradient)
+#
+#     def __call__(self, *args, **kwargs):
+#         key, tensors, natives, kwargs, batch_shape = key_from_args_pack_batch(args, kwargs, self.f_params, cache=True)
+#         if not key.backend.supports(Backend.jacobian):
+#             if math.default_backend().supports(Backend.jacobian):
+#                 warnings.warn(f"Using {math.default_backend()} for gradient computation because {key.backend} does not support jacobian()", RuntimeWarning)
+#                 key.backend = math.default_backend()
+#             else:
+#                 raise AssertionError(f"jacobian() not supported by {key.backend}.")
+#         wrt_tensors: List[int] = self._track_wrt(kwargs)
+#         wrt_natives: List[int] = self._track_wrt_natives(wrt_tensors, disassemble_tree(kwargs)[1])
+#         if key not in self.traces:
+#             self.traces[key] = self._trace_hessian(key, wrt_natives)
+#         native_result = self.traces[key](*natives)
+#         assert len(native_result) == 1 + int(self.get_output) + int(self.get_gradient)
+#         output_key = match_output_signature(key, self.recorded_mappings, self)
+#         result = ()
+#         if self.get_output:
+#             output_tensors = assemble_tensors(native_result[0], output_key.specs)
+#             output_tensors = [unpack_dim(t, 'batch', batch_shape) for t in output_tensors]
+#             # output_tensors = [math.reshaped_tensor(n, [batch_shape, *shape.non_batch]) for n, shape in zip(native_result[0], output_key.shapes)]
+#             result += assemble_tree(output_key.tree, output_tensors),
+#         if self.get_gradient:
+#             grad_tensors = assemble_tensors(native_result[int(self.get_output)], [key.specs[i] for i in wrt_tensors])
+#             grad_tensors = [unpack_dim(t, 'batch', batch_shape) for t in grad_tensors]
+#             grads = assemble_tree([key.tree[i] for i in self._wrt_tuple], grad_tensors)
+#             if not isinstance(self.wrt, tuple):
+#                 grads = grads[0]
+#             result += grads,
+#         if len(wrt_natives) == 1:
+#             native_hessian = native_result[-1][0][0]
+#             hessian_tensor = math.reshaped_tensor(native_hessian, [batch_shape, *self.shape_with_suffixes(key.shapes[0].non_batch, self.dim_suffixes[0]),
+#                                                                    *self.shape_with_suffixes(key.shapes[0].non_batch, self.dim_suffixes[1])], check_sizes=True)
+#             hessian_tree = assemble_tree(key.tree[self.wrt[0] if isinstance(self.wrt, tuple) else self.wrt], [hessian_tensor])
+#             result += [hessian_tree] if isinstance(self.wrt, tuple) else hessian_tree,
+#         else:
+#             assert all([t is None for t in key.tree]), "When computing the Hessian w.r.t. multiple tensors, all inputs must be Tensors."
+#             raise NotImplementedError()
+#             hessian_tree = [[] for _ in self.wrt]
+#             for i in range(len(self.wrt)):
+#                 for j in range(len(self.wrt)):
+#                     native_hessian_ij = native_result[-1][i][j]
+#                     hessian_tensor_ij = math.reshaped_tensor(native_hessian_ij, [batch_shape, *key.shapes[i].non_batch, *self.dupli_shape(key.shapes[j].non_batch)], check_sizes=True)
+#                     hessian_tree[i].append(hessian_tensor_ij)
+#             result += tuple([tuple(col) for col in hessian_tree]),
+#         return result
+#
+#     def shape_with_suffixes(self, shape: Shape, suffix: str):
+#         return shape._with_names([n + suffix for n in shape.names])
+#
+#     def __repr__(self):
+#         return f"grad({f_name(self.f)})"
+#
+#     @property
+#     def __name__(self):
+#         return f_name(self.f)
+#
+#     def _track_wrt(self, kwargs: dict):
+#         wrt_tensors = []
+#         for name, arg in kwargs.items():
+#             _, tensors = disassemble_tree(arg)
+#             wrt_tensors.extend([name] * len(tensors))
+#         return [t_i for t_i, name in enumerate(wrt_tensors) if name in self._wrt_tuple]
+#
+#     @staticmethod
+#     def _track_wrt_natives(wrt_tensors, values):
+#         wrt_natives = []
+#         for i, value in enumerate(values):
+#             wrt_natives.extend([i] * len(value._natives()))
+#         return [n_i for n_i, t_i in enumerate(wrt_natives) if t_i in wrt_tensors]
+#
+#
+# def hessian(f: Callable, wrt: str, get_output=True, get_gradient=True, dim_suffixes=('', '_')) -> Callable:
+#     """
+#     *Experimental. This function currently only supports PyTorch and the Hessian can only be computed w.r.t. one argument.*
+#
+#     Creates a function which computes the Hessian (second derivative) of `f`.
+#
+#     Example:
+#     ```python
+#     def loss_function(x, y):
+#         prediction = f(x)
+#         loss = math.l2_loss(prediction - y)
+#         return loss, prediction
+#
+#     hess, = hessian(loss_function, 'x', get_output=False, get_gradient=False)(x, y)
+#
+#     (loss, prediction), (dx, dy), ((dx_dx, dx_dy), (dy_dx, dy_dy)) = hessian(loss_function,
+#                                         wrt='x,y', get_output=True)(x, y)
+#     ```
+#
+#     When the gradient function is invoked, `f` is called with tensors that track the gradient.
+#     For PyTorch, `arg.requires_grad = True` for all positional arguments of `f`.
+#
+#     Args:
+#         f: Function to be differentiated.
+#             `f` must return a floating point `Tensor` with rank zero.
+#             It can return additional tensors which are treated as auxiliary data and will be returned by the gradient function if `return_values=True`.
+#             All arguments for which the gradient is computed must be of dtype float or complex.
+#         wrt: Comma-separated parameter names of `f` with respect to which the gradient should be computed.
+#             If not specified, the gradient will be computed w.r.t. the first positional argument (highly discouraged).
+#         get_output: Whether the Hessian function should also return the return values of `f`.
+#         get_gradient: Whether the Hessian function should also return the gradient of `f`.
+#         dim_suffixes: `tuple` containing two strings.
+#             All Non-batch dimensions of the parameters occur twice in the corresponding Hessian.
+#             To avoid duplicate names, suffixes are added to non-batch dimensions.
+#             The dimensions from the first derivative computation are appended with `dim_suffixes[0]` and the second ones with `dim_suffixes[1]`.
+#             This argument has no effect on the dimension names of the gradient if `get_gradient=True`.
+#
+#     Returns:
+#         Function with the same arguments as `f` that returns `(f(x), g(x), H(x))` or less depending on `get_output` and `get_gradient`.
+#     """
+#     f_params, wrt = simplify_wrt(f, wrt)
+#     return HessianFunction(f, f_params, wrt, get_output, get_gradient, dim_suffixes)
+
+
+class CustomGradientFunction:
+
+    def __init__(self, f: Callable, gradient: Callable, auxiliary_args: Set[str]):
+        self.f = f
+        self.f_params = function_parameters(f)
+        self.gradient = gradient
+        self.auxiliary_args = auxiliary_args
+        self.traces: Dict[SignatureKey, Callable] = {}
+        self.recorded_mappings: Dict[SignatureKey, SignatureKey] = {}
+
+    def _trace(self, in_key: SignatureKey):
+        def forward_native(*natives):
+            in_tensors = assemble_tensors(natives, in_key.specs)
+            kwargs = assemble_tree(in_key.tree, in_tensors)
+            PHI_LOGGER.debug(f"Running forward pass of custom op {forward_native.__name__} given args {tuple(kwargs.keys())} containing {len(natives)} native tensors")
+            result = self.f(**kwargs, **in_key.auxiliary_kwargs)  # Tensor or tuple/list of Tensors
+            nest, out_tensors = disassemble_tree(result)
+            result_natives, result_shapes, specs = disassemble_tensors(out_tensors, expand=True)
+            self.recorded_mappings[in_key] = SignatureKey(forward_native, nest, result_shapes, specs, in_key.backend, in_key.tracing)
+            return result_natives
+
+        def backward_native(x_natives, y_natives, dy_natives):
+            PHI_LOGGER.debug(f"Running backward pass of custom op {backward_native.__name__}")
+            out_key = self.recorded_mappings[in_key]
+            # del self.recorded_mappings[in_key]  # this may be required multiple times
+            x_tensors = assemble_tensors(x_natives, in_key.specs)
+            y_tensors = assemble_tensors(y_natives, out_key.specs)
+            dy_tensors = assemble_tensors(dy_natives, out_key.specs)
+            kwargs = assemble_tree(in_key.tree, x_tensors)
+            if in_key.auxiliary_kwargs:
+                kwargs = {**kwargs, **in_key.auxiliary_kwargs}
+            y = assemble_tree(out_key.tree, y_tensors)
+            dy = assemble_tree(out_key.tree, dy_tensors)
+            result = self.gradient(kwargs, y, dy)
+            assert isinstance(result, dict) and all(key in kwargs for key in result.keys()), f"gradient function must return a dict containing only parameter names of the forward function. Forward '{f_name(self.f)}' has arguments {kwargs}."
+            full_result = tuple(result.get(name, None) for name in in_key.tree.keys())
+            result_natives = self.incomplete_tree_to_natives(full_result, tuple(in_key.tree.values()), list(in_key.shapes))
+            PHI_LOGGER.debug(f"Backward pass of custom op {backward_native.__name__} returned gradients for {tuple(result.keys())} out of {tuple(in_key.tree.keys())} containing {len(result_natives)} native tensors")
+            return result_natives
+
+        forward_native.__name__ = f"forward '{f_name(self.f) if isinstance(self.f, types.FunctionType) else str(self.f)}'"
+        backward_native.__name__ = f"{self.gradient.__name__ if isinstance(self.gradient, types.FunctionType) else str(self.gradient)} (of '{f_name(self.f) if isinstance(self.f, types.FunctionType) else str(self.f)}')"
+
+        return in_key.backend.custom_gradient(forward_native, backward_native, get_external_cache=lambda: self.recorded_mappings[in_key], on_call_skipped=partial(self.recorded_mappings.__setitem__, in_key))
+
+    def __call__(self, *args, **kwargs):
+        key, _, natives, _ = key_from_args(args, kwargs, self.f_params, cache=False, aux=self.auxiliary_args)
+        if not key.backend.supports(Backend.jacobian) and not key.backend.supports(Backend.jacobian):
+            return self.f(*args, **kwargs)  # no need to use custom gradient if gradients aren't supported anyway
+        elif not key.backend.supports(Backend.custom_gradient):
+            warnings.warn(f"custom_gradient() not supported by {key.backend}. Running function '{f_name(self.f)}' as-is.", RuntimeWarning)
+            return self.f(*args, **kwargs)
+        if key not in self.traces:
+            self.traces[key] = self._trace(key)
+            if len(self.traces) >= 8:
+                warnings.warn(f"""{self.__name__} has been traced {len(self.traces)} times.
+To avoid memory leaks, call {f_name(self.f)}.traces.clear(), {f_name(self.f)}.recorded_mappings.clear().
+Traces can be avoided by jit-compiling the code that calls custom gradient functions.
+""", RuntimeWarning, stacklevel=2)
+        native_result = self.traces[key](*natives)  # With PyTorch + jit, this does not call forward_native every time
+        output_key = match_output_signature(key, self.recorded_mappings, self)
+        output_tensors = assemble_tensors(native_result, output_key.specs)
+        return assemble_tree(output_key.tree, output_tensors)
+
+    def __repr__(self):
+        return f"custom_gradient(forward={f_name(self.f)}, backward={self.gradient.__name__}, id={id(self)})"
+
+    @property
+    def __name__(self):
+        return f"custom_grad({f_name(self.f)})"
+
+    @staticmethod
+    def incomplete_tree_to_natives(incomplete, tree, complete_shapes: List[Shape]) -> list:
+        """ None in nest means there is a tensor. """
+        if tree is None:
+            c_shape = complete_shapes.pop(0)
+            if incomplete is None:
+                return [None] * c_shape.shape.without('dims').volume
+            else:
+                assert isinstance(incomplete, Tensor)
+                return list(incomplete._natives())
+        elif isinstance(tree, (tuple, list)):
+            if incomplete is None:
+                return sum([CustomGradientFunction.incomplete_tree_to_natives(None, item, complete_shapes) for item in tree], [])
+            else:
+                assert type(tree) == type(incomplete) and len(tree) == len(incomplete)
+                return sum([CustomGradientFunction.incomplete_tree_to_natives(i_item, c_item, complete_shapes) for i_item, c_item in zip(incomplete, tree)], [])
+        elif isinstance(tree, dict):
+            if incomplete is None:
+                return sum([CustomGradientFunction.incomplete_tree_to_natives(None, item, complete_shapes) for item in tree.values()], [])
+            else:
+                assert type(tree) == type(incomplete) and len(tree) == len(incomplete) and set(tree.keys()) == set(incomplete.keys())
+                return sum([CustomGradientFunction.incomplete_tree_to_natives(incomplete[key], c_item, complete_shapes) for key, c_item in tree.items()], [])
+        elif isinstance(tree, PhiTreeNode):
+            attributes = variable_attributes(tree)
+            natives = []
+            for attr in attributes:
+                n_val = getattr(tree, attr)
+                i_val = getattr(incomplete, attr) if incomplete is not None else None
+                natives_item = CustomGradientFunction.incomplete_tree_to_natives(i_val, n_val, complete_shapes)
+                natives.extend(natives_item)
+            return natives
+        else:
+            assert incomplete is None
+            return []
+
+
+def custom_gradient(f: Callable, gradient: Callable, auxiliary_args: str = ''):
+    """
+    Creates a function based on `f` that uses a custom gradient for the backpropagation pass.
+
+    *Warning* This method can lead to memory leaks if the gradient function is not called.
+    Make sure to pass tensors without gradients if the gradient is not required, see `stop_gradient()`.
+
+    Args:
+        f: Forward function mapping `Tensor` arguments `x` to a single `Tensor` output or sequence of tensors `y`.
+        gradient: Function to compute the vector-Jacobian product for backpropagation.
+            Will be called as `gradient(input_dict, *y, *dy) -> output_dict` where `input_dict` contains all named arguments passed to the forward function
+            and `output_dict` contains only those parameters for which a gradient is defined.
+        auxiliary_args: Comma-separated parameter names of arguments that are not relevant to backpropagation.
+
+    Returns:
+        Function with similar signature and return values as `f`. However, the returned function does not support keyword arguments.
+    """
+    auxiliary_args = set(s.strip() for s in auxiliary_args.split(',') if s.strip())
+    return CustomGradientFunction(f, gradient, auxiliary_args)
+
+
+def print_gradient(value: Tensor, name="", detailed=False) -> Tensor:
+    """
+    Prints the gradient vector of `value` when computed.
+    The gradient at `value` is the vector-Jacobian product of all operations between the output of this function and the loss value.
+
+    The gradient is not printed in jit mode, see `jit_compile()`.
+
+    Example:
+        ```python
+        def f(x):
+            x = math.print_gradient(x, 'dx')
+            return math.l1_loss(x)
+
+        math.jacobian(f)(math.ones(x=6))
+        ```
+
+    Args:
+        value: `Tensor` for which the gradient may be computed later.
+        name: (Optional) Name to print along with the gradient values
+        detailed: If `False`, prints a short summary of the gradient tensor.
+
+    Returns:
+        `identity(value)` which when differentiated, prints the gradient vector.
+    """
+
+    def print_grad(params: dict, _y, dx):
+        param_name, x = next(iter(params.items()))
+        if math.all_available(x, dx):
+            if detailed:
+                math.print_(dx, name=name)
+            else:
+                print(f"{name}:  \t{dx}")
+        else:
+            print(f"Cannot print gradient for {param_name}, data not available.")
+        return {param_name: dx}
+
+    identity = custom_gradient(lambda x: x, print_grad)
+    return identity(value)
+
+
+def trace_check(f, *args, **kwargs):
+    """
+    Tests if `f(*args, **kwargs)` has already been traced.
+    If true, jit-compiled functions are very fast since the Python function is not actually called anymore.
+
+    Args:
+        f: Transformed Function, e.g. jit-compiled or linear function.
+        *args: Hypothetical arguments to be passed to `f`
+        **kwargs: Hypothetical keyword arugments to be passed to `f`
+
+    Returns:
+        result: `True` if there is an existing trace that can be used, `False` if `f` would have to be re-traced.
+        reason: Message giving hints as to why `f` needs to be re-traced given `args` and `kwargs`.
+    """
+    if isinstance(f, (JitFunction, GradientFunction, HessianFunction, CustomGradientFunction)):
+        keys = f.traces.keys()
+    elif isinstance(f, LinearFunction):
+        keys = f.matrices_and_biases.keys()
+    else:
+        raise ValueError(f"{f_name(f)} is not a traceable function. Only supports jit_compile, jit_compile_linear, functional_gradient, custom_gradient, jacobian, hessian")
+    key, *_ = key_from_args(args, kwargs, f.f_params, aux=f.auxiliary_args)
+    if not keys:
+        return False, "Function has not yet been traced"
+    if key in keys:
+        return True, ""
+    traced_key = next(iter(keys))  # ToDo compare against all
+    cond_equal = key.auxiliary_kwargs == traced_key.auxiliary_kwargs
+    if isinstance(cond_equal, Tensor):
+        cond_equal = cond_equal.all
+    if not cond_equal:
+        return False, "Auxiliary arguments do not match"
+    # shapes need not be compared because they are included in specs
+    if traced_key.tree.keys() != key.tree.keys():
+        return False, f"Different primary arguments passed: {set(traced_key.tree.keys())} vs {set(key.tree.keys())}"
+    for name in traced_key.tree.keys():
+        if traced_key.tree[name] != key.tree[name]:
+            return False, f"Primary argument '{name}' differs in non-traced variables: {traced_key.tree[name]} vs {key.tree[name]}. Make sure the corresponding class overrides __eq__()."
+    if traced_key.specs != key.specs:
+        return False, "Traced variables differ in shape"
+    if traced_key.backend != key.backend:
+        return False, f"Function was not traced with backend {key.backend}"
+    if traced_key.spatial_derivative_order != key.spatial_derivative_order:
+        return False, f"Different in spatial_derivative_order. This is likely an internal problem."
+    return True
+
+
+def map_types(f: Callable, dims: Union[Shape, tuple, list, str, Callable], dim_type: Union[Callable, str]) -> Callable:
+    """
+    Wraps a function to change the dimension types of its `Tensor` and `phi.math.magic.PhiTreeNode` arguments.
+
+    Args:
+        f: Function to wrap.
+        dims: Concrete dimensions or dimension type, such as `spatial` or `batch`.
+            These dimensions will be mapped to `dim_type` for all positional function arguments.
+        dim_type: Dimension type, such as `spatial` or `batch`.
+            `f` will be called with dimensions remapped to this type.
+
+    Returns:
+        Function with signature matching `f`.
+    """
+
+    def forward_retype(obj, input_types: Shape):
+        tree, tensors = disassemble_tree(obj)
+        retyped = []
+        for t in tensors:
+            for dim in t.shape.only(dims):
+                t = t.dimension(dim).as_type(dim_type)
+                input_types = math.merge_shapes(input_types, dim.with_size(None))
+            retyped.append(t)
+        return assemble_tree(tree, retyped), input_types
+
+    def reverse_retype(obj, input_types: Shape):
+        tree, tensors = disassemble_tree(obj)
+        retyped = []
+        for t in tensors:
+            for dim in t.shape.only(input_types.names):
+                t = t.dimension(dim).as_type(input_types.get_type(dim))
+            retyped.append(t)
+        return assemble_tree(tree, retyped)
+
+    @wraps(f)
+    def retyped_f(*args, **kwargs):
+        input_types = EMPTY_SHAPE
+        retyped_args = []
+        for arg in args:
+            retyped_arg, input_types = forward_retype(arg, input_types)
+            retyped_args.append(retyped_arg)
+        output = f(*retyped_args, **kwargs)
+        restored_output = reverse_retype(output, input_types)
+        return restored_output
+
+    return retyped_f
+
+
+def map_s2b(f: Callable) -> Callable:
+    """ Map spatial dimensions to batch dimensions. Short for `map_types(f, spatial, batch)`. """
+    return map_types(f, spatial, batch)
+
+
+def map_i2b(f: Callable) -> Callable:
+    """ Map instance dimensions to batch dimensions. Short for `map_types(f, instance, batch)`. """
+    return map_types(f, instance, batch)
+
+
+def map_c2b(f: Callable) -> Callable:
+    """ Map channel dimensions to batch dimensions. Short for `map_types(f, instance, batch)`. """
+    return map_types(f, channel, batch)
+
+
+def broadcast(f):
+    """
+    Function decorator for non-vectorized functions.
+    When passing a `Tensor` argument to a broadcast function, the function is called once for each element of the tensor.
+
+    Only positionsl arguments, not keyword arguments are broadcast.
+
+    See Also:
+        `phi.math.map`
+
+    Args:
+        f: Function.
+
+    Returns:
+        Broadcast function
+    """
+    @wraps(f)
+    def broadcast_(*args, **kwargs):
+        return math.map_(f, *args, **kwargs)
+    return broadcast_
+
+
+def iterate(f: Callable,
+            iterations: Union[int, Shape],
+            *x0,
+            f_kwargs: dict = None,
+            range: Callable = range,
+            measure: Callable = None,
+            **f_kwargs_):
+    """
+    Repeatedly call `function`, passing the previous output as the next input.
+
+    Args:
+        f: Function to call. Must be callable as `f(x0, **f_kwargs)` and `f(f(x0, **f_kwargs), **f_kwargs)`.
+        iterations: Number of iterations as `int` or single-dimension `Shape`.
+            If `int`, returns the final output of `f`.
+            If `Shape`, returns the trajectory (`x0` and all outputs of `f`), stacking the values along this dimension.
+        x0: Initial positional arguments for `f`.
+        range: Range function. Can be used to generate tqdm output by passing `trange`.
+        measure: Function without arguments to call at the start and end (and in between if `isinstance(iterations, Shape)`) calls to `f`.
+            The measure of each call to `f` is `measure()` after minus `measure()` before the call.
+        f_kwargs: Additional keyword arguments to be passed to `f`.
+            These arguments can be of any type.
+        f_kwargs_: More keyword arguments.
+
+    Returns:
+        trajectory: Trajectory of final output of `f`, depending on `iterations`.
+        measured: Only if `measure` was specified, returns the measured value or trajectory tensor.
+    """
+    if f_kwargs is None:
+        f_kwargs = {}
+    f_kwargs.update(f_kwargs_)
+    x = x0
+    if isinstance(iterations, int):
+        start_time = measure() if measure else None
+        for _ in range(iterations):
+            x = f(*x, **f_kwargs)
+            if not isinstance(x, tuple):
+                x = (x,)
+            assert len(x) == len(x0), f"Function to iterate must return {len(x0)} outputs to match input but got {x}"
+        result = x[0] if len(x0) == 1 else x
+        return (result, measure() - start_time) if measure else result
+    elif isinstance(iterations, Shape):
+        xs = [x0]
+        ts = [measure()] if measure else None
+        for _ in range(iterations.size):
+            x = f(*x, **f_kwargs)
+            if not isinstance(x, tuple):
+                x = (x,)
+            assert len(x) == len(x0), f"Function to iterate must return {len(x0)} outputs to match input but got {x}"
+            xs.append(x)
+            if measure:
+                ts.append(measure())
+        xs = [stack(item, iterations.with_size(None)) for item in zip(*xs)]
+        result = xs[0] if len(x0) == 1 else xs
+        ts = np.asarray(ts)
+        return (result, wrap(ts[1:] - ts[:-1], iterations.with_size(None))) if measure else result
+    else:
+        raise ValueError(f"iterations must be an int or Shape but got {type(iterations)}")
+
+
+def identity(x):
+    """
+    Identity function for one argument.
+    Vararg functions cannot be transformed as the argument names are unknown.
+
+    Args:
+        x: Positional argument.
+
+    Returns:
+        `x`
+    """
+    return x
```

### Comparing `phiflow-2.3.4/phi/math/_magic_ops.py` & `phiflow-2.4.0/phi/math/_magic_ops.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,725 +1,757 @@
-import copy
-import warnings
-from numbers import Number
-from typing import TypeVar, Tuple, Set, Dict, Union
-
-import dataclasses
-
-from . import channel
-from .backend import choose_backend, NoBackendFound
-from .backend._dtype import DType
-from ._shape import Shape, DimFilter, batch, instance, shape, non_batch, merge_shapes, concat_shapes, spatial, parse_dim_order
-from .magic import Sliceable, Shaped, Shapable, PhiTreeNode
-
-
-class MagicNotImplemented(Exception): pass
-
-
-def slice_(value, slices: Dict[str, Union[int, slice, str, tuple, list]]):
-    """
-    Slices a `Tensor` or `phi.math.magic.PhiTreeNode` along named dimensions.
-
-    See Also:
-        `unstack`.
-
-    Args:
-        value: `Tensor` or `phi.math.magic.PhiTreeNode`
-        slices: `dict` mapping dimension names to slices. A slice can be one of the following:
-
-            * An index (`int`)
-            * A range (`slice`)
-            * An item name (`str`)
-            * Multiple item names (comma-separated `str`)
-            * Multiple indices or item names (`tuple` or `list`)
-
-    Returns:
-        `Tensor` or `phi.math.magic.PhiTreeNode` of the same type as `value`.
-
-    Examples:
-        >>> math.slice([vec(x=0, y=1), vec(x=2, y=3)], {'vector': 'y'})
-        [1, 3]
-    """
-    if isinstance(value, (bool, Number)):
-        return value
-    if isinstance(value, tuple):
-        return tuple([slice_(v, slices) for v in value])
-    if isinstance(value, list):
-        return [slice_(v, slices) for v in value]
-    if isinstance(value, dict):
-        return {k: slice_(v, slices) for k, v in value.items()}
-    if isinstance(value, Shape):
-        raise NotImplementedError
-    if hasattr(value, '__getitem__'):
-        return value[slices]
-    if isinstance(value, PhiTreeNode):
-        attrs = {key: getattr(value, key) for key in value_attributes(value)}
-        new_attrs = {k: slice_(v, slices) for k, v in attrs.items()}
-        return copy_with(value, **new_attrs)
-    raise ValueError(f"value must be a PhiTreeNode but got {type(value)}")
-
-
-def unstack(value, dim: DimFilter):
-    """
-    Un-stacks a `Sliceable` along one or multiple dimensions.
-
-    If multiple dimensions are given, the order of elements will be according to the dimension order in `dim`, i.e. elements along the last dimension will be neighbors in the returned `tuple`.
-
-    See Also:
-        `phi.math.slice`.
-
-    Args:
-        value: `phi.math.magic.Shapable`, such as `phi.math.Tensor`
-        dim: Dimensions as `Shape` or comma-separated `str` or dimension type, i.e. `channel`, `spatial`, `instance`, `batch`.
-
-    Returns:
-        `tuple` of `Tensor` objects.
-
-    Examples:
-        >>> unstack(expand(0, spatial(x=5)), 'x')
-        (0.0, 0.0, 0.0, 0.0, 0.0)
-    """
-    assert isinstance(value, Sliceable) and isinstance(value, Shaped), f"Cannot unstack {type(value).__name__}. Must be Sliceable and Shaped, see https://tum-pbs.github.io/PhiFlow/phi/math/magic.html"
-    dims = shape(value).only(dim)
-    assert dims.rank > 0, "unstack() requires at least one dimension"
-    if dims.rank == 1:
-        if hasattr(value, '__unstack__'):
-            result = value.__unstack__(dims.names)
-            if result is not NotImplemented:
-                assert isinstance(result, tuple), f"__unstack__ must return a tuple but got {type(result)}"
-                assert all([isinstance(item, Sliceable) for item in result]), f"__unstack__ must return a tuple of Sliceable objects but not all items were sliceable in {result}"
-                return result
-        return tuple([slice_(value, {dims.name: i}) for i in range(dims.size)])
-    else:  # multiple dimensions
-        if hasattr(value, '__pack_dims__'):
-            packed_dim = batch('_unstack')
-            value_packed = value.__pack_dims__(dims.names, packed_dim, pos=None)
-            if value_packed is not NotImplemented:
-                return unstack(value_packed, packed_dim)
-        unstack_dim = _any_uniform_dim(dims)
-        first_unstacked = unstack(value, unstack_dim)
-        inner_unstacked = [unstack(v, dims.without(unstack_dim)) for v in first_unstacked]
-        return sum(inner_unstacked, ())
-
-
-def _any_uniform_dim(dims: Shape):
-    for dim in dims:
-        if dim.is_uniform:
-            return dim
-    raise ValueError(f"Uniform dimension required but found only non-uniform dimensions {dims}")
-
-
-def stack(values: Union[tuple, list, dict], dim: Shape, expand_values=False, **kwargs):
-    """
-    Stacks `values` along the new dimension `dim`.
-    All values must have the same spatial, instance and channel dimensions. If the dimension sizes vary, the resulting tensor will be non-uniform.
-    Batch dimensions will be added as needed.
-
-    Stacking tensors is performed lazily, i.e. the memory is allocated only when needed.
-    This makes repeated stacking and slicing along the same dimension very efficient, i.e. jit-compiled functions will not perform these operations.
-
-    Args:
-        values: Collection of `phi.math.magic.Shapable`, such as `phi.math.Tensor`
-            If a `dict`, keys must be of type `str` and are used as item names along `dim`.
-        dim: `Shape` with a least one dimension. None of these dimensions can be present with any of the `values`.
-            If `dim` is a single-dimension shape, its size is determined from `len(values)` and can be left undefined (`None`).
-            If `dim` is a multi-dimension shape, its volume must be equal to `len(values)`.
-        expand_values: If `True`, will first add missing dimensions to all values, not just batch dimensions.
-            This allows tensors with different dimensions to be stacked.
-            The resulting tensor will have all dimensions that are present in `values`.
-        **kwargs: Additional keyword arguments required by specific implementations.
-            Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
-            Adding batch dimensions must always work without keyword arguments.
-
-    Returns:
-        `Tensor` containing `values` stacked along `dim`.
-
-    Examples:
-        >>> stack({'x': 0, 'y': 1}, channel('vector'))
-        (x=0, y=1)
-
-        >>> stack([math.zeros(batch(b=2)), math.ones(batch(b=2))], channel(c='x,y'))
-        (x=0.000, y=1.000); (x=0.000, y=1.000) (bᵇ=2, cᶜ=x,y)
-
-        >>> stack([vec(x=1, y=0), vec(x=2, y=3.)], batch('b'))
-        (x=1.000, y=0.000); (x=2.000, y=3.000) (bᵇ=2, vectorᶜ=x,y)
-    """
-    assert len(values) > 0, f"stack() got empty sequence {values}"
-    assert isinstance(dim, Shape)
-    values_ = tuple(values.values()) if isinstance(values, dict) else values
-    if not expand_values:
-        for v in values_[1:]:
-            assert set(non_batch(v).names) == set(non_batch(values_[0]).names), f"Stacked values must have the same non-batch dimensions but got {non_batch(values_[0])} and {non_batch(v)}"
-    # --- Add missing dimensions ---
-    if expand_values:
-        all_dims = merge_shapes(*values_, allow_varying_sizes=True)
-        if isinstance(values, dict):
-            values = {k: expand(v, all_dims.without(shape(v))) for k, v in values.items()}
-        else:
-            values = [expand(v, all_dims.without(shape(v))) for v in values]
-    else:
-        all_batch_dims = merge_shapes(*[batch(v) for v in values_], allow_varying_sizes=True)
-        if isinstance(values, dict):
-            values = {k: expand(v, all_batch_dims.without(shape(v))) for k, v in values.items()}
-        else:
-            values = [expand(v, all_batch_dims.without(shape(v))) for v in values]
-    if dim.rank == 1:
-        assert dim.size == len(values) or dim.size is None, f"stack dim size must match len(values) or be undefined but got {dim} for {len(values)} values"
-        if dim.size is None:
-            dim = dim.with_size(len(values))
-        if isinstance(values, dict):
-            dim_item_names = tuple(values.keys())
-            values = tuple(values.values())
-            dim = dim.with_size(dim_item_names)
-        # --- First try __stack__ ---
-        for v in values:
-            if hasattr(v, '__stack__'):
-                result = v.__stack__(values, dim, **kwargs)
-                if result is not NotImplemented:
-                    assert isinstance(result, Shapable), "__stack__ must return a Shapable object"
-                    return result
-        # --- Next: try stacking attributes for tree nodes ---
-        if all(isinstance(v, PhiTreeNode) for v in values):
-            attributes = all_attributes(values[0])
-            if attributes and all(all_attributes(v) == attributes for v in values):
-                new_attrs = {}
-                for a in attributes:
-                    assert all(dim not in shape(getattr(v, a)) for v in values), f"Cannot stack attribute {a} because one values contains the stack dimension {dim}."
-                    a_values = [getattr(v, a) for v in values]
-                    if all(v is a_values[0] for v in a_values[1:]):
-                        new_attrs[a] = expand(a_values[0], dim, **kwargs)
-                    else:
-                        new_attrs[a] = stack(a_values, dim, expand_values=expand_values, **kwargs)
-                return copy_with(values[0], **new_attrs)
-            else:
-                warnings.warn(f"Failed to concat values using value attributes because attributes differ among values {values}")
-        # --- Fallback: use expand and concat ---
-        for v in values:
-            if not hasattr(v, '__stack__') and hasattr(v, '__concat__') and hasattr(v, '__expand__'):
-                expanded_values = tuple([expand(v, dim.with_size(1 if dim.item_names[0] is None else dim.item_names[0][i]), **kwargs) for i, v in enumerate(values)])
-                if len(expanded_values) > 8:
-                    warnings.warn(f"stack() default implementation is slow on large dimensions ({dim.name}={len(expanded_values)}). Please implement __stack__()", RuntimeWarning, stacklevel=2)
-                result = v.__concat__(expanded_values, dim.name, **kwargs)
-                if result is not NotImplemented:
-                    assert isinstance(result, Shapable), "__concat__ must return a Shapable object"
-                    return result
-        # --- else maybe all values are native scalars ---
-        from ._tensors import wrap
-        try:
-            values = tuple([wrap(v) for v in values])
-        except ValueError:
-            raise MagicNotImplemented(f"At least one item in values must be Shapable but got types {[type(v) for v in values]}")
-        return values[0].__stack__(values, dim, **kwargs)
-    else:  # multi-dim stack
-        assert dim.volume == len(values), f"When passing multiple stack dims, their volume must equal len(values) but got {dim} for {len(values)} values"
-        if isinstance(values, dict):
-            warnings.warn(f"When stacking a dict along multiple dimensions, the key names are discarded. Got keys {tuple(values.keys())}", RuntimeWarning, stacklevel=2)
-            values = tuple(values.values())
-        # --- if any value implements Shapable, use stack and unpack_dim ---
-        for v in values:
-            if hasattr(v, '__stack__') and hasattr(v, '__unpack_dim__'):
-                stack_dim = batch('_stack')
-                stacked = v.__stack__(values, stack_dim, **kwargs)
-                if stacked is not NotImplemented:
-                    assert isinstance(stacked, Shapable), "__stack__ must return a Shapable object"
-                    assert hasattr(stacked, '__unpack_dim__'), "If a value supports __unpack_dim__, the result of __stack__ must also support it."
-                    reshaped = stacked.__unpack_dim__(stack_dim.name, dim, **kwargs)
-                    if kwargs is NotImplemented:
-                        warnings.warn("__unpack_dim__ is overridden but returned NotImplemented during multi-dimensional stack. This results in unnecessary stack operations.", RuntimeWarning, stacklevel=2)
-                    else:
-                        return reshaped
-        # --- Fallback: multi-level stack ---
-        for dim_ in reversed(dim):
-            values = [stack(values[i:i + dim_.size], dim_, **kwargs) for i in range(0, len(values), dim_.size)]
-        return values[0]
-
-
-def concat(values: Union[tuple, list], dim: Union[str, Shape], **kwargs):
-    """
-    Concatenates a sequence of `phi.math.magic.Shapable` objects, e.g. `Tensor`, along one dimension.
-    All values must have the same spatial, instance and channel dimensions and their sizes must be equal, except for `dim`.
-    Batch dimensions will be added as needed.
-
-    Args:
-        values: Tuple or list of `phi.math.magic.Shapable`, such as `phi.math.Tensor`
-        dim: Concatenation dimension, must be present in all `values`.
-            The size along `dim` is determined from `values` and can be set to undefined (`None`).
-        **kwargs: Additional keyword arguments required by specific implementations.
-            Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
-            Adding batch dimensions must always work without keyword arguments.
-
-    Returns:
-        Concatenated `Tensor`
-
-    Examples:
-        >>> concat([math.zeros(batch(b=10)), math.ones(batch(b=10))], 'b')
-        (bᵇ=20) 0.500 ± 0.500 (0e+00...1e+00)
-
-        >>> concat([vec(x=1, y=0), vec(z=2.)], 'vector')
-        (x=1.000, y=0.000, z=2.000) float64
-    """
-    assert len(values) > 0, f"concat() got empty sequence {values}"
-    if isinstance(dim, Shape):
-        dim = dim.name
-    assert isinstance(dim, str), f"dim must be a str or Shape but got '{dim}' of type {type(dim)}"
-    for v in values:
-        assert dim in shape(v), f"dim must be present in the shapes of all values bot got value {type(v).__name__} with shape {shape(v)}"
-    for v in values[1:]:
-        assert set(non_batch(v).names) == set(non_batch(values[0]).names), f"Concatenated values must have the same non-batch dimensions but got {non_batch(values[0])} and {non_batch(v)}"
-    # Add missing batch dimensions
-    all_batch_dims = merge_shapes(*[batch(v) for v in values])
-    values = [expand(v, all_batch_dims) for v in values]
-    # --- First try __concat__ ---
-    for v in values:
-        if isinstance(v, Shapable):
-            if hasattr(v, '__concat__'):
-                result = v.__concat__(values, dim, **kwargs)
-                if result is not NotImplemented:
-                    assert isinstance(result, Shapable), f"__concat__ must return a Shapable object but got {type(result).__name__} from {type(v).__name__} {v}"
-                    return result
-    # --- Next: try concat attributes for tree nodes ---
-    if all(isinstance(v, PhiTreeNode) for v in values):
-        attributes = all_attributes(values[0])
-        if attributes and all(all_attributes(v) == attributes for v in values):
-            new_attrs = {}
-            for a in attributes:
-                common_shape = merge_shapes(*[shape(getattr(v, a)).without(dim) for v in values])
-                a_values = [expand(getattr(v, a), common_shape & shape(v).only(dim)) for v in values]  # expand by dim if missing, and dims of others
-                new_attrs[a] = concat(a_values, dim, **kwargs)
-            return copy_with(values[0], **new_attrs)
-        else:
-            warnings.warn(f"Failed to concat values using value attributes because attributes differ among values {values}")
-    # --- Fallback: slice and stack ---
-    try:
-        unstacked = sum([unstack(v, dim) for v in values], ())
-    except MagicNotImplemented:
-        raise MagicNotImplemented(f"concat: No value implemented __concat__ and not all values were Sliceable along {dim}. values = {[type(v) for v in values]}")
-    if len(unstacked) > 8:
-        warnings.warn(f"concat() default implementation is slow on large dimensions ({dim}={len(unstacked)}). Please implement __concat__()", RuntimeWarning, stacklevel=2)
-    dim = shape(values[0])[dim].with_size(None)
-    try:
-        return stack(unstacked, dim, **kwargs)
-    except MagicNotImplemented:
-        raise MagicNotImplemented(f"concat: No value implemented __concat__ and slices could not be stacked. values = {[type(v) for v in values]}")
-
-
-def expand(value, *dims: Shape, **kwargs):
-    """
-    Adds dimensions to a `Tensor` or tensor-like object by implicitly repeating the tensor values along the new dimensions.
-    If `value` already contains any of the new dimensions, a size and type check is performed for these instead.
-
-    If any of `dims` varies along a dimension that is present neither in `value` nor on `dims`, it will also be added to `value`.
-
-    This function replaces the usual `tile` / `repeat` functions of
-    [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.tile.html),
-    [PyTorch](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.repeat),
-    [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/tile) and
-    [Jax](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.tile.html).
-
-    Additionally, it replaces the traditional `unsqueeze` / `expand_dims` functions.
-
-    Args:
-        value: `phi.math.magic.Shapable`, such as `phi.math.Tensor`
-            For tree nodes, expands all value attributes by `dims` or the first variable attribute if no value attributes are set.
-        *dims: Dimensions to be added as `Shape`
-        **kwargs: Additional keyword arguments required by specific implementations.
-            Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
-            Adding batch dimensions must always work without keyword arguments.
-
-    Returns:
-        Same type as `value`.
-    """
-    dims = concat_shapes(*dims)
-    combined = merge_shapes(value, dims)  # check that existing sizes match
-    if not dims.without(shape(value)):  # no new dims to add
-        if set(dims) == set(shape(value).only(dims)):  # sizes and item names might differ, though
-            return value
-    dims &= combined.shape.without('dims')  # add missing non-uniform dims
-    # --- First try __stack__
-    if hasattr(value, '__expand__'):
-        result = value.__expand__(dims, **kwargs)
-        if result is not NotImplemented:
-            return result
-    # --- Next try Tree Node ---
-    if isinstance(value, PhiTreeNode):
-        attributes = value_attributes(value) if hasattr(value, '__value_attrs__') else [variable_attributes(value)[0]]
-        new_attributes = {a: expand(getattr(value, a), dims, **kwargs) for a in attributes}
-        return copy_with(value, **new_attributes)
-    # --- Fallback: stack ---
-    if hasattr(value, '__stack__'):
-        if dims.volume > 8:
-            warnings.warn(f"expand() default implementation is slow on large shapes {dims}. Please implement __expand__() for {type(value).__name__} as defined in phi.math.magic", RuntimeWarning, stacklevel=2)
-        for dim in reversed(dims):
-            value = stack((value,) * dim.size, dim, **kwargs)
-            assert value is not NotImplemented, "Value must implement either __expand__ or __stack__"
-        return value
-    try:  # value may be a native scalar
-        from ._ops import expand_tensor
-        from ._tensors import wrap
-        value = wrap(value)
-    except ValueError:
-        raise AssertionError(f"Cannot expand non-shapable object {type(value)}")
-    return expand_tensor(value, dims)
-
-
-def rename_dims(value,
-                dims: DimFilter,
-                names: DimFilter,
-                **kwargs):
-    """
-    Change the name and optionally the type of some dimensions of `value`.
-
-    Dimensions that are not present on value will be ignored. The corresponding new dimensions given by `names` will not be added.
-
-    Args:
-        value: `Shape` or `Tensor` or `Shapable`.
-        dims: Existing dimensions of `value` as comma-separated `str`, `tuple`, `list`, `Shape` or filter function.
-        names: Either
-
-            * Sequence of names matching `dims` as `tuple`, `list` or `str`. This replaces only the dimension names but leaves the types untouched.
-            * `Shape` matching `dims` to replace names and types.
-            * Dimension type function to replace only types.
-
-        **kwargs: Additional keyword arguments required by specific implementations.
-            Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
-            Adding batch dimensions must always work without keyword arguments.
-
-    Returns:
-        Same type as `value`.
-    """
-    if isinstance(value, Shape):
-        return value._replace_names_and_types(dims, names)
-    elif isinstance(value, (Number, bool)):
-        return value
-    assert isinstance(value, Shapable) and isinstance(value, Shaped), f"value must be a Shape or Shapable but got {type(value).__name__}"
-    dims = shape(value).only(dims).names if callable(dims) else parse_dim_order(dims)
-    if isinstance(names, str):
-        names = parse_dim_order(names)
-    elif callable(names):
-        names: Shape = names(*dims).with_sizes(shape(value))
-    assert len(dims) == len(names), f"names and dims must be of equal length but got #dims={len(dims)} and #names={len(names)}"
-    existing_dims = shape(value).only(dims, reorder=True)
-    if not existing_dims:
-        return value
-    existing_names = [n for i, n in enumerate(names) if dims[i] in existing_dims]
-    existing_names = existing_dims._replace_names_and_types(existing_dims, existing_names)
-    # --- First try __replace_dims__ ---
-    if hasattr(value, '__replace_dims__'):
-        result = value.__replace_dims__(existing_dims.names, existing_names, **kwargs)
-        if result is not NotImplemented:
-            return result
-    # --- Next try Tree Node ---
-    if isinstance(value, PhiTreeNode):
-        new_attributes = {a: rename_dims(getattr(value, a), existing_dims, existing_names, **kwargs) for a in all_attributes(value)}
-        return copy_with(value, **new_attributes)
-    # --- Fallback: unstack and stack ---
-    if shape(value).only(existing_dims).volume > 8:
-        warnings.warn(f"rename_dims() default implementation is slow on large dimensions ({shape(value).only(dims)}). Please implement __replace_dims__() for {type(value).__name__} as defined in phi.math.magic", RuntimeWarning, stacklevel=2)
-    for old_name, new_dim in zip(existing_dims.names, existing_names):
-        value = stack(unstack(value, old_name), new_dim, **kwargs)
-    return value
-
-
-def pack_dims(value, dims: DimFilter, packed_dim: Shape, pos: Union[int, None] = None, **kwargs):
-    """
-    Compresses multiple dimensions into a single dimension by concatenating the elements.
-    Elements along the new dimensions are laid out according to the order of `dims`.
-    If the order of `dims` differs from the current dimension order, the tensor is transposed accordingly.
-    This function replaces the traditional `reshape` for these cases.
-
-    The type of the new dimension will be equal to the types of `dims`.
-    If `dims` have varying types, the new dimension will be a batch dimension.
-
-    If none of `dims` exist on `value`, `packed_dim` will be added only if it is given with a definite size and `value` is not a primitive type.
-
-    See Also:
-        `unpack_dim()`
-
-    Args:
-        value: `phi.math.magic.Shapable`, such as `phi.math.Tensor`.
-        dims: Dimensions to be compressed in the specified order.
-        packed_dim: Single-dimension `Shape`.
-        pos: Index of new dimension. `None` for automatic, `-1` for last, `0` for first.
-        **kwargs: Additional keyword arguments required by specific implementations.
-            Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
-            Adding batch dimensions must always work without keyword arguments.
-
-    Returns:
-        Same type as `value`.
-
-    Examples:
-        >>> pack_dims(math.zeros(spatial(x=4, y=3)), spatial, instance('points'))
-        (pointsⁱ=12) const 0.0
-    """
-    if isinstance(value, (Number, bool)):
-        return value
-    assert isinstance(value, Shapable) and isinstance(value, Sliceable) and isinstance(value, Shaped), f"value must be Shapable but got {type(value)}"
-    dims = shape(value).only(dims, reorder=True)
-    if packed_dim in shape(value):
-        assert packed_dim in dims, f"Cannot pack dims into new dimension {packed_dim} because it already exists on value {value} and is not packed."
-    if len(dims) == 0 or all(dim not in shape(value) for dim in dims):
-        return value if packed_dim.size is None else expand(value, packed_dim, **kwargs)  # Inserting size=1 can cause shape errors
-    elif len(dims) == 1:
-        return rename_dims(value, dims, packed_dim, **kwargs)
-    # --- First try __pack_dims__ ---
-    if hasattr(value, '__pack_dims__'):
-        result = value.__pack_dims__(dims.names, packed_dim, pos, **kwargs)
-        if result is not NotImplemented:
-            return result
-    # --- Next try Tree Node ---
-    if isinstance(value, PhiTreeNode):
-        new_attributes = {a: pack_dims(getattr(value, a), dims, packed_dim, pos=pos, **kwargs) for a in all_attributes(value)}
-        return copy_with(value, **new_attributes)
-    # --- Fallback: unstack and stack ---
-    if shape(value).only(dims).volume > 8:
-        warnings.warn(f"pack_dims() default implementation is slow on large dimensions ({shape(value).only(dims)}). Please implement __pack_dims__() for {type(value).__name__} as defined in phi.math.magic", RuntimeWarning, stacklevel=2)
-    return stack(unstack(value, dims), packed_dim, **kwargs)
-
-
-
-
-def unpack_dim(value, dim: Union[str, Shape], *unpacked_dims: Shape, **kwargs):
-    """
-    Decompresses a dimension by unstacking the elements along it.
-    This function replaces the traditional `reshape` for these cases.
-    The compressed dimension `dim` is assumed to contain elements laid out according to the order of `unpacked_dims`.
-
-    If `dim` does not exist on `value`, this function will return `value` as-is. This includes primitive types.
-
-    See Also:
-        `pack_dims()`
-
-    Args:
-        value: `phi.math.magic.Shapable`, such as `Tensor`, for which one dimension should be split.
-        dim: Dimension to be decompressed.
-        *unpacked_dims: Vararg `Shape`, ordered dimensions to replace `dim`, fulfilling `unpacked_dims.volume == shape(self)[dim].rank`.
-        **kwargs: Additional keyword arguments required by specific implementations.
-            Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
-            Adding batch dimensions must always work without keyword arguments.
-
-    Returns:
-        Same type as `value`.
-
-    Examples:
-        >>> unpack_dim(math.zeros(instance(points=12)), 'points', spatial(x=4, y=3))
-        (xˢ=4, yˢ=3) const 0.0
-    """
-    if isinstance(value, (Number, bool)):
-        return value
-    assert isinstance(value, Shapable) and isinstance(value, Sliceable) and isinstance(value, Shaped), f"value must be Shapable but got {type(value)}"
-    if isinstance(dim, Shape):
-        dim = dim.name
-    assert isinstance(dim, str), f"dim must be a str or Shape but got {type(dim)}"
-    if dim not in shape(value):
-        return value  # Nothing to do, maybe expand?
-    unpacked_dims = concat_shapes(*unpacked_dims)
-    if unpacked_dims.rank == 0:
-        return value[{dim: 0}]  # remove dim
-    elif unpacked_dims.rank == 1:
-        return rename_dims(value, dim, unpacked_dims, **kwargs)
-    # --- First try __unpack_dim__
-    if hasattr(value, '__unpack_dim__'):
-        result = value.__unpack_dim__(dim, unpacked_dims, **kwargs)
-        if result is not NotImplemented:
-            return result
-    # --- Next try Tree Node ---
-    if isinstance(value, PhiTreeNode) and all_attributes(value):
-        new_attributes = {a: unpack_dim(getattr(value, a), dim, unpacked_dims, **kwargs) for a in all_attributes(value)}
-        return copy_with(value, **new_attributes)
-    # --- Fallback: unstack and stack ---
-    if shape(value).only(dim).volume > 8:
-        warnings.warn(f"pack_dims() default implementation is slow on large dimensions ({shape(value).only(dim)}). Please implement __unpack_dim__() for {type(value).__name__} as defined in phi.math.magic", RuntimeWarning, stacklevel=2)
-    unstacked = unstack(value, dim)
-    for dim in reversed(unpacked_dims):
-        unstacked = [stack(unstacked[i:i+dim.size], dim, **kwargs) for i in range(0, len(unstacked), dim.size)]
-    return unstacked[0]
-
-
-def flatten(value, flat_dim: Shape = instance('flat'), flatten_batch=False, **kwargs):
-    """
-    Returns a `Tensor` with the same values as `value` but only a single dimension `flat_dim`.
-    The order of the values in memory is not changed.
-
-    Args:
-        value: `phi.math.magic.Shapable`, such as `Tensor`.
-        flat_dim: Dimension name and type as `Shape` object. The size is ignored.
-        flatten_batch: Whether to flatten batch dimensions as well.
-            If `False`, batch dimensions are kept, only onn-batch dimensions are flattened.
-        **kwargs: Additional keyword arguments required by specific implementations.
-            Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
-            Adding batch dimensions must always work without keyword arguments.
-
-    Returns:
-        Same type as `value`.
-
-    Examples:
-        >>> flatten(math.zeros(spatial(x=4, y=3)))
-        (flatⁱ=12) const 0.0
-    """
-    assert isinstance(flat_dim, Shape) and flat_dim.rank == 1, flat_dim
-    assert isinstance(value, Shapable) and isinstance(value, Shaped), f"value must be Shapable but got {type(value)}"
-    # --- First try __flatten__ ---
-    if hasattr(value, '__flatten__'):
-        result = value.__flatten__(flat_dim, flatten_batch, **kwargs)
-        if result is not NotImplemented:
-            return result
-    # There is no tree node implementation for flatten because pack_dims is just as fast
-    # --- Fallback: pack_dims ---
-    return pack_dims(value, shape(value) if flatten_batch else non_batch(value), flat_dim, **kwargs)
-
-
-# PhiTreeNode
-
-PhiTreeNodeType = TypeVar('PhiTreeNodeType')  # Defined in phi.math.magic: tuple, list, dict, custom
-
-
-def variable_attributes(obj) -> Tuple[str]:
-    if hasattr(obj, '__variable_attrs__'):
-        return obj.__variable_attrs__()
-    elif hasattr(obj, '__value_attrs__'):
-        return obj.__value_attrs__()
-    elif dataclasses.is_dataclass(obj):
-        return tuple([f.name for f in dataclasses.fields(obj)])
-    else:
-        raise ValueError(f"Not a PhiTreeNode: {type(obj).__name__}")
-
-
-def value_attributes(obj) -> Tuple[str, ...]:
-    if hasattr(obj, '__value_attrs__'):
-        return obj.__value_attrs__()
-    if dataclasses.is_dataclass(obj):
-        return tuple([f.name for f in dataclasses.fields(obj)])
-    raise ValueError(f"{type(obj).__name__} must implement '__value_attrs__()' or be a dataclass to be used with value functions.")
-
-
-def variable_values(obj) -> Tuple[str, ...]:
-    if hasattr(obj, '__variable_attrs__'):
-        values = obj.__value_attrs__()
-        variables = obj.__variable_attrs__()
-        return tuple([a for a in values if a in variables])
-    else:
-        return obj.__value_attrs__()  # this takes care of dataclasses as well
-
-
-def all_attributes(obj, assert_any=False) -> Set[str]:
-    if not isinstance(obj, PhiTreeNode):
-        raise ValueError(f"Not a PhiTreeNode: {type(obj).__name__}")
-    result = set()
-    if hasattr(obj, '__variable_attrs__'):
-        result.update(obj.__variable_attrs__())
-    if hasattr(obj, '__value_attrs__'):
-        result.update(obj.__value_attrs__())
-    if dataclasses.is_dataclass(obj) and not hasattr(obj, '__variable_attrs__') and not hasattr(obj, '__value_attrs__'):
-        result.update([f.name for f in dataclasses.fields(obj)])
-    if assert_any:
-        assert result, f"{type(obj).__name__} is not a valid tree node because it has no tensor-like attributes."
-    return result
-
-
-def replace(obj: PhiTreeNodeType, **updates) -> PhiTreeNodeType:
-    """
-    Creates a copy of the given `phi.math.magic.PhiTreeNode` with updated values as specified in `updates`.
-
-    If `obj` overrides `__with_attrs__`, the copy will be created via that specific implementation.
-    Otherwise, the `copy` module and `setattr` will be used.
-
-    Args:
-        obj: `phi.math.magic.PhiTreeNode`
-        **updates: Values to be replaced.
-
-    Returns:
-        Copy of `obj` with updated values.
-    """
-    if hasattr(obj, '__with_attrs__'):
-        return obj.__with_attrs__(**updates)
-    elif isinstance(obj, (Number, bool)):
-        return obj
-    elif dataclasses.is_dataclass(obj):
-        return dataclasses.replace(obj, **updates)
-    else:
-        cpy = copy.copy(obj)
-        for attr, value in updates.items():
-            setattr(cpy, attr, value)
-        return cpy
-
-
-copy_with = replace
-
-
-# Other Ops
-
-MagicType = TypeVar('MagicType')
-OtherMagicType = TypeVar('OtherMagicType')
-
-
-def cast(x: MagicType, dtype: Union[DType, type]) -> OtherMagicType:
-    """
-    Casts `x` to a different data type.
-
-    Implementations:
-
-    * NumPy: [`x.astype()`](numpy.ndarray.astype)
-    * PyTorch: [`x.to()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.to)
-    * TensorFlow: [`tf.cast`](https://www.tensorflow.org/api_docs/python/tf/cast)
-    * Jax: [`jax.numpy.array`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.array.html)
-
-    See Also:
-        `to_float`, `to_int32`, `to_int64`, `to_complex`.
-
-    Args:
-        x: `Tensor`
-        dtype: New data type as `phi.math.DType`, e.g. `DType(int, 16)`.
-
-    Returns:
-        `Tensor` with data type `dtype`
-    """
-    if not isinstance(dtype, DType):
-        dtype = DType.as_dtype(dtype)
-    if hasattr(x, '__cast__'):
-        return x.__cast__(dtype)
-    elif isinstance(x, (Number, bool)):
-        return dtype.kind(x)
-    elif isinstance(x, PhiTreeNode):
-        attrs = {key: getattr(x, key) for key in value_attributes(x)}
-        new_attrs = {k: cast(v, dtype) for k, v in attrs.items()}
-        return copy_with(x, **new_attrs)
-    try:
-        backend = choose_backend(x)
-        return backend.cast(x, dtype)
-    except NoBackendFound:
-        if dtype.kind == bool:
-            return bool(x)
-        raise ValueError(f"Cannot cast object of type '{type(x).__name__}'")
-
-
-def bool_to_int(x: MagicType, bits=32):
-    if isinstance(x, bool):
-        return int(x)
-    if isinstance(x, Number):
-        return x
-    if hasattr(x, 'dtype') and isinstance(x.dtype, DType):
-        return cast(x, DType(int, bits)) if x.dtype.kind == bool else x
-    elif isinstance(x, PhiTreeNode):
-        return tree_map(bool_to_int, x, bits=32)
-    try:
-        backend = choose_backend(x)
-        return backend.cast(x, DType(int, bits)) if backend.dtype(x).kind == bool else x
-    except NoBackendFound:
-        raise ValueError(f"Cannot cast object of type '{type(x).__name__}'")
-
-
-def tree_map(f, tree, **f_kwargs):
-    from ._tensors import Tensor
-    if isinstance(tree, Tensor):
-        return f(tree, **f_kwargs)
-    if isinstance(tree, list):
-        return [tree_map(f, e, **f_kwargs) for e in tree]
-    elif isinstance(tree, tuple):
-        return tuple([tree_map(f, e, **f_kwargs) for e in tree])
-    elif isinstance(tree, dict):
-        return {k: tree_map(f, e, **f_kwargs) for k, e in tree.items()}
-    elif isinstance(tree, PhiTreeNode):
-        attrs = {key: getattr(tree, key) for key in value_attributes(tree)}
-        new_attrs = {k: tree_map(f, v, **f_kwargs) for k, v in attrs.items()}
-        return copy_with(tree, **new_attrs)
-    else:
-        return f(tree, **f_kwargs)  # try anyway
+import copy
+import warnings
+from numbers import Number
+from typing import TypeVar, Tuple, Set, Dict, Union, Optional
+
+import dataclasses
+
+from . import channel
+from .backend import choose_backend, NoBackendFound
+from .backend._dtype import DType
+from ._shape import Shape, DimFilter, batch, instance, shape, non_batch, merge_shapes, concat_shapes, spatial, parse_dim_order, dual
+from .magic import Sliceable, Shaped, Shapable, PhiTreeNode
+
+
+class MagicNotImplemented(Exception): pass
+
+
+def slice_(value, slices: Dict[str, Union[int, slice, str, tuple, list]]):
+    """
+    Slices a `Tensor` or `phi.math.magic.PhiTreeNode` along named dimensions.
+
+    See Also:
+        `unstack`.
+
+    Args:
+        value: `Tensor` or `phi.math.magic.PhiTreeNode`
+        slices: `dict` mapping dimension names to slices. A slice can be one of the following:
+
+            * An index (`int`)
+            * A range (`slice`)
+            * An item name (`str`)
+            * Multiple item names (comma-separated `str`)
+            * Multiple indices or item names (`tuple` or `list`)
+
+    Returns:
+        `Tensor` or `phi.math.magic.PhiTreeNode` of the same type as `value`.
+
+    Examples:
+        >>> math.slice([vec(x=0, y=1), vec(x=2, y=3)], {'vector': 'y'})
+        [1, 3]
+    """
+    if isinstance(value, (bool, Number)):
+        return value
+    if isinstance(value, tuple):
+        return tuple([slice_(v, slices) for v in value])
+    if isinstance(value, list):
+        return [slice_(v, slices) for v in value]
+    if isinstance(value, dict):
+        return {k: slice_(v, slices) for k, v in value.items()}
+    if isinstance(value, Shape):
+        raise NotImplementedError
+    if hasattr(value, '__getitem__'):
+        return value[slices]
+    if isinstance(value, PhiTreeNode):
+        attrs = {key: getattr(value, key) for key in value_attributes(value)}
+        new_attrs = {k: slice_(v, slices) for k, v in attrs.items()}
+        return copy_with(value, **new_attrs)
+    raise ValueError(f"value must be a PhiTreeNode but got {type(value)}")
+
+
+def unstack(value, dim: DimFilter):
+    """
+    Un-stacks a `Sliceable` along one or multiple dimensions.
+
+    If multiple dimensions are given, the order of elements will be according to the dimension order in `dim`, i.e. elements along the last dimension will be neighbors in the returned `tuple`.
+
+    See Also:
+        `phi.math.slice`.
+
+    Args:
+        value: `phi.math.magic.Shapable`, such as `phi.math.Tensor`
+        dim: Dimensions as `Shape` or comma-separated `str` or dimension type, i.e. `channel`, `spatial`, `instance`, `batch`.
+
+    Returns:
+        `tuple` of `Tensor` objects.
+
+    Examples:
+        >>> unstack(expand(0, spatial(x=5)), 'x')
+        (0.0, 0.0, 0.0, 0.0, 0.0)
+    """
+    assert isinstance(value, Sliceable) and isinstance(value, Shaped), f"Cannot unstack {type(value).__name__}. Must be Sliceable and Shaped, see https://tum-pbs.github.io/PhiFlow/phi/math/magic.html"
+    dims = shape(value).only(dim)
+    assert dims.rank > 0, "unstack() requires at least one dimension"
+    if dims.rank == 1:
+        if hasattr(value, '__unstack__'):
+            result = value.__unstack__(dims.names)
+            if result is not NotImplemented:
+                assert isinstance(result, tuple), f"__unstack__ must return a tuple but got {type(result)}"
+                assert all([isinstance(item, Sliceable) for item in result]), f"__unstack__ must return a tuple of Sliceable objects but not all items were sliceable in {result}"
+                return result
+        return tuple([slice_(value, {dims.name: i}) for i in range(dims.size)])
+    else:  # multiple dimensions
+        if hasattr(value, '__pack_dims__'):
+            packed_dim = batch('_unstack')
+            value_packed = value.__pack_dims__(dims.names, packed_dim, pos=None)
+            if value_packed is not NotImplemented:
+                return unstack(value_packed, packed_dim)
+        unstack_dim = _any_uniform_dim(dims)
+        first_unstacked = unstack(value, unstack_dim)
+        inner_unstacked = [unstack(v, dims.without(unstack_dim)) for v in first_unstacked]
+        return sum(inner_unstacked, ())
+
+
+def _any_uniform_dim(dims: Shape):
+    for dim in dims:
+        if dim.is_uniform:
+            return dim
+    raise ValueError(f"Uniform dimension required but found only non-uniform dimensions {dims}")
+
+
+def stack(values: Union[tuple, list, dict], dim: Shape, expand_values=False, **kwargs):
+    """
+    Stacks `values` along the new dimension `dim`.
+    All values must have the same spatial, instance and channel dimensions. If the dimension sizes vary, the resulting tensor will be non-uniform.
+    Batch dimensions will be added as needed.
+
+    Stacking tensors is performed lazily, i.e. the memory is allocated only when needed.
+    This makes repeated stacking and slicing along the same dimension very efficient, i.e. jit-compiled functions will not perform these operations.
+
+    Args:
+        values: Collection of `phi.math.magic.Shapable`, such as `phi.math.Tensor`
+            If a `dict`, keys must be of type `str` and are used as item names along `dim`.
+        dim: `Shape` with a least one dimension. None of these dimensions can be present with any of the `values`.
+            If `dim` is a single-dimension shape, its size is determined from `len(values)` and can be left undefined (`None`).
+            If `dim` is a multi-dimension shape, its volume must be equal to `len(values)`.
+        expand_values: If `True`, will first add missing dimensions to all values, not just batch dimensions.
+            This allows tensors with different dimensions to be stacked.
+            The resulting tensor will have all dimensions that are present in `values`.
+        **kwargs: Additional keyword arguments required by specific implementations.
+            Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
+            Adding batch dimensions must always work without keyword arguments.
+
+    Returns:
+        `Tensor` containing `values` stacked along `dim`.
+
+    Examples:
+        >>> stack({'x': 0, 'y': 1}, channel('vector'))
+        (x=0, y=1)
+
+        >>> stack([math.zeros(batch(b=2)), math.ones(batch(b=2))], channel(c='x,y'))
+        (x=0.000, y=1.000); (x=0.000, y=1.000) (bᵇ=2, cᶜ=x,y)
+
+        >>> stack([vec(x=1, y=0), vec(x=2, y=3.)], batch('b'))
+        (x=1.000, y=0.000); (x=2.000, y=3.000) (bᵇ=2, vectorᶜ=x,y)
+    """
+    assert len(values) > 0, f"stack() got empty sequence {values}"
+    assert isinstance(dim, Shape)
+    values_ = tuple(values.values()) if isinstance(values, dict) else values
+    if not expand_values:
+        for v in values_[1:]:
+            assert set(non_batch(v).names) == set(non_batch(values_[0]).names), f"Stacked values must have the same non-batch dimensions but got {non_batch(values_[0])} and {non_batch(v)}"
+    # --- Add missing dimensions ---
+    if expand_values:
+        all_dims = merge_shapes(*values_, allow_varying_sizes=True)
+        if isinstance(values, dict):
+            values = {k: expand(v, all_dims.without(shape(v))) for k, v in values.items()}
+        else:
+            values = [expand(v, all_dims.without(shape(v))) for v in values]
+    else:
+        all_batch_dims = merge_shapes(*[batch(v) for v in values_], allow_varying_sizes=True)
+        if isinstance(values, dict):
+            values = {k: expand(v, all_batch_dims.without(shape(v))) for k, v in values.items()}
+        else:
+            values = [expand(v, all_batch_dims.without(shape(v))) for v in values]
+    if dim.rank == 1:
+        assert dim.size == len(values) or dim.size is None, f"stack dim size must match len(values) or be undefined but got {dim} for {len(values)} values"
+        if dim.size is None:
+            dim = dim.with_size(len(values))
+        if isinstance(values, dict):
+            dim_item_names = tuple(values.keys())
+            values = tuple(values.values())
+            dim = dim.with_size(dim_item_names)
+        # --- First try __stack__ ---
+        for v in values:
+            if hasattr(v, '__stack__'):
+                result = v.__stack__(values, dim, **kwargs)
+                if result is not NotImplemented:
+                    assert isinstance(result, Shapable), "__stack__ must return a Shapable object"
+                    return result
+        # --- Next: try stacking attributes for tree nodes ---
+        if all(isinstance(v, PhiTreeNode) for v in values):
+            attributes = all_attributes(values[0])
+            if attributes and all(all_attributes(v) == attributes for v in values):
+                new_attrs = {}
+                for a in attributes:
+                    assert all(dim not in shape(getattr(v, a)) for v in values), f"Cannot stack attribute {a} because one values contains the stack dimension {dim}."
+                    a_values = [getattr(v, a) for v in values]
+                    if all(v is a_values[0] for v in a_values[1:]):
+                        new_attrs[a] = expand(a_values[0], dim, **kwargs)
+                    else:
+                        new_attrs[a] = stack(a_values, dim, expand_values=expand_values, **kwargs)
+                return copy_with(values[0], **new_attrs)
+            else:
+                warnings.warn(f"Failed to concat values using value attributes because attributes differ among values {values}")
+        # --- Fallback: use expand and concat ---
+        for v in values:
+            if not hasattr(v, '__stack__') and hasattr(v, '__concat__') and hasattr(v, '__expand__'):
+                expanded_values = tuple([expand(v, dim.with_size(1 if dim.item_names[0] is None else dim.item_names[0][i]), **kwargs) for i, v in enumerate(values)])
+                if len(expanded_values) > 8:
+                    warnings.warn(f"stack() default implementation is slow on large dimensions ({dim.name}={len(expanded_values)}). Please implement __stack__()", RuntimeWarning, stacklevel=2)
+                result = v.__concat__(expanded_values, dim.name, **kwargs)
+                if result is not NotImplemented:
+                    assert isinstance(result, Shapable), "__concat__ must return a Shapable object"
+                    return result
+        # --- else maybe all values are native scalars ---
+        from ._tensors import wrap
+        try:
+            values = tuple([wrap(v) for v in values])
+        except ValueError:
+            raise MagicNotImplemented(f"At least one item in values must be Shapable but got types {[type(v) for v in values]}")
+        return values[0].__stack__(values, dim, **kwargs)
+    else:  # multi-dim stack
+        assert dim.volume == len(values), f"When passing multiple stack dims, their volume must equal len(values) but got {dim} for {len(values)} values"
+        if isinstance(values, dict):
+            warnings.warn(f"When stacking a dict along multiple dimensions, the key names are discarded. Got keys {tuple(values.keys())}", RuntimeWarning, stacklevel=2)
+            values = tuple(values.values())
+        # --- if any value implements Shapable, use stack and unpack_dim ---
+        for v in values:
+            if hasattr(v, '__stack__') and hasattr(v, '__unpack_dim__'):
+                stack_dim = batch('_stack')
+                stacked = v.__stack__(values, stack_dim, **kwargs)
+                if stacked is not NotImplemented:
+                    assert isinstance(stacked, Shapable), "__stack__ must return a Shapable object"
+                    assert hasattr(stacked, '__unpack_dim__'), "If a value supports __unpack_dim__, the result of __stack__ must also support it."
+                    reshaped = stacked.__unpack_dim__(stack_dim.name, dim, **kwargs)
+                    if kwargs is NotImplemented:
+                        warnings.warn("__unpack_dim__ is overridden but returned NotImplemented during multi-dimensional stack. This results in unnecessary stack operations.", RuntimeWarning, stacklevel=2)
+                    else:
+                        return reshaped
+        # --- Fallback: multi-level stack ---
+        for dim_ in reversed(dim):
+            values = [stack(values[i:i + dim_.size], dim_, **kwargs) for i in range(0, len(values), dim_.size)]
+        return values[0]
+
+
+def concat(values: Union[tuple, list], dim: Union[str, Shape], expand_values=False, **kwargs):
+    """
+    Concatenates a sequence of `phi.math.magic.Shapable` objects, e.g. `Tensor`, along one dimension.
+    All values must have the same spatial, instance and channel dimensions and their sizes must be equal, except for `dim`.
+    Batch dimensions will be added as needed.
+
+    Args:
+        values: Tuple or list of `phi.math.magic.Shapable`, such as `phi.math.Tensor`
+        dim: Concatenation dimension, must be present in all `values`.
+            The size along `dim` is determined from `values` and can be set to undefined (`None`).
+        expand_values: If `True`, will first add missing dimensions to all values, not just batch dimensions.
+            This allows tensors with different dimensions to be concatenated.
+            The resulting tensor will have all dimensions that are present in `values`.
+        **kwargs: Additional keyword arguments required by specific implementations.
+            Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
+            Adding batch dimensions must always work without keyword arguments.
+
+    Returns:
+        Concatenated `Tensor`
+
+    Examples:
+        >>> concat([math.zeros(batch(b=10)), math.ones(batch(b=10))], 'b')
+        (bᵇ=20) 0.500 ± 0.500 (0e+00...1e+00)
+
+        >>> concat([vec(x=1, y=0), vec(z=2.)], 'vector')
+        (x=1.000, y=0.000, z=2.000) float64
+    """
+    assert len(values) > 0, f"concat() got empty sequence {values}"
+    if isinstance(dim, Shape):
+        dim = dim.name
+    assert isinstance(dim, str), f"dim must be a str or Shape but got '{dim}' of type {type(dim)}"
+    # Add missing dimensions
+    if expand_values:
+        all_dims = merge_shapes(*values, allow_varying_sizes=True)
+        all_dims = all_dims.with_dim_size(dim, 1, keep_item_names=False)
+        values = [expand(v, all_dims.without(shape(v))) for v in values]
+    else:
+        for v in values:
+            assert dim in shape(v), f"dim must be present in the shapes of all values bot got value {type(v).__name__} with shape {shape(v)}"
+        for v in values[1:]:
+            assert set(non_batch(v).names) == set(non_batch(values[0]).names), f"Concatenated values must have the same non-batch dimensions but got {non_batch(values[0])} and {non_batch(v)}"
+        all_batch_dims = merge_shapes(*[batch(v) for v in values])
+        values = [expand(v, all_batch_dims) for v in values]
+    # --- First try __concat__ ---
+    for v in values:
+        if isinstance(v, Shapable):
+            if hasattr(v, '__concat__'):
+                result = v.__concat__(values, dim, **kwargs)
+                if result is not NotImplemented:
+                    assert isinstance(result, Shapable), f"__concat__ must return a Shapable object but got {type(result).__name__} from {type(v).__name__} {v}"
+                    return result
+    # --- Next: try concat attributes for tree nodes ---
+    if all(isinstance(v, PhiTreeNode) for v in values):
+        attributes = all_attributes(values[0])
+        if attributes and all(all_attributes(v) == attributes for v in values):
+            new_attrs = {}
+            for a in attributes:
+                common_shape = merge_shapes(*[shape(getattr(v, a)).without(dim) for v in values])
+                a_values = [expand(getattr(v, a), common_shape & shape(v).only(dim)) for v in values]  # expand by dim if missing, and dims of others
+                new_attrs[a] = concat(a_values, dim, **kwargs)
+            return copy_with(values[0], **new_attrs)
+        else:
+            warnings.warn(f"Failed to concat values using value attributes because attributes differ among values {values}")
+    # --- Fallback: slice and stack ---
+    try:
+        unstacked = sum([unstack(v, dim) for v in values], ())
+    except MagicNotImplemented:
+        raise MagicNotImplemented(f"concat: No value implemented __concat__ and not all values were Sliceable along {dim}. values = {[type(v) for v in values]}")
+    if len(unstacked) > 8:
+        warnings.warn(f"concat() default implementation is slow on large dimensions ({dim}={len(unstacked)}). Please implement __concat__()", RuntimeWarning, stacklevel=2)
+    dim = shape(values[0])[dim].with_size(None)
+    try:
+        return stack(unstacked, dim, **kwargs)
+    except MagicNotImplemented:
+        raise MagicNotImplemented(f"concat: No value implemented __concat__ and slices could not be stacked. values = {[type(v) for v in values]}")
+
+
+def expand(value, *dims: Shape, **kwargs):
+    """
+    Adds dimensions to a `Tensor` or tensor-like object by implicitly repeating the tensor values along the new dimensions.
+    If `value` already contains any of the new dimensions, a size and type check is performed for these instead.
+
+    If any of `dims` varies along a dimension that is present neither in `value` nor on `dims`, it will also be added to `value`.
+
+    This function replaces the usual `tile` / `repeat` functions of
+    [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.tile.html),
+    [PyTorch](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.repeat),
+    [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/tile) and
+    [Jax](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.tile.html).
+
+    Additionally, it replaces the traditional `unsqueeze` / `expand_dims` functions.
+
+    Args:
+        value: `phi.math.magic.Shapable`, such as `phi.math.Tensor`
+            For tree nodes, expands all value attributes by `dims` or the first variable attribute if no value attributes are set.
+        *dims: Dimensions to be added as `Shape`
+        **kwargs: Additional keyword arguments required by specific implementations.
+            Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
+            Adding batch dimensions must always work without keyword arguments.
+
+    Returns:
+        Same type as `value`.
+    """
+    dims = concat_shapes(*dims)
+    combined = merge_shapes(value, dims)  # check that existing sizes match
+    if not dims.without(shape(value)):  # no new dims to add
+        if set(dims) == set(shape(value).only(dims)):  # sizes and item names might differ, though
+            return value
+    dims &= combined.shape.without('dims')  # add missing non-uniform dims
+    # --- First try __stack__
+    if hasattr(value, '__expand__'):
+        result = value.__expand__(dims, **kwargs)
+        if result is not NotImplemented:
+            return result
+    # --- Next try Tree Node ---
+    if isinstance(value, PhiTreeNode):
+        attributes = value_attributes(value) if hasattr(value, '__value_attrs__') else [variable_attributes(value)[0]]
+        new_attributes = {a: expand(getattr(value, a), dims, **kwargs) for a in attributes}
+        return copy_with(value, **new_attributes)
+    # --- Fallback: stack ---
+    if hasattr(value, '__stack__'):
+        if dims.volume > 8:
+            warnings.warn(f"expand() default implementation is slow on large shapes {dims}. Please implement __expand__() for {type(value).__name__} as defined in phi.math.magic", RuntimeWarning, stacklevel=2)
+        for dim in reversed(dims):
+            value = stack((value,) * dim.size, dim, **kwargs)
+            assert value is not NotImplemented, "Value must implement either __expand__ or __stack__"
+        return value
+    try:  # value may be a native scalar
+        from ._tensors import expand_tensor, wrap
+        value = wrap(value)
+    except ValueError:
+        raise AssertionError(f"Cannot expand non-shapable object {type(value)}")
+    return expand_tensor(value, dims)
+
+
+def rename_dims(value,
+                dims: DimFilter,
+                names: DimFilter,
+                **kwargs):
+    """
+    Change the name and optionally the type of some dimensions of `value`.
+
+    Dimensions that are not present on value will be ignored. The corresponding new dimensions given by `names` will not be added.
+
+    Args:
+        value: `Shape` or `Tensor` or `Shapable`.
+        dims: Existing dimensions of `value` as comma-separated `str`, `tuple`, `list`, `Shape` or filter function.
+        names: Either
+
+            * Sequence of names matching `dims` as `tuple`, `list` or `str`. This replaces only the dimension names but leaves the types untouched.
+            * `Shape` matching `dims` to replace names and types.
+            * Dimension type function to replace only types.
+
+        **kwargs: Additional keyword arguments required by specific implementations.
+            Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
+            Adding batch dimensions must always work without keyword arguments.
+
+    Returns:
+        Same type as `value`.
+    """
+    if isinstance(value, Shape):
+        return value._replace_names_and_types(dims, names)
+    elif isinstance(value, (Number, bool)):
+        return value
+    assert isinstance(value, Shapable) and isinstance(value, Shaped), f"value must be a Shape or Shapable but got {type(value).__name__}"
+    dims = shape(value).only(dims).names if callable(dims) else parse_dim_order(dims)
+    if isinstance(names, str):
+        names = parse_dim_order(names)
+    elif callable(names):
+        names: Shape = names(*dims).with_sizes(shape(value))
+    assert len(dims) == len(names), f"names and dims must be of equal length but got #dims={len(dims)} and #names={len(names)}"
+    existing_dims = shape(value).only(dims, reorder=True)
+    if not existing_dims:
+        return value
+    existing_names = [n for i, n in enumerate(names) if dims[i] in existing_dims]
+    existing_names = existing_dims._replace_names_and_types(existing_dims, existing_names)
+    # --- First try __replace_dims__ ---
+    if hasattr(value, '__replace_dims__'):
+        result = value.__replace_dims__(existing_dims.names, existing_names, **kwargs)
+        if result is not NotImplemented:
+            return result
+    # --- Next try Tree Node ---
+    if isinstance(value, PhiTreeNode):
+        new_attributes = {a: rename_dims(getattr(value, a), existing_dims, existing_names, **kwargs) for a in all_attributes(value)}
+        return copy_with(value, **new_attributes)
+    # --- Fallback: unstack and stack ---
+    if shape(value).only(existing_dims).volume > 8:
+        warnings.warn(f"rename_dims() default implementation is slow on large dimensions ({shape(value).only(dims)}). Please implement __replace_dims__() for {type(value).__name__} as defined in phi.math.magic", RuntimeWarning, stacklevel=2)
+    for old_name, new_dim in zip(existing_dims.names, existing_names):
+        value = stack(unstack(value, old_name), new_dim, **kwargs)
+    return value
+
+
+def b2i(value):
+    """ Change the type of all *batch* dimensions of `value` to *instance* dimensions. See `rename_dims`. """
+    return rename_dims(value, batch, instance)
+
+
+def c2b(value):
+    """ Change the type of all *channel* dimensions of `value` to *batch* dimensions. See `rename_dims`. """
+    return rename_dims(value, channel, batch)
+
+
+def s2b(value):
+    """ Change the type of all *spatial* dimensions of `value` to *batch* dimensions. See `rename_dims`. """
+    return rename_dims(value, spatial, batch)
+
+
+def si2d(value):
+    """ Change the type of all *spatial* and *instance* dimensions of `value` to *dual* dimensions. See `rename_dims`. """
+    return rename_dims(value, lambda s: s.non_channel.non_dual.non_batch, dual)
+
+
+def i2b(value):
+    """ Change the type of all *instance* dimensions of `value` to *batch* dimensions. See `rename_dims`. """
+    return rename_dims(value, instance, batch)
+
+
+def pack_dims(value, dims: DimFilter, packed_dim: Shape, pos: Optional[int] = None, **kwargs):
+    """
+    Compresses multiple dimensions into a single dimension by concatenating the elements.
+    Elements along the new dimensions are laid out according to the order of `dims`.
+    If the order of `dims` differs from the current dimension order, the tensor is transposed accordingly.
+    This function replaces the traditional `reshape` for these cases.
+
+    The type of the new dimension will be equal to the types of `dims`.
+    If `dims` have varying types, the new dimension will be a batch dimension.
+
+    If none of `dims` exist on `value`, `packed_dim` will be added only if it is given with a definite size and `value` is not a primitive type.
+
+    See Also:
+        `unpack_dim()`
+
+    Args:
+        value: `phi.math.magic.Shapable`, such as `phi.math.Tensor`.
+        dims: Dimensions to be compressed in the specified order.
+        packed_dim: Single-dimension `Shape`.
+        pos: Index of new dimension. `None` for automatic, `-1` for last, `0` for first.
+        **kwargs: Additional keyword arguments required by specific implementations.
+            Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
+            Adding batch dimensions must always work without keyword arguments.
+
+    Returns:
+        Same type as `value`.
+
+    Examples:
+        >>> pack_dims(math.zeros(spatial(x=4, y=3)), spatial, instance('points'))
+        (pointsⁱ=12) const 0.0
+    """
+    if isinstance(value, (Number, bool)):
+        return value
+    assert isinstance(value, Shapable) and isinstance(value, Sliceable) and isinstance(value, Shaped), f"value must be Shapable but got {type(value)}"
+    dims = shape(value).only(dims, reorder=True)
+    if packed_dim in shape(value):
+        assert packed_dim in dims, f"Cannot pack dims into new dimension {packed_dim} because it already exists on value {value} and is not packed."
+    if len(dims) == 0 or all(dim not in shape(value) for dim in dims):
+        return value if packed_dim.size is None else expand(value, packed_dim, **kwargs)  # Inserting size=1 can cause shape errors
+    elif len(dims) == 1:
+        return rename_dims(value, dims, packed_dim, **kwargs)
+    # --- First try __pack_dims__ ---
+    if hasattr(value, '__pack_dims__'):
+        result = value.__pack_dims__(dims.names, packed_dim, pos, **kwargs)
+        if result is not NotImplemented:
+            return result
+    # --- Next try Tree Node ---
+    if isinstance(value, PhiTreeNode):
+        new_attributes = {a: pack_dims(getattr(value, a), dims, packed_dim, pos=pos, **kwargs) for a in all_attributes(value)}
+        return copy_with(value, **new_attributes)
+    # --- Fallback: unstack and stack ---
+    if shape(value).only(dims).volume > 8:
+        warnings.warn(f"pack_dims() default implementation is slow on large dimensions ({shape(value).only(dims)}). Please implement __pack_dims__() for {type(value).__name__} as defined in phi.math.magic", RuntimeWarning, stacklevel=2)
+    return stack(unstack(value, dims), packed_dim, **kwargs)
+
+
+
+
+def unpack_dim(value, dim: DimFilter, *unpacked_dims: Shape, **kwargs):
+    """
+    Decompresses a dimension by unstacking the elements along it.
+    This function replaces the traditional `reshape` for these cases.
+    The compressed dimension `dim` is assumed to contain elements laid out according to the order of `unpacked_dims`.
+
+    If `dim` does not exist on `value`, this function will return `value` as-is. This includes primitive types.
+
+    See Also:
+        `pack_dims()`
+
+    Args:
+        value: `phi.math.magic.Shapable`, such as `Tensor`, for which one dimension should be split.
+        dim: Single dimension to be decompressed.
+        *unpacked_dims: Vararg `Shape`, ordered dimensions to replace `dim`, fulfilling `unpacked_dims.volume == shape(self)[dim].rank`.
+        **kwargs: Additional keyword arguments required by specific implementations.
+            Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
+            Adding batch dimensions must always work without keyword arguments.
+
+    Returns:
+        Same type as `value`.
+
+    Examples:
+        >>> unpack_dim(math.zeros(instance(points=12)), 'points', spatial(x=4, y=3))
+        (xˢ=4, yˢ=3) const 0.0
+    """
+    if isinstance(value, (Number, bool)):
+        return value
+    assert isinstance(value, Shapable) and isinstance(value, Sliceable) and isinstance(value, Shaped), f"value must be Shapable but got {type(value)}"
+    dim = shape(value).only(dim)
+    if dim.is_empty:
+        return value  # Nothing to do, maybe expand?
+    assert dim.rank == 1, f"unpack_dim requires as single dimension to be unpacked but got {dim}"
+    dim = dim.name
+    unpacked_dims = concat_shapes(*unpacked_dims)
+    if unpacked_dims.rank == 0:
+        return value[{dim: 0}]  # remove dim
+    elif unpacked_dims.rank == 1:
+        return rename_dims(value, dim, unpacked_dims, **kwargs)
+    # --- First try __unpack_dim__
+    if hasattr(value, '__unpack_dim__'):
+        result = value.__unpack_dim__(dim, unpacked_dims, **kwargs)
+        if result is not NotImplemented:
+            return result
+    # --- Next try Tree Node ---
+    if isinstance(value, PhiTreeNode) and all_attributes(value):
+        new_attributes = {a: unpack_dim(getattr(value, a), dim, unpacked_dims, **kwargs) for a in all_attributes(value)}
+        return copy_with(value, **new_attributes)
+    # --- Fallback: unstack and stack ---
+    if shape(value).only(dim).volume > 8:
+        warnings.warn(f"pack_dims() default implementation is slow on large dimensions ({shape(value).only(dim)}). Please implement __unpack_dim__() for {type(value).__name__} as defined in phi.math.magic", RuntimeWarning, stacklevel=2)
+    unstacked = unstack(value, dim)
+    for dim in reversed(unpacked_dims):
+        unstacked = [stack(unstacked[i:i+dim.size], dim, **kwargs) for i in range(0, len(unstacked), dim.size)]
+    return unstacked[0]
+
+
+def flatten(value, flat_dim: Shape = instance('flat'), flatten_batch=False, **kwargs):
+    """
+    Returns a `Tensor` with the same values as `value` but only a single dimension `flat_dim`.
+    The order of the values in memory is not changed.
+
+    Args:
+        value: `phi.math.magic.Shapable`, such as `Tensor`.
+        flat_dim: Dimension name and type as `Shape` object. The size is ignored.
+        flatten_batch: Whether to flatten batch dimensions as well.
+            If `False`, batch dimensions are kept, only onn-batch dimensions are flattened.
+        **kwargs: Additional keyword arguments required by specific implementations.
+            Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
+            Adding batch dimensions must always work without keyword arguments.
+
+    Returns:
+        Same type as `value`.
+
+    Examples:
+        >>> flatten(math.zeros(spatial(x=4, y=3)))
+        (flatⁱ=12) const 0.0
+    """
+    assert isinstance(flat_dim, Shape) and flat_dim.rank == 1, flat_dim
+    assert isinstance(value, Shapable) and isinstance(value, Shaped), f"value must be Shapable but got {type(value)}"
+    # --- First try __flatten__ ---
+    if hasattr(value, '__flatten__'):
+        result = value.__flatten__(flat_dim, flatten_batch, **kwargs)
+        if result is not NotImplemented:
+            return result
+    # There is no tree node implementation for flatten because pack_dims is just as fast
+    # --- Fallback: pack_dims ---
+    return pack_dims(value, shape(value) if flatten_batch else non_batch(value), flat_dim, **kwargs)
+
+
+# PhiTreeNode
+
+PhiTreeNodeType = TypeVar('PhiTreeNodeType')  # Defined in phi.math.magic: tuple, list, dict, custom
+
+
+def variable_attributes(obj) -> Tuple[str]:
+    if hasattr(obj, '__variable_attrs__'):
+        return obj.__variable_attrs__()
+    elif hasattr(obj, '__value_attrs__'):
+        return obj.__value_attrs__()
+    elif dataclasses.is_dataclass(obj):
+        return tuple([f.name for f in dataclasses.fields(obj)])
+    else:
+        raise ValueError(f"Not a PhiTreeNode: {type(obj).__name__}")
+
+
+def value_attributes(obj) -> Tuple[str, ...]:
+    if hasattr(obj, '__value_attrs__'):
+        return obj.__value_attrs__()
+    if dataclasses.is_dataclass(obj):
+        return tuple([f.name for f in dataclasses.fields(obj)])
+    raise ValueError(f"{type(obj).__name__} must implement '__value_attrs__()' or be a dataclass to be used with value functions.")
+
+
+def variable_values(obj) -> Tuple[str, ...]:
+    if hasattr(obj, '__variable_attrs__'):
+        values = obj.__value_attrs__()
+        variables = obj.__variable_attrs__()
+        return tuple([a for a in values if a in variables])
+    else:
+        return obj.__value_attrs__()  # this takes care of dataclasses as well
+
+
+def all_attributes(obj, assert_any=False) -> Set[str]:
+    if not isinstance(obj, PhiTreeNode):
+        raise ValueError(f"Not a PhiTreeNode: {type(obj).__name__}")
+    result = set()
+    if hasattr(obj, '__variable_attrs__'):
+        result.update(obj.__variable_attrs__())
+    if hasattr(obj, '__value_attrs__'):
+        result.update(obj.__value_attrs__())
+    if dataclasses.is_dataclass(obj) and not hasattr(obj, '__variable_attrs__') and not hasattr(obj, '__value_attrs__'):
+        result.update([f.name for f in dataclasses.fields(obj)])
+    if assert_any:
+        assert result, f"{type(obj).__name__} is not a valid tree node because it has no tensor-like attributes."
+    return result
+
+
+def replace(obj: PhiTreeNodeType, **updates) -> PhiTreeNodeType:
+    """
+    Creates a copy of the given `phi.math.magic.PhiTreeNode` with updated values as specified in `updates`.
+
+    If `obj` overrides `__with_attrs__`, the copy will be created via that specific implementation.
+    Otherwise, the `copy` module and `setattr` will be used.
+
+    Args:
+        obj: `phi.math.magic.PhiTreeNode`
+        **updates: Values to be replaced.
+
+    Returns:
+        Copy of `obj` with updated values.
+    """
+    if hasattr(obj, '__with_attrs__'):
+        return obj.__with_attrs__(**updates)
+    elif isinstance(obj, (Number, bool)):
+        return obj
+    elif dataclasses.is_dataclass(obj):
+        return dataclasses.replace(obj, **updates)
+    else:
+        cpy = copy.copy(obj)
+        for attr, value in updates.items():
+            setattr(cpy, attr, value)
+        return cpy
+
+
+copy_with = replace
+
+
+# Other Ops
+
+MagicType = TypeVar('MagicType')
+OtherMagicType = TypeVar('OtherMagicType')
+
+
+def cast(x: MagicType, dtype: Union[DType, type]) -> OtherMagicType:
+    """
+    Casts `x` to a different data type.
+
+    Implementations:
+
+    * NumPy: [`x.astype()`](numpy.ndarray.astype)
+    * PyTorch: [`x.to()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.to)
+    * TensorFlow: [`tf.cast`](https://www.tensorflow.org/api_docs/python/tf/cast)
+    * Jax: [`jax.numpy.array`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.array.html)
+
+    See Also:
+        `to_float`, `to_int32`, `to_int64`, `to_complex`.
+
+    Args:
+        x: `Tensor`
+        dtype: New data type as `phi.math.DType`, e.g. `DType(int, 16)`.
+
+    Returns:
+        `Tensor` with data type `dtype`
+    """
+    if not isinstance(dtype, DType):
+        dtype = DType.as_dtype(dtype)
+    if hasattr(x, '__cast__'):
+        return x.__cast__(dtype)
+    elif isinstance(x, (Number, bool)):
+        return dtype.kind(x)
+    elif isinstance(x, PhiTreeNode):
+        attrs = {key: getattr(x, key) for key in value_attributes(x)}
+        new_attrs = {k: cast(v, dtype) for k, v in attrs.items()}
+        return copy_with(x, **new_attrs)
+    try:
+        backend = choose_backend(x)
+        return backend.cast(x, dtype)
+    except NoBackendFound:
+        if dtype.kind == bool:
+            return bool(x)
+        raise ValueError(f"Cannot cast object of type '{type(x).__name__}'")
+
+
+def bool_to_int(x: MagicType, bits=32):
+    if isinstance(x, bool):
+        return int(x)
+    if isinstance(x, Number):
+        return x
+    if hasattr(x, 'dtype') and isinstance(x.dtype, DType):
+        return cast(x, DType(int, bits)) if x.dtype.kind == bool else x
+    elif isinstance(x, PhiTreeNode):
+        return tree_map(bool_to_int, x, bits=32)
+    try:
+        backend = choose_backend(x)
+        return backend.cast(x, DType(int, bits)) if backend.dtype(x).kind == bool else x
+    except NoBackendFound:
+        raise ValueError(f"Cannot cast object of type '{type(x).__name__}'")
+
+
+def tree_map(f, tree, **f_kwargs):
+    from ._tensors import Tensor
+    if isinstance(tree, Tensor):
+        return f(tree, **f_kwargs)
+    if isinstance(tree, list):
+        return [tree_map(f, e, **f_kwargs) for e in tree]
+    elif isinstance(tree, tuple):
+        return tuple([tree_map(f, e, **f_kwargs) for e in tree])
+    elif isinstance(tree, dict):
+        return {k: tree_map(f, e, **f_kwargs) for k, e in tree.items()}
+    elif isinstance(tree, PhiTreeNode):
+        attrs = {key: getattr(tree, key) for key in value_attributes(tree)}
+        new_attrs = {k: tree_map(f, v, **f_kwargs) for k, v in attrs.items()}
+        return copy_with(tree, **new_attrs)
+    else:
+        return f(tree, **f_kwargs)  # try anyway
```

### Comparing `phiflow-2.3.4/phi/math/_nd.py` & `phiflow-2.4.0/phi/math/_nd.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,751 +1,748 @@
-from functools import partial
-from typing import Tuple, Optional, List, Union
-
-import numpy as np
-
-from ._shape import Shape, channel, batch, spatial, DimFilter, parse_dim_order, shape, instance
-from .magic import PhiTreeNode
-from ._magic_ops import stack, rename_dims, concat, variable_values
-from ._tensors import Tensor, wrap, tensor
-from . import extrapolation as extrapolation
-from .extrapolation import Extrapolation
-from . import _ops as math
-from ._functional import jit_compile_linear
-from ._optimize import solve_linear
-
-
-def vec(name: Union[str, Shape] = 'vector', *sequence, tuple_dim=spatial('sequence'), list_dim=instance('sequence'), **components) -> Tensor:
-    """
-    Lay out the given values along a channel dimension without converting them to the current backend.
-
-    Args:
-        name: Dimension name.
-        *sequence: Component values that will also be used as item names.
-            If specified, `components` must be empty.
-        **components: Values by component name.
-            If specified, no additional positional arguments must be given.
-        tuple_dim: Dimension for `tuple` values passed as components, e.g. `vec(x=(0, 1), ...)`
-        list_dim: Dimension for `list` values passed as components, e.g. `vec(x=[0, 1], ...)`
-
-    Returns:
-        `Tensor`
-
-    Examples:
-        >>> vec(x=1, y=0, z=-1)
-        (x=1, y=0, z=-1)
-
-        >>> vec(x=1., z=0)
-        (x=1.000, z=0.000)
-
-        >>> vec(x=tensor([1, 2, 3], instance('particles')), y=0)
-        (x=1, y=0); (x=2, y=0); (x=3, y=0) (particlesⁱ=3, vectorᶜ=x,y)
-
-        >>> vec(x=0, y=[0, 1])
-        (x=0, y=0); (x=0, y=1) (vectorᶜ=x,y, sequenceⁱ=2)
-
-        >>> vec(x=0, y=(0, 1))
-        (x=0, y=0); (x=0, y=1) (sequenceˢ=2, vectorᶜ=x,y)
-    """
-    dim = channel(name) if isinstance(name, str) else name
-    assert isinstance(dim, Shape), f"name must be a str or Shape but got '{type(name)}'"
-    if sequence:
-        assert not components, "vec() must be given either positional or keyword arguments but not both"
-        if len(sequence) == 1 and isinstance(sequence[0], (tuple, list)):
-            sequence = sequence[0]
-        dim = dim.with_size([str(v) for v in sequence])
-        return wrap(sequence, dim)
-    else:
-        def wrap_sequence(value):
-            if isinstance(value, tuple):
-                return wrap(value, tuple_dim)
-            elif isinstance(value, list):
-                return wrap(value, list_dim)
-            else:
-                return value
-        components = {n: wrap_sequence(v) for n, v in components.items()}
-        return stack(components, dim, expand_values=True)
-
-
-def const_vec(value: Union[float, Tensor], dim: Union[Shape, tuple, list, str]):
-    """
-    Creates a single-dimension tensor with all values equal to `value`.
-    `value` is not converted to the default backend, even when it is a Python primitive.
-
-    Args:
-        value: Value for filling the vector.
-        dim: Either single-dimension non-spatial Shape or `Shape` consisting of any number of spatial dimensions.
-            In the latter case, a new channel dimension named `'vector'` will be created from the spatial shape.
-
-    Returns:
-        `Tensor`
-    """
-    if isinstance(dim, Shape):
-        if dim.spatial:
-            assert not dim.non_spatial, f"When creating a vector given spatial dimensions, the shape may only contain spatial dimensions but got {dim}"
-            shape = channel(vector=dim.names)
-        else:
-            assert dim.rank == 1, f"Cannot create vector from {dim}"
-            shape = dim
-    else:
-        dims = parse_dim_order(dim)
-        shape = channel(vector=dims)
-    return wrap([value] * shape.size, shape)
-
-
-def vec_abs(vec: Tensor, vec_dim: DimFilter = channel, eps: Union[float, Tensor] = None):
-    """
-    Computes the vector length of `vec`.
-
-    Args:
-        eps: Minimum vector length. Use to avoid `inf` gradients for zero-length vectors.
-    """
-    if vec.dtype.kind == complex:
-        vec = stack([vec.real, vec.imag], channel('_ReIm'))
-    squared = vec_squared(vec, vec_dim)
-    if eps is not None:
-        squared = math.maximum(squared, eps)
-    return math.sqrt(squared)
-
-
-def vec_squared(vec: Tensor, vec_dim: DimFilter = channel):
-    """ Computes the squared length of `vec`. If `vec_dim` is None, the combined channel dimensions of `vec` are interpreted as a vector. """
-    return math.sum_(vec ** 2, dim=vec_dim)
-
-
-def vec_normalize(vec: Tensor, vec_dim: DimFilter = channel):
-    """ Normalizes the vectors in `vec`. If `vec_dim` is None, the combined channel dimensions of `vec` are interpreted as a vector. """
-    return vec / vec_abs(vec, vec_dim=vec_dim)
-
-
-def cross_product(vec1: Tensor, vec2: Tensor) -> Tensor:
-    """
-    Computes the cross product of two vectors in 2D.
-
-    Args:
-        vec1: `Tensor` with a single channel dimension called `'vector'`
-        vec2: `Tensor` with a single channel dimension called `'vector'`
-
-    Returns:
-        `Tensor`
-    """
-    vec1 = math.tensor(vec1)
-    vec2 = math.tensor(vec2)
-    spatial_rank = vec1.vector.size if 'vector' in vec1.shape else vec2.vector.size
-    if spatial_rank == 2:  # Curl in 2D
-        assert vec2.vector.exists
-        if vec1.vector.exists:
-            v1_x, v1_y = vec1.vector
-            v2_x, v2_y = vec2.vector
-            return v1_x * v2_y - v1_y * v2_x
-        else:
-            v2_x, v2_y = vec2.vector
-            return vec1 * math.stack_tensors([-v2_y, v2_x], channel('vector'))
-    elif spatial_rank == 3:  # Curl in 3D
-        raise NotImplementedError(f'spatial_rank={spatial_rank} not yet implemented')
-    else:
-        raise AssertionError(f'dims = {spatial_rank}. Vector product not available in > 3 dimensions')
-
-
-def rotate_vector(vector: math.Tensor, angle: Union[float, math.Tensor]) -> Tensor:
-    """
-    Rotates `vector` around the origin.
-
-    Args:
-        vector: n-dimensional vector with a channel dimension called `'vector'`
-        angle: Euler angle. The direction is the rotation axis and the length is the amount (in radians).
-
-    Returns:
-        Rotated vector as `Tensor`
-    """
-    assert 'vector' in vector.shape, "vector must have 'vector' dimension."
-    if vector.vector.size == 2:
-        sin = wrap(math.sin(angle))
-        cos = wrap(math.cos(angle))
-        x, y = vector.vector
-        rot_x = cos * x - sin * y
-        rot_y = sin * x + cos * y
-        return math.stack_tensors([rot_x, rot_y], channel(vector=vector.vector.item_names))
-    elif vector.vector.size == 1:
-        raise AssertionError(f"Cannot rotate a 1D vector. shape={vector.shape}")
-    else:
-        raise NotImplementedError(f"Rotation in {vector.vector.size}D not yet implemented.")
-
-
-def dim_mask(all_dims: Union[Shape, tuple, list], dims: DimFilter, mask_dim=channel('vector')) -> Tensor:
-    """
-    Creates a masked vector with 1 elements for `dims` and 0 for all other dimensions in `all_dims`.
-
-    Args:
-        all_dims: All dimensions for which the vector should have an entry.
-        dims: Dimensions marked as 1.
-        mask_dim: Dimension of the masked vector. Item names are assigned automatically.
-
-    Returns:
-        `Tensor`
-    """
-    assert isinstance(all_dims, (Shape, tuple, list)), f"all_dims must be a tuple or Shape but got {type(all_dims)}"
-    assert isinstance(mask_dim, Shape) and mask_dim.rank == 1, f"mask_dim must be a single-dimension Shape but got {mask_dim}"
-    if isinstance(all_dims, (tuple, list)):
-        all_dims = spatial(*all_dims)
-    dims = all_dims.only(dims)
-    mask = [1 if dim in dims else 0 for dim in all_dims]
-    mask_dim = mask_dim.with_size(all_dims.names)
-    return wrap(mask, mask_dim)
-
-
-def normalize_to(target: Tensor, source: Union[float, Tensor], epsilon=1e-5):
-    """
-    Multiplies the target so that its sum matches the source.
-
-    Args:
-        target: `Tensor`
-        source: `Tensor` or constant
-        epsilon: Small number to prevent division by zero.
-
-    Returns:
-        Normalized tensor of the same shape as target
-    """
-    target_total = math.sum_(target)
-    denominator = math.maximum(target_total, epsilon) if epsilon is not None else target_total
-    source_total = math.sum_(source)
-    return target * (source_total / denominator)
-
-
-def l1_loss(x, reduce: DimFilter = math.non_batch) -> Tensor:
-    """
-    Computes *∑<sub>i</sub> ||x<sub>i</sub>||<sub>1</sub>*, summing over all non-batch dimensions.
-
-    Args:
-        x: `Tensor` or `phi.math.magic.PhiTreeNode` or 0D or 1D native tensor.
-            For `phi.math.magic.PhiTreeNode` objects, only value the sum over all value attributes is computed.
-        reduce: Dimensions to reduce as `DimFilter`.
-
-    Returns:
-        loss: `Tensor`
-    """
-    if isinstance(x, Tensor):
-        return math.sum_(abs(x), reduce)
-    elif isinstance(x, PhiTreeNode):
-        return sum([l1_loss(getattr(x, a), reduce) for a in variable_values(x)])
-    else:
-        try:
-            backend = math.choose_backend(x)
-            shape = backend.staticshape(x)
-            if len(shape) == 0:
-                return abs(x)
-            elif len(shape) == 1:
-                return backend.sum(abs(x))
-            else:
-                raise ValueError("l2_loss is only defined for 0D and 1D native tensors. For higher-dimensional data, use Φ-Flow tensors.")
-        except math.NoBackendFound:
-            raise ValueError(x)
-
-
-def l2_loss(x, reduce: DimFilter = math.non_batch) -> Tensor:
-    """
-    Computes *∑<sub>i</sub> ||x<sub>i</sub>||<sub>2</sub><sup>2</sup> / 2*, summing over all non-batch dimensions.
-
-    Args:
-        x: `Tensor` or `phi.math.magic.PhiTreeNode` or 0D or 1D native tensor.
-            For `phi.math.magic.PhiTreeNode` objects, only value the sum over all value attributes is computed.
-        reduce: Dimensions to reduce as `DimFilter`.
-
-    Returns:
-        loss: `Tensor`
-    """
-    if isinstance(x, Tensor):
-        if x.dtype.kind == complex:
-            x = abs(x)
-        return math.sum_(x ** 2, reduce) * 0.5
-    elif isinstance(x, PhiTreeNode):
-        return sum([l2_loss(getattr(x, a), reduce) for a in variable_values(x)])
-    else:
-        try:
-            backend = math.choose_backend(x)
-            shape = backend.staticshape(x)
-            if len(shape) == 0:
-                return x ** 2 * 0.5
-            elif len(shape) == 1:
-                return backend.sum(x ** 2) * 0.5
-            else:
-                raise ValueError("l2_loss is only defined for 0D and 1D native tensors. For higher-dimensional data, use Φ-Flow tensors.")
-        except math.NoBackendFound:
-            raise ValueError(x)
-
-
-def frequency_loss(x,
-                   frequency_falloff: float = 100,
-                   threshold=1e-5,
-                   ignore_mean=False,
-                   n=2) -> Tensor:
-    """
-    Penalizes the squared `values` in frequency (Fourier) space.
-    Lower frequencies are weighted more strongly then higher frequencies, depending on `frequency_falloff`.
-
-    Args:
-        x: `Tensor` or `phi.math.magic.PhiTreeNode` Values to penalize, typically `actual - target`.
-        frequency_falloff: Large values put more emphasis on lower frequencies, 1.0 weights all frequencies equally.
-            *Note*: The total loss is not normalized. Varying the value will result in losses of different magnitudes.
-        threshold: Frequency amplitudes below this value are ignored.
-            Setting this to zero may cause infinities or NaN values during backpropagation.
-        ignore_mean: If `True`, does not penalize the mean value (frequency=0 component).
-
-    Returns:
-      Scalar loss value
-    """
-    assert n in (1, 2)
-    if isinstance(x, Tensor):
-        if ignore_mean:
-            x -= math.mean(x, x.shape.non_batch)
-        k_squared = vec_squared(math.fftfreq(x.shape.spatial))
-        weights = math.exp(-0.5 * k_squared * frequency_falloff ** 2)
-
-        diff_fft = abs_square(math.fft(x) * weights)
-        diff_fft = math.sqrt(math.maximum(diff_fft, threshold))
-        return l2_loss(diff_fft) if n == 2 else l1_loss(diff_fft)
-    elif isinstance(x, PhiTreeNode):
-        losses = [frequency_loss(getattr(x, a), frequency_falloff, threshold, ignore_mean, n) for a in variable_values(x)]
-        return sum(losses)
-    else:
-        raise ValueError(x)
-
-
-def abs_square(complex_values: Tensor) -> Tensor:
-    """
-    Squared magnitude of complex values.
-
-    Args:
-      complex_values: complex `Tensor`
-
-    Returns:
-        Tensor: real valued magnitude squared
-
-    """
-    return math.imag(complex_values) ** 2 + math.real(complex_values) ** 2
-
-
-# Divergence
-
-# def divergence(tensor, dx=1, difference='central', padding='constant', dimensions=None):
-#     """
-#     Computes the spatial divergence of a vector channel from finite differences.
-#
-#     :param tensor: vector field; tensor of shape (batch size, spatial dimensions..., spatial rank)
-#     :param dx: distance between adjacent grid points (default 1)
-#     :param difference: type of difference, one of ('forward', 'central') (default 'forward')
-#     :return: tensor of shape (batch size, spatial dimensions..., 1)
-#     """
-#     assert difference in ('central', 'forward', 'backward'), difference
-#     rank = spatial_rank(tensor)
-#     if difference == 'forward':
-#         return _divergence_nd(tensor, padding, (0, 1), dims) / dx ** rank  # TODO why dx^rank?
-#     elif difference == 'backward':
-#         return _divergence_nd(tensor, padding, (-1, 0), dims) / dx ** rank
-#     else:
-#         return _divergence_nd(tensor, padding, (-1, 1), dims) / (2 * dx) ** rank
-#
-#
-# def _divergence_nd(x_, padding, relative_shifts, dims=None):
-#     x = tensor(x_)
-#     assert x.shape.channel.rank == 1
-#     dims = dims if dims is not None else x.shape.spatial.names
-#     x = math.pad(x, {axis: (-relative_shifts[0], relative_shifts[1]) for axis in dims}, mode=padding)
-#     components = []
-#     for dimension in dims:
-#         dim_index_in_spatial = x.shape.spatial.reset_indices().index(dimension)
-#         lower, upper = _multi_roll(x, dimension, relative_shifts, diminish_others=(-relative_shifts[0], relative_shifts[1]), names=dims, base_selection={0: rank - dimension - 1})
-#         components.append(upper - lower)
-#     return math.sum_(components, 0)
-
-
-def shift(x: Tensor,
-          offsets: tuple,
-          dims: DimFilter = math.spatial,
-          padding: Union[Extrapolation, Tensor, float, None] = extrapolation.BOUNDARY,
-          stack_dim: Optional[Shape] = channel('shift'),
-          extend_bounds=0) -> list:
-    """
-    shift Tensor by a fixed offset and abiding by extrapolation
-
-    Args:
-        x: Input data
-        offsets: Shift size
-        dims: Dimensions along which to shift, defaults to None
-        padding: padding to be performed at the boundary, defaults to extrapolation.BOUNDARY
-        stack_dim: dimensions to be stacked, defaults to 'shift'
-
-    Returns:
-        list: offset_tensor
-
-    """
-    if dims is None:
-        raise ValueError("dims=None is not supported anymore.")
-    dims = x.shape.only(dims).names
-    if stack_dim is None:
-        assert len(dims) == 1
-    x = wrap(x)
-    pad_lower = max(0, -min(offsets))
-    pad_upper = max(0, max(offsets))
-    if padding is not None:
-        x = math.pad(x, {axis: (pad_lower + extend_bounds, pad_upper + extend_bounds) for axis in dims}, mode=padding)
-    if extend_bounds:
-        assert padding is not None
-    offset_tensors = []
-    for offset in offsets:
-        components = []
-        for dimension in dims:
-            if padding:
-                slices = {dim: slice(pad_lower + offset, (-pad_upper + offset) or None) if dim == dimension else slice(pad_lower, -pad_upper or None) for dim in dims}
-            else:
-                slices = {dim: slice(pad_lower + offset, (-pad_upper + offset) or None) if dim == dimension else slice(None, None) for dim in dims}
-            components.append(x[slices])
-        offset_tensors.append(stack(components, stack_dim) if stack_dim is not None else components[0])
-    return offset_tensors
-
-
-def masked_fill(values: Tensor, valid: Tensor, distance: int = 1) -> Tuple[Tensor, Tensor]:
-    """
-    Extrapolates the values of `values` which are marked by the nonzero values of `valid` for `distance` steps in all spatial directions.
-    Overlapping extrapolated values get averaged. Extrapolation also includes diagonals.
-
-    Args:
-        values: Tensor which holds the values for extrapolation
-        valid: Tensor with same size as `x` marking the values for extrapolation with nonzero values
-        distance: Number of extrapolation steps
-
-    Returns:
-        values: Extrapolation result
-        valid: mask marking all valid values after extrapolation
-    """
-    def binarize(x):
-        return math.divide_no_nan(x, x)
-    distance = min(distance, max(values.shape.sizes))
-    for _ in range(distance):
-        valid = binarize(valid)
-        valid_values = valid * values
-        overlap = valid  # count how many values we are adding
-        for dim in values.shape.spatial.names:
-            values_l, values_r = shift(valid_values, (-1, 1), dims=dim, padding=extrapolation.ZERO)
-            valid_values = math.sum_(values_l + values_r + valid_values, dim='shift')
-            mask_l, mask_r = shift(overlap, (-1, 1), dims=dim, padding=extrapolation.ZERO)
-            overlap = math.sum_(mask_l + mask_r + overlap, dim='shift')
-        extp = math.divide_no_nan(valid_values, overlap)  # take mean where extrapolated values overlap
-        values = math.where(valid, values, math.where(binarize(overlap), extp, values))
-        valid = overlap
-    return values, binarize(valid)
-
-
-def finite_fill(values: Tensor, dims: DimFilter = spatial, distance: int = 1, diagonal: bool = True, padding=extrapolation.BOUNDARY) -> Tuple[Tensor, Tensor]:
-    """
-    Fills non-finite (NaN, inf, -inf) values from nearby finite values.
-    Extrapolates the finite values of `values` for `distance` steps along `dims`.
-    Where multiple finite values could fill an invalid value, the average is computed.
-
-    Args:
-        values: Floating-point `Tensor`. All non-numeric values (`NaN`, `inf`, `-inf`) are interpreted as invalid.
-        dims: Dimensions along which to fill invalid values from finite ones.
-        distance: Number of extrapolation steps, each extrapolating one cell out.
-        diagonal: Whether to extrapolate values to their diagonal neighbors per step.
-        padding: Extrapolation of `values`. Determines whether to extrapolate from the edges as well.
-
-    Returns:
-        `Tensor` of same shape as `values`.
-    """
-    if diagonal:
-        distance = min(distance, max(values.shape.sizes))
-        dims = values.shape.only(dims)
-        for _ in range(distance):
-            valid = math.is_finite(values)
-            valid_values = math.where(valid, values, 0)
-            overlap = valid
-            for dim in dims:
-                values_l, values_r = shift(valid_values, (-1, 1), dims=dim, padding=padding)
-                valid_values = math.sum_(values_l + values_r + valid_values, dim='shift')
-                mask_l, mask_r = shift(overlap, (-1, 1), dims=dim, padding=padding)
-                overlap = math.sum_(mask_l + mask_r + overlap, dim='shift')
-            values = math.where(valid, values, valid_values / overlap)
-    else:
-        distance = min(distance, sum(values.shape.sizes))
-        for _ in range(distance):
-            neighbors = concat(shift(values, (-1, 1), dims, padding=padding, stack_dim=channel('neighbors')), 'neighbors')
-            finite = math.is_finite(neighbors)
-            avg_neighbors = math.sum_(math.where(finite, neighbors, 0), 'neighbors') / math.sum_(finite, 'neighbors')
-            values = math.where(math.is_finite(values), values, avg_neighbors)
-    return values
-
-
-# Gradient
-
-def spatial_gradient(grid: Tensor,
-                     dx: Union[float, Tensor] = 1,
-                     difference: str = 'central',
-                     padding: Union[Extrapolation, None] = extrapolation.BOUNDARY,
-                     dims: DimFilter = spatial,
-                     stack_dim: Union[Shape, None] = channel('gradient'),
-                     pad=0) -> Tensor:
-    """
-    Calculates the spatial_gradient of a scalar channel from finite differences.
-    The spatial_gradient vectors are in reverse order, lowest dimension first.
-
-    Args:
-        grid: grid values
-        dims: (Optional) Dimensions along which the spatial derivative will be computed. sequence of dimension names
-        dx: Physical distance between grid points, `float` or `Tensor`.
-            When passing a vector-valued `Tensor`, the dx values should be listed along `stack_dim`, matching `dims`.
-        difference: type of difference, one of ('forward', 'backward', 'central') (default 'forward')
-        padding: tensor padding mode
-        stack_dim: name of the new vector dimension listing the spatial_gradient w.r.t. the various axes
-        pad: How many cells to extend the result compared to `grid`.
-            This value is added to the internal padding. For non-trivial extrapolations, this gives the correct result while manual padding before or after this operation would not respect the boundary locations.
-
-    Returns:
-        `Tensor`
-    """
-    grid = wrap(grid)
-    if stack_dim is not None and stack_dim in grid.shape:
-        assert grid.shape.only(stack_dim).size == 1, f"spatial_gradient() cannot list components along {stack_dim.name} because that dimension already exists on grid {grid}"
-        grid = grid[{stack_dim.name: 0}]
-    dims = grid.shape.only(dims)
-    dx = wrap(dx)
-    if dx.vector.exists:
-        dx = dx.vector[dims]
-        if dx.vector.size in (None, 1):
-            dx = dx.vector[0]
-    if difference.lower() == 'central':
-        left, right = shift(grid, (-1, 1), dims, padding, stack_dim=stack_dim, extend_bounds=pad)
-        return (right - left) / (dx * 2)
-    elif difference.lower() == 'forward':
-        left, right = shift(grid, (0, 1), dims, padding, stack_dim=stack_dim, extend_bounds=pad)
-        return (right - left) / dx
-    elif difference.lower() == 'backward':
-        left, right = shift(grid, (-1, 0), dims, padding, stack_dim=stack_dim, extend_bounds=pad)
-        return (right - left) / dx
-    else:
-        raise ValueError('Invalid difference type: {}. Can be CENTRAL or FORWARD'.format(difference))
-
-
-# Laplace
-
-def laplace(x: Tensor,
-            dx: Union[Tensor, float] = 1,
-            padding: Extrapolation = extrapolation.BOUNDARY,
-            dims: DimFilter = spatial,
-            weights: Tensor = None):
-    """
-    Spatial Laplace operator as defined for scalar fields.
-    If a vector field is passed, the laplace is computed component-wise.
-
-    Args:
-        x: n-dimensional field of shape (batch, spacial dimensions..., components)
-        dx: scalar or 1d tensor
-        padding: extrapolation
-        dims: The second derivative along these dimensions is summed over
-        weights: (Optional) Multiply the axis terms by these factors before summation.
-            Must be a Tensor with a single channel dimension that lists all laplace dims by name.
-
-    Returns:
-        `phi.math.Tensor` of same shape as `x`
-    """
-    if isinstance(dx, (tuple, list)):
-        dx = wrap(dx, batch('_laplace'))
-    elif isinstance(dx, Tensor) and dx.vector.exists:
-        dx = rename_dims(dx, 'vector', batch('_laplace'))
-    if isinstance(x, Extrapolation):
-        return x.spatial_gradient()
-    left, center, right = shift(wrap(x), (-1, 0, 1), dims, padding, stack_dim=batch('_laplace'))
-    result = (left + right - 2 * center) / (dx ** 2)
-    if weights is not None:
-        dim_names = x.shape.only(dims).names
-        assert channel(weights).rank == 1 and channel(weights).item_names is not None, f"weights must have one channel dimension listing the laplace dims but got {shape(weights)}"
-        assert set(channel(weights).item_names[0]) >= set(dim_names), f"the channel dim of weights must contain all laplace dims {dim_names} but only has {channel(weights).item_names}"
-        result *= rename_dims(weights, channel, batch('_laplace'))
-    result = math.sum_(result, '_laplace')
-    return result
-
-
-def fourier_laplace(grid: Tensor,
-                    dx: Union[Tensor, Shape, float, list, tuple],
-                    times: int = 1):
-    """
-    Applies the spatial laplace operator to the given tensor with periodic boundary conditions.
-    
-    *Note:* The results of `fourier_laplace` and `laplace` are close but not identical.
-    
-    This implementation computes the laplace operator in Fourier space.
-    The result for periodic fields is exact, i.e. no numerical instabilities can occur, even for higher-order derivatives.
-
-    Args:
-      grid: tensor, assumed to have periodic boundary conditions
-      dx: distance between grid points, tensor-like, scalar or vector
-      times: number of times the laplace operator is applied. The computational cost is independent of this parameter.
-      grid: Tensor: 
-      dx: Tensor or Shape or float or list or tuple: 
-      times: int:  (Default value = 1)
-
-    Returns:
-      tensor of same shape as `tensor`
-
-    """
-    frequencies = math.fft(math.to_complex(grid))
-    k_squared = math.sum_(math.fftfreq(grid.shape) ** 2, 'vector')
-    fft_laplace = -(2 * np.pi) ** 2 * k_squared
-    result = math.real(math.ifft(frequencies * fft_laplace ** times))
-    return math.cast(result / wrap(dx) ** 2, grid.dtype)
-
-
-def fourier_poisson(grid: Tensor,
-                    dx: Union[Tensor, Shape, float, list, tuple],
-                    times: int = 1):
-    """
-    Inverse operation to `fourier_laplace`.
-
-    Args:
-      grid: Tensor: 
-      dx: Tensor or Shape or float or list or tuple: 
-      times: int:  (Default value = 1)
-
-    Returns:
-
-    """
-    frequencies = math.fft(math.to_complex(grid))
-    k_squared = math.sum_(math.fftfreq(grid.shape) ** 2, 'vector')
-    fft_laplace = -(2 * np.pi) ** 2 * k_squared
-    # fft_laplace.tensor[(0,) * math.ndims(k_squared)] = math.inf  # assume NumPy array to edit
-    result = math.real(math.ifft(math.divide_no_nan(frequencies, math.to_complex(fft_laplace ** times))))
-    return math.cast(result * wrap(dx) ** 2, grid.dtype)
-
-
-# Downsample / Upsample
-
-def downsample2x(grid: Tensor,
-                 padding: Extrapolation = extrapolation.BOUNDARY,
-                 dims: DimFilter = spatial) -> Tensor:
-    """
-    Resamples a regular grid to half the number of spatial sample points per dimension.
-    The grid values at the new points are determined via mean (linear interpolation).
-
-    Args:
-      grid: full size grid
-      padding: grid extrapolation. Used to insert an additional value for odd spatial dims
-      dims: dims along which down-sampling is applied. If None, down-sample along all spatial dims.
-      grid: Tensor: 
-      padding: Extrapolation:  (Default value = extrapolation.BOUNDARY)
-      dims: tuple or None:  (Default value = None)
-
-    Returns:
-      half-size grid
-
-    """
-    dims = grid.shape.only(dims).names
-    odd_dimensions = [dim for dim in dims if grid.shape.get_size(dim) % 2 != 0]
-    grid = math.pad(grid, {dim: (0, 1) for dim in odd_dimensions}, padding)
-    for dim in dims:
-        grid = (grid[{dim: slice(1, None, 2)}] + grid[{dim: slice(0, None, 2)}]) / 2
-    return grid
-
-
-def upsample2x(grid: Tensor,
-               padding: Extrapolation = extrapolation.BOUNDARY,
-               dims: DimFilter = spatial) -> Tensor:
-    """
-    Resamples a regular grid to double the number of spatial sample points per dimension.
-    The grid values at the new points are determined via linear interpolation.
-
-    Args:
-      grid: half-size grid
-      padding: grid extrapolation
-      dims: dims along which up-sampling is applied. If None, up-sample along all spatial dims.
-      grid: Tensor: 
-      padding: Extrapolation:  (Default value = extrapolation.BOUNDARY)
-      dims: tuple or None:  (Default value = None)
-
-    Returns:
-      double-size grid
-
-    """
-    for dim in grid.shape.only(dims):
-        left, center, right = shift(grid, (-1, 0, 1), dim.names, padding, None)
-        interp_left = 0.25 * left + 0.75 * center
-        interp_right = 0.75 * center + 0.25 * right
-        stacked = math.stack_tensors([interp_left, interp_right], channel(_interleave='left,right'))
-        grid = math.pack_dims(stacked, (dim.name, '_interleave'), dim)
-    return grid
-
-
-def sample_subgrid(grid: Tensor, start: Tensor, size: Shape) -> Tensor:
-    """
-    Samples a sub-grid from `grid` with equal distance between sampling points.
-    The values at the new sample points are determined via linear interpolation.
-
-    Args:
-        grid: `Tensor` to be resampled. Values are assumed to be sampled at cell centers.
-        start: Origin point of sub-grid within `grid`, measured in number of cells.
-            Must have a single dimension called `vector`.
-            Example: `start=(1, 0.5)` would slice off the first grid point in dim 1 and take the mean of neighbouring points in dim 2.
-            The order of dims must be equal to `size` and `grid.shape.spatial`.
-        size: Resolution of the sub-grid. Must not be larger than the resolution of `grid`.
-            The order of dims must be equal to `start` and `grid.shape.spatial`.
-
-    Returns:
-      Sub-grid as `Tensor`
-    """
-    assert start.shape.names == ('vector',)
-    assert grid.shape.spatial.names == size.names
-    assert math.all_available(start), "Cannot perform sample_subgrid() during tracing, 'start' must be known."
-    crop = {}
-    for dim, d_start, d_size in zip(grid.shape.spatial.names, start, size.sizes):
-        crop[dim] = slice(int(d_start), int(d_start) + d_size + (0 if d_start % 1 in (0, 1) else 1))
-    grid = grid[crop]
-    upper_weight = start % 1
-    lower_weight = 1 - upper_weight
-    for i, dim in enumerate(grid.shape.spatial.names):
-        if upper_weight[i].native() not in (0, 1):
-            lower, upper = shift(grid, (0, 1), [dim], padding=None, stack_dim=None)
-            grid = upper * upper_weight[i] + lower * lower_weight[i]
-    return grid
-
-
-# Poisson Brackets
-
-
-def poisson_bracket(grid1, grid2):
-    if all([grid1.rank == grid2.rank == 2,
-            grid1.boundary == grid2.boundary == extrapolation.PERIODIC,
-            len(set(list(grid1.dx) + list(grid2.dx))) == 1]):
-        return _periodic_2d_arakawa_poisson_bracket(grid1.values, grid2.values, grid1.dx)
-    else:
-        raise NotImplementedError("\n".join([
-                                      "Not implemented for:"
-                                      f"ranks ({grid1.rank}, {grid2.rank}) != 2",
-                                      f"boundary ({grid1.boundary}, {grid2.boundary}) != {extrapolation.PERIODIC}",
-                                      f"dx uniform ({grid1.dx}, {grid2.dx})"
-                                  ]))
-
-
-def _periodic_2d_arakawa_poisson_bracket(tensor1: Tensor, tensor2: Tensor, dx: float):
-    """
-    Solves the poisson bracket using the Arakawa Scheme [tensor1, tensor2]
-    
-    Only works in 2D, with equal spaced grids, and periodic boundary conditions
-
-    Args:
-      tensor1(Tensor): first field in the poisson bracket
-      tensor2(Tensor): second field in the poisson bracket
-      dx(float): Grid size (equal in x-y)
-      tensor1: Tensor: 
-      tensor2: Tensor: 
-      dx: float: 
-
-    Returns:
-
-    """
-    zeta = math.pad(value=tensor1, widths={'x': (1, 1), 'y': (1, 1)}, mode=extrapolation.PERIODIC)
-    psi = math.pad(value=tensor2, widths={'x': (1, 1), 'y': (1, 1)}, mode=extrapolation.PERIODIC)
-    return (zeta.x[2:].y[1:-1] * (psi.x[1:-1].y[2:] - psi.x[1:-1].y[0:-2] + psi.x[2:].y[2:] - psi.x[2:].y[0:-2])
-            - zeta.x[0:-2].y[1:-1] * (psi.x[1:-1].y[2:] - psi.x[1:-1].y[0:-2] + psi.x[0:-2].y[2:] - psi.x[0:-2].y[0:-2])
-            - zeta.x[1:-1].y[2:] * (psi.x[2:].y[1:-1] - psi.x[0:-2].y[1:-1] + psi.x[2:].y[2:] - psi.x[0:-2].y[2:])
-            + zeta.x[1:-1].y[0:-2] * (psi.x[2:].y[1:-1] - psi.x[0:-2].y[1:-1] + psi.x[2:].y[0:-2] - psi.x[0:-2].y[0:-2])
-            + zeta.x[2:].y[0:-2] * (psi.x[2:].y[1:-1] - psi.x[1:-1].y[0:-2])
-            + zeta.x[2:].y[2:] * (psi.x[1:-1].y[2:] - psi.x[2:].y[1:-1])
-            - zeta.x[0:-2].y[2:] * (psi.x[1:-1].y[2:] - psi.x[0:-2].y[1:-1])
-            - zeta.x[0:-2].y[0:-2] * (psi.x[0:-2].y[1:-1] - psi.x[1:-1].y[0:-2])) / (12 * dx ** 2)
+from typing import Tuple, Optional, Union
+
+import numpy as np
+
+from . import _ops as math
+from . import extrapolation as extrapolation
+from ._magic_ops import stack, rename_dims, concat, variable_values
+from ._shape import Shape, channel, batch, spatial, DimFilter, parse_dim_order, shape, instance
+from ._tensors import Tensor, wrap, tensor
+from .extrapolation import Extrapolation
+from .magic import PhiTreeNode
+
+
+def vec(name: Union[str, Shape] = 'vector', *sequence, tuple_dim=spatial('sequence'), list_dim=instance('sequence'), **components) -> Tensor:
+    """
+    Lay out the given values along a channel dimension without converting them to the current backend.
+
+    Args:
+        name: Dimension name.
+        *sequence: Component values that will also be used as item names.
+            If specified, `components` must be empty.
+        **components: Values by component name.
+            If specified, no additional positional arguments must be given.
+        tuple_dim: Dimension for `tuple` values passed as components, e.g. `vec(x=(0, 1), ...)`
+        list_dim: Dimension for `list` values passed as components, e.g. `vec(x=[0, 1], ...)`
+
+    Returns:
+        `Tensor`
+
+    Examples:
+        >>> vec(x=1, y=0, z=-1)
+        (x=1, y=0, z=-1)
+
+        >>> vec(x=1., z=0)
+        (x=1.000, z=0.000)
+
+        >>> vec(x=tensor([1, 2, 3], instance('particles')), y=0)
+        (x=1, y=0); (x=2, y=0); (x=3, y=0) (particlesⁱ=3, vectorᶜ=x,y)
+
+        >>> vec(x=0, y=[0, 1])
+        (x=0, y=0); (x=0, y=1) (vectorᶜ=x,y, sequenceⁱ=2)
+
+        >>> vec(x=0, y=(0, 1))
+        (x=0, y=0); (x=0, y=1) (sequenceˢ=2, vectorᶜ=x,y)
+    """
+    dim = channel(name) if isinstance(name, str) else name
+    assert isinstance(dim, Shape), f"name must be a str or Shape but got '{type(name)}'"
+    if sequence:
+        assert not components, "vec() must be given either positional or keyword arguments but not both"
+        if len(sequence) == 1 and isinstance(sequence[0], (tuple, list)):
+            sequence = sequence[0]
+        dim = dim.with_size([str(v) for v in sequence])
+        return wrap(sequence, dim)
+    else:
+        def wrap_sequence(value):
+            if isinstance(value, tuple):
+                return wrap(value, tuple_dim)
+            elif isinstance(value, list):
+                return wrap(value, list_dim)
+            else:
+                return value
+        components = {n: wrap_sequence(v) for n, v in components.items()}
+        return stack(components, dim, expand_values=True)
+
+
+def const_vec(value: Union[float, Tensor], dim: Union[Shape, tuple, list, str]):
+    """
+    Creates a single-dimension tensor with all values equal to `value`.
+    `value` is not converted to the default backend, even when it is a Python primitive.
+
+    Args:
+        value: Value for filling the vector.
+        dim: Either single-dimension non-spatial Shape or `Shape` consisting of any number of spatial dimensions.
+            In the latter case, a new channel dimension named `'vector'` will be created from the spatial shape.
+
+    Returns:
+        `Tensor`
+    """
+    if isinstance(dim, Shape):
+        if dim.spatial:
+            assert not dim.non_spatial, f"When creating a vector given spatial dimensions, the shape may only contain spatial dimensions but got {dim}"
+            shape = channel(vector=dim.names)
+        else:
+            assert dim.rank == 1, f"Cannot create vector from {dim}"
+            shape = dim
+    else:
+        dims = parse_dim_order(dim)
+        shape = channel(vector=dims)
+    return wrap([value] * shape.size, shape)
+
+
+def vec_abs(vec: Tensor, vec_dim: DimFilter = channel, eps: Union[float, Tensor] = None):
+    """
+    Computes the vector length of `vec`.
+
+    Args:
+        eps: Minimum vector length. Use to avoid `inf` gradients for zero-length vectors.
+    """
+    if vec.dtype.kind == complex:
+        vec = stack([vec.real, vec.imag], channel('_ReIm'))
+    squared = vec_squared(vec, vec_dim)
+    if eps is not None:
+        squared = math.maximum(squared, eps)
+    return math.sqrt(squared)
+
+
+def vec_squared(vec: Tensor, vec_dim: DimFilter = channel):
+    """ Computes the squared length of `vec`. If `vec_dim` is None, the combined channel dimensions of `vec` are interpreted as a vector. """
+    return math.sum_(vec ** 2, dim=vec_dim)
+
+
+def vec_normalize(vec: Tensor, vec_dim: DimFilter = channel):
+    """ Normalizes the vectors in `vec`. If `vec_dim` is None, the combined channel dimensions of `vec` are interpreted as a vector. """
+    return vec / vec_abs(vec, vec_dim=vec_dim)
+
+
+def cross_product(vec1: Tensor, vec2: Tensor) -> Tensor:
+    """
+    Computes the cross product of two vectors in 2D.
+
+    Args:
+        vec1: `Tensor` with a single channel dimension called `'vector'`
+        vec2: `Tensor` with a single channel dimension called `'vector'`
+
+    Returns:
+        `Tensor`
+    """
+    vec1 = math.tensor(vec1)
+    vec2 = math.tensor(vec2)
+    spatial_rank = vec1.vector.size if 'vector' in vec1.shape else vec2.vector.size
+    if spatial_rank == 2:  # Curl in 2D
+        assert vec2.vector.exists
+        if vec1.vector.exists:
+            v1_x, v1_y = vec1.vector
+            v2_x, v2_y = vec2.vector
+            return v1_x * v2_y - v1_y * v2_x
+        else:
+            v2_x, v2_y = vec2.vector
+            return vec1 * math.stack_tensors([-v2_y, v2_x], channel('vector'))
+    elif spatial_rank == 3:  # Curl in 3D
+        raise NotImplementedError(f'spatial_rank={spatial_rank} not yet implemented')
+    else:
+        raise AssertionError(f'dims = {spatial_rank}. Vector product not available in > 3 dimensions')
+
+
+def rotate_vector(vector: math.Tensor, angle: Union[float, math.Tensor]) -> Tensor:
+    """
+    Rotates `vector` around the origin.
+
+    Args:
+        vector: n-dimensional vector with a channel dimension called `'vector'`
+        angle: Euler angle. The direction is the rotation axis and the length is the amount (in radians).
+
+    Returns:
+        Rotated vector as `Tensor`
+    """
+    assert 'vector' in vector.shape, "vector must have 'vector' dimension."
+    if vector.vector.size == 2:
+        sin = wrap(math.sin(angle))
+        cos = wrap(math.cos(angle))
+        x, y = vector.vector
+        rot_x = cos * x - sin * y
+        rot_y = sin * x + cos * y
+        return math.stack_tensors([rot_x, rot_y], channel(vector=vector.vector.item_names))
+    elif vector.vector.size == 1:
+        raise AssertionError(f"Cannot rotate a 1D vector. shape={vector.shape}")
+    else:
+        raise NotImplementedError(f"Rotation in {vector.vector.size}D not yet implemented.")
+
+
+def dim_mask(all_dims: Union[Shape, tuple, list], dims: DimFilter, mask_dim=channel('vector')) -> Tensor:
+    """
+    Creates a masked vector with 1 elements for `dims` and 0 for all other dimensions in `all_dims`.
+
+    Args:
+        all_dims: All dimensions for which the vector should have an entry.
+        dims: Dimensions marked as 1.
+        mask_dim: Dimension of the masked vector. Item names are assigned automatically.
+
+    Returns:
+        `Tensor`
+    """
+    assert isinstance(all_dims, (Shape, tuple, list)), f"all_dims must be a tuple or Shape but got {type(all_dims)}"
+    assert isinstance(mask_dim, Shape) and mask_dim.rank == 1, f"mask_dim must be a single-dimension Shape but got {mask_dim}"
+    if isinstance(all_dims, (tuple, list)):
+        all_dims = spatial(*all_dims)
+    dims = all_dims.only(dims)
+    mask = [1 if dim in dims else 0 for dim in all_dims]
+    mask_dim = mask_dim.with_size(all_dims.names)
+    return wrap(mask, mask_dim)
+
+
+def normalize_to(target: Tensor, source: Union[float, Tensor], epsilon=1e-5):
+    """
+    Multiplies the target so that its sum matches the source.
+
+    Args:
+        target: `Tensor`
+        source: `Tensor` or constant
+        epsilon: Small number to prevent division by zero.
+
+    Returns:
+        Normalized tensor of the same shape as target
+    """
+    target_total = math.sum_(target)
+    denominator = math.maximum(target_total, epsilon) if epsilon is not None else target_total
+    source_total = math.sum_(source)
+    return target * (source_total / denominator)
+
+
+def l1_loss(x, reduce: DimFilter = math.non_batch) -> Tensor:
+    """
+    Computes *∑<sub>i</sub> ||x<sub>i</sub>||<sub>1</sub>*, summing over all non-batch dimensions.
+
+    Args:
+        x: `Tensor` or `phi.math.magic.PhiTreeNode` or 0D or 1D native tensor.
+            For `phi.math.magic.PhiTreeNode` objects, only value the sum over all value attributes is computed.
+        reduce: Dimensions to reduce as `DimFilter`.
+
+    Returns:
+        loss: `Tensor`
+    """
+    if isinstance(x, Tensor):
+        return math.sum_(abs(x), reduce)
+    elif isinstance(x, PhiTreeNode):
+        return sum([l1_loss(getattr(x, a), reduce) for a in variable_values(x)])
+    else:
+        try:
+            backend = math.choose_backend(x)
+            shape = backend.staticshape(x)
+            if len(shape) == 0:
+                return abs(x)
+            elif len(shape) == 1:
+                return backend.sum(abs(x))
+            else:
+                raise ValueError("l2_loss is only defined for 0D and 1D native tensors. For higher-dimensional data, use Φ-Flow tensors.")
+        except math.NoBackendFound:
+            raise ValueError(x)
+
+
+def l2_loss(x, reduce: DimFilter = math.non_batch) -> Tensor:
+    """
+    Computes *∑<sub>i</sub> ||x<sub>i</sub>||<sub>2</sub><sup>2</sup> / 2*, summing over all non-batch dimensions.
+
+    Args:
+        x: `Tensor` or `phi.math.magic.PhiTreeNode` or 0D or 1D native tensor.
+            For `phi.math.magic.PhiTreeNode` objects, only value the sum over all value attributes is computed.
+        reduce: Dimensions to reduce as `DimFilter`.
+
+    Returns:
+        loss: `Tensor`
+    """
+    if isinstance(x, Tensor):
+        if x.dtype.kind == complex:
+            x = abs(x)
+        return math.sum_(x ** 2, reduce) * 0.5
+    elif isinstance(x, PhiTreeNode):
+        return sum([l2_loss(getattr(x, a), reduce) for a in variable_values(x)])
+    else:
+        try:
+            backend = math.choose_backend(x)
+            shape = backend.staticshape(x)
+            if len(shape) == 0:
+                return x ** 2 * 0.5
+            elif len(shape) == 1:
+                return backend.sum(x ** 2) * 0.5
+            else:
+                raise ValueError("l2_loss is only defined for 0D and 1D native tensors. For higher-dimensional data, use Φ-Flow tensors.")
+        except math.NoBackendFound:
+            raise ValueError(x)
+
+
+def frequency_loss(x,
+                   frequency_falloff: float = 100,
+                   threshold=1e-5,
+                   ignore_mean=False,
+                   n=2) -> Tensor:
+    """
+    Penalizes the squared `values` in frequency (Fourier) space.
+    Lower frequencies are weighted more strongly then higher frequencies, depending on `frequency_falloff`.
+
+    Args:
+        x: `Tensor` or `phi.math.magic.PhiTreeNode` Values to penalize, typically `actual - target`.
+        frequency_falloff: Large values put more emphasis on lower frequencies, 1.0 weights all frequencies equally.
+            *Note*: The total loss is not normalized. Varying the value will result in losses of different magnitudes.
+        threshold: Frequency amplitudes below this value are ignored.
+            Setting this to zero may cause infinities or NaN values during backpropagation.
+        ignore_mean: If `True`, does not penalize the mean value (frequency=0 component).
+
+    Returns:
+      Scalar loss value
+    """
+    assert n in (1, 2)
+    if isinstance(x, Tensor):
+        if ignore_mean:
+            x -= math.mean(x, x.shape.non_batch)
+        k_squared = vec_squared(math.fftfreq(x.shape.spatial))
+        weights = math.exp(-0.5 * k_squared * frequency_falloff ** 2)
+
+        diff_fft = abs_square(math.fft(x) * weights)
+        diff_fft = math.sqrt(math.maximum(diff_fft, threshold))
+        return l2_loss(diff_fft) if n == 2 else l1_loss(diff_fft)
+    elif isinstance(x, PhiTreeNode):
+        losses = [frequency_loss(getattr(x, a), frequency_falloff, threshold, ignore_mean, n) for a in variable_values(x)]
+        return sum(losses)
+    else:
+        raise ValueError(x)
+
+
+def abs_square(complex_values: Tensor) -> Tensor:
+    """
+    Squared magnitude of complex values.
+
+    Args:
+      complex_values: complex `Tensor`
+
+    Returns:
+        Tensor: real valued magnitude squared
+
+    """
+    return math.imag(complex_values) ** 2 + math.real(complex_values) ** 2
+
+
+# Divergence
+
+# def divergence(tensor, dx=1, difference='central', padding='constant', dimensions=None):
+#     """
+#     Computes the spatial divergence of a vector channel from finite differences.
+#
+#     :param tensor: vector field; tensor of shape (batch size, spatial dimensions..., spatial rank)
+#     :param dx: distance between adjacent grid points (default 1)
+#     :param difference: type of difference, one of ('forward', 'central') (default 'forward')
+#     :return: tensor of shape (batch size, spatial dimensions..., 1)
+#     """
+#     assert difference in ('central', 'forward', 'backward'), difference
+#     rank = spatial_rank(tensor)
+#     if difference == 'forward':
+#         return _divergence_nd(tensor, padding, (0, 1), dims) / dx ** rank  # TODO why dx^rank?
+#     elif difference == 'backward':
+#         return _divergence_nd(tensor, padding, (-1, 0), dims) / dx ** rank
+#     else:
+#         return _divergence_nd(tensor, padding, (-1, 1), dims) / (2 * dx) ** rank
+#
+#
+# def _divergence_nd(x_, padding, relative_shifts, dims=None):
+#     x = tensor(x_)
+#     assert x.shape.channel.rank == 1
+#     dims = dims if dims is not None else x.shape.spatial.names
+#     x = math.pad(x, {axis: (-relative_shifts[0], relative_shifts[1]) for axis in dims}, mode=padding)
+#     components = []
+#     for dimension in dims:
+#         dim_index_in_spatial = x.shape.spatial.reset_indices().index(dimension)
+#         lower, upper = _multi_roll(x, dimension, relative_shifts, diminish_others=(-relative_shifts[0], relative_shifts[1]), names=dims, base_selection={0: rank - dimension - 1})
+#         components.append(upper - lower)
+#     return math.sum_(components, 0)
+
+
+def shift(x: Tensor,
+          offsets: tuple,
+          dims: DimFilter = math.spatial,
+          padding: Union[Extrapolation, Tensor, float, None] = extrapolation.BOUNDARY,
+          stack_dim: Optional[Shape] = channel('shift'),
+          extend_bounds=0) -> list:
+    """
+    shift Tensor by a fixed offset and abiding by extrapolation
+
+    Args:
+        x: Input data
+        offsets: Shift size
+        dims: Dimensions along which to shift, defaults to None
+        padding: padding to be performed at the boundary, defaults to extrapolation.BOUNDARY
+        stack_dim: dimensions to be stacked, defaults to 'shift'
+
+    Returns:
+        list: offset_tensor
+
+    """
+    if dims is None:
+        raise ValueError("dims=None is not supported anymore.")
+    dims = x.shape.only(dims).names
+    if stack_dim is None:
+        assert len(dims) == 1
+    x = wrap(x)
+    pad_lower = max(0, -min(offsets))
+    pad_upper = max(0, max(offsets))
+    if padding is not None:
+        x = math.pad(x, {axis: (pad_lower + extend_bounds, pad_upper + extend_bounds) for axis in dims}, mode=padding)
+    if extend_bounds:
+        assert padding is not None
+    offset_tensors = []
+    for offset in offsets:
+        components = []
+        for dimension in dims:
+            if padding:
+                slices = {dim: slice(pad_lower + offset, (-pad_upper + offset) or None) if dim == dimension else slice(pad_lower, -pad_upper or None) for dim in dims}
+            else:
+                slices = {dim: slice(pad_lower + offset, (-pad_upper + offset) or None) if dim == dimension else slice(None, None) for dim in dims}
+            components.append(x[slices])
+        offset_tensors.append(stack(components, stack_dim) if stack_dim is not None else components[0])
+    return offset_tensors
+
+
+def masked_fill(values: Tensor, valid: Tensor, distance: int = 1) -> Tuple[Tensor, Tensor]:
+    """
+    Extrapolates the values of `values` which are marked by the nonzero values of `valid` for `distance` steps in all spatial directions.
+    Overlapping extrapolated values get averaged. Extrapolation also includes diagonals.
+
+    Args:
+        values: Tensor which holds the values for extrapolation
+        valid: Tensor with same size as `x` marking the values for extrapolation with nonzero values
+        distance: Number of extrapolation steps
+
+    Returns:
+        values: Extrapolation result
+        valid: mask marking all valid values after extrapolation
+    """
+    def binarize(x):
+        return math.safe_div(x, x)
+    distance = min(distance, max(values.shape.sizes))
+    for _ in range(distance):
+        valid = binarize(valid)
+        valid_values = valid * values
+        overlap = valid  # count how many values we are adding
+        for dim in values.shape.spatial.names:
+            values_l, values_r = shift(valid_values, (-1, 1), dims=dim, padding=extrapolation.ZERO)
+            valid_values = math.sum_(values_l + values_r + valid_values, dim='shift')
+            mask_l, mask_r = shift(overlap, (-1, 1), dims=dim, padding=extrapolation.ZERO)
+            overlap = math.sum_(mask_l + mask_r + overlap, dim='shift')
+        extp = math.safe_div(valid_values, overlap)  # take mean where extrapolated values overlap
+        values = math.where(valid, values, math.where(binarize(overlap), extp, values))
+        valid = overlap
+    return values, binarize(valid)
+
+
+def finite_fill(values: Tensor, dims: DimFilter = spatial, distance: int = 1, diagonal: bool = True, padding=extrapolation.BOUNDARY) -> Tuple[Tensor, Tensor]:
+    """
+    Fills non-finite (NaN, inf, -inf) values from nearby finite values.
+    Extrapolates the finite values of `values` for `distance` steps along `dims`.
+    Where multiple finite values could fill an invalid value, the average is computed.
+
+    Args:
+        values: Floating-point `Tensor`. All non-numeric values (`NaN`, `inf`, `-inf`) are interpreted as invalid.
+        dims: Dimensions along which to fill invalid values from finite ones.
+        distance: Number of extrapolation steps, each extrapolating one cell out.
+        diagonal: Whether to extrapolate values to their diagonal neighbors per step.
+        padding: Extrapolation of `values`. Determines whether to extrapolate from the edges as well.
+
+    Returns:
+        `Tensor` of same shape as `values`.
+    """
+    if diagonal:
+        distance = min(distance, max(values.shape.sizes))
+        dims = values.shape.only(dims)
+        for _ in range(distance):
+            valid = math.is_finite(values)
+            valid_values = math.where(valid, values, 0)
+            overlap = valid
+            for dim in dims:
+                values_l, values_r = shift(valid_values, (-1, 1), dims=dim, padding=padding)
+                valid_values = math.sum_(values_l + values_r + valid_values, dim='shift')
+                mask_l, mask_r = shift(overlap, (-1, 1), dims=dim, padding=padding)
+                overlap = math.sum_(mask_l + mask_r + overlap, dim='shift')
+            values = math.where(valid, values, valid_values / overlap)
+    else:
+        distance = min(distance, sum(values.shape.sizes))
+        for _ in range(distance):
+            neighbors = concat(shift(values, (-1, 1), dims, padding=padding, stack_dim=channel('neighbors')), 'neighbors')
+            finite = math.is_finite(neighbors)
+            avg_neighbors = math.sum_(math.where(finite, neighbors, 0), 'neighbors') / math.sum_(finite, 'neighbors')
+            values = math.where(math.is_finite(values), values, avg_neighbors)
+    return values
+
+
+# Gradient
+
+def spatial_gradient(grid: Tensor,
+                     dx: Union[float, Tensor] = 1,
+                     difference: str = 'central',
+                     padding: Union[Extrapolation, None] = extrapolation.BOUNDARY,
+                     dims: DimFilter = spatial,
+                     stack_dim: Union[Shape, None] = channel('gradient'),
+                     pad=0) -> Tensor:
+    """
+    Calculates the spatial_gradient of a scalar channel from finite differences.
+    The spatial_gradient vectors are in reverse order, lowest dimension first.
+
+    Args:
+        grid: grid values
+        dims: (Optional) Dimensions along which the spatial derivative will be computed. sequence of dimension names
+        dx: Physical distance between grid points, `float` or `Tensor`.
+            When passing a vector-valued `Tensor`, the dx values should be listed along `stack_dim`, matching `dims`.
+        difference: type of difference, one of ('forward', 'backward', 'central') (default 'forward')
+        padding: tensor padding mode
+        stack_dim: name of the new vector dimension listing the spatial_gradient w.r.t. the various axes
+        pad: How many cells to extend the result compared to `grid`.
+            This value is added to the internal padding. For non-trivial extrapolations, this gives the correct result while manual padding before or after this operation would not respect the boundary locations.
+
+    Returns:
+        `Tensor`
+    """
+    grid = wrap(grid)
+    if stack_dim is not None and stack_dim in grid.shape:
+        assert grid.shape.only(stack_dim).size == 1, f"spatial_gradient() cannot list components along {stack_dim.name} because that dimension already exists on grid {grid}"
+        grid = grid[{stack_dim.name: 0}]
+    dims = grid.shape.only(dims)
+    dx = wrap(dx)
+    if dx.vector.exists:
+        dx = dx.vector[dims]
+        if dx.vector.size in (None, 1):
+            dx = dx.vector[0]
+    if difference.lower() == 'central':
+        left, right = shift(grid, (-1, 1), dims, padding, stack_dim=stack_dim, extend_bounds=pad)
+        return (right - left) / (dx * 2)
+    elif difference.lower() == 'forward':
+        left, right = shift(grid, (0, 1), dims, padding, stack_dim=stack_dim, extend_bounds=pad)
+        return (right - left) / dx
+    elif difference.lower() == 'backward':
+        left, right = shift(grid, (-1, 0), dims, padding, stack_dim=stack_dim, extend_bounds=pad)
+        return (right - left) / dx
+    else:
+        raise ValueError('Invalid difference type: {}. Can be CENTRAL or FORWARD'.format(difference))
+
+
+# Laplace
+
+def laplace(x: Tensor,
+            dx: Union[Tensor, float] = 1,
+            padding: Union[Extrapolation, float, Tensor] = extrapolation.BOUNDARY,
+            dims: DimFilter = spatial,
+            weights: Tensor = None):
+    """
+    Spatial Laplace operator as defined for scalar fields.
+    If a vector field is passed, the laplace is computed component-wise.
+
+    Args:
+        x: n-dimensional field of shape (batch, spacial dimensions..., components)
+        dx: scalar or 1d tensor
+        padding: extrapolation
+        dims: The second derivative along these dimensions is summed over
+        weights: (Optional) Multiply the axis terms by these factors before summation.
+            Must be a Tensor with a single channel dimension that lists all laplace dims by name.
+
+    Returns:
+        `phi.math.Tensor` of same shape as `x`
+    """
+    if isinstance(dx, (tuple, list)):
+        dx = wrap(dx, batch('_laplace'))
+    elif isinstance(dx, Tensor) and dx.vector.exists:
+        dx = rename_dims(dx, 'vector', batch('_laplace'))
+    if isinstance(x, Extrapolation):
+        return x.spatial_gradient()
+    left, center, right = shift(wrap(x), (-1, 0, 1), dims, padding, stack_dim=batch('_laplace'))
+    result = (left + right - 2 * center) / (dx ** 2)
+    if weights is not None:
+        dim_names = x.shape.only(dims).names
+        assert channel(weights).rank == 1 and channel(weights).item_names is not None, f"weights must have one channel dimension listing the laplace dims but got {shape(weights)}"
+        assert set(channel(weights).item_names[0]) >= set(dim_names), f"the channel dim of weights must contain all laplace dims {dim_names} but only has {channel(weights).item_names}"
+        result *= rename_dims(weights, channel, batch('_laplace'))
+    result = math.sum_(result, '_laplace')
+    return result
+
+
+def fourier_laplace(grid: Tensor,
+                    dx: Union[Tensor, Shape, float, list, tuple],
+                    times: int = 1):
+    """
+    Applies the spatial laplace operator to the given tensor with periodic boundary conditions.
+    
+    *Note:* The results of `fourier_laplace` and `laplace` are close but not identical.
+    
+    This implementation computes the laplace operator in Fourier space.
+    The result for periodic fields is exact, i.e. no numerical instabilities can occur, even for higher-order derivatives.
+
+    Args:
+      grid: tensor, assumed to have periodic boundary conditions
+      dx: distance between grid points, tensor-like, scalar or vector
+      times: number of times the laplace operator is applied. The computational cost is independent of this parameter.
+      grid: Tensor: 
+      dx: Tensor or Shape or float or list or tuple: 
+      times: int:  (Default value = 1)
+
+    Returns:
+      tensor of same shape as `tensor`
+
+    """
+    frequencies = math.fft(math.to_complex(grid))
+    k_squared = math.sum_(math.fftfreq(grid.shape) ** 2, 'vector')
+    fft_laplace = -(2 * np.pi) ** 2 * k_squared
+    result = math.real(math.ifft(frequencies * fft_laplace ** times))
+    return math.cast(result / wrap(dx) ** 2, grid.dtype)
+
+
+def fourier_poisson(grid: Tensor,
+                    dx: Union[Tensor, Shape, float, list, tuple],
+                    times: int = 1):
+    """
+    Inverse operation to `fourier_laplace`.
+
+    Args:
+      grid: Tensor: 
+      dx: Tensor or Shape or float or list or tuple: 
+      times: int:  (Default value = 1)
+
+    Returns:
+
+    """
+    frequencies = math.fft(math.to_complex(grid))
+    k_squared = math.sum_(math.fftfreq(grid.shape) ** 2, 'vector')
+    fft_laplace = -(2 * np.pi) ** 2 * k_squared
+    # fft_laplace.tensor[(0,) * math.ndims(k_squared)] = math.inf  # assume NumPy array to edit
+    result = math.real(math.ifft(math.safe_div(frequencies, math.to_complex(fft_laplace ** times))))
+    return math.cast(result * wrap(dx) ** 2, grid.dtype)
+
+
+# Downsample / Upsample
+
+def downsample2x(grid: Tensor,
+                 padding: Extrapolation = extrapolation.BOUNDARY,
+                 dims: DimFilter = spatial) -> Tensor:
+    """
+    Resamples a regular grid to half the number of spatial sample points per dimension.
+    The grid values at the new points are determined via mean (linear interpolation).
+
+    Args:
+      grid: full size grid
+      padding: grid extrapolation. Used to insert an additional value for odd spatial dims
+      dims: dims along which down-sampling is applied. If None, down-sample along all spatial dims.
+      grid: Tensor: 
+      padding: Extrapolation:  (Default value = extrapolation.BOUNDARY)
+      dims: tuple or None:  (Default value = None)
+
+    Returns:
+      half-size grid
+
+    """
+    dims = grid.shape.only(dims).names
+    odd_dimensions = [dim for dim in dims if grid.shape.get_size(dim) % 2 != 0]
+    grid = math.pad(grid, {dim: (0, 1) for dim in odd_dimensions}, padding)
+    for dim in dims:
+        grid = (grid[{dim: slice(1, None, 2)}] + grid[{dim: slice(0, None, 2)}]) / 2
+    return grid
+
+
+def upsample2x(grid: Tensor,
+               padding: Extrapolation = extrapolation.BOUNDARY,
+               dims: DimFilter = spatial) -> Tensor:
+    """
+    Resamples a regular grid to double the number of spatial sample points per dimension.
+    The grid values at the new points are determined via linear interpolation.
+
+    Args:
+      grid: half-size grid
+      padding: grid extrapolation
+      dims: dims along which up-sampling is applied. If None, up-sample along all spatial dims.
+      grid: Tensor: 
+      padding: Extrapolation:  (Default value = extrapolation.BOUNDARY)
+      dims: tuple or None:  (Default value = None)
+
+    Returns:
+      double-size grid
+
+    """
+    for dim in grid.shape.only(dims):
+        left, center, right = shift(grid, (-1, 0, 1), dim.names, padding, None)
+        interp_left = 0.25 * left + 0.75 * center
+        interp_right = 0.75 * center + 0.25 * right
+        stacked = math.stack_tensors([interp_left, interp_right], channel(_interleave='left,right'))
+        grid = math.pack_dims(stacked, (dim.name, '_interleave'), dim)
+    return grid
+
+
+def sample_subgrid(grid: Tensor, start: Tensor, size: Shape) -> Tensor:
+    """
+    Samples a sub-grid from `grid` with equal distance between sampling points.
+    The values at the new sample points are determined via linear interpolation.
+
+    Args:
+        grid: `Tensor` to be resampled. Values are assumed to be sampled at cell centers.
+        start: Origin point of sub-grid within `grid`, measured in number of cells.
+            Must have a single dimension called `vector`.
+            Example: `start=(1, 0.5)` would slice off the first grid point in dim 1 and take the mean of neighbouring points in dim 2.
+            The order of dims must be equal to `size` and `grid.shape.spatial`.
+        size: Resolution of the sub-grid. Must not be larger than the resolution of `grid`.
+            The order of dims must be equal to `start` and `grid.shape.spatial`.
+
+    Returns:
+      Sub-grid as `Tensor`
+    """
+    assert start.shape.names == ('vector',)
+    assert grid.shape.spatial.names == size.names
+    assert math.all_available(start), "Cannot perform sample_subgrid() during tracing, 'start' must be known."
+    crop = {}
+    for dim, d_start, d_size in zip(grid.shape.spatial.names, start, size.sizes):
+        crop[dim] = slice(int(d_start), int(d_start) + d_size + (0 if d_start % 1 in (0, 1) else 1))
+    grid = grid[crop]
+    upper_weight = start % 1
+    lower_weight = 1 - upper_weight
+    for i, dim in enumerate(grid.shape.spatial.names):
+        if upper_weight[i].native() not in (0, 1):
+            lower, upper = shift(grid, (0, 1), [dim], padding=None, stack_dim=None)
+            grid = upper * upper_weight[i] + lower * lower_weight[i]
+    return grid
+
+
+# Poisson Brackets
+
+
+def poisson_bracket(grid1, grid2):
+    if all([grid1.rank == grid2.rank == 2,
+            grid1.boundary == grid2.boundary == extrapolation.PERIODIC,
+            len(set(list(grid1.dx) + list(grid2.dx))) == 1]):
+        return _periodic_2d_arakawa_poisson_bracket(grid1.values, grid2.values, grid1.dx)
+    else:
+        raise NotImplementedError("\n".join([
+                                      "Not implemented for:"
+                                      f"ranks ({grid1.rank}, {grid2.rank}) != 2",
+                                      f"boundary ({grid1.boundary}, {grid2.boundary}) != {extrapolation.PERIODIC}",
+                                      f"dx uniform ({grid1.dx}, {grid2.dx})"
+                                  ]))
+
+
+def _periodic_2d_arakawa_poisson_bracket(tensor1: Tensor, tensor2: Tensor, dx: float):
+    """
+    Solves the poisson bracket using the Arakawa Scheme [tensor1, tensor2]
+    
+    Only works in 2D, with equal spaced grids, and periodic boundary conditions
+
+    Args:
+      tensor1(Tensor): first field in the poisson bracket
+      tensor2(Tensor): second field in the poisson bracket
+      dx(float): Grid size (equal in x-y)
+      tensor1: Tensor: 
+      tensor2: Tensor: 
+      dx: float: 
+
+    Returns:
+
+    """
+    zeta = math.pad(value=tensor1, widths={'x': (1, 1), 'y': (1, 1)}, mode=extrapolation.PERIODIC)
+    psi = math.pad(value=tensor2, widths={'x': (1, 1), 'y': (1, 1)}, mode=extrapolation.PERIODIC)
+    return (zeta.x[2:].y[1:-1] * (psi.x[1:-1].y[2:] - psi.x[1:-1].y[0:-2] + psi.x[2:].y[2:] - psi.x[2:].y[0:-2])
+            - zeta.x[0:-2].y[1:-1] * (psi.x[1:-1].y[2:] - psi.x[1:-1].y[0:-2] + psi.x[0:-2].y[2:] - psi.x[0:-2].y[0:-2])
+            - zeta.x[1:-1].y[2:] * (psi.x[2:].y[1:-1] - psi.x[0:-2].y[1:-1] + psi.x[2:].y[2:] - psi.x[0:-2].y[2:])
+            + zeta.x[1:-1].y[0:-2] * (psi.x[2:].y[1:-1] - psi.x[0:-2].y[1:-1] + psi.x[2:].y[0:-2] - psi.x[0:-2].y[0:-2])
+            + zeta.x[2:].y[0:-2] * (psi.x[2:].y[1:-1] - psi.x[1:-1].y[0:-2])
+            + zeta.x[2:].y[2:] * (psi.x[1:-1].y[2:] - psi.x[2:].y[1:-1])
+            - zeta.x[0:-2].y[2:] * (psi.x[1:-1].y[2:] - psi.x[0:-2].y[1:-1])
+            - zeta.x[0:-2].y[0:-2] * (psi.x[0:-2].y[1:-1] - psi.x[1:-1].y[0:-2])) / (12 * dx ** 2)
```

### Comparing `phiflow-2.3.4/phi/math/_ops.py` & `phiflow-2.4.0/phi/math/_ops.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,2461 +1,2710 @@
-import functools
-import math
-import warnings
-from numbers import Number
-from typing import Tuple, Callable, Any, Union
-
-import numpy as np
-
-from . import extrapolation as e_
-from ._magic_ops import expand, pack_dims, unpack_dim, cast, copy_with, value_attributes, bool_to_int
-from ._shape import (Shape, EMPTY_SHAPE,
-                     spatial, batch, channel, instance, merge_shapes, parse_dim_order, concat_shapes,
-                     IncompatibleShapes, DimFilter, non_batch, non_channel)
-from ._sparse import CompressedSparseMatrix, dot_compressed_dense, dense, SparseCoordinateTensor, dot_coordinate_dense
-from ._tensors import Tensor, wrap, tensor, broadcastable_native_tensors, NativeTensor, TensorStack, CollapsedTensor, \
-    custom_op2, compatible_tensor, variable_attributes, disassemble_tree, assemble_tree, \
-    is_scalar, Layout
-from .backend import default_backend, choose_backend, Backend, get_precision, convert as b_convert, BACKENDS, \
-    NoBackendFound
-from .backend._dtype import DType, combine_types
-from .magic import PhiTreeNode
-
-
-def choose_backend_t(*values, prefer_default=False) -> Backend:
-    """
-    Choose backend for given `Tensor` or native tensor values.
-    Backends need to be registered to be available, e.g. via the global import `phi.<backend>` or `phi.detect_backends()`.
-
-    Args:
-        *values: Sequence of `Tensor`s, native tensors or constants.
-        prefer_default: Whether to always select the default backend if it can work with `values`, see `default_backend()`.
-
-    Returns:
-        The selected `phi.math.backend.Backend`
-    """
-    natives = sum([v._natives() if isinstance(v, Tensor) else (v,) for v in values], ())
-    return choose_backend(*natives, prefer_default=prefer_default)
-
-
-def convert(x, backend: Backend = None, use_dlpack=True):
-    """
-    Convert the native representation of a `Tensor` or `phi.math.magic.PhiTreeNode` to the native format of `backend`.
-
-    *Warning*: This operation breaks the automatic differentiation chain.
-
-    See Also:
-        `phi.math.backend.convert()`.
-
-    Args:
-        x: `Tensor` to convert. If `x` is a `phi.math.magic.PhiTreeNode`, its variable attributes are converted.
-        backend: Target backend. If `None`, uses the current default backend, see `phi.math.backend.default_backend()`.
-
-    Returns:
-        `Tensor` with native representation belonging to `backend`.
-    """
-    if isinstance(x, Tensor):
-        return x._op1(lambda native: b_convert(native, backend, use_dlpack=use_dlpack))
-    elif isinstance(x, PhiTreeNode):
-        return copy_with(x, **{a: convert(getattr(x, a), backend, use_dlpack=use_dlpack) for a in variable_attributes(x)})
-    else:
-        return choose_backend(x).as_tensor(x)
-
-
-def all_available(*values: Tensor) -> bool:
-    """
-    Tests if the values of all given tensors are known and can be read at this point.
-    Tracing placeholders are considered not available, even when they hold example values.
-
-    Tensors are not available during `jit_compile()`, `jit_compile_linear()` or while using TensorFlow's legacy graph mode.
-    
-    Tensors are typically available when the backend operates in eager mode and is not currently tracing a function.
-
-    This can be used instead of the native checks
-
-    * PyTorch: `torch._C._get_tracing_state()`
-    * TensorFlow: `tf.executing_eagerly()`
-    * Jax: `isinstance(x, jax.core.Tracer)`
-
-    Args:
-      values: Tensors to check.
-
-    Returns:
-        `True` if no value is a placeholder or being traced, `False` otherwise.
-    """
-    return all([v.available for v in values])
-
-
-def seed(seed: int):
-    """
-    Sets the current seed of all backends and the built-in `random` package.
-
-    Calling this function with a fixed value at the start of an application yields reproducible results
-    as long as the same backend is used.
-
-    Args:
-        seed: Seed to use.
-    """
-    for backend in BACKENDS:
-        backend.seed(seed)
-    import random
-    random.seed(0)
-
-
-def native(value: Union[Tensor, Number, tuple, list, Any]):
-    """
-    Returns the native tensor representation of `value`.
-    If `value` is a `phi.math.Tensor`, this is equal to calling `phi.math.Tensor.native()`.
-    Otherwise, checks that `value` is a valid tensor object and returns it.
-
-    Args:
-        value: `Tensor` or native tensor or tensor-like.
-
-    Returns:
-        Native tensor representation
-
-    Raises:
-        ValueError if the tensor cannot be transposed to match target_shape
-    """
-    if isinstance(value, Tensor):
-        return value.native()
-    else:
-        choose_backend(value)  # check that value is a native tensor
-        return value
-
-
-def numpy(value: Union[Tensor, Number, tuple, list, Any]):
-    """
-    Converts `value` to a `numpy.ndarray` where value must be a `Tensor`, backend tensor or tensor-like.
-    If `value` is a `phi.math.Tensor`, this is equal to calling `phi.math.Tensor.numpy()`.
-
-    *Note*: Using this function breaks the autograd chain. The returned tensor is not differentiable.
-    To get a differentiable tensor, use `Tensor.native()` instead.
-
-    Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
-    If a dimension of the tensor is not listed in `order`, a `ValueError` is raised.
-
-    If `value` is a NumPy array, it may be returned directly.
-
-    Returns:
-        NumPy representation of `value`
-
-    Raises:
-        ValueError if the tensor cannot be transposed to match target_shape
-    """
-    if isinstance(value, Tensor):
-        return value.numpy()
-    else:
-        backend = choose_backend(value)
-        return backend.numpy(value)
-
-
-def reshaped_native(value: Tensor,
-                    groups: Union[tuple, list],
-                    force_expand: Any = False,
-                    to_numpy=False):
-    """
-    Returns a native representation of `value` where dimensions are laid out according to `groups`.
-
-    See Also:
-        `native()`, `pack_dims()`, `reshaped_tensor()`, `reshaped_numpy()`.
-
-    Args:
-        value: `Tensor`
-        groups: `tuple` or `list` of dimensions to be packed into one native dimension. Each entry must be one of the following:
-
-            * `str`: the name of one dimension that is present on `value`.
-            * `Shape`: Dimensions to be packed. If `force_expand`, missing dimensions are first added, otherwise they are ignored.
-            * Filter function: Packs all dimensions of this type that are present on `value`.
-
-        force_expand: `bool` or sequence of dimensions.
-            If `True`, repeats the tensor along missing dimensions.
-            If `False`, puts singleton dimensions where possible.
-            If a sequence of dimensions is provided, only forces the expansion for groups containing those dimensions.
-        to_numpy: If True, converts the native tensor to a `numpy.ndarray`.
-
-    Returns:
-        Native tensor with dimensions matching `groups`.
-    """
-    assert isinstance(value, Tensor), f"value must be a Tensor but got {type(value)}"
-    assert isinstance(groups, (tuple, list)), f"groups must be a tuple or list but got {type(value)}"
-    order = []
-    groups = [group(value) if callable(group) else group for group in groups]
-    for i, group in enumerate(groups):
-        if isinstance(group, Shape):
-            present = value.shape.only(group)
-            if force_expand is True or present.volume > 1 or (force_expand is not False and group.only(force_expand).volume > 1):
-                value = expand(value, group)
-            value = pack_dims(value, group, batch(f"group{i}"))
-            order.append(f"group{i}")
-        else:
-            assert isinstance(group, str), f"Groups must be either single-dim str or Shape but got {group}"
-            assert ',' not in group, f"When packing multiple dimensions, pass a well-defined Shape instead of a comma-separated str. Got {group}"
-            order.append(group)
-    return value.numpy(order) if to_numpy else value.native(order)
-
-
-def reshaped_numpy(value: Tensor, groups: Union[tuple, list], force_expand: Any = False):
-    """
-    Returns the NumPy representation of `value` where dimensions are laid out according to `groups`.
-
-    See Also:
-        `numpy()`, `reshaped_native()`, `pack_dims()`, `reshaped_tensor()`.
-
-    Args:
-        value: `Tensor`
-        groups: Sequence of dimension names as `str` or groups of dimensions to be packed_dim as `Shape`.
-        force_expand: `bool` or sequence of dimensions.
-            If `True`, repeats the tensor along missing dimensions.
-            If `False`, puts singleton dimensions where possible.
-            If a sequence of dimensions is provided, only forces the expansion for groups containing those dimensions.
-
-    Returns:
-        NumPy `ndarray` with dimensions matching `groups`.
-    """
-    return reshaped_native(value, groups, force_expand=force_expand, to_numpy=True)
-
-
-def reshaped_tensor(value: Any,
-                    groups: Union[tuple, list],
-                    check_sizes=False,
-                    convert=True):
-    """
-    Creates a `Tensor` from a native tensor or tensor-like whereby the dimensions of `value` are split according to `groups`.
-
-    See Also:
-        `phi.math.tensor()`, `reshaped_native()`, `unpack_dim()`.
-
-    Args:
-        value: Native tensor or tensor-like.
-        groups: Sequence of dimension groups to be packed_dim as `tuple[Shape]` or `list[Shape]`.
-        check_sizes: If True, group sizes must match the sizes of `value` exactly. Otherwise, allows singleton dimensions.
-        convert: If True, converts the data to the native format of the current default backend.
-            If False, wraps the data in a `Tensor` but keeps the given data reference if possible.
-
-    Returns:
-        `Tensor` with all dimensions from `groups`
-    """
-    assert all(isinstance(g, Shape) for g in groups), "groups must be a sequence of Shapes"
-    dims = [batch(f'group{i}') for i, group in enumerate(groups)]
-    try:
-        value = tensor(value, *dims, convert=convert)
-    except IncompatibleShapes:
-        raise IncompatibleShapes(f"Cannot reshape native tensor {type(value)} with sizes {value.shape} given groups {groups}")
-    for i, group in enumerate(groups):
-        if value.shape.get_size(f'group{i}') == group.volume:
-            value = unpack_dim(value, f'group{i}', group)
-        elif check_sizes:
-            raise AssertionError(f"Group {group} does not match dimension {i} of value {value.shape}")
-        else:
-            value = unpack_dim(value, f'group{i}', group)
-    return value
-
-
-def copy(value: Tensor):
-    """
-    Copies the data buffer and encapsulating `Tensor` object.
-
-    Args:
-        value: `Tensor` to be copied.
-
-    Returns:
-        Copy of `value`.
-    """
-    if value._is_tracer:
-        warnings.warn("Tracing tensors cannot be copied.", RuntimeWarning)
-        return value
-    return value._op1(lambda native: choose_backend(native).copy(native))
-
-
-def native_call(f: Callable, *inputs: Tensor, channels_last=None, channel_dim='vector', spatial_dim=None):
-    """
-    Calls `f` with the native representations of the `inputs` tensors in standard layout and returns the result as a `Tensor`.
-
-    All inputs are converted to native tensors (including precision cast) depending on `channels_last`:
-
-    * `channels_last=True`: Dimension layout `(total_batch_size, spatial_dims..., total_channel_size)`
-    * `channels_last=False`: Dimension layout `(total_batch_size, total_channel_size, spatial_dims...)`
-
-    All batch dimensions are compressed into a single dimension with `total_batch_size = input.shape.batch.volume`.
-    The same is done for all channel dimensions.
-
-    Additionally, missing batch and spatial dimensions are added so that all `inputs` have the same batch and spatial shape.
-
-    Args:
-        f: Function to be called on native tensors of `inputs`.
-            The function output must have the same dimension layout as the inputs, unless overridden by `spatial_dim`,
-            and the batch size must be identical.
-        *inputs: Uniform `Tensor` arguments
-        channels_last: (Optional) Whether to put channels as the last dimension of the native representation.
-            If `None`, the channels are put in the default position associated with the current backend,
-            see `phi.math.backend.Backend.prefers_channels_last()`.
-        channel_dim: Name of the channel dimension of the result.
-        spatial_dim: Name of the spatial dimension of the result.
-
-    Returns:
-        `Tensor` with batch and spatial dimensions of `inputs`, unless overridden by `spatial_dim`,
-        and single channel dimension `channel_dim`.
-    """
-    if channels_last is None:
-        try:
-            backend = choose_backend(f)
-        except NoBackendFound:
-            backend = choose_backend_t(*inputs, prefer_default=True)
-        channels_last = backend.prefers_channels_last()
-    batch = merge_shapes(*[i.shape.batch for i in inputs])
-    spatial = merge_shapes(*[i.shape.spatial for i in inputs])
-    natives = []
-    for i in inputs:
-        groups = (batch, *i.shape.spatial.names, i.shape.channel) if channels_last else (batch, i.shape.channel, *i.shape.spatial.names)
-        natives.append(reshaped_native(i, groups))
-    output = f(*natives)
-    if isinstance(channel_dim, str):
-        channel_dim = channel(channel_dim)
-    assert isinstance(channel_dim, Shape), "channel_dim must be a Shape or str"
-    if isinstance(output, (tuple, list)):
-        raise NotImplementedError()
-    else:
-        if spatial_dim is None:
-            groups = (batch, *spatial, channel_dim) if channels_last else (batch, channel_dim, *spatial)
-        else:
-            if isinstance(spatial_dim, str):
-                spatial_dim = spatial(spatial_dim)
-            assert isinstance(spatial_dim, Shape), "spatial_dim must be a Shape or str"
-            groups = (batch, *spatial_dim, channel_dim) if channels_last else (batch, channel_dim, *spatial_dim)
-        result = reshaped_tensor(output, groups, convert=False)
-        if result.shape.get_size(channel_dim.name) == 1:
-            result = result.dimension(channel_dim.name)[0]  # remove vector dim if not required
-        return result
-
-
-def print_(obj: Union[Tensor, PhiTreeNode, Number, tuple, list, None] = None, name: str = ""):
-    """
-    Print a tensor with no more than two spatial dimensions, slicing it along all batch and channel dimensions.
-    
-    Unlike NumPy's array printing, the dimensions are sorted.
-    Elements along the alphabetically first dimension is printed to the right, the second dimension upward.
-    Typically, this means x right, y up.
-
-    Args:
-        obj: tensor-like
-        name: name of the tensor
-
-    Returns:
-
-    """
-    def variables(obj) -> dict:
-        if hasattr(obj, '__variable_attrs__') or hasattr(obj, '__value_attrs__'):
-            return {f".{a}": getattr(obj, a) for a in variable_attributes(obj)}
-        elif isinstance(obj, (tuple, list)):
-            return {f"[{i}]": item for i, item in enumerate(obj)}
-        elif isinstance(obj, dict):
-            return obj
-        else:
-            raise ValueError(f"Not PhiTreeNode: {type(obj)}")
-
-    if name:
-        print(" " * 12 + name)
-    if obj is None:
-        print("None")
-    elif isinstance(obj, Tensor):
-        print(f"{obj:full}")
-    elif isinstance(obj, PhiTreeNode):
-        for n, val in variables(obj).items():
-            print_(val, name + n)
-    else:
-        print(f"{wrap(obj):full}")
-
-
-def map_(function, *values, range=range, **kwargs) -> Union[Tensor, None]:
-    """
-    Calls `function` on all elements of `values`.
-
-    Args:
-        function: Function to be called on single elements contained in `value`. Must return a value that can be stored in tensors.
-        *values: `Tensors` containing positional arguments for `function`.
-            Number of tensors must match `function` signature.
-        range: Range function. Can be used to generate tqdm output by passing `trange`.
-        **kwargs: Non-`Tensor` keyword arguments for `function`.
-            Their shapes are not broadcast with the positional arguments.
-
-    Returns:
-        `Tensor` of same shape as `value`.
-    """
-    values = [wrap(v) for v in values]
-    shape = merge_shapes(*[v.shape for v in values])
-    flat = [pack_dims(expand(v, shape), shape, batch('flat')) for v in values]
-    result = []
-    results = None
-    for _, items in zip(range(flat[0].flat.size_or_1), zip(*flat)):
-        f_output = function(*items, **kwargs)
-        if isinstance(f_output, tuple):
-            if results is None:
-                results = [[] for _ in f_output]
-            for result_i, output_i in zip(results, f_output):
-                result_i.append(output_i)
-        else:
-            result.append(f_output)
-    if results is None:
-        if any(r is None for r in result):
-            assert all(r is None for r in result), f"map function returned None for some elements, {result}"
-            return None
-        return unpack_dim(wrap(result, channel('_c')), '_c', shape)
-    else:
-        for i, result_i in enumerate(results):
-            if any(r is None for r in result_i):
-                assert all(r is None for r in result_i), f"map function returned None for some elements at output index {i}, {result_i}"
-                results[i] = None
-        return tuple([unpack_dim(wrap(result_i, channel('_c')), '_c', shape) for result_i in results])
-
-
-def _initialize(uniform_initializer, shapes: Tuple[Shape]) -> Tensor:
-    shape = concat_shapes(*shapes)
-    if shape.is_non_uniform:
-        stack_dim = shape.shape.without('dims')[0:1]
-        shapes = shape.unstack(stack_dim.name)
-        tensors = [_initialize(uniform_initializer, s) for s in shapes]
-        return stack_tensors(tensors, stack_dim)
-    else:
-        return uniform_initializer(shape)
-
-
-def zeros(*shape: Shape, dtype=None) -> Tensor:
-    """
-    Define a tensor with specified shape with value `0.0` / `0` / `False` everywhere.
-    
-    This method may not immediately allocate the memory to store the values.
-
-    See Also:
-        `zeros_like()`, `ones()`.
-
-    Args:
-        *shape: This (possibly empty) sequence of `Shape`s is concatenated, preserving the order.
-        dtype: Data type as `DType` object. Defaults to `float` matching the current precision setting.
-
-    Returns:
-        `Tensor`
-    """
-    return _initialize(lambda shape: expand_tensor(NativeTensor(default_backend().zeros((), dtype=DType.as_dtype(dtype)), EMPTY_SHAPE), shape), shape)
-
-
-def zeros_like(obj: Union[Tensor, PhiTreeNode]) -> Union[Tensor, PhiTreeNode]:
-    """ Create a `Tensor` containing only `0.0` / `0` / `False` with the same shape and dtype as `obj`. """
-    nest, values = disassemble_tree(obj)
-    zeros_ = []
-    for val in values:
-        val = wrap(val)
-        with val.default_backend:
-            zeros_.append(zeros(val.shape, dtype=val.dtype))
-    return assemble_tree(nest, zeros_)
-
-
-def ones(*shape: Shape, dtype=None) -> Tensor:
-    """
-    Define a tensor with specified shape with value `1.0`/ `1` / `True` everywhere.
-    
-    This method may not immediately allocate the memory to store the values.
-
-    See Also:
-        `ones_like()`, `zeros()`.
-
-    Args:
-        *shape: This (possibly empty) sequence of `Shape`s is concatenated, preserving the order.
-        dtype: Data type as `DType` object. Defaults to `float` matching the current precision setting.
-
-    Returns:
-        `Tensor`
-    """
-    return _initialize(lambda shape: expand_tensor(NativeTensor(default_backend().ones((), dtype=DType.as_dtype(dtype)), EMPTY_SHAPE), shape), shape)
-
-
-def ones_like(value: Tensor) -> Tensor:
-    """ Create a `Tensor` containing only `1.0` / `1` / `True` with the same shape and dtype as `obj`. """
-    return zeros_like(value) + 1
-
-
-def random_normal(*shape: Shape, dtype=None) -> Tensor:
-    """
-    Creates a `Tensor` with the specified shape, filled with random values sampled from a normal / Gaussian distribution.
-
-    Implementations:
-
-    * NumPy: [`numpy.random.standard_normal`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.standard_normal.html)
-    * PyTorch: [`torch.randn`](https://pytorch.org/docs/stable/generated/torch.randn.html)
-    * TensorFlow: [`tf.random.normal`](https://www.tensorflow.org/api_docs/python/tf/random/normal)
-    * Jax: [`jax.random.normal`](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.normal.html)
-
-    Args:
-        *shape: This (possibly empty) sequence of `Shape`s is concatenated, preserving the order.
-        dtype: (optional) floating point `DType`. If `None`, a float tensor with the current default precision is created, see `get_precision()`.
-
-    Returns:
-        `Tensor`
-    """
-
-    def uniform_random_normal(shape):
-        native = choose_backend(*shape.sizes, prefer_default=True).random_normal(shape.sizes, DType.as_dtype(dtype))
-        return NativeTensor(native, shape)
-
-    return _initialize(uniform_random_normal, shape)
-
-
-def random_uniform(*shape: Shape,
-                   low: Union[Tensor, float] = 0,
-                   high: Union[Tensor, float] = 1,
-                   dtype: Union[DType, tuple] = None) -> Tensor:
-    """
-    Creates a `Tensor` with the specified shape, filled with random values sampled from a uniform distribution.
-
-    Args:
-        *shape: This (possibly empty) sequence of `Shape`s is concatenated, preserving the order.
-        dtype: (optional) `DType` or `(kind, bits)`.
-            The dtype kind must be one of `float`, `int`, `complex`.
-            If not specified, a `float` tensor with the current default precision is created, see `get_precision()`.
-        low: Minimum value, included.
-        high: Maximum value, excluded.
-    Returns:
-        `Tensor`
-    """
-    def uniform_random_uniform(shape):
-        native = choose_backend(low, high, *shape.sizes, prefer_default=True).random_uniform(shape.sizes, low, high, DType.as_dtype(dtype))
-        return NativeTensor(native, shape)
-
-    return _initialize(uniform_random_uniform, shape)
-
-
-def transpose(x: Tensor, axes):
-    """
-    Swap the dimension order of `x`.
-    This operation is superfluous since tensors will be reshaped under the hood or when getting the native/numpy representations.
-
-    Implementations:
-
-    * NumPy: [`numpy.transpose`](https://numpy.org/doc/stable/reference/generated/numpy.transpose.html)
-    * PyTorch: [`x.permute`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute)
-    * TensorFlow: [`tf.transpose`](https://www.tensorflow.org/api_docs/python/tf/transpose)
-    * Jax: [`jax.numpy.transpose`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.transpose.html)
-
-    Args:
-        x: `Tensor` or native tensor.
-        axes: `tuple` or `list`
-
-    Returns:
-        `Tensor` or native tensor, depending on `x`.
-    """
-    if isinstance(x, Tensor):
-        return expand(x, x.shape[axes])
-    else:
-        return choose_backend(x).transpose(x, axes)
-
-
-def cumulative_sum(x: Tensor, dim: DimFilter):
-    """
-    Performs a cumulative sum of `x` along `dim`.
-
-    Implementations:
-
-    * NumPy: [`cumsum`](https://numpy.org/doc/stable/reference/generated/numpy.cumsum.html)
-    * PyTorch: [`cumsum`](https://pytorch.org/docs/stable/generated/torch.cumsum.html)
-    * TensorFlow: [`cumsum`](https://www.tensorflow.org/api_docs/python/tf/math/cumsum)
-    * Jax: [`cumsum`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.cumsum.html)
-
-    Args:
-        x: `Tensor`
-        dim: Dimension along which to sum, as `str` or `Shape`.
-
-    Returns:
-        `Tensor` with the same shape as `x`.
-    """
-    dim = x.shape.only(dim)
-    assert len(dim) == 1, f"dim must be a single dimension but got {dim}"
-    native_x = x.native(x.shape)
-    native_result = choose_backend(native_x).cumsum(native_x, x.shape.index(dim))
-    return NativeTensor(native_result, x.shape)
-
-
-def fftfreq(resolution: Shape, dx: Union[Tensor, float] = 1, dtype: DType = None):
-    """
-    Returns the discrete Fourier transform sample frequencies.
-    These are the frequencies corresponding to the components of the result of `math.fft` on a tensor of shape `resolution`.
-
-    Args:
-        resolution: Grid resolution measured in cells
-        dx: Distance between sampling points in real space.
-        dtype: Data type of the returned tensor (Default value = None)
-
-    Returns:
-        `Tensor` holding the frequencies of the corresponding values computed by math.fft
-    """
-    k = meshgrid(**{dim: np.fft.fftfreq(int(n)) for dim, n in resolution.spatial._named_sizes})
-    k /= dx
-    return to_float(k) if dtype is None else cast(k, dtype)
-
-
-def meshgrid(dim_type=spatial, stack_dim=channel('vector'), assign_item_names=True, **dimensions: Union[int, Tensor]) -> Tensor:
-    """
-    Generate a mesh-grid `Tensor` from keyword dimensions.
-
-    Args:
-        **dimensions: Mesh-grid dimensions, mapping names to values.
-            Values may be `int`, 1D `Tensor` or 1D native tensor.
-        dim_type: Dimension type of mesh-grid dimensions, one of `spatial`, `channel`, `batch`, `instance`.
-        stack_dim: Vector dimension along which grids are stacked.
-        assign_item_names: Whether to use the dimension names from `**dimensions` as item names for `stack_dim`.
-
-    Returns:
-        Mesh-grid `Tensor`
-    """
-    assert 'vector' not in dimensions
-    dim_values = []
-    dim_sizes = []
-    for dim, spec in dimensions.items():
-        if isinstance(spec, int):
-            dim_values.append(tuple(range(spec)))
-            dim_sizes.append(spec)
-        elif isinstance(spec, Tensor):
-            assert spec.rank == 1, f"Only 1D sequences allowed, got {spec} for dimension '{dim}'."
-            dim_values.append(spec.native())
-            dim_sizes.append(spec.shape.volume)
-        else:
-            backend = choose_backend(spec)
-            shape = backend.staticshape(spec)
-            assert len(shape) == 1, "Only 1D sequences allowed, got {spec} for dimension '{dim}'."
-            dim_values.append(spec)
-            dim_sizes.append(shape[0])
-    backend = choose_backend(*dim_values, prefer_default=True)
-    indices_list = backend.meshgrid(*dim_values)
-    grid_shape = dim_type(**{dim: size for dim, size in zip(dimensions.keys(), dim_sizes)})
-    channels = [NativeTensor(t, grid_shape) for t in indices_list]
-    if assign_item_names:
-        return stack_tensors(channels, stack_dim.with_size(tuple(dimensions.keys())))
-    else:
-        return stack_tensors(channels, stack_dim)
-
-
-def linspace(start: Union[int, Tensor], stop, dim: Shape) -> Tensor:
-    """
-    Returns `number` evenly spaced numbers between `start` and `stop`.
-
-    See Also:
-        `arange()`, `meshgrid()`.
-
-    Args:
-        start: First value, `int` or `Tensor`.
-        stop: Last value, `int` or `Tensor`.
-        dim: Linspace dimension of integer size.
-            The size determines how many values to linearly space between `start` and `stop`.
-            The values will be laid out along `dim`.
-
-    Returns:
-        `Tensor`
-
-    Examples:
-        >>> math.linspace(0, 1, spatial(x=5))
-        (0.000, 0.250, 0.500, 0.750, 1.000) along xˢ
-
-        >>> math.linspace(0, (-1, 1), spatial(x=3))
-        (0.000, 0.000); (-0.500, 0.500); (-1.000, 1.000) (xˢ=3, vectorᶜ=2)
-    """
-    assert isinstance(dim, Shape) and dim.rank == 1, f"dim must be a single-dimension Shape but got {dim}"
-    if is_scalar(start) and is_scalar(stop):
-        if isinstance(start, Tensor):
-            start = start.native()
-        if isinstance(stop, Tensor):
-            stop = stop.native()
-        native_linspace = choose_backend(start, stop, prefer_default=True).linspace(start, stop, dim.size)
-        return NativeTensor(native_linspace, dim)
-    else:
-        return map_(linspace, start, stop, dim=dim)
-
-
-def arange(dim: Shape, start_or_stop: Union[int, None] = None, stop: Union[int, None] = None, step=1):
-    """
-    Returns evenly spaced values between `start` and `stop`.
-    If only one limit is given, `0` is used for the start.
-
-    See Also:
-        `range_tensor()`, `linspace()`, `meshgrid()`.
-
-    Args:
-        dim: Dimension name and type as `Shape` object.
-            The `size` of `dim` is interpreted as `stop` unless `start_or_stop` is specified.
-        start_or_stop: (Optional) `int`. Interpreted as `start` if `stop` is specified as well. Otherwise this is `stop`.
-        stop: (Optional) `int`. `stop` value.
-        step: Distance between values.
-
-    Returns:
-        `Tensor`
-    """
-    if start_or_stop is None:
-        assert stop is None, "start_or_stop must be specified when stop is given."
-        assert isinstance(dim.size, int), "When start_or_stop is not specified, dim.size must be an integer."
-        start, stop = 0, dim.size
-    elif stop is None:
-        start, stop = 0, start_or_stop
-    else:
-        start = start_or_stop
-    native = choose_backend(start, stop, prefer_default=True).range(start, stop, step, DType(int, 32))
-    return NativeTensor(native, dim.with_sizes([stop - start]))
-
-
-def range_tensor(*shape: Shape):
-    """
-    Returns a `Tensor` with given `shape` containing the linear indices of each element.
-    For 1D tensors, this equivalent to `arange()` with `step=1`.
-
-    See Also:
-        `arange()`, `meshgrid()`.
-
-    Args:
-        shape: Tensor shape.
-
-    Returns:
-        `Tensor`
-    """
-    shape = concat_shapes(*shape)
-    data = arange(spatial('range'), 0, shape.volume)
-    return unpack_dim(data, 'range', shape)
-
-
-def stack_tensors(values: Union[tuple, list], dim: Shape):
-    if len(values) == 1 and not dim:
-        return values[0]
-    values = [wrap(v) for v in values]
-    values = cast_same(*values)
-
-    def inner_stack(*values):
-        if len(values) > 1:
-            return TensorStack(values, dim)
-        else:
-            return CollapsedTensor(values[0], values[0].shape & dim.with_size(1))
-
-    result = broadcast_op(inner_stack, values)
-    return result
-
-
-def concat_tensor(values: Union[tuple, list], dim: str) -> Tensor:
-    assert len(values) > 0, "concat() got empty sequence"
-    assert isinstance(dim, str), f"dim must be a single-dimension Shape but got '{dim}' of type {type(dim)}"
-
-    def inner_concat(*values):
-        broadcast_shape: Shape = values[0].shape  # merge_shapes(*[t.shape.with_sizes([None] * t.shape.rank) for t in values])
-        dim_index = broadcast_shape.index(dim)
-        natives = [v.native(order=broadcast_shape.names) for v in values]
-        concatenated = choose_backend(*natives).concat(natives, dim_index)
-        if all([v.shape.get_item_names(dim) is not None for v in values]):
-            broadcast_shape = broadcast_shape.with_dim_size(dim, sum([v.shape.get_item_names(dim) for v in values], ()))
-        else:
-            broadcast_shape = broadcast_shape.with_dim_size(dim, sum([v.shape.get_size(dim) for v in values]))
-        return NativeTensor(concatenated, broadcast_shape)
-
-    result = broadcast_op(inner_concat, values)
-    return result
-
-
-def pad(value: Tensor, widths: dict, mode: Union['e_.Extrapolation', Tensor, Number], **kwargs) -> Tensor:
-    """
-    Pads a tensor along the specified dimensions, determining the added values using the given extrapolation.
-    Unlike `Extrapolation.pad()`, this function can handle negative widths which slice off outer values.
-
-    Args:
-        value: `Tensor` to be padded
-        widths: `dict` mapping dimension name (`str`) to `(lower, upper)`
-            where `lower` and `upper` are `int` that can be positive (pad), negative (slice) or zero (pass).
-        mode: `Extrapolation` used to determine values added from positive `widths`.
-            Assumes constant extrapolation if given a number or `Tensor` instead.
-        kwargs: Additional padding arguments.
-            These are ignored by the standard extrapolations defined in `phi.math.extrapolation` but can be used to pass additional contextual information to custom extrapolations.
-            Grid classes from `phi.field` will pass the argument `bounds: Box`.
-
-    Returns:
-        Padded `Tensor`
-
-    Examples:
-        >>> math.pad(math.ones(spatial(x=10, y=10)), {'x': (1, 1), 'y': (2, 1)}, 0)
-        (xˢ=12, yˢ=13) 0.641 ± 0.480 (0e+00...1e+00)
-
-        >>> math.pad(math.ones(spatial(x=10, y=10)), {'x': (1, -1)}, 0)
-        (xˢ=10, yˢ=10) 0.900 ± 0.300 (0e+00...1e+00)
-    """
-    mode = mode if isinstance(mode, e_.Extrapolation) else e_.ConstantExtrapolation(mode)
-    has_negative_widths = any(w0 < 0 or w1 < 0 for w0, w1 in widths.values())
-    has_positive_widths = any(w0 > 0 or w1 > 0 for w0, w1 in widths.values())
-    slices = None
-    if has_negative_widths:
-        slices = {dim: slice(max(0, -w[0]), min(0, w[1]) or None) for dim, w in widths.items()}
-        widths = {dim: (max(0, w[0]), max(0, w[1])) for dim, w in widths.items()}
-    result_padded = mode.pad(value, widths, **kwargs) if has_positive_widths else value
-    result_sliced = result_padded[slices] if has_negative_widths else result_padded
-    return result_sliced
-
-
-def closest_grid_values(grid: Tensor,
-                        coordinates: Tensor,
-                        extrap: 'e_.Extrapolation',
-                        stack_dim_prefix='closest_',
-                        **kwargs):
-    """
-    Finds the neighboring grid points in all spatial directions and returns their values.
-    The result will have 2^d values for each vector in coordiantes in d dimensions.
-
-    Args:
-      grid: grid data. The grid is spanned by the spatial dimensions of the tensor
-      coordinates: tensor with 1 channel dimension holding vectors pointing to locations in grid index space
-      extrap: grid extrapolation
-      stack_dim_prefix: For each spatial dimension `dim`, stacks lower and upper closest values along dimension `stack_dim_prefix+dim`.
-      kwargs: Additional information for the extrapolation.
-
-    Returns:
-      Tensor of shape (batch, coord_spatial, grid_spatial=(2, 2,...), grid_channel)
-
-    """
-    return broadcast_op(functools.partial(_closest_grid_values, extrap=extrap, stack_dim_prefix=stack_dim_prefix, pad_kwargs=kwargs), [grid, coordinates])
-
-
-def _closest_grid_values(grid: Tensor,
-                         coordinates: Tensor,
-                         extrap: 'e_.Extrapolation',
-                         stack_dim_prefix: str,
-                         pad_kwargs: dict):
-    # alternative method: pad array for all 2^d combinations, then stack to simplify gather.
-    # --- Pad tensor where transform is not possible ---
-    non_copy_pad = {dim: (0 if extrap.is_copy_pad(dim, False) else 1, 0 if extrap.is_copy_pad(dim, True) else 1) for dim in grid.shape.spatial.names}
-    grid = extrap.pad(grid, non_copy_pad, **pad_kwargs)
-    coordinates += wrap([not extrap.is_copy_pad(dim, False) for dim in grid.shape.spatial.names], channel('vector'))
-    # --- Transform coordiantes ---
-    min_coords = to_int32(floor(coordinates))
-    max_coords = extrap.transform_coordinates(min_coords + 1, grid.shape)
-    min_coords = extrap.transform_coordinates(min_coords, grid.shape)
-
-    def left_right(is_hi_by_axis_left, ax_idx):
-        is_hi_by_axis_right = is_hi_by_axis_left | np.array([ax == ax_idx for ax in range(grid.shape.spatial_rank)])
-        coords_left = where(is_hi_by_axis_left, max_coords, min_coords)
-        coords_right = where(is_hi_by_axis_right, max_coords, min_coords)
-        if ax_idx == grid.shape.spatial_rank - 1:
-            values_left = gather(grid, coords_left)
-            values_right = gather(grid, coords_right)
-        else:
-            values_left = left_right(is_hi_by_axis_left, ax_idx + 1)
-            values_right = left_right(is_hi_by_axis_right, ax_idx + 1)
-        return stack_tensors([values_left, values_right], channel(f"{stack_dim_prefix}{grid.shape.spatial.names[ax_idx]}"))
-
-    result = left_right(np.array([False] * grid.shape.spatial_rank), 0)
-    return result
-
-
-def grid_sample(grid: Tensor, coordinates: Tensor, extrap: 'e_.Extrapolation', **kwargs):
-    """
-    Samples values of `grid` at the locations referenced by `coordinates`.
-    Values lying in between sample points are determined via linear interpolation.
-
-    For values outside the valid bounds of `grid` (`coord < 0 or coord > grid.shape - 1`), `extrap` is used to determine the neighboring grid values.
-    If the extrapolation does not support resampling, the grid is padded by one cell layer before resampling.
-    In that case, values lying further outside will not be sampled according to the extrapolation.
-
-    Args:
-        grid: Grid with at least one spatial dimension and no instance dimensions.
-        coordinates: Coordinates with a single channel dimension called `'vector'`.
-            The size of the `vector` dimension must match the number of spatial dimensions of `grid`.
-        extrap: Extrapolation used to determine the values of `grid` outside its valid bounds.
-        kwargs: Additional information for the extrapolation.
-
-    Returns:
-        `Tensor` with channel dimensions of `grid`, spatial and instance dimensions of `coordinates` and combined batch dimensions.
-    """
-    result = broadcast_op(functools.partial(_grid_sample, extrap=extrap, pad_kwargs=kwargs), [grid, coordinates])
-    return result
-
-
-def _grid_sample(grid: Tensor, coordinates: Tensor, extrap: Union['e_.Extrapolation', None], pad_kwargs: dict):
-    if grid.shape.batch == coordinates.shape.batch or grid.shape.batch.volume == 1 or coordinates.shape.batch.volume == 1:
-        # call backend.grid_sample()
-        batch = grid.shape.batch & coordinates.shape.batch
-        backend = choose_backend_t(grid, coordinates)
-        result = NotImplemented
-        if extrap is None:
-            result = backend.grid_sample(reshaped_native(grid, [batch, *grid.shape.spatial, grid.shape.channel]),
-                                         reshaped_native(coordinates, [batch, *coordinates.shape.instance, *coordinates.shape.spatial, 'vector']),
-                                         'undefined')
-        elif extrap.native_grid_sample_mode:
-            result = backend.grid_sample(reshaped_native(grid, [batch, *grid.shape.spatial, grid.shape.channel]),
-                                         reshaped_native(coordinates, [batch, *coordinates.shape.instance, *coordinates.shape.spatial, 'vector']),
-                                         extrap.native_grid_sample_mode)
-        if result is NotImplemented:
-            # pad one layer
-            grid_padded = pad(grid, {dim: (1, 1) for dim in grid.shape.spatial.names}, extrap or e_.ZERO, **pad_kwargs)
-            if extrap is not None:
-                from .extrapolation import _CopyExtrapolation
-                if isinstance(extrap, _CopyExtrapolation):
-                    inner_coordinates = extrap.transform_coordinates(coordinates, grid.shape) + 1
-                else:
-                    inner_coordinates = extrap.transform_coordinates(coordinates + 1, grid_padded.shape)
-            else:
-                inner_coordinates = coordinates + 1
-            result = backend.grid_sample(reshaped_native(grid_padded, [batch, *grid_padded.shape.spatial.names, grid.shape.channel]),
-                                         reshaped_native(inner_coordinates, [batch, *coordinates.shape.instance, *coordinates.shape.spatial, 'vector']),
-                                         'boundary')
-        if result is not NotImplemented:
-            result = reshaped_tensor(result, [grid.shape.batch & coordinates.shape.batch, *coordinates.shape.instance, *coordinates.shape.spatial, grid.shape.channel])
-            return result
-    # fallback to slower grid sampling
-    neighbors = _closest_grid_values(grid, coordinates, extrap or e_.ZERO, '_closest_', pad_kwargs)
-    binary = meshgrid(**{f'_closest_{dim}': (0, 1) for dim in grid.shape.spatial.names}, dim_type=channel, assign_item_names=False)
-    right_weights = coordinates % 1
-    weights = prod(binary * right_weights + (1 - binary) * (1 - right_weights), 'vector')
-    result = sum_(neighbors * weights, dim=[f"_closest_{dim}" for dim in grid.shape.spatial.names])
-    return result
-
-
-def broadcast_op(operation: Callable,
-                 tensors: Union[tuple, list],
-                 iter_dims: Union[set, tuple, list, Shape] = None,
-                 no_return=False):
-    if iter_dims is None:
-        iter_dims = set()
-        for tensor in tensors:
-            if isinstance(tensor, TensorStack) and tensor.requires_broadcast:
-                iter_dims.add(tensor._stack_dim.name)
-    if len(iter_dims) == 0:
-        return operation(*tensors)
-    else:
-        if isinstance(iter_dims, Shape):
-            iter_dims = iter_dims.names
-        dim = next(iter(iter_dims))
-        dim_type = None
-        size = None
-        item_names = None
-        unstacked = []
-        for tensor in tensors:
-            if dim in tensor.shape.names:
-                unstacked_tensor = tensor.unstack(dim)
-                unstacked.append(unstacked_tensor)
-                if size is None:
-                    size = len(unstacked_tensor)
-                    dim_type = tensor.shape.get_type(dim)
-                else:
-                    assert size == len(unstacked_tensor)
-                    assert dim_type == tensor.shape.get_type(dim)
-                if item_names is None:
-                    item_names = tensor.shape.get_item_names(dim)
-            else:
-                unstacked.append(tensor)
-        result_unstacked = []
-        for i in range(size):
-            gathered = [t[i] if isinstance(t, tuple) else t for t in unstacked]
-            result_unstacked.append(broadcast_op(operation, gathered, iter_dims=set(iter_dims) - {dim}))
-        if not no_return:
-            return TensorStack(result_unstacked, Shape((None,), (dim,), (dim_type,), (item_names,)))
-
-
-def where(condition: Union[Tensor, float, int], value_true: Union[Tensor, float, int], value_false: Union[Tensor, float, int]):
-    """
-    Builds a tensor by choosing either values from `value_true` or `value_false` depending on `condition`.
-    If `condition` is not of type boolean, non-zero values are interpreted as True.
-    
-    This function requires non-None values for `value_true` and `value_false`.
-    To get the indices of True / non-zero values, use :func:`nonzero`.
-
-    Args:
-      condition: determines where to choose values from value_true or from value_false
-      value_true: Values to pick where `condition != 0 / True`
-      value_false: Values to pick where `condition == 0 / False`
-
-    Returns:
-        `Tensor` containing dimensions of all inputs.
-    """
-    condition = wrap(condition)
-    value_true = wrap(value_true)
-    value_false = wrap(value_false)
-
-    def inner_where(c: Tensor, vt: Tensor, vf: Tensor):
-        if vt._is_tracer or vf._is_tracer or c._is_tracer:
-            return c * vt + (1 - c) * vf  # ToDo this does not take NaN into account
-        shape, (c, vt, vf) = broadcastable_native_tensors(c, vt, vf)
-        result = choose_backend(c, vt, vf).where(c, vt, vf)
-        return NativeTensor(result, shape)
-
-    return broadcast_op(inner_where, [condition, value_true, value_false])
-
-
-def nonzero(value: Tensor, list_dim: Union[Shape, str] = instance('nonzero'), index_dim: Shape = channel('vector')):
-    """
-    Get spatial indices of non-zero / True values.
-    
-    Batch dimensions are preserved by this operation.
-    If channel dimensions are present, this method returns the indices where any component is nonzero.
-
-    Implementations:
-
-    * NumPy: [`numpy.argwhere`](https://numpy.org/doc/stable/reference/generated/numpy.argwhere.html)
-    * PyTorch: [`torch.nonzero`](https://pytorch.org/docs/stable/generated/torch.nonzero.html)
-    * TensorFlow: [`tf.where(tf.not_equal(values, 0))`](https://www.tensorflow.org/api_docs/python/tf/where)
-    * Jax: [`jax.numpy.nonzero`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.nonzero.html)
-
-    Args:
-        value: spatial tensor to find non-zero / True values in.
-        list_dim: Dimension listing non-zero values.
-        index_dim: Index dimension.
-
-    Returns:
-        `Tensor` of shape (batch dims..., `list_dim`=#non-zero, `index_dim`=value.shape.spatial_rank)
-
-    """
-    if value.shape.channel_rank > 0:
-        value = sum_(abs(value), value.shape.channel)
-
-    if isinstance(list_dim, str):
-        list_dim = instance(list_dim)
-
-    def unbatched_nonzero(value: Tensor):
-        native = reshaped_native(value, [*value.shape.spatial])
-        backend = choose_backend(native)
-        indices = backend.nonzero(native)
-        indices_shape = Shape(backend.staticshape(indices), (list_dim.name, index_dim.name), (list_dim.type, index_dim.type), (None, value.shape.spatial.names))
-        return NativeTensor(indices, indices_shape)
-
-    return broadcast_op(unbatched_nonzero, [value], iter_dims=value.shape.batch.names)
-
-
-def reduce_(f, value, dims, require_all_dims_present=False, required_kind: type = None):
-    if dims in ((), [], EMPTY_SHAPE):
-        return value
-    else:
-        if isinstance(value, (tuple, list)):
-            values = [wrap(v) for v in value]
-            value = stack_tensors(values, instance('0'))
-            dims = value.shape.only(dims)
-            assert '0' in dims, "When passing a sequence of tensors to be reduced, the sequence dimension '0' must be reduced."
-        elif isinstance(value, Layout):
-            if not value.shape.without(dims):  # reduce all
-                dims = batch('_flat_layout')
-                values = value._as_list()
-                if required_kind is not None:
-                    values = [required_kind(v) for v in values]
-                value = wrap(values, dims)
-        else:
-            value = wrap(value)
-        dims = value.shape.only(dims)
-        if require_all_dims_present and any(d not in value.shape for d in dims):
-            raise ValueError(f"Cannot sum dimensions {dims} because tensor {value.shape} is missing at least one of them")
-        return f(value._simplify(), dims)
-
-
-def sum_(value: Union[Tensor, list, tuple], dim: DimFilter = non_batch) -> Tensor:
-    """
-    Sums `values` along the specified dimensions.
-
-    Args:
-        value: `Tensor` or `list` / `tuple` of Tensors.
-        dim: Dimension or dimensions to be reduced. One of
-
-            * `None` to reduce all non-batch dimensions
-            * `str` containing single dimension or comma-separated list of dimensions
-            * `Tuple[str]` or `List[str]`
-            * `Shape`
-            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
-            * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors
-
-    Returns:
-        `Tensor` without the reduced dimensions.
-    """
-    return reduce_(_sum, bool_to_int(value), dim, require_all_dims_present=True)
-
-
-def _sum(value: Tensor, dims: Shape) -> Tensor:
-    if not dims:
-        return value
-    if isinstance(value, NativeTensor):
-        result = value.default_backend.sum(value.native(value.shape), value.shape.indices(dims))
-        return NativeTensor(result, value.shape.without(dims))
-    elif isinstance(value, CollapsedTensor):
-        result = _sum(value._inner, dims.only(value._inner.shape)) * value.collapsed_dims.only(dims).volume
-        return expand_tensor(result, value.shape.without(dims))
-    elif isinstance(value, TensorStack):
-        reduced_inners = [_sum(t, dims.without(value._stack_dim)) for t in value._tensors]
-        return functools.reduce(lambda x, y: x + y, reduced_inners) if value._stack_dim in dims else TensorStack(reduced_inners, value._stack_dim)
-    elif isinstance(value, CompressedSparseMatrix):
-        if value.sparse_dims in dims:  # reduce all sparse dims
-            return _sum(value._values, dims.without(value.sparse_dims) & instance(value._values))
-        value_only_dims = dims.only(value._values.shape).without(value.sparsity_batch)
-        if value_only_dims:
-            value = value._with_values(_sum(value._values, value_only_dims))
-        dims = dims.without(value_only_dims)
-        if value._compressed_dims in dims and value._uncompressed_dims.isdisjoint(dims):
-            # We can ignore the pointers
-            result_base = zeros(value.shape.without(value._compressed_dims))
-            return scatter(result_base, value._indices, value._values, mode='add', outside_handling='undefined')
-        elif value.sparse_dims.only(dims):  # reduce some sparse dims
-            return dot(value, dims, ones(dims), dims)  # this is what SciPy does in both axes, actually.
-        return value
-        # first sum value dims that are not part of indices
-    else:
-        raise ValueError(type(value))
-
-
-def prod(value: Union[Tensor, list, tuple], dim: DimFilter = non_batch) -> Tensor:
-    """
-    Multiplies `values` along the specified dimensions.
-
-    Args:
-        value: `Tensor` or `list` / `tuple` of Tensors.
-        dim: Dimension or dimensions to be reduced. One of
-
-            * `None` to reduce all non-batch dimensions
-            * `str` containing single dimension or comma-separated list of dimensions
-            * `Tuple[str]` or `List[str]`
-            * `Shape`
-            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
-            * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors
-
-    Returns:
-        `Tensor` without the reduced dimensions.
-    """
-    return reduce_(_prod, value, dim, require_all_dims_present=True)
-
-
-def _prod(value: Tensor, dims: Shape) -> Tensor:
-    if isinstance(value, NativeTensor):
-        result = value.default_backend.prod(value.native(value.shape), value.shape.indices(dims))
-        return NativeTensor(result, value.shape.without(dims))
-    elif isinstance(value, CollapsedTensor):
-        result = _prod(value._inner, dims.only(value._inner.shape)) ** value.collapsed_dims.only(dims).volume
-        return expand_tensor(result, value.shape.without(dims))
-    elif isinstance(value, TensorStack):
-        reduced_inners = [_prod(t, dims.without(value._stack_dim)) for t in value._tensors]
-        return functools.reduce(lambda x, y: x * y, reduced_inners) if value._stack_dim in dims else TensorStack(reduced_inners, value._stack_dim)
-    else:
-        raise ValueError(type(value))
-
-
-def mean(value: Union[Tensor, list, tuple], dim: DimFilter = non_batch) -> Tensor:
-    """
-    Computes the mean over `values` along the specified dimensions.
-
-    Args:
-        value: `Tensor` or `list` / `tuple` of Tensors.
-        dim: Dimension or dimensions to be reduced. One of
-
-            * `None` to reduce all non-batch dimensions
-            * `str` containing single dimension or comma-separated list of dimensions
-            * `Tuple[str]` or `List[str]`
-            * `Shape`
-            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
-            * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors
-
-    Returns:
-        `Tensor` without the reduced dimensions.
-    """
-    return reduce_(_mean, value, dim)
-
-
-def _mean(value: Tensor, dims: Shape) -> Tensor:
-    if not dims:
-        return value
-    if isinstance(value, NativeTensor):
-        result = value.default_backend.mean(value.native(value.shape), value.shape.indices(dims))
-        return NativeTensor(result, value.shape.without(dims))
-    elif isinstance(value, CollapsedTensor):
-        result = _mean(value._inner, dims.only(value._inner.shape))
-        return expand_tensor(result, value.shape.without(dims))
-    elif isinstance(value, TensorStack):
-        reduced_inners = [_mean(t, dims.without(value._stack_dim)) for t in value._tensors]
-        return functools.reduce(lambda x, y: x + y, reduced_inners) / len(reduced_inners) if value._stack_dim in dims else TensorStack(reduced_inners, value._stack_dim)
-    else:
-        raise ValueError(type(value))
-
-
-def std(value: Union[Tensor, list, tuple], dim: DimFilter = non_batch) -> Tensor:
-    """
-    Computes the standard deviation over `values` along the specified dimensions.
-
-    *Warning*: The standard deviation of non-uniform tensors along the stack dimension is undefined.
-
-    Args:
-        value: `Tensor` or `list` / `tuple` of Tensors.
-        dim: Dimension or dimensions to be reduced. One of
-
-            * `None` to reduce all non-batch dimensions
-            * `str` containing single dimension or comma-separated list of dimensions
-            * `Tuple[str]` or `List[str]`
-            * `Shape`
-            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
-            * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors
-
-    Returns:
-        `Tensor` without the reduced dimensions.
-    """
-    return reduce_(_std, value, dim)
-
-
-def _std(value: Tensor, dims: Shape) -> Tensor:
-    result = value.default_backend.std(value.native(value.shape), value.shape.indices(dims))
-    return NativeTensor(result, value.shape.without(dims))
-
-
-def any_(boolean_tensor: Union[Tensor, list, tuple], dim: DimFilter = non_batch) -> Tensor:
-    """
-    Tests whether any entry of `boolean_tensor` is `True` along the specified dimensions.
-
-    Args:
-        boolean_tensor: `Tensor` or `list` / `tuple` of Tensors.
-        dim: Dimension or dimensions to be reduced. One of
-
-            * `None` to reduce all non-batch dimensions
-            * `str` containing single dimension or comma-separated list of dimensions
-            * `Tuple[str]` or `List[str]`
-            * `Shape`
-            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
-            * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors
-
-    Returns:
-        `Tensor` without the reduced dimensions.
-    """
-    return reduce_(_any, boolean_tensor, dim)
-
-
-def _any(value: Tensor, dims: Shape) -> Tensor:
-    if isinstance(value, NativeTensor):
-        result = value.default_backend.any(value.native(value.shape), value.shape.indices(dims))
-        return NativeTensor(result, value.shape.without(dims))
-    elif isinstance(value, CollapsedTensor):
-        result = _any(value._inner, dims.only(value._inner.shape))
-        return expand_tensor(result, value.shape.without(dims))
-    elif isinstance(value, TensorStack):
-        reduced_inners = [_any(t, dims.without(value._stack_dim)) for t in value._tensors]
-        return functools.reduce(lambda x, y: x | y, reduced_inners) if value._stack_dim in dims else TensorStack(reduced_inners, value._stack_dim)
-    else:
-        raise ValueError(type(value))
-
-
-def all_(boolean_tensor: Union[Tensor, list, tuple], dim: DimFilter = non_batch) -> Tensor:
-    """
-    Tests whether all entries of `boolean_tensor` are `True` along the specified dimensions.
-
-    Args:
-        boolean_tensor: `Tensor` or `list` / `tuple` of Tensors.
-        dim: Dimension or dimensions to be reduced. One of
-
-            * `None` to reduce all non-batch dimensions
-            * `str` containing single dimension or comma-separated list of dimensions
-            * `Tuple[str]` or `List[str]`
-            * `Shape`
-            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
-            * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors
-
-    Returns:
-        `Tensor` without the reduced dimensions.
-    """
-    return reduce_(_all, boolean_tensor, dim)
-
-
-def _all(value: Tensor, dims: Shape) -> Tensor:
-    if isinstance(value, NativeTensor):
-        result = value.default_backend.all(value.native(value.shape), value.shape.indices(dims))
-        return NativeTensor(result, value.shape.without(dims))
-    elif isinstance(value, CollapsedTensor):
-        result = _all(value._inner, dims.only(value._inner.shape))
-        return expand_tensor(result, value.shape.without(dims))
-    elif isinstance(value, TensorStack):
-        reduced_inners = [_all(t, dims.without(value._stack_dim)) for t in value._tensors]
-        return functools.reduce(lambda x, y: x & y, reduced_inners) if value._stack_dim in dims else TensorStack(reduced_inners, value._stack_dim)
-    else:
-        raise ValueError(type(value))
-
-
-def max_(value: Union[Tensor, list, tuple], dim: DimFilter = non_batch) -> Tensor:
-    """
-    Determines the maximum value of `values` along the specified dimensions.
-
-    Args:
-        value: `Tensor` or `list` / `tuple` of Tensors.
-        dim: Dimension or dimensions to be reduced. One of
-
-            * `None` to reduce all non-batch dimensions
-            * `str` containing single dimension or comma-separated list of dimensions
-            * `Tuple[str]` or `List[str]`
-            * `Shape`
-            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
-            * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors
-
-    Returns:
-        `Tensor` without the reduced dimensions.
-    """
-    return reduce_(_max, value, dim)
-
-
-def _max(value: Tensor, dims: Shape) -> Tensor:
-    if isinstance(value, NativeTensor):
-        result = value.default_backend.max(value.native(value.shape), value.shape.indices(dims))
-        return NativeTensor(result, value.shape.without(dims))
-    elif isinstance(value, CollapsedTensor):
-        result = _max(value._inner, dims.only(value._inner.shape))
-        return expand_tensor(result, value.shape.without(dims))
-    elif isinstance(value, TensorStack):
-        reduced_inners = [_max(t, dims.without(value._stack_dim)) for t in value._tensors]
-        return functools.reduce(lambda x, y: maximum(x, y), reduced_inners) if value._stack_dim in dims else TensorStack(reduced_inners, value._stack_dim)
-    else:
-        raise ValueError(type(value))
-
-
-def min_(value: Union[Tensor, list, tuple], dim: DimFilter = non_batch) -> Tensor:
-    """
-    Determines the minimum value of `values` along the specified dimensions.
-
-    Args:
-        value: `Tensor` or `list` / `tuple` of Tensors.
-        dim: Dimension or dimensions to be reduced. One of
-
-            * `None` to reduce all non-batch dimensions
-            * `str` containing single dimension or comma-separated list of dimensions
-            * `Tuple[str]` or `List[str]`
-            * `Shape`
-            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
-            * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors
-
-    Returns:
-        `Tensor` without the reduced dimensions.
-    """
-    return reduce_(_min, value, dim)
-
-
-def _min(value: Tensor, dims: Shape) -> Tensor:
-    if isinstance(value, NativeTensor):
-        result = value.default_backend.min(value.native(value.shape), value.shape.indices(dims))
-        return NativeTensor(result, value.shape.without(dims))
-    elif isinstance(value, CollapsedTensor):
-        result = _min(value._inner, dims.only(value._inner.shape))
-        return expand_tensor(result, value.shape.without(dims))
-    elif isinstance(value, TensorStack):
-        reduced_inners = [_min(t, dims.without(value._stack_dim)) for t in value._tensors]
-        return functools.reduce(lambda x, y: minimum(x, y), reduced_inners) if value._stack_dim in dims else TensorStack(reduced_inners, value._stack_dim)
-    else:
-        raise ValueError(type(value))
-
-
-def finite_min(value, dim: DimFilter = non_batch, default: Union[complex, float] = float('NaN')):
-    """
-    Finds the minimum along `dim` ignoring all non-finite values.
-
-    Args:
-        value: `Tensor` or `list` / `tuple` of Tensors.
-        dim: Dimension or dimensions to be reduced. One of
-
-            * `None` to reduce all non-batch dimensions
-            * `str` containing single dimension or comma-separated list of dimensions
-            * `Tuple[str]` or `List[str]`
-            * `Shape`
-            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
-            * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors
-
-        default: Value to use where no finite value was encountered.
-
-    Returns:
-        `Tensor` without the reduced dimensions.
-    """
-    value_inf = where(is_finite(value), value, float('inf'))
-    result_inf = min_(value_inf, dim)
-    return where(is_finite(result_inf), result_inf, default)
-
-
-def finite_max(value, dim: DimFilter = non_batch, default: Union[complex, float] = float('NaN')):
-    """
-    Finds the maximum along `dim` ignoring all non-finite values.
-
-    Args:
-        value: `Tensor` or `list` / `tuple` of Tensors.
-        dim: Dimension or dimensions to be reduced. One of
-
-            * `None` to reduce all non-batch dimensions
-            * `str` containing single dimension or comma-separated list of dimensions
-            * `Tuple[str]` or `List[str]`
-            * `Shape`
-            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
-            * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors
-
-        default: Value to use where no finite value was encountered.
-
-    Returns:
-        `Tensor` without the reduced dimensions.
-    """
-    value_inf = where(is_finite(value), value, float('-inf'))
-    result_inf = max_(value_inf, dim)
-    return where(is_finite(result_inf), result_inf, default)
-
-
-def finite_sum(value, dim: DimFilter = non_batch, default: Union[complex, float] = float('NaN')):
-    """
-    Sums all finite values in `value` along `dim`.
-
-    Args:
-        value: `Tensor` or `list` / `tuple` of Tensors.
-        dim: Dimension or dimensions to be reduced. One of
-
-            * `None` to reduce all non-batch dimensions
-            * `str` containing single dimension or comma-separated list of dimensions
-            * `Tuple[str]` or `List[str]`
-            * `Shape`
-            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
-            * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors
-
-        default: Value to use where no finite value was encountered.
-
-    Returns:
-        `Tensor` without the reduced dimensions.
-    """
-    finite = is_finite(value)
-    summed = sum_(where(finite, value, 0), dim)
-    return where(any_(finite, dim), summed, default)
-
-
-def finite_mean(value, dim: DimFilter = non_batch, default: Union[complex, float] = float('NaN')):
-    """
-    Computes the mean value of all finite values in `value` along `dim`.
-
-    Args:
-        value: `Tensor` or `list` / `tuple` of Tensors.
-        dim: Dimension or dimensions to be reduced. One of
-
-            * `None` to reduce all non-batch dimensions
-            * `str` containing single dimension or comma-separated list of dimensions
-            * `Tuple[str]` or `List[str]`
-            * `Shape`
-            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
-            * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors
-
-        default: Value to use where no finite value was encountered.
-
-    Returns:
-        `Tensor` without the reduced dimensions.
-    """
-    finite = is_finite(value)
-    summed = sum_(where(finite, value, 0), dim)
-    count = sum_(finite, dim)
-    mean_nan = summed / count
-    return where(is_finite(mean_nan), mean_nan, default)
-
-
-def quantile(value: Tensor,
-             quantiles: Union[float, tuple, list, Tensor],
-             dim: DimFilter = non_batch):
-    """
-    Compute the q-th quantile of `value` along `dim` for each q in `quantiles`.
-
-    Implementations:
-
-    * NumPy: [`quantile`](https://numpy.org/doc/stable/reference/generated/numpy.quantile.html)
-    * PyTorch: [`quantile`](https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile)
-    * TensorFlow: [`tfp.stats.percentile`](https://www.tensorflow.org/probability/api_docs/python/tfp/stats/percentile)
-    * Jax: [`quantile`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.quantile.html)
-
-    Args:
-        value: `Tensor`
-        quantiles: Single quantile or tensor of quantiles to compute.
-            Must be of type `float`, `tuple`, `list` or `Tensor`.
-        dim: Dimension or dimensions to be reduced. One of
-
-            * `None` to reduce all non-batch dimensions
-            * `str` containing single dimension or comma-separated list of dimensions
-            * `Tuple[str]` or `List[str]`
-            * `Shape`
-            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
-            * `'0'` when `isinstance(value, (tuple, list))` to reduce the sequence of Tensors
-
-    Returns:
-        `Tensor` with dimensions of `quantiles` and non-reduced dimensions of `value`.
-    """
-    dims = value.shape.only(dim)
-    native_values = reshaped_native(value, [*value.shape.without(dims), value.shape.only(dims)])
-    backend = choose_backend(native_values)
-    q = tensor(quantiles, default_list_dim=instance('quantiles'))
-    native_quantiles = reshaped_native(q, [q.shape])
-    native_result = backend.quantile(native_values, native_quantiles)
-    return reshaped_tensor(native_result, [q.shape, *value.shape.without(dims)])
-
-
-def median(value, dim: DimFilter = non_batch):
-    """
-    Reduces `dim` of `value` by picking the median value.
-    For odd dimension sizes (ambigous choice), the linear average of the two median values is computed.
-
-    Currently implemented via `quantile()`.
-
-    Args:
-        value: `Tensor`
-        dim: Dimension or dimensions to be reduced. One of
-
-            * `None` to reduce all non-batch dimensions
-            * `str` containing single dimension or comma-separated list of dimensions
-            * `Tuple[str]` or `List[str]`
-            * `Shape`
-            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
-            * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors
-
-    Returns:
-        `Tensor`
-    """
-    return quantile(value, 0.5, dim)
-
-
-def dot(x: Tensor,
-        x_dims: DimFilter,
-        y: Tensor,
-        y_dims: DimFilter) -> Tensor:
-    """
-    Computes the dot product along the specified dimensions.
-    Contracts `x_dims` with `y_dims` by first multiplying the elements and then summing them up.
-
-    For one dimension, this is equal to matrix-matrix or matrix-vector multiplication.
-
-    The function replaces the traditional `dot` / `tensordot` / `matmul` / `einsum` functions.
-
-    * NumPy: [`numpy.tensordot`](https://numpy.org/doc/stable/reference/generated/numpy.tensordot.html), [`numpy.einsum`](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html)
-    * PyTorch: [`torch.tensordot`](https://pytorch.org/docs/stable/generated/torch.tensordot.html#torch.tensordot), [`torch.einsum`](https://pytorch.org/docs/stable/generated/torch.einsum.html)
-    * TensorFlow: [`tf.tensordot`](https://www.tensorflow.org/api_docs/python/tf/tensordot), [`tf.einsum`](https://www.tensorflow.org/api_docs/python/tf/einsum)
-    * Jax: [`jax.numpy.tensordot`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.tensordot.html), [`jax.numpy.einsum`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.einsum.html)
-
-    Args:
-        x: First `Tensor`
-        x_dims: Dimensions of `x` to reduce against `y`
-        y: Second `Tensor`
-        y_dims: Dimensions of `y` to reduce against `x`.
-
-    Returns:
-        Dot product as `Tensor`.
-    """
-    x_dims = x.shape.only(x_dims)
-    y_dims = y.shape.only(y_dims)
-    if not x_dims:
-        assert y_dims.volume == 1, f"Cannot compute dot product between dimensions {x_dims} on {x.shape} and {y_dims} on {y.shape}"
-        y = y[{d: 0 for d in y_dims.names}]
-        return x * y
-    if not y_dims:
-        assert x_dims.volume == 1, f"Cannot compute dot product between dimensions {x_dims} on {x.shape} and {y_dims} on {y.shape}"
-        x = x[{d: 0 for d in x_dims.names}]
-        return x * y
-    if isinstance(x, CompressedSparseMatrix):
-        if isinstance(y, (CompressedSparseMatrix, SparseCoordinateTensor)):
-            raise NotImplementedError("sparse-sparse multiplication not yet supported")
-        return dot_compressed_dense(x, x_dims, y, y_dims)
-    elif isinstance(y, CompressedSparseMatrix):
-        if isinstance(x, (CompressedSparseMatrix, SparseCoordinateTensor)):
-            raise NotImplementedError("sparse-sparse multiplication not yet supported")
-        return dot_compressed_dense(y, y_dims, x, x_dims)
-    if isinstance(x, SparseCoordinateTensor):
-        if isinstance(y, (CompressedSparseMatrix, SparseCoordinateTensor)):
-            raise NotImplementedError("sparse-sparse multiplication not yet supported")
-        return dot_coordinate_dense(x, x_dims, y, y_dims)
-    elif isinstance(y, SparseCoordinateTensor):
-        if isinstance(x, (CompressedSparseMatrix, SparseCoordinateTensor)):
-            raise NotImplementedError("sparse-sparse multiplication not yet supported")
-        return dot_coordinate_dense(y, y_dims, x, x_dims)
-    x_native = x.native(x.shape)
-    y_native = y.native(y.shape)
-    backend = choose_backend(x_native, y_native)
-    remaining_shape_x = x.shape.without(x_dims)
-    remaining_shape_y = y.shape.without(y_dims)
-    assert x_dims.volume == y_dims.volume, f"Failed to reduce {x_dims} against {y_dims} in dot product of {x.shape} and {y.shape}. Sizes do not match."
-    if remaining_shape_y.isdisjoint(remaining_shape_x):  # no shared batch dimensions -> tensordot
-        result_native = backend.tensordot(x_native, x.shape.indices(x_dims), y_native, y.shape.indices(y_dims))
-        result_shape = concat_shapes(remaining_shape_x, remaining_shape_y)
-    else:  # shared batch dimensions -> einsum
-        result_shape = merge_shapes(x.shape.without(x_dims), y.shape.without(y_dims))
-        REDUCE_LETTERS = list('ijklmn')
-        KEEP_LETTERS = list('abcdefgh')
-        x_letters = [(REDUCE_LETTERS if dim in x_dims else KEEP_LETTERS).pop(0) for dim in x.shape.names]
-        letter_map = {dim: letter for dim, letter in zip(x.shape.names, x_letters)}
-        REDUCE_LETTERS = list('ijklmn')
-        y_letters = []
-        for dim in y.shape.names:
-            if dim in y_dims:
-                y_letters.append(REDUCE_LETTERS.pop(0))
-            else:
-                if dim in x.shape and dim not in x_dims:
-                    y_letters.append(letter_map[dim])
-                else:
-                    next_letter = KEEP_LETTERS.pop(0)
-                    letter_map[dim] = next_letter
-                    y_letters.append(next_letter)
-        keep_letters = [letter_map[dim] for dim in result_shape.names]
-        subscripts = f'{"".join(x_letters)},{"".join(y_letters)}->{"".join(keep_letters)}'
-        result_native = backend.einsum(subscripts, x_native, y_native)
-    return NativeTensor(result_native, result_shape)
-
-
-def _backend_op1(x, unbound_method) -> Union[Tensor, PhiTreeNode]:
-    if isinstance(x, Tensor):
-        def apply_op(native_tensor):
-            backend = choose_backend(native_tensor)
-            return getattr(backend, unbound_method.__name__)(backend.auto_cast(native_tensor)[0])
-        apply_op.__name__ = unbound_method.__name__
-        return x._op1(apply_op)
-    elif isinstance(x, PhiTreeNode):
-        return copy_with(x, **{a: _backend_op1(getattr(x, a), unbound_method) for a in value_attributes(x)})
-    else:
-        backend = choose_backend(x)
-        y = getattr(backend, unbound_method.__name__)(backend.auto_cast(x)[0])
-        return y
-
-
-def abs_(x) -> Union[Tensor, PhiTreeNode]:
-    """
-    Computes *||x||<sub>1</sub>*.
-    Complex `x` result in matching precision float values.
-
-    *Note*: The gradient of this operation is undefined for *x=0*.
-    TensorFlow and PyTorch return 0 while Jax returns 1.
-
-    Args:
-        x: `Tensor` or `phi.math.magic.PhiTreeNode`
-
-    Returns:
-        Absolute value of `x` of same type as `x`.
-    """
-    return _backend_op1(x, Backend.abs)
-
-
-def sign(x) -> Union[Tensor, PhiTreeNode]:
-    """
-    The sign of positive numbers is 1 and -1 for negative numbers.
-    The sign of 0 is undefined.
-
-    Args:
-        x: `Tensor` or `phi.math.magic.PhiTreeNode`
-
-    Returns:
-        `Tensor` or `phi.math.magic.PhiTreeNode` matching `x`.
-    """
-    return _backend_op1(x, Backend.sign)
-
-
-def round_(x) -> Union[Tensor, PhiTreeNode]:
-    """ Rounds the `Tensor` or `phi.math.magic.PhiTreeNode` `x` to the closest integer. """
-    return _backend_op1(x, Backend.round)
-
-
-def ceil(x) -> Union[Tensor, PhiTreeNode]:
-    """ Computes *⌈x⌉* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
-    return _backend_op1(x, Backend.ceil)
-
-
-def floor(x) -> Union[Tensor, PhiTreeNode]:
-    """ Computes *⌊x⌋* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
-    return _backend_op1(x, Backend.floor)
-
-
-def sqrt(x) -> Union[Tensor, PhiTreeNode]:
-    """ Computes *sqrt(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
-    return _backend_op1(x, Backend.sqrt)
-
-
-def exp(x) -> Union[Tensor, PhiTreeNode]:
-    """ Computes *exp(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
-    return _backend_op1(x, Backend.exp)
-
-
-def to_float(x) -> Union[Tensor, PhiTreeNode]:
-    """
-    Converts the given tensor to floating point format with the currently specified precision.
-    
-    The precision can be set globally using `math.set_global_precision()` and locally using `with math.precision()`.
-    
-    See the `phi.math` module documentation at https://tum-pbs.github.io/PhiFlow/Math.html
-
-    See Also:
-        `cast()`.
-
-    Args:
-        x: `Tensor` or `phi.math.magic.PhiTreeNode` to convert
-
-    Returns:
-        `Tensor` or `phi.math.magic.PhiTreeNode` matching `x`.
-    """
-    return _backend_op1(x, Backend.to_float)
-
-
-def to_int32(x) -> Union[Tensor, PhiTreeNode]:
-    """ Converts the `Tensor` or `phi.math.magic.PhiTreeNode` `x` to 32-bit integer. """
-    return _backend_op1(x, Backend.to_int32)
-
-
-def to_int64(x) -> Union[Tensor, PhiTreeNode]:
-    """ Converts the `Tensor` or `phi.math.magic.PhiTreeNode` `x` to 64-bit integer. """
-    return _backend_op1(x, Backend.to_int64)
-
-
-def to_complex(x) -> Union[Tensor, PhiTreeNode]:
-    """
-    Converts the given tensor to complex floating point format with the currently specified precision.
-
-    The precision can be set globally using `math.set_global_precision()` and locally using `with math.precision()`.
-
-    See the `phi.math` module documentation at https://tum-pbs.github.io/PhiFlow/Math.html
-
-    See Also:
-        `cast()`.
-
-    Args:
-        x: values to convert
-
-    Returns:
-        `Tensor` of same shape as `x`
-    """
-    return _backend_op1(x, Backend.to_complex)
-
-
-def is_finite(x) -> Union[Tensor, PhiTreeNode]:
-    """ Returns a `Tensor` or `phi.math.magic.PhiTreeNode` matching `x` with values `True` where `x` has a finite value and `False` otherwise. """
-    return _backend_op1(x, Backend.isfinite)
-
-
-def real(x) -> Union[Tensor, PhiTreeNode]:
-    """
-    See Also:
-        `imag()`, `conjugate()`.
-
-    Args:
-        x: `Tensor` or `phi.math.magic.PhiTreeNode` or native tensor.
-
-    Returns:
-        Real component of `x`.
-    """
-    return _backend_op1(x, Backend.real)
-
-
-def imag(x) -> Union[Tensor, PhiTreeNode]:
-    """
-    Returns the imaginary part of `x`.
-    If `x` does not store complex numbers, returns a zero tensor with the same shape and dtype as this tensor.
-
-    See Also:
-        `real()`, `conjugate()`.
-
-    Args:
-        x: `Tensor` or `phi.math.magic.PhiTreeNode` or native tensor.
-
-    Returns:
-        Imaginary component of `x` if `x` is complex, zeros otherwise.
-    """
-    return _backend_op1(x, Backend.imag)
-
-
-def conjugate(x) -> Union[Tensor, PhiTreeNode]:
-    """
-    See Also:
-        `imag()`, `real()`.
-
-    Args:
-        x: Real or complex `Tensor` or `phi.math.magic.PhiTreeNode` or native tensor.
-
-    Returns:
-        Complex conjugate of `x` if `x` is complex, else `x`.
-    """
-    return _backend_op1(x, Backend.conj)
-
-
-def degrees(deg):
-    """ Convert degrees to radians. """
-    return deg * (3.1415 / 180.)
-
-
-def sin(x) -> Union[Tensor, PhiTreeNode]:
-    """ Computes *sin(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
-    return _backend_op1(x, Backend.sin)
-
-
-def arcsin(x) -> Union[Tensor, PhiTreeNode]:
-    """ Computes the inverse of *sin(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`.
-    For real arguments, the result lies in the range [-π/2, π/2].
-    """
-    return _backend_op1(x, Backend.arcsin)
-
-
-def cos(x) -> Union[Tensor, PhiTreeNode]:
-    """ Computes *cos(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
-    return _backend_op1(x, Backend.cos)
-
-
-def arccos(x) -> Union[Tensor, PhiTreeNode]:
-    """ Computes the inverse of *cos(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`.
-    For real arguments, the result lies in the range [0, π].
-    """
-    return _backend_op1(x, Backend.cos)
-
-
-def tan(x) -> Union[Tensor, PhiTreeNode]:
-    """ Computes *tan(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
-    return _backend_op1(x, Backend.tan)
-
-
-def arctan(x, divide_by=None) -> Union[Tensor, PhiTreeNode]:
-    """
-    Computes the inverse of *tan(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`.
-
-    Args:
-        x: Input. The single-argument `arctan` function cannot output π/2 or -π/2 since tan(π/2) is infinite.
-        divide_by: If specified, computes `arctan(x/divide_by)` so that it can return π/2 and -π/2.
-            This is equivalent to the common `arctan2` function.
-    """
-    if divide_by is None:
-        return _backend_op1(x, Backend.arctan)
-    else:
-        divide_by = to_float(divide_by)
-        return custom_op2(x, divide_by, arctan, lambda a, b: choose_backend(a, b).arctan2(a, b), 'arctan')
-
-
-def sinh(x) -> Union[Tensor, PhiTreeNode]:
-    """ Computes *sinh(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
-    return _backend_op1(x, Backend.sinh)
-
-
-def arcsinh(x) -> Union[Tensor, PhiTreeNode]:
-    """ Computes the inverse of *sinh(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
-    return _backend_op1(x, Backend.arcsinh)
-
-
-def cosh(x) -> Union[Tensor, PhiTreeNode]:
-    """ Computes *cosh(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
-    return _backend_op1(x, Backend.cosh)
-
-
-def arccosh(x) -> Union[Tensor, PhiTreeNode]:
-    """ Computes the inverse of *cosh(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
-    return _backend_op1(x, Backend.arccosh)
-
-
-def tanh(x) -> Union[Tensor, PhiTreeNode]:
-    """ Computes *tanh(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
-    return _backend_op1(x, Backend.tanh)
-
-
-def arctanh(x) -> Union[Tensor, PhiTreeNode]:
-    """ Computes the inverse of *tanh(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
-    return _backend_op1(x, Backend.arctanh)
-
-
-def log(x) -> Union[Tensor, PhiTreeNode]:
-    """ Computes the natural logarithm of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
-    return _backend_op1(x, Backend.log)
-
-
-def log2(x) -> Union[Tensor, PhiTreeNode]:
-    """ Computes *log(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x` with base 2. """
-    return _backend_op1(x, Backend.log2)
-
-
-def log10(x) -> Union[Tensor, PhiTreeNode]:
-    """ Computes *log(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x` with base 10. """
-    return _backend_op1(x, Backend.log10)
-
-
-def sigmoid(x) -> Union[Tensor, PhiTreeNode]:
-    """ Computes the sigmoid function of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
-    return _backend_op1(x, Backend.sigmoid)
-
-
-def cast_same(*values: Tensor) -> Tuple[Tensor]:
-    """
-    Casts all tensors to the same `DType`.
-    If all data types are of the same kind, returns the largest occurring data type.
-    Otherwise casts `bool` &rarr; `int` &rarr; `float` &rarr; `complex`.
-
-    Args:
-        *values: tensors to cast
-
-    Returns:
-        Tuple of Tensors with same data type.
-    """
-    assert all(isinstance(v, Tensor) for v in values), f"Only Tensor arguments allowed but got {values}"
-    dtypes = [v.dtype for v in values]
-    if any(dt != dtypes[0] for dt in dtypes):
-        common_type = combine_types(*dtypes, fp_precision=get_precision())
-        return tuple([cast(v, common_type) for v in values])
-    else:
-        return values
-
-
-def divide_no_nan(x: Union[float, Tensor], y: Union[float, Tensor]):
-    """ Computes *x/y* with the `Tensor`s `x` and `y` but returns 0 where *y=0*. """
-    return custom_op2(x, y,
-                      l_operator=divide_no_nan,
-                      l_native_function=lambda x_, y_: choose_backend(x_, y_).divide_no_nan(x_, y_),
-                      r_operator=lambda y_, x_: divide_no_nan(x_, y_),
-                      r_native_function=lambda y_, x_: choose_backend(x_, y_).divide_no_nan(x_, y_),
-                      op_name='divide_no_nan')
-
-
-def maximum(x: Union[Tensor, float], y: Union[Tensor, float]):
-    """ Computes the element-wise maximum of `x` and `y`. """
-    return custom_op2(x, y, maximum, lambda x_, y_: choose_backend(x_, y_).maximum(x_, y_), op_name='maximum')
-
-
-def minimum(x: Union[Tensor, float], y: Union[Tensor, float]):
-    """ Computes the element-wise minimum of `x` and `y`. """
-    return custom_op2(x, y, minimum, lambda x_, y_: choose_backend(x_, y_).minimum(x_, y_), op_name='minimum')
-
-
-def clip(x: Tensor, lower_limit: Union[float, Tensor], upper_limit: Union[float, Tensor]):
-    """ Limits the values of the `Tensor` `x` to lie between `lower_limit` and `upper_limit` (inclusive). """
-    if isinstance(lower_limit, Number) and isinstance(upper_limit, Number):
-
-        def clip_(x):
-            return x._op1(lambda native: choose_backend(native).clip(native, lower_limit, upper_limit))
-
-        return broadcast_op(clip_, [x])
-    else:
-        return maximum(lower_limit, minimum(x, upper_limit))
-
-
-def convolve(value: Tensor,
-             kernel: Tensor,
-             extrapolation: 'e_.Extrapolation' = None) -> Tensor:
-    """
-    Computes the convolution of `value` and `kernel` along the spatial axes of `kernel`.
-
-    The channel dimensions of `value` are reduced against the equally named dimensions of `kernel`.
-    The result will have the non-reduced channel dimensions of `kernel`.
-
-    Args:
-        value: `Tensor` whose shape includes all spatial dimensions of `kernel`.
-        kernel: `Tensor` used as convolutional filter.
-        extrapolation: If not None, pads `value` so that the result has the same shape as `value`.
-
-    Returns:
-        `Tensor`
-    """
-    assert all(dim in value.shape for dim in kernel.shape.spatial.names), f"Value must have all spatial dimensions of kernel but got value {value} kernel {kernel}"
-    conv_shape = kernel.shape.spatial
-    in_channels = value.shape.channel
-    out_channels = kernel.shape.channel.without(in_channels)
-    batch = value.shape.batch & kernel.shape.batch
-    if extrapolation is not None and extrapolation != e_.ZERO:
-        value = pad(value, {dim: (kernel.shape.get_size(dim) // 2, (kernel.shape.get_size(dim) - 1) // 2) for dim in conv_shape.names}, extrapolation)
-    native_kernel = reshaped_native(kernel, (batch, out_channels, in_channels, *conv_shape.names), force_expand=in_channels)
-    native_value = reshaped_native(value, (batch, in_channels, *conv_shape.names), force_expand=batch)
-    backend = choose_backend(native_value, native_kernel)
-    native_result = backend.conv(native_value, native_kernel, zero_padding=extrapolation == e_.ZERO)
-    result = reshaped_tensor(native_result, (batch, out_channels, *conv_shape))
-    return result
-
-
-def boolean_mask(x: Tensor, dim: str, mask: Tensor):
-    """
-    Discards values `x.dim[i]` where `mask.dim[i]=False`.
-    All dimensions of `mask` that are not `dim` are treated as batch dimensions.
-
-    Alternative syntax: `x.dim[mask]`.
-
-    Implementations:
-
-    * NumPy: Slicing
-    * PyTorch: [`masked_select`](https://pytorch.org/docs/stable/generated/torch.masked_select.html)
-    * TensorFlow: [`tf.boolean_mask`](https://www.tensorflow.org/api_docs/python/tf/boolean_mask)
-    * Jax: Slicing
-
-    Args:
-        x: `Tensor` of values.
-        dim: Dimension of `x` to along which to discard slices.
-        mask: Boolean `Tensor` marking which values to keep. Must have the dimension `dim` matching `x´.
-
-    Returns:
-        Selected values of `x` as `Tensor` with dimensions from `x` and `mask`.
-    """
-    assert dim in mask.shape, f"mask dimension '{dim}' must be present on the mask but got {mask.shape}"
-    
-    def uniform_boolean_mask(x: Tensor, mask_1d: Tensor):
-        if dim in x.shape:
-            x_native = x.native(x.shape.names)  # order does not matter
-            mask_native = mask_1d.native()  # only has 1 dim
-            backend = choose_backend(x_native, mask_native)
-            result_native = backend.boolean_mask(x_native, mask_native, axis=x.shape.index(dim))
-            new_shape = x.shape.with_sizes(backend.staticshape(result_native))
-            return NativeTensor(result_native, new_shape)
-        else:
-            total = int(sum_(to_int64(mask_1d), mask_1d.shape))
-            new_shape = mask_1d.shape.with_sizes([total])
-            return expand(x, new_shape)
-
-    return broadcast_op(uniform_boolean_mask, [x, mask], iter_dims=mask.shape.without(dim))
-
-
-def gather(values: Tensor, indices: Tensor, dims: Union[DimFilter, None] = None):
-    """
-    Gathers the entries of `values` at positions described by `indices`.
-    All non-channel dimensions of `indices` that are part of `values` but not indexed are treated as batch dimensions.
-
-    See Also:
-        `scatter()`.
-
-    Args:
-        values: `Tensor` containing values to gather.
-        indices: `int` `Tensor`. Multidimensional position references in `values`.
-            Must contain a single channel dimension for the index vector matching the number of dimensons to index.
-            This channel dimension should list the dimension names to index as item names unless explicitly specified as `dims`.
-        dims: (Optional) Dimensions indexed by `indices`.
-            Alternatively, the dimensions can be specified as the item names of the channel dimension of `indices`.
-            If `None` and no index item names are specified, will default to all spatial dimensions or all instance dimensions, depending on which ones are present (but not both).
-
-    Returns:
-        `Tensor` with combined batch dimensions, channel dimensions of `values` and spatial/instance dimensions of `indices`.
-    """
-    assert channel(indices).rank < 2, f"indices can at most have one channel dimension but got {indices.shape}"
-    if dims is None:
-        if channel(indices) and channel(indices).item_names[0]:
-            dims = channel(indices).item_names[0]
-        else:  # Fallback to spatial / instance
-            warnings.warn(f"Indexing without item names is not recommended. Got indices {indices.shape}", SyntaxWarning, stacklevel=2)
-            assert values.shape.instance.is_empty or values.shape.spatial.is_empty, f"Specify gather dimensions for values with both instance and spatial dimensions. Got {values.shape}"
-            dims = values.shape.instance if values.shape.spatial.is_empty else values.shape.spatial
-    if indices.dtype.kind == bool:
-        indices = to_int32(indices)
-    dims = parse_dim_order(dims)
-    assert dims in values.shape, f"Trying to index non-existant dimensions with indices {indices.shape} into values {values.shape}"
-    treat_as_batch = non_channel(indices).only(values.shape).without(dims)
-    batch_ = (values.shape.batch & indices.shape.batch).without(dims) & treat_as_batch
-    channel_ = values.shape.without(dims).without(batch_)
-    index_list_dims = indices.shape.non_channel.without(batch_)
-    squeeze_index_list = False
-    if not index_list_dims:
-        index_list_dims = instance('_single_index')
-        squeeze_index_list = True
-    native_values = reshaped_native(values, [batch_, *dims, channel_])
-    native_indices = reshaped_native(indices, [batch_, *index_list_dims, channel(indices)])
-    backend = choose_backend(native_values, native_indices)
-    native_result = backend.batched_gather_nd(native_values, native_indices)
-    result = reshaped_tensor(native_result, [batch_, *index_list_dims, channel_], convert=False)
-    if squeeze_index_list:
-        result = result[{'_single_index': 0}]
-    return result
-
-
-def scatter(base_grid: Union[Tensor, Shape],
-            indices: Tensor,
-            values: Union[Tensor, float],
-            mode: str = 'update',
-            outside_handling: str = 'discard',
-            indices_gradient=False):
-    """
-    Scatters `values` into `base_grid` at `indices`.
-    instance dimensions of `indices` and/or `values` are reduced during scattering.
-    Depending on `mode`, this method has one of the following effects:
-
-    * `mode='update'`: Replaces the values of `base_grid` at `indices` by `values`. The result is undefined if `indices` contains duplicates.
-    * `mode='add'`: Adds `values` to `base_grid` at `indices`. The values corresponding to duplicate indices are accumulated.
-    * `mode='mean'`: Replaces the values of `base_grid` at `indices` by the mean of all `values` with the same index.
-
-    Implementations:
-
-    * NumPy: Slice assignment / `numpy.add.at`
-    * PyTorch: [`torch.scatter`](https://pytorch.org/docs/stable/generated/torch.scatter.html), [`torch.scatter_add`](https://pytorch.org/docs/stable/generated/torch.scatter_add.html)
-    * TensorFlow: [`tf.tensor_scatter_nd_add`](https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_add), [`tf.tensor_scatter_nd_update`](https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update)
-    * Jax: [`jax.lax.scatter_add`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scatter_add.html), [`jax.lax.scatter`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scatter.html)
-
-    See Also:
-        `gather()`.
-
-    Args:
-        base_grid: `Tensor` into which `values` are scattered.
-        indices: `Tensor` of n-dimensional indices at which to place `values`.
-            Must have a single channel dimension with size matching the number of spatial dimensions of `base_grid`.
-            This dimension is optional if the spatial rank is 1.
-            Must also contain all `scatter_dims`.
-        values: `Tensor` of values to scatter at `indices`.
-        mode: Scatter mode as `str`. One of ('add', 'mean', 'update')
-        outside_handling: Defines how indices lying outside the bounds of `base_grid` are handled.
-
-            * `'discard'`: outside indices are ignored.
-            * `'clamp'`: outside indices are projected onto the closest point inside the grid.
-            * `'undefined'`: All points are expected to lie inside the grid. Otherwise an error may be thrown or an undefined tensor may be returned.
-        indices_gradient: Whether to allow the gradient of this operation to be backpropagated through `indices`.
-
-    Returns:
-        Copy of `base_grid` with updated values at `indices`.
-    """
-    assert mode in ('update', 'add', 'mean')
-    assert outside_handling in ('discard', 'clamp', 'undefined')
-    assert isinstance(indices_gradient, bool)
-    grid_shape = base_grid if isinstance(base_grid, Shape) else base_grid.shape
-    assert channel(indices).rank < 2
-    if channel(indices) and channel(indices).item_names[0]:
-        indexed_dims = channel(indices).item_names[0]
-        assert indexed_dims in grid_shape, f"Scatter indices {indices.shape} point to missing dimensions in grid {grid_shape}"
-        if indexed_dims != grid_shape.only(indexed_dims).names:
-            indices = indices.vector[grid_shape.only(indexed_dims).names]
-        indexed_dims = grid_shape.only(indexed_dims)
-    else:
-        assert channel(indices).rank == 1 or (grid_shape.spatial_rank + grid_shape.instance_rank == 1 and indices.shape.channel_rank == 0)
-        indexed_dims = grid_shape.spatial
-        assert channel(indices).volume == indexed_dims.rank
-    values = wrap(values)
-    batches = values.shape.non_channel.non_instance & indices.shape.non_channel.non_instance
-    channels = grid_shape.without(indexed_dims).without(batches) & values.shape.channel
-    # --- Set up grid ---
-    if isinstance(base_grid, Shape):
-        with choose_backend_t(indices, values):
-            base_grid = zeros(base_grid & batches & values.shape.channel, dtype=values.dtype)
-        if mode != 'add':
-            base_grid += math.nan
-    # --- Handle outside indices ---
-    if outside_handling == 'clamp':
-        indices = clip(indices, 0, tensor(indexed_dims, channel('vector')) - 1)
-    elif outside_handling == 'discard':
-        indices_linear = pack_dims(indices, instance, instance(_scatter_instance=1))
-        indices_inside = min_((round_(indices_linear) >= 0) & (round_(indices_linear) < tensor(indexed_dims, channel('vector'))), 'vector')
-        indices_linear = boolean_mask(indices_linear, '_scatter_instance', indices_inside)
-        if instance(values).rank > 0:
-            values_linear = pack_dims(values, instance, instance(_scatter_instance=1))
-            values_linear = boolean_mask(values_linear, '_scatter_instance', indices_inside)
-            values = unpack_dim(values_linear, '_scatter_instance', instance(values))
-        indices = unpack_dim(indices_linear, '_scatter_instance', instance(indices))
-        if indices.shape.is_non_uniform:
-            raise NotImplementedError()
-    lists = indices.shape.instance & values.shape.instance
-
-    def scatter_forward(base_grid, indices, values):
-        indices = to_int32(round_(indices))
-        native_grid = reshaped_native(base_grid, [batches, *indexed_dims, channels], force_expand=True)
-        native_values = reshaped_native(values, [batches, lists, channels], force_expand=True)
-        native_indices = reshaped_native(indices, [batches, lists, 'vector'], force_expand=True)
-        backend = choose_backend(native_indices, native_values, native_grid)
-        if mode in ('add', 'update'):
-            native_result = backend.scatter(native_grid, native_indices, native_values, mode=mode)
-        else:  # mean
-            zero_grid = backend.zeros_like(native_grid)
-            summed = backend.scatter(zero_grid, native_indices, native_values, mode='add')
-            count = backend.scatter(zero_grid, native_indices, backend.ones_like(native_values), mode='add')
-            native_result = summed / backend.maximum(count, 1)
-            native_result = backend.where(count == 0, native_grid, native_result)
-        return reshaped_tensor(native_result, [batches, *indexed_dims, channels], check_sizes=True)
-
-    def scatter_backward(args: dict, _output, d_output):
-        from ._nd import spatial_gradient
-        values_grad = gather(d_output, args['indices'])
-        spatial_gradient_indices = gather(spatial_gradient(d_output, dims=indexed_dims), args['indices'])
-        indices_grad = mean(spatial_gradient_indices * args['values'], 'vector_')
-        return None, indices_grad, values_grad
-
-    from ._functional import custom_gradient
-    scatter_function = custom_gradient(scatter_forward, scatter_backward) if indices_gradient else scatter_forward
-    result = scatter_function(base_grid, indices, values)
-    return result
-
-
-def fft(x: Tensor, dims: DimFilter = spatial) -> Tensor:
-    """
-    Performs a fast Fourier transform (FFT) on all spatial dimensions of x.
-    
-    The inverse operation is `ifft()`.
-
-    Implementations:
-
-    * NumPy: [`np.fft.fft`](https://numpy.org/doc/stable/reference/generated/numpy.fft.fft.html),
-      [`numpy.fft.fft2`](https://numpy.org/doc/stable/reference/generated/numpy.fft.fft2.html),
-      [`numpy.fft.fftn`](https://numpy.org/doc/stable/reference/generated/numpy.fft.fftn.html)
-    * PyTorch: [`torch.fft.fft`](https://pytorch.org/docs/stable/fft.html)
-    * TensorFlow: [`tf.signal.fft`](https://www.tensorflow.org/api_docs/python/tf/signal/fft),
-      [`tf.signal.fft2d`](https://www.tensorflow.org/api_docs/python/tf/signal/fft2d),
-      [`tf.signal.fft3d`](https://www.tensorflow.org/api_docs/python/tf/signal/fft3d)
-    * Jax: [`jax.numpy.fft.fft`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.fft.fft.html),
-      [`jax.numpy.fft.fft2`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.fft.fft2.html)
-      [`jax.numpy.fft.fft`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.fft.fftn.html)
-
-    Args:
-        x: Uniform complex or float `Tensor` with at least one spatial dimension.
-        dims: Dimensions along which to perform the FFT.
-            If `None`, performs the FFT along all spatial dimensions of `x`.
-
-    Returns:
-        *Ƒ(x)* as complex `Tensor`
-    """
-    dims = x.shape.only(dims)
-    x_native = x.native(x.shape)
-    result_native = choose_backend(x_native).fft(x_native, x.shape.indices(dims))
-    return NativeTensor(result_native, x.shape)
-
-
-def ifft(k: Tensor, dims: DimFilter = spatial):
-    """
-    Inverse of `fft()`.
-
-    Args:
-        k: Complex or float `Tensor` with at least one spatial dimension.
-        dims: Dimensions along which to perform the inverse FFT.
-            If `None`, performs the inverse FFT along all spatial dimensions of `k`.
-
-    Returns:
-        *Ƒ<sup>-1</sup>(k)* as complex `Tensor`
-    """
-    dims = k.shape.only(dims)
-    k_native = k.native(k.shape)
-    result_native = choose_backend(k_native).ifft(k_native, k.shape.indices(dims))
-    return NativeTensor(result_native, k.shape)
-
-
-def dtype(x) -> DType:
-    """
-    Returns the data type of `x`.
-
-    Args:
-        x: `Tensor` or native tensor.
-
-    Returns:
-        `DType`
-    """
-    if isinstance(x, Tensor):
-        return x.dtype
-    else:
-        return choose_backend(x).dtype(x)
-
-
-def expand_tensor(value: Union[float, Tensor], dims: Shape):
-    if not dims:
-        return value
-    value = wrap(value)
-    shape = value.shape
-    for dim in reversed(dims):
-        if dim in value.shape:
-            shape &= dim  # checks sizes, copies item names
-        else:
-            if dim.size is None:
-                dim = dim.with_sizes([1])
-            shape = concat_shapes(dim, shape)
-    if shape == value.shape:  # no changes made
-        return value
-    elif shape.rank == value.rank:  # changes made but same dims
-        return value._with_shape_replaced(shape._reorder(value.shape))
-    else:
-        return CollapsedTensor(value, shape)
-
-
-def close(*tensors, rel_tolerance=1e-5, abs_tolerance=0) -> bool:
-    """
-    Checks whether all tensors have equal values within the specified tolerance.
-    
-    Does not check that the shapes exactly match.
-    Tensors with different shapes are reshaped before comparing.
-
-    Args:
-        *tensors: `Tensor` or tensor-like (constant) each
-        rel_tolerance: relative tolerance (Default value = 1e-5)
-        abs_tolerance: absolute tolerance (Default value = 0)
-
-    Returns:
-        Whether all given tensors are equal to the first tensor within the specified tolerance.
-    """
-    tensors = [wrap(t) for t in tensors]
-    for other in tensors[1:]:
-        if not _close(tensors[0], other, rel_tolerance=rel_tolerance, abs_tolerance=abs_tolerance):
-            return False
-    return True
-
-
-def _close(tensor1, tensor2, rel_tolerance=1e-5, abs_tolerance=0):
-    if tensor2 is tensor1:
-        return True
-    new_shape, (native1, native2) = broadcastable_native_tensors(tensor1, tensor2)
-    np1 = choose_backend(native1).numpy(native1)
-    np2 = choose_backend(native2).numpy(native2)
-    return np.allclose(np1, np2, rel_tolerance, abs_tolerance)
-
-
-def assert_close(*values,
-                 rel_tolerance: float = 1e-5,
-                 abs_tolerance: float = 0,
-                 msg: str = "",
-                 verbose: bool = True):
-    """
-    Checks that all given tensors have equal values within the specified tolerance.
-    Raises an AssertionError if the values of this tensor are not within tolerance of any of the other tensors.
-    
-    Does not check that the shapes match as long as they can be broadcast to a common shape.
-
-    Args:
-      values: Tensors or native tensors or numbers or sequences of numbers.
-      rel_tolerance: Relative tolerance.
-      abs_tolerance: Absolute tolerance.
-      msg: Optional error message.
-      verbose: Whether to print conflicting values.
-    """
-    if not values:
-        return
-    phi_tensors = [t for t in values if isinstance(t, Tensor)]
-    if phi_tensors:
-        values = [compatible_tensor(t, phi_tensors[0].shape)._simplify() for t in values]  # use Tensor to infer dimensions
-        for other in values[1:]:
-            _assert_close(values[0], other, rel_tolerance, abs_tolerance, msg, verbose)
-    elif all(isinstance(v, PhiTreeNode) for v in values):
-        tree0, tensors0 = disassemble_tree(values[0])
-        for value in values[1:]:
-            tree, tensors_ = disassemble_tree(value)
-            assert tree0 == tree, f"Tree structures do not match: {tree0} and {tree}"
-            for t0, t in zip(tensors0, tensors_):
-                _assert_close(t0, t, rel_tolerance, abs_tolerance, msg, verbose)
-    else:
-        np_values = [choose_backend(t).numpy(t) for t in values]
-        for other in np_values[1:]:
-            np.testing.assert_allclose(np_values[0], other, rel_tolerance, abs_tolerance, err_msg=msg, verbose=verbose)
-
-
-def _assert_close(tensor1: Tensor, tensor2: Tensor, rel_tolerance: float, abs_tolerance: float, msg: str, verbose: bool):
-    if tensor2 is tensor1:
-        return
-    # if isinstance(tensor2, (int, float, bool)):
-    #     np.testing.assert_allclose(tensor1.numpy(), tensor2, rel_tolerance, abs_tolerance)
-    if isinstance(tensor1, Layout):
-        tensor1._assert_close(tensor2, rel_tolerance, abs_tolerance, msg, verbose)
-    elif isinstance(tensor2, Layout):
-        tensor2._assert_close(tensor1, rel_tolerance, abs_tolerance, msg, verbose)
-    elif isinstance(tensor1, CompressedSparseMatrix):
-        if isinstance(tensor2, CompressedSparseMatrix):
-            _assert_close(tensor1._values, tensor2._values, rel_tolerance, abs_tolerance, msg, verbose)
-            _assert_close(tensor1._indices, tensor2._indices, 0, 0, msg, verbose)
-            _assert_close(tensor1._pointers, tensor2._pointers, 0, 0, msg, verbose)
-        elif tensor1._compressed_dims.only(tensor2.shape):
-            _assert_close(dense(tensor1), tensor2, rel_tolerance, abs_tolerance, msg, verbose)
-        else:
-            _assert_close(tensor1._values, tensor2._values, rel_tolerance, abs_tolerance, msg, verbose)
-    elif isinstance(tensor2, CompressedSparseMatrix):
-        return _assert_close(tensor2, tensor1, rel_tolerance, abs_tolerance, msg, verbose)
-    else:
-        def inner_assert_close(tensor1, tensor2):
-            new_shape, (native1, native2) = broadcastable_native_tensors(tensor1, tensor2)
-            np1 = choose_backend(native1).numpy(native1)
-            np2 = choose_backend(native2).numpy(native2)
-            if not np.allclose(np1, np2, rel_tolerance, abs_tolerance):
-                np.testing.assert_allclose(np1, np2, rel_tolerance, abs_tolerance, err_msg=msg, verbose=verbose)
-
-        broadcast_op(inner_assert_close, [tensor1, tensor2], no_return=True)
-
-
-def _native_wrapper(tensor_function: Callable, create_native_function: Callable, persistent_refs=False):
-    INPUT_TENSORS = []
-    OUTPUT_TENSORS = []
-
-    def native_function(*natives):
-        natives = list(natives)
-        values = [t._op1(lambda _: natives.pop(0)) for t in INPUT_TENSORS]
-        assert len(natives) == 0, "Not all arguments were converted"
-        result = tensor_function(*values)
-        results = [result] if not isinstance(result, (tuple, list)) else result
-        OUTPUT_TENSORS.clear()
-        OUTPUT_TENSORS.extend(results)
-        return sum([v._natives() for v in results], ())
-
-    backend = default_backend()
-    traced = create_native_function(native_function, backend)
-    if traced is NotImplemented:
-        warnings.warn(f"Backend '{backend}' not supported. Returning original function.", RuntimeWarning)
-        return tensor_function, None, INPUT_TENSORS, OUTPUT_TENSORS
-
-    def wrapper(*values: Tensor):
-        INPUT_TENSORS.clear()
-        INPUT_TENSORS.extend(values)
-        for v in values:
-            v._expand()
-        natives = sum([v._natives() for v in values], ())
-        results_native = list(traced(*natives))
-        results = [t._with_natives_replaced(results_native) for t in OUTPUT_TENSORS]
-        if not persistent_refs:
-            INPUT_TENSORS.clear()
-            # OUTPUT_TENSORS.clear()  outputs need to be saved because native_function may be called only the first time. Will get garbage collected once the function is not referenced anymore.
-        assert len(results_native) == 0
-        return results[0] if len(results) == 1 else results
-
-    return wrapper, traced, INPUT_TENSORS, OUTPUT_TENSORS
-
-
-def stop_gradient(x):
-    """
-    Disables gradients for the given tensor.
-    This may switch off the gradients for `x` itself or create a copy of `x` with disabled gradients.
-
-    Implementations:
-
-    * PyTorch: [`x.detach()`](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach)
-    * TensorFlow: [`tf.stop_gradient`](https://www.tensorflow.org/api_docs/python/tf/stop_gradient)
-    * Jax: [`jax.lax.stop_gradient`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.stop_gradient.html)
-
-    Args:
-        x: `Tensor` or `phi.math.magic.PhiTreeNode` for which gradients should be disabled.
-
-    Returns:
-        Copy of `x`.
-    """
-    if isinstance(x, Tensor):
-        return x._op1(lambda native: choose_backend(native).stop_gradient(native))
-    elif isinstance(x, PhiTreeNode):
-        nest, values = disassemble_tree(x)
-        new_values = [stop_gradient(v) for v in values]
-        return assemble_tree(nest, new_values)
-    else:
-        return wrap(choose_backend(x).stop_gradient(x))
-
-
-def pairwise_distances(positions: Tensor, max_distance: Union[float, Tensor] = None, others_dims=instance('others'), format='dense') -> Tensor:
-    """
-    Computes the distance matrix containing the pairwise position differences between each pair of points.
-    Points that are further apart than `max_distance` are assigned a distance value of `0`.
-    The diagonal of the matrix (self-distance) also consists purely of zero-vectors.
-
-    Args:
-        positions: `Tensor`.
-            Channel dimensions are interpreted as position components.
-            Instance and spatial dimensions list nodes.
-        max_distance: Scalar or `Tensor` specifying a max_radius for each point separately.
-            Can contain additional batch dimensions but spatial/instance dimensions must match `positions` if present.
-            If not specified, uses an infinite cutoff radius, i.e. all points will be considered neighbors.
-        others_dims: These dimensions will be added to the result to list the neighbours of each point.
-            If `positions` contains multiple spatial/instance dimensions, it is recommended to specify a neighbor dim for each of them.
-        format:
-            One of `'dense', 'csr'`
-
-    Returns:
-        `Tensor`
-
-    Examples:
-        >>> pos = vec(x=0, y=tensor([0, 1, 2.5], instance('particles')))
-        >>> dx = pairwise_distances(pos, format='dense', max_distance=2)
-        >>> dx.particles[0]
-        (x=0.000, y=0.000); (x=0.000, y=1.000); (x=0.000, y=0.000) (othersⁱ=3, vectorᶜ=x,y)
-    """
-    if format == 'dense':
-        # if not count_self:
-        #     warnings.warn(f"count_self has no effect when using format '{format}'", SyntaxWarning, stacklevel=2)
-        dx = unpack_dim(pack_dims(positions, non_batch(positions).non_channel, instance('_tmp')), '_tmp', others_dims) - positions
-        if max_distance is not None:
-            neighbors = sum_(dx ** 2, channel) <= max_distance ** 2
-            dx = where(neighbors, dx, 0)
-        return dx
-    else:  # sparse
-        assert max_distance is not None, "max_distance must be specified when computing distance in sparse format"
-        backend = choose_backend_t(positions, max_distance)
-        batch_shape = batch(positions) & batch(max_distance)
-        pos_i_shape = non_batch(positions).non_channel
-        native_positions = reshaped_native(positions, [batch_shape, pos_i_shape, channel(positions)], force_expand=True)
-        if isinstance(max_distance, Tensor):
-            if max_distance.shape:
-                rad_i_shape = non_batch(max_distance).non_channel
-                if rad_i_shape:  # different values for each particle
-                    assert rad_i_shape == pos_i_shape, f"spatial/instance dimensions of max_radius {rad_i_shape} must match positions {pos_i_shape} if present."
-                    max_distance = reshaped_native(max_distance, [batch_shape, rad_i_shape], force_expand=True)
-                else:
-                    max_distance = reshaped_native(max_distance, [batch_shape], force_expand=True)
-            else:
-                max_distance = max_distance.native()
-        if not others_dims.well_defined:
-            assert others_dims.rank == 1, f"others_dims sizes must be specified when passing more then one dimension but got {others_dims}"
-            others_dims = others_dims.with_size(pos_i_shape.volume)
-        sparse_natives = backend.pairwise_distances(native_positions, max_distance, format)
-        tensors = []
-        if format == 'csr':
-            for indices, pointers, values in sparse_natives:
-                indices = wrap(indices, instance('nnz'))
-                pointers = wrap(pointers, instance('pointers'))
-                values = wrap(values, instance('nnz'), channel(positions))
-                tensors.append(CompressedSparseMatrix(indices, pointers, values, others_dims, pos_i_shape))
-        elif format == 'coo':
-            raise NotImplementedError
-        elif format == 'csc':
-            raise NotImplementedError
-        else:
-            raise ValueError(format)
-        return stack_tensors(tensors, batch_shape)
+import functools
+import math
+import warnings
+from numbers import Number
+from typing import Tuple, Callable, Any, Union, Optional
+
+import numpy as np
+
+from . import extrapolation as e_
+from ._magic_ops import expand, pack_dims, unpack_dim, cast, copy_with, value_attributes, bool_to_int, tree_map, concat, stack
+from ._shape import (Shape, EMPTY_SHAPE,
+                     spatial, batch, channel, instance, merge_shapes, parse_dim_order, concat_shapes,
+                     IncompatibleShapes, DimFilter, non_batch, dual, non_channel, shape)
+from ._sparse import CompressedSparseMatrix, dot_compressed_dense, dense, SparseCoordinateTensor, dot_coordinate_dense, get_format, to_format, stored_indices, tensor_like, sparse_dims, same_sparsity_pattern, is_sparse
+from ._tensors import (Tensor, wrap, tensor, broadcastable_native_tensors, NativeTensor, TensorStack,
+                       custom_op2, compatible_tensor, variable_attributes, disassemble_tree, assemble_tree,
+                       is_scalar, Layout, expand_tensor)
+from .backend import default_backend, choose_backend, Backend, get_precision, convert as b_convert, BACKENDS, NoBackendFound, ComputeDevice, NUMPY
+from .backend._dtype import DType, combine_types
+from .magic import PhiTreeNode, Shapable
+
+
+def choose_backend_t(*values, prefer_default=False) -> Backend:
+    """
+    Choose backend for given `Tensor` or native tensor values.
+    Backends need to be registered to be available, e.g. via the global import `phi.<backend>` or `phi.detect_backends()`.
+
+    Args:
+        *values: Sequence of `Tensor`s, native tensors or constants.
+        prefer_default: Whether to always select the default backend if it can work with `values`, see `default_backend()`.
+
+    Returns:
+        The selected `phi.math.backend.Backend`
+    """
+    natives = sum([v._natives() if isinstance(v, Tensor) else (v,) for v in values], ())
+    return choose_backend(*natives, prefer_default=prefer_default)
+
+
+def convert(x, backend: Backend = None, use_dlpack=True):
+    """
+    Convert the native representation of a `Tensor` or `phi.math.magic.PhiTreeNode` to the native format of `backend`.
+
+    *Warning*: This operation breaks the automatic differentiation chain.
+
+    See Also:
+        `phi.math.backend.convert()`.
+
+    Args:
+        x: `Tensor` to convert. If `x` is a `phi.math.magic.PhiTreeNode`, its variable attributes are converted.
+        backend: Target backend. If `None`, uses the current default backend, see `phi.math.backend.default_backend()`.
+
+    Returns:
+        `Tensor` with native representation belonging to `backend`.
+    """
+    if isinstance(x, Tensor):
+        return x._op1(lambda native: b_convert(native, backend, use_dlpack=use_dlpack))
+    elif isinstance(x, PhiTreeNode):
+        return copy_with(x, **{a: convert(getattr(x, a), backend, use_dlpack=use_dlpack) for a in variable_attributes(x)})
+    else:
+        return b_convert(x, backend, use_dlpack=use_dlpack)
+
+
+def to_device(value, device: ComputeDevice or str, convert=True, use_dlpack=True):
+    """
+    Allocates the tensors of `value` on `device`.
+    If the value already exists on that device, this function may either create a copy of `value` or return `value` directly.
+
+    See Also:
+        `to_cpu()`.
+
+    Args:
+        value: `Tensor` or `phi.math.magic.PhiTreeNode` or native tensor.
+        device: Device to allocate value on.
+            Either `ComputeDevice` or category `str`, such as `'CPU'` or `'GPU'`.
+        convert: Whether to convert tensors that do not belong to the corresponding backend to compatible native tensors.
+            If `False`, this function has no effect on numpy tensors.
+        use_dlpack: Only if `convert==True`.
+            Whether to use the DLPack library to convert from one GPU-enabled backend to another.
+
+    Returns:
+        Same type as `value`.
+    """
+    assert isinstance(device, (ComputeDevice, str)), f"device must be a ComputeDevice or str but got {type(device)}"
+    return tree_map(_to_device, value, device=device, convert_to_backend=convert, use_dlpack=use_dlpack)
+
+
+def _to_device(value: Tensor or Any, device: ComputeDevice or str, convert_to_backend: bool, use_dlpack: bool):
+    if isinstance(value, Tensor):
+        if not convert and value.default_backend == NUMPY:
+            return value
+        natives = [_to_device(n, device, convert_to_backend, use_dlpack) for n in value._natives()]
+        return value._with_natives_replaced(natives)
+    else:
+        old_backend = choose_backend(value)
+        if isinstance(device, str):
+            device = old_backend.list_devices(device)[0]
+        if old_backend != device.backend:
+            if convert_to_backend:
+                value = b_convert(value, device.backend, use_dlpack=use_dlpack)
+            else:
+                return value
+        return device.backend.allocate_on_device(value, device)
+
+
+def all_available(*values: Tensor) -> bool:
+    """
+    Tests if the values of all given tensors are known and can be read at this point.
+    Tracing placeholders are considered not available, even when they hold example values.
+
+    Tensors are not available during `jit_compile()`, `jit_compile_linear()` or while using TensorFlow's legacy graph mode.
+    
+    Tensors are typically available when the backend operates in eager mode and is not currently tracing a function.
+
+    This can be used instead of the native checks
+
+    * PyTorch: `torch._C._get_tracing_state()`
+    * TensorFlow: `tf.executing_eagerly()`
+    * Jax: `isinstance(x, jax.core.Tracer)`
+
+    Args:
+      values: Tensors to check.
+
+    Returns:
+        `True` if no value is a placeholder or being traced, `False` otherwise.
+    """
+    return all([v.available for v in values])
+
+
+def seed(seed: int):
+    """
+    Sets the current seed of all backends and the built-in `random` package.
+
+    Calling this function with a fixed value at the start of an application yields reproducible results
+    as long as the same backend is used.
+
+    Args:
+        seed: Seed to use.
+    """
+    for backend in BACKENDS:
+        backend.seed(seed)
+    import random
+    random.seed(0)
+
+
+def native(value: Union[Tensor, Number, tuple, list, Any]):
+    """
+    Returns the native tensor representation of `value`.
+    If `value` is a `phi.math.Tensor`, this is equal to calling `phi.math.Tensor.native()`.
+    Otherwise, checks that `value` is a valid tensor object and returns it.
+
+    Args:
+        value: `Tensor` or native tensor or tensor-like.
+
+    Returns:
+        Native tensor representation
+
+    Raises:
+        ValueError if the tensor cannot be transposed to match target_shape
+    """
+    if isinstance(value, Tensor):
+        return value.native()
+    else:
+        choose_backend(value)  # check that value is a native tensor
+        return value
+
+
+def numpy(value: Union[Tensor, Number, tuple, list, Any]):
+    """
+    Converts `value` to a `numpy.ndarray` where value must be a `Tensor`, backend tensor or tensor-like.
+    If `value` is a `phi.math.Tensor`, this is equal to calling `phi.math.Tensor.numpy()`.
+
+    *Note*: Using this function breaks the autograd chain. The returned tensor is not differentiable.
+    To get a differentiable tensor, use `Tensor.native()` instead.
+
+    Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
+    If a dimension of the tensor is not listed in `order`, a `ValueError` is raised.
+
+    If `value` is a NumPy array, it may be returned directly.
+
+    Returns:
+        NumPy representation of `value`
+
+    Raises:
+        ValueError if the tensor cannot be transposed to match target_shape
+    """
+    if isinstance(value, Tensor):
+        return value.numpy()
+    else:
+        backend = choose_backend(value)
+        return backend.numpy(value)
+
+
+def reshaped_native(value: Tensor,
+                    groups: Union[tuple, list],
+                    force_expand: Any = True,
+                    to_numpy=False):
+    """
+    Returns a native representation of `value` where dimensions are laid out according to `groups`.
+
+    See Also:
+        `native()`, `pack_dims()`, `reshaped_tensor()`, `reshaped_numpy()`.
+
+    Args:
+        value: `Tensor`
+        groups: `tuple` or `list` of dimensions to be packed into one native dimension. Each entry must be one of the following:
+
+            * `str`: the name of one dimension that is present on `value`.
+            * `Shape`: Dimensions to be packed. If `force_expand`, missing dimensions are first added, otherwise they are ignored.
+            * Filter function: Packs all dimensions of this type that are present on `value`.
+
+        force_expand: `bool` or sequence of dimensions.
+            If `True`, repeats the tensor along missing dimensions.
+            If `False`, puts singleton dimensions where possible.
+            If a sequence of dimensions is provided, only forces the expansion for groups containing those dimensions.
+        to_numpy: If True, converts the native tensor to a `numpy.ndarray`.
+
+    Returns:
+        Native tensor with dimensions matching `groups`.
+    """
+    assert isinstance(value, Tensor), f"value must be a Tensor but got {type(value)}"
+    assert value.shape.is_uniform, f"Only uniform (homogenous) tensors can be converted to native but got shape {value.shape}"
+    assert isinstance(groups, (tuple, list)), f"groups must be a tuple or list but got {type(value)}"
+    order = []
+    groups = [group(value) if callable(group) else group for group in groups]
+    for i, group in enumerate(groups):
+        if isinstance(group, Shape):
+            present = value.shape.only(group)
+            if force_expand is True or present.volume > 1 or (force_expand is not False and group.only(force_expand).volume > 1):
+                value = expand(value, group)
+            value = pack_dims(value, group, batch(f"group{i}"))
+            order.append(f"group{i}")
+        else:
+            assert isinstance(group, str), f"Groups must be either single-dim str or Shape but got {group}"
+            assert ',' not in group, f"When packing multiple dimensions, pass a well-defined Shape instead of a comma-separated str. Got {group}"
+            order.append(group)
+    return value.numpy(order) if to_numpy else value.native(order)
+
+
+def reshaped_numpy(value: Tensor, groups: Union[tuple, list], force_expand: Any = True):
+    """
+    Returns the NumPy representation of `value` where dimensions are laid out according to `groups`.
+
+    See Also:
+        `numpy()`, `reshaped_native()`, `pack_dims()`, `reshaped_tensor()`.
+
+    Args:
+        value: `Tensor`
+        groups: Sequence of dimension names as `str` or groups of dimensions to be packed_dim as `Shape`.
+        force_expand: `bool` or sequence of dimensions.
+            If `True`, repeats the tensor along missing dimensions.
+            If `False`, puts singleton dimensions where possible.
+            If a sequence of dimensions is provided, only forces the expansion for groups containing those dimensions.
+
+    Returns:
+        NumPy `ndarray` with dimensions matching `groups`.
+    """
+    return reshaped_native(value, groups, force_expand=force_expand, to_numpy=True)
+
+
+def reshaped_tensor(value: Any,
+                    groups: Union[tuple, list],
+                    check_sizes=False,
+                    convert=True):
+    """
+    Creates a `Tensor` from a native tensor or tensor-like whereby the dimensions of `value` are split according to `groups`.
+
+    See Also:
+        `phi.math.tensor()`, `reshaped_native()`, `unpack_dim()`.
+
+    Args:
+        value: Native tensor or tensor-like.
+        groups: Sequence of dimension groups to be packed_dim as `tuple[Shape]` or `list[Shape]`.
+        check_sizes: If True, group sizes must match the sizes of `value` exactly. Otherwise, allows singleton dimensions.
+        convert: If True, converts the data to the native format of the current default backend.
+            If False, wraps the data in a `Tensor` but keeps the given data reference if possible.
+
+    Returns:
+        `Tensor` with all dimensions from `groups`
+    """
+    assert all(isinstance(g, Shape) for g in groups), "groups must be a sequence of Shapes"
+    dims = [batch(f'group{i}') for i, group in enumerate(groups)]
+    try:
+        value = tensor(value, *dims, convert=convert)
+    except IncompatibleShapes:
+        raise IncompatibleShapes(f"Cannot reshape native tensor {type(value)} with sizes {value.shape} given groups {groups}")
+    for i, group in enumerate(groups):
+        if value.shape.get_size(f'group{i}') == group.volume:
+            value = unpack_dim(value, f'group{i}', group)
+        elif check_sizes:
+            raise AssertionError(f"Group {group} does not match dimension {i} of value {value.shape}")
+        else:
+            value = unpack_dim(value, f'group{i}', group)
+    return value
+
+
+def copy(value: Tensor):
+    """
+    Copies the data buffer and encapsulating `Tensor` object.
+
+    Args:
+        value: `Tensor` to be copied.
+
+    Returns:
+        Copy of `value`.
+    """
+    if value._is_tracer:
+        warnings.warn("Tracing tensors cannot be copied.", RuntimeWarning)
+        return value
+    return value._op1(lambda native: choose_backend(native).copy(native))
+
+
+def native_call(f: Callable, *inputs: Tensor, channels_last=None, channel_dim='vector', spatial_dim=None):
+    """
+    Calls `f` with the native representations of the `inputs` tensors in standard layout and returns the result as a `Tensor`.
+
+    All inputs are converted to native tensors (including precision cast) depending on `channels_last`:
+
+    * `channels_last=True`: Dimension layout `(total_batch_size, spatial_dims..., total_channel_size)`
+    * `channels_last=False`: Dimension layout `(total_batch_size, total_channel_size, spatial_dims...)`
+
+    All batch dimensions are compressed into a single dimension with `total_batch_size = input.shape.batch.volume`.
+    The same is done for all channel dimensions.
+
+    Additionally, missing batch and spatial dimensions are added so that all `inputs` have the same batch and spatial shape.
+
+    Args:
+        f: Function to be called on native tensors of `inputs`.
+            The function output must have the same dimension layout as the inputs, unless overridden by `spatial_dim`,
+            and the batch size must be identical.
+        *inputs: Uniform `Tensor` arguments
+        channels_last: (Optional) Whether to put channels as the last dimension of the native representation.
+            If `None`, the channels are put in the default position associated with the current backend,
+            see `phi.math.backend.Backend.prefers_channels_last()`.
+        channel_dim: Name of the channel dimension of the result.
+        spatial_dim: Name of the spatial dimension of the result.
+
+    Returns:
+        `Tensor` with batch and spatial dimensions of `inputs`, unless overridden by `spatial_dim`,
+        and single channel dimension `channel_dim`.
+    """
+    if channels_last is None:
+        try:
+            backend = choose_backend(f)
+        except NoBackendFound:
+            backend = choose_backend_t(*inputs, prefer_default=True)
+        channels_last = backend.prefers_channels_last()
+    batch = merge_shapes(*[i.shape.batch for i in inputs])
+    spatial = merge_shapes(*[i.shape.spatial for i in inputs])
+    natives = []
+    for i in inputs:
+        groups = (batch, *i.shape.spatial.names, i.shape.channel) if channels_last else (batch, i.shape.channel, *i.shape.spatial.names)
+        natives.append(reshaped_native(i, groups, force_expand=False))
+    output = f(*natives)
+    if isinstance(channel_dim, str):
+        channel_dim = channel(channel_dim)
+    assert isinstance(channel_dim, Shape), "channel_dim must be a Shape or str"
+    if isinstance(output, (tuple, list)):
+        raise NotImplementedError()
+    else:
+        if spatial_dim is None:
+            groups = (batch, *spatial, channel_dim) if channels_last else (batch, channel_dim, *spatial)
+        else:
+            if isinstance(spatial_dim, str):
+                spatial_dim = spatial(spatial_dim)
+            assert isinstance(spatial_dim, Shape), "spatial_dim must be a Shape or str"
+            groups = (batch, *spatial_dim, channel_dim) if channels_last else (batch, channel_dim, *spatial_dim)
+        result = reshaped_tensor(output, groups, convert=False)
+        if result.shape.get_size(channel_dim.name) == 1 and not channel_dim.item_names[0]:
+            result = result.dimension(channel_dim.name)[0]  # remove vector dim if not required
+        return result
+
+
+def print_(obj: Union[Tensor, PhiTreeNode, Number, tuple, list, None] = None, name: str = ""):
+    """
+    Print a tensor with no more than two spatial dimensions, slicing it along all batch and channel dimensions.
+    
+    Unlike NumPy's array printing, the dimensions are sorted.
+    Elements along the alphabetically first dimension is printed to the right, the second dimension upward.
+    Typically, this means x right, y up.
+
+    Args:
+        obj: tensor-like
+        name: name of the tensor
+
+    Returns:
+
+    """
+    def variables(obj) -> dict:
+        if hasattr(obj, '__variable_attrs__') or hasattr(obj, '__value_attrs__'):
+            return {f".{a}": getattr(obj, a) for a in variable_attributes(obj)}
+        elif isinstance(obj, (tuple, list)):
+            return {f"[{i}]": item for i, item in enumerate(obj)}
+        elif isinstance(obj, dict):
+            return obj
+        else:
+            raise ValueError(f"Not PhiTreeNode: {type(obj)}")
+
+    if name:
+        print(" " * 12 + name)
+    if obj is None:
+        print("None")
+    elif isinstance(obj, Tensor):
+        print(f"{obj:full}")
+    elif isinstance(obj, PhiTreeNode):
+        for n, val in variables(obj).items():
+            print_(val, name + n)
+    else:
+        print(f"{wrap(obj):full}")
+
+
+def map_(function, *values, range=range, **kwargs) -> Union[Tensor, None]:
+    """
+    Calls `function` on all elements of `values`.
+
+    Args:
+        function: Function to be called on single elements contained in `value`. Must return a value that can be stored in tensors.
+        *values: `Tensors` containing positional arguments for `function`.
+            Number of tensors must match `function` signature.
+        range: Range function. Can be used to generate tqdm output by passing `trange`.
+        **kwargs: Non-`Tensor` keyword arguments for `function`.
+            Their shapes are not broadcast with the positional arguments.
+
+    Returns:
+        `Tensor` of same shape as `value`.
+    """
+    if not values:
+        return function(**kwargs)
+    values = [v if isinstance(v, Shapable) else wrap(v) for v in values]
+    shape = merge_shapes(*[v.shape for v in values])
+    flat = [pack_dims(expand(v, shape), shape, channel(flat=shape.volume)) for v in values]
+    result = []
+    results = None
+    for _, items in zip(range(flat[0].flat.size_or_1), zip(*flat)):
+        f_output = function(*items, **kwargs)
+        if isinstance(f_output, tuple):
+            if results is None:
+                results = [[] for _ in f_output]
+            for result_i, output_i in zip(results, f_output):
+                result_i.append(output_i)
+        else:
+            result.append(f_output)
+    if results is None:
+        if any(r is None for r in result):
+            assert all(r is None for r in result), f"map function returned None for some elements, {result}"
+            return None
+        return unpack_dim(stack(result, channel('_c')) if isinstance(result, Shapable) else wrap(result, channel('_c')), '_c', shape)
+    else:
+        for i, result_i in enumerate(results):
+            if any(r is None for r in result_i):
+                assert all(r is None for r in result_i), f"map function returned None for some elements at output index {i}, {result_i}"
+                results[i] = None
+        return tuple([unpack_dim(stack(result_i, channel('_c')) if isinstance(result_i, Shapable) else wrap(result_i, channel('_c')), '_c', shape) for result_i in results])
+
+
+def _initialize(uniform_initializer, shapes: Tuple[Shape]) -> Tensor:
+    shape = concat_shapes(*shapes)
+    assert shape.well_defined, f"When creating a Tensor, shape needs to have definitive sizes but got {shape}"
+    if shape.is_non_uniform:
+        stack_dim = shape.shape.without('dims')[0:1]
+        shapes = shape.unstack(stack_dim.name)
+        tensors = [_initialize(uniform_initializer, s) for s in shapes]
+        return stack_tensors(tensors, stack_dim)
+    else:
+        return uniform_initializer(shape)
+
+
+def zeros(*shape: Shape, dtype=None) -> Tensor:
+    """
+    Define a tensor with specified shape with value `0.0` / `0` / `False` everywhere.
+    
+    This method may not immediately allocate the memory to store the values.
+
+    See Also:
+        `zeros_like()`, `ones()`.
+
+    Args:
+        *shape: This (possibly empty) sequence of `Shape`s is concatenated, preserving the order.
+        dtype: Data type as `DType` object. Defaults to `float` matching the current precision setting.
+
+    Returns:
+        `Tensor`
+    """
+    return _initialize(lambda shape: expand_tensor(NativeTensor(default_backend().zeros((), dtype=DType.as_dtype(dtype)), EMPTY_SHAPE), shape), shape)
+
+
+def zeros_like(obj: Union[Tensor, PhiTreeNode]) -> Union[Tensor, PhiTreeNode]:
+    """ Create a `Tensor` containing only `0.0` / `0` / `False` with the same shape and dtype as `obj`. """
+    nest, values = disassemble_tree(obj)
+    zeros_ = []
+    for val in values:
+        val = wrap(val)
+        with val.default_backend:
+            zeros_.append(zeros(val.shape, dtype=val.dtype))
+    return assemble_tree(nest, zeros_)
+
+
+def ones(*shape: Shape, dtype=None) -> Tensor:
+    """
+    Define a tensor with specified shape with value `1.0`/ `1` / `True` everywhere.
+    
+    This method may not immediately allocate the memory to store the values.
+
+    See Also:
+        `ones_like()`, `zeros()`.
+
+    Args:
+        *shape: This (possibly empty) sequence of `Shape`s is concatenated, preserving the order.
+        dtype: Data type as `DType` object. Defaults to `float` matching the current precision setting.
+
+    Returns:
+        `Tensor`
+    """
+    return _initialize(lambda shape: expand_tensor(NativeTensor(default_backend().ones((), dtype=DType.as_dtype(dtype)), EMPTY_SHAPE), shape), shape)
+
+
+def ones_like(value: Tensor) -> Tensor:
+    """ Create a `Tensor` containing only `1.0` / `1` / `True` with the same shape and dtype as `obj`. """
+    return zeros_like(value) + 1
+
+
+def random_normal(*shape: Shape, dtype=None) -> Tensor:
+    """
+    Creates a `Tensor` with the specified shape, filled with random values sampled from a normal / Gaussian distribution.
+
+    Implementations:
+
+    * NumPy: [`numpy.random.standard_normal`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.standard_normal.html)
+    * PyTorch: [`torch.randn`](https://pytorch.org/docs/stable/generated/torch.randn.html)
+    * TensorFlow: [`tf.random.normal`](https://www.tensorflow.org/api_docs/python/tf/random/normal)
+    * Jax: [`jax.random.normal`](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.normal.html)
+
+    Args:
+        *shape: This (possibly empty) sequence of `Shape`s is concatenated, preserving the order.
+        dtype: (optional) floating point `DType`. If `None`, a float tensor with the current default precision is created, see `get_precision()`.
+
+    Returns:
+        `Tensor`
+    """
+
+    def uniform_random_normal(shape):
+        native = choose_backend(*shape.sizes, prefer_default=True).random_normal(shape.sizes, DType.as_dtype(dtype))
+        return NativeTensor(native, shape)
+
+    return _initialize(uniform_random_normal, shape)
+
+
+def random_uniform(*shape: Shape,
+                   low: Union[Tensor, float] = 0,
+                   high: Union[Tensor, float] = 1,
+                   dtype: Union[DType, tuple] = None) -> Tensor:
+    """
+    Creates a `Tensor` with the specified shape, filled with random values sampled from a uniform distribution.
+
+    Args:
+        *shape: This (possibly empty) sequence of `Shape`s is concatenated, preserving the order.
+        dtype: (optional) `DType` or `(kind, bits)`.
+            The dtype kind must be one of `float`, `int`, `complex`.
+            If not specified, a `float` tensor with the current default precision is created, see `get_precision()`.
+        low: Minimum value, included.
+        high: Maximum value, excluded.
+    Returns:
+        `Tensor`
+    """
+    def uniform_random_uniform(shape):
+        native = choose_backend(low, high, *shape.sizes, prefer_default=True).random_uniform(shape.sizes, low, high, DType.as_dtype(dtype))
+        return NativeTensor(native, shape)
+
+    return _initialize(uniform_random_uniform, shape)
+
+
+def transpose(x: Tensor, axes):
+    """
+    Swap the dimension order of `x`.
+    This operation is superfluous since tensors will be reshaped under the hood or when getting the native/numpy representations.
+
+    Implementations:
+
+    * NumPy: [`numpy.transpose`](https://numpy.org/doc/stable/reference/generated/numpy.transpose.html)
+    * PyTorch: [`x.permute`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute)
+    * TensorFlow: [`tf.transpose`](https://www.tensorflow.org/api_docs/python/tf/transpose)
+    * Jax: [`jax.numpy.transpose`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.transpose.html)
+
+    Args:
+        x: `Tensor` or native tensor.
+        axes: `tuple` or `list`
+
+    Returns:
+        `Tensor` or native tensor, depending on `x`.
+    """
+    if isinstance(x, Tensor):
+        return expand(x, x.shape[axes])
+    else:
+        return choose_backend(x).transpose(x, axes)
+
+
+def cumulative_sum(x: Tensor, dim: DimFilter):
+    """
+    Performs a cumulative sum of `x` along `dim`.
+
+    Implementations:
+
+    * NumPy: [`cumsum`](https://numpy.org/doc/stable/reference/generated/numpy.cumsum.html)
+    * PyTorch: [`cumsum`](https://pytorch.org/docs/stable/generated/torch.cumsum.html)
+    * TensorFlow: [`cumsum`](https://www.tensorflow.org/api_docs/python/tf/math/cumsum)
+    * Jax: [`cumsum`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.cumsum.html)
+
+    Args:
+        x: `Tensor`
+        dim: Dimension along which to sum, as `str` or `Shape`.
+
+    Returns:
+        `Tensor` with the same shape as `x`.
+    """
+    dim = x.shape.only(dim)
+    assert len(dim) == 1, f"dim must be a single dimension but got {dim}"
+    native_x = x.native(x.shape)
+    native_result = choose_backend(native_x).cumsum(native_x, x.shape.index(dim))
+    return NativeTensor(native_result, x.shape)
+
+
+def fftfreq(resolution: Shape, dx: Union[Tensor, float] = 1, dtype: DType = None):
+    """
+    Returns the discrete Fourier transform sample frequencies.
+    These are the frequencies corresponding to the components of the result of `math.fft` on a tensor of shape `resolution`.
+
+    Args:
+        resolution: Grid resolution measured in cells
+        dx: Distance between sampling points in real space.
+        dtype: Data type of the returned tensor (Default value = None)
+
+    Returns:
+        `Tensor` holding the frequencies of the corresponding values computed by math.fft
+    """
+    k = meshgrid(**{dim: np.fft.fftfreq(int(n)) for dim, n in resolution.spatial._named_sizes})
+    k /= dx
+    return to_float(k) if dtype is None else cast(k, dtype)
+
+
+def meshgrid(dims: Union[Callable, Shape] = spatial, stack_dim=channel('vector'), **dimensions: Union[int, Tensor]) -> Tensor:
+    """
+    Generate a mesh-grid `Tensor` from keyword dimensions.
+
+    Args:
+        **dimensions: Mesh-grid dimensions, mapping names to values.
+            Values may be `int`, 1D `Tensor` or 1D native tensor.
+        dims: Dimension type of mesh-grid dimensions, one of `spatial`, `channel`, `batch`, `instance`.
+        stack_dim: Channel dim along which grids are stacked.
+            This is optional for 1D mesh-grids. In that case returns a `Tensor` without a stack dim if `None` or an empty `Shape` is passed.
+
+    Returns:
+        Mesh-grid `Tensor` with the dimensions of `dims` / `dimensions` and `stack_dim`.
+
+    Examples:
+        >>> math.meshgrid(x=2, y=2)
+        (xˢ=2, yˢ=2, vectorᶜ=x,y) 0.500 ± 0.500 (0e+00...1e+00)
+
+        >>> math.meshgrid(x=2, y=(-1, 1))
+        (xˢ=2, yˢ=2, vectorᶜ=x,y) 0.250 ± 0.829 (-1e+00...1e+00)
+
+        >>> math.meshgrid(x=2, stack_dim=None)
+        (0, 1) along xˢ
+    """
+    assert 'dim_type' not in dimensions, f"dim_type has been renamed to dims"
+    assert not stack_dim or stack_dim.name not in dimensions
+    if isinstance(dims, Shape):
+        assert not dimensions, f"When passing a Shape to meshgrid(), no kwargs are allowed"
+        dimensions = {d: s for d, s in zip(dims.names, dims.sizes)}
+        grid_shape = dims
+        dim_values = [tuple(range(s)) for s in dims.sizes]
+    else:
+        dim_type = dims
+        assert callable(dim_type), f"dims must be a Shape or dimension type but got {dims}"
+        dim_values = []
+        dim_sizes = []
+        for dim, spec in dimensions.items():
+            if isinstance(spec, int):
+                dim_values.append(tuple(range(spec)))
+                dim_sizes.append(spec)
+            elif isinstance(spec, Tensor):
+                assert spec.rank == 1, f"Only 1D sequences allowed, got {spec} for dimension '{dim}'."
+                dim_values.append(spec.native())
+                dim_sizes.append(spec.shape.volume)
+            else:
+                backend = choose_backend(spec)
+                shape = backend.staticshape(spec)
+                assert len(shape) == 1, "Only 1D sequences allowed, got {spec} for dimension '{dim}'."
+                dim_values.append(spec)
+                dim_sizes.append(shape[0])
+        grid_shape = dim_type(**{dim: size for dim, size in zip(dimensions.keys(), dim_sizes)})
+    backend = choose_backend(*dim_values, prefer_default=True)
+    indices_list = backend.meshgrid(*dim_values)
+    channels = [NativeTensor(t, grid_shape) for t in indices_list]
+    if not stack_dim:
+        assert len(channels) == 1, f"meshgrid with multiple dimension requires a valid stack_dim but got {stack_dim}"
+        return channels[0]
+    if stack_dim.item_names[0] is None:
+        stack_dim = stack_dim.with_size(tuple(dimensions.keys()))
+    return stack_tensors(channels, stack_dim)
+
+
+def linspace(start: Union[float, Tensor], stop: Union[float, Tensor], dim: Shape) -> Tensor:
+    """
+    Returns `number` evenly spaced numbers between `start` and `stop`.
+
+    See Also:
+        `arange()`, `meshgrid()`.
+
+    Args:
+        start: First value, `int` or `Tensor`.
+        stop: Last value, `int` or `Tensor`.
+        dim: Linspace dimension of integer size.
+            The size determines how many values to linearly space between `start` and `stop`.
+            The values will be laid out along `dim`.
+
+    Returns:
+        `Tensor`
+
+    Examples:
+        >>> math.linspace(0, 1, spatial(x=5))
+        (0.000, 0.250, 0.500, 0.750, 1.000) along xˢ
+
+        >>> math.linspace(0, (-1, 1), spatial(x=3))
+        (0.000, 0.000); (-0.500, 0.500); (-1.000, 1.000) (xˢ=3, vectorᶜ=2)
+    """
+    assert isinstance(dim, Shape) and dim.rank == 1, f"dim must be a single-dimension Shape but got {dim}"
+    if is_scalar(start) and is_scalar(stop):
+        if isinstance(start, Tensor):
+            start = start.native()
+        if isinstance(stop, Tensor):
+            stop = stop.native()
+        native_linspace = choose_backend(start, stop, prefer_default=True).linspace(start, stop, dim.size)
+        return NativeTensor(native_linspace, dim)
+    else:
+        return map_(linspace, start, stop, dim=dim)
+
+
+def arange(dim: Shape, start_or_stop: Union[int, None] = None, stop: Union[int, None] = None, step=1):
+    """
+    Returns evenly spaced values between `start` and `stop`.
+    If only one limit is given, `0` is used for the start.
+
+    See Also:
+        `range_tensor()`, `linspace()`, `meshgrid()`.
+
+    Args:
+        dim: Dimension name and type as `Shape` object.
+            The `size` of `dim` is interpreted as `stop` unless `start_or_stop` is specified.
+        start_or_stop: (Optional) `int`. Interpreted as `start` if `stop` is specified as well. Otherwise this is `stop`.
+        stop: (Optional) `int`. `stop` value.
+        step: Distance between values.
+
+    Returns:
+        `Tensor`
+    """
+    if start_or_stop is None:
+        assert stop is None, "start_or_stop must be specified when stop is given."
+        assert isinstance(dim.size, int), "When start_or_stop is not specified, dim.size must be an integer."
+        start, stop = 0, dim.size
+    elif stop is None:
+        start, stop = 0, start_or_stop
+    else:
+        start = start_or_stop
+    native = choose_backend(start, stop, prefer_default=True).range(start, stop, step, DType(int, 32))
+    return NativeTensor(native, dim.with_sizes([stop - start]))
+
+
+def range_tensor(*shape: Shape):
+    """
+    Returns a `Tensor` with given `shape` containing the linear indices of each element.
+    For 1D tensors, this equivalent to `arange()` with `step=1`.
+
+    See Also:
+        `arange()`, `meshgrid()`.
+
+    Args:
+        shape: Tensor shape.
+
+    Returns:
+        `Tensor`
+    """
+    shape = concat_shapes(*shape)
+    data = arange(spatial('range'), 0, shape.volume)
+    return unpack_dim(data, 'range', shape)
+
+
+def stack_tensors(values: Union[tuple, list], dim: Shape):
+    if len(values) == 1 and not dim:
+        return values[0]
+    values = [wrap(v) for v in values]
+    values = cast_same(*values)
+
+    def inner_stack(*values):
+        if len(values) > 1 or not isinstance(values[0], NativeTensor):
+            if all(isinstance(t, SparseCoordinateTensor) for t in values):
+                if all(values[0]._indices is t._indices for t in values):
+                    return values[0]._with_values(stack_tensors([v._values for v in values], dim))
+            return TensorStack(values, dim)
+        else:
+            value: NativeTensor = values[0]
+            return NativeTensor(value._native, value._native_shape, value.shape & dim.with_size(1))
+
+    result = broadcast_op(inner_stack, values)
+    return result
+
+
+def concat_tensor(values: Union[tuple, list], dim: str) -> Tensor:
+    assert len(values) > 0, "concat() got empty sequence"
+    assert isinstance(dim, str), f"dim must be a single-dimension Shape but got '{dim}' of type {type(dim)}"
+
+    def inner_concat(*values):
+        broadcast_shape: Shape = values[0].shape  # merge_shapes(*[t.shape.with_sizes([None] * t.shape.rank) for t in values])
+        dim_index = broadcast_shape.index(dim)
+        natives = [v.native(order=broadcast_shape.names) for v in values]
+        concatenated = choose_backend(*natives).concat(natives, dim_index)
+        if all([v.shape.get_item_names(dim) is not None for v in values]):
+            broadcast_shape = broadcast_shape.with_dim_size(dim, sum([v.shape.get_item_names(dim) for v in values], ()))
+        else:
+            broadcast_shape = broadcast_shape.with_dim_size(dim, sum([v.shape.get_size(dim) for v in values]))
+        return NativeTensor(concatenated, broadcast_shape)
+
+    result = broadcast_op(inner_concat, values)
+    return result
+
+
+def pad(value: Tensor, widths: dict, mode: Union['e_.Extrapolation', Tensor, Number], **kwargs) -> Tensor:
+    """
+    Pads a tensor along the specified dimensions, determining the added values using the given extrapolation.
+    Unlike `Extrapolation.pad()`, this function can handle negative widths which slice off outer values.
+
+    Args:
+        value: `Tensor` to be padded
+        widths: `dict` mapping dimension name (`str`) to `(lower, upper)`
+            where `lower` and `upper` are `int` that can be positive (pad), negative (slice) or zero (pass).
+        mode: `Extrapolation` used to determine values added from positive `widths`.
+            Assumes constant extrapolation if given a number or `Tensor` instead.
+        kwargs: Additional padding arguments.
+            These are ignored by the standard extrapolations defined in `phi.math.extrapolation` but can be used to pass additional contextual information to custom extrapolations.
+            Grid classes from `phi.field` will pass the argument `bounds: Box`.
+
+    Returns:
+        Padded `Tensor`
+
+    Examples:
+        >>> math.pad(math.ones(spatial(x=10, y=10)), {'x': (1, 1), 'y': (2, 1)}, 0)
+        (xˢ=12, yˢ=13) 0.641 ± 0.480 (0e+00...1e+00)
+
+        >>> math.pad(math.ones(spatial(x=10, y=10)), {'x': (1, -1)}, 0)
+        (xˢ=10, yˢ=10) 0.900 ± 0.300 (0e+00...1e+00)
+    """
+    mode = mode if isinstance(mode, e_.Extrapolation) else e_.ConstantExtrapolation(mode)
+    has_negative_widths = any(w0 < 0 or w1 < 0 for w0, w1 in widths.values())
+    has_positive_widths = any(w0 > 0 or w1 > 0 for w0, w1 in widths.values())
+    slices = None
+    if has_negative_widths:
+        slices = {dim: slice(max(0, -w[0]), min(0, w[1]) or None) for dim, w in widths.items()}
+        widths = {dim: (max(0, w[0]), max(0, w[1])) for dim, w in widths.items()}
+    result_padded = mode.pad(value, widths, **kwargs) if has_positive_widths else value
+    result_sliced = result_padded[slices] if has_negative_widths else result_padded
+    return result_sliced
+
+
+def closest_grid_values(grid: Tensor,
+                        coordinates: Tensor,
+                        extrap: 'e_.Extrapolation',
+                        stack_dim_prefix='closest_',
+                        **kwargs):
+    """
+    Finds the neighboring grid points in all spatial directions and returns their values.
+    The result will have 2^d values for each vector in coordiantes in d dimensions.
+
+    Args:
+      grid: grid data. The grid is spanned by the spatial dimensions of the tensor
+      coordinates: tensor with 1 channel dimension holding vectors pointing to locations in grid index space
+      extrap: grid extrapolation
+      stack_dim_prefix: For each spatial dimension `dim`, stacks lower and upper closest values along dimension `stack_dim_prefix+dim`.
+      kwargs: Additional information for the extrapolation.
+
+    Returns:
+      Tensor of shape (batch, coord_spatial, grid_spatial=(2, 2,...), grid_channel)
+
+    """
+    return broadcast_op(functools.partial(_closest_grid_values, extrap=extrap, stack_dim_prefix=stack_dim_prefix, pad_kwargs=kwargs), [grid, coordinates])
+
+
+def _closest_grid_values(grid: Tensor,
+                         coordinates: Tensor,
+                         extrap: 'e_.Extrapolation',
+                         stack_dim_prefix: str,
+                         pad_kwargs: dict):
+    # alternative method: pad array for all 2^d combinations, then stack to simplify gather.
+    # --- Pad tensor where transform is not possible ---
+    non_copy_pad = {dim: (0 if extrap.is_copy_pad(dim, False) else 1, 0 if extrap.is_copy_pad(dim, True) else 1) for dim in grid.shape.spatial.names}
+    grid = extrap.pad(grid, non_copy_pad, **pad_kwargs)
+    coordinates += wrap([not extrap.is_copy_pad(dim, False) for dim in grid.shape.spatial.names], channel('vector'))
+    # --- Transform coordiantes ---
+    min_coords = to_int32(floor(coordinates))
+    max_coords = extrap.transform_coordinates(min_coords + 1, grid.shape)
+    min_coords = extrap.transform_coordinates(min_coords, grid.shape)
+
+    def left_right(is_hi_by_axis_left, ax_idx):
+        is_hi_by_axis_right = is_hi_by_axis_left | np.array([ax == ax_idx for ax in range(grid.shape.spatial_rank)])
+        coords_left = where(is_hi_by_axis_left, max_coords, min_coords)
+        coords_right = where(is_hi_by_axis_right, max_coords, min_coords)
+        if ax_idx == grid.shape.spatial_rank - 1:
+            values_left = gather(grid, coords_left)
+            values_right = gather(grid, coords_right)
+        else:
+            values_left = left_right(is_hi_by_axis_left, ax_idx + 1)
+            values_right = left_right(is_hi_by_axis_right, ax_idx + 1)
+        return stack_tensors([values_left, values_right], channel(f"{stack_dim_prefix}{grid.shape.spatial.names[ax_idx]}"))
+
+    result = left_right(np.array([False] * grid.shape.spatial_rank), 0)
+    return result
+
+
+def grid_sample(grid: Tensor, coordinates: Tensor, extrap: 'e_.Extrapolation', **kwargs):
+    """
+    Samples values of `grid` at the locations referenced by `coordinates`.
+    Values lying in between sample points are determined via linear interpolation.
+
+    For values outside the valid bounds of `grid` (`coord < 0 or coord > grid.shape - 1`), `extrap` is used to determine the neighboring grid values.
+    If the extrapolation does not support resampling, the grid is padded by one cell layer before resampling.
+    In that case, values lying further outside will not be sampled according to the extrapolation.
+
+    Args:
+        grid: Grid with at least one spatial dimension and no instance dimensions.
+        coordinates: Coordinates with a single channel dimension called `'vector'`.
+            The size of the `vector` dimension must match the number of spatial dimensions of `grid`.
+        extrap: Extrapolation used to determine the values of `grid` outside its valid bounds.
+        kwargs: Additional information for the extrapolation.
+
+    Returns:
+        `Tensor` with channel dimensions of `grid`, spatial and instance dimensions of `coordinates` and combined batch dimensions.
+    """
+    result = broadcast_op(functools.partial(_grid_sample, extrap=extrap, pad_kwargs=kwargs), [grid, coordinates])
+    return result
+
+
+def _grid_sample(grid: Tensor, coordinates: Tensor, extrap: Union['e_.Extrapolation', None], pad_kwargs: dict):
+    if grid.shape.batch == coordinates.shape.batch or grid.shape.batch.volume == 1 or coordinates.shape.batch.volume == 1:
+        # call backend.grid_sample()
+        batch = grid.shape.batch & coordinates.shape.batch
+        backend = choose_backend_t(grid, coordinates)
+        result = NotImplemented
+        if extrap is None:
+            result = backend.grid_sample(reshaped_native(grid, [batch, *grid.shape.spatial, grid.shape.channel]),
+                                         reshaped_native(coordinates, [batch, *coordinates.shape.instance, *coordinates.shape.spatial, 'vector']),
+                                         'undefined')
+        elif extrap.native_grid_sample_mode:
+            result = backend.grid_sample(reshaped_native(grid, [batch, *grid.shape.spatial, grid.shape.channel]),
+                                         reshaped_native(coordinates, [batch, *coordinates.shape.instance, *coordinates.shape.spatial, 'vector']),
+                                         extrap.native_grid_sample_mode)
+        if result is NotImplemented:
+            # pad one layer
+            grid_padded = pad(grid, {dim: (1, 1) for dim in grid.shape.spatial.names}, extrap or e_.ZERO, **pad_kwargs)
+            if extrap is not None:
+                from .extrapolation import _CopyExtrapolation
+                if isinstance(extrap, _CopyExtrapolation):
+                    inner_coordinates = extrap.transform_coordinates(coordinates, grid.shape) + 1
+                else:
+                    inner_coordinates = extrap.transform_coordinates(coordinates + 1, grid_padded.shape)
+            else:
+                inner_coordinates = coordinates + 1
+            result = backend.grid_sample(reshaped_native(grid_padded, [batch, *grid_padded.shape.spatial.names, grid.shape.channel]),
+                                         reshaped_native(inner_coordinates, [batch, *coordinates.shape.instance, *coordinates.shape.spatial, 'vector']),
+                                         'boundary')
+        if result is not NotImplemented:
+            result = reshaped_tensor(result, [grid.shape.batch & coordinates.shape.batch, *coordinates.shape.instance, *coordinates.shape.spatial, grid.shape.channel])
+            return result
+    # fallback to slower grid sampling
+    neighbors = _closest_grid_values(grid, coordinates, extrap or e_.ZERO, '_closest_', pad_kwargs)
+    binary = meshgrid(channel, **{f'_closest_{dim}': (0, 1) for dim in grid.shape.spatial.names}, stack_dim=channel(coordinates))
+    right_weights = coordinates % 1
+    weights = prod(binary * right_weights + (1 - binary) * (1 - right_weights), 'vector')
+    result = sum_(neighbors * weights, dim=[f"_closest_{dim}" for dim in grid.shape.spatial.names])
+    return result
+
+
+def broadcast_op(operation: Callable,
+                 tensors: Union[tuple, list],
+                 iter_dims: Union[set, tuple, list, Shape] = None,
+                 no_return=False):
+    if iter_dims is None:
+        iter_dims = set()
+        for tensor in tensors:
+            iter_dims.update(tensor.shape.shape.without('dims').names)
+            if isinstance(tensor, TensorStack) and tensor.requires_broadcast:
+                iter_dims.add(tensor._stack_dim.name)
+    if len(iter_dims) == 0:
+        return operation(*tensors)
+    else:
+        if isinstance(iter_dims, Shape):
+            iter_dims = iter_dims.names
+        dim = next(iter(iter_dims))
+        dim_type = None
+        size = None
+        item_names = None
+        unstacked = []
+        for tensor in tensors:
+            if dim in tensor.shape.names:
+                unstacked_tensor = tensor.unstack(dim)
+                unstacked.append(unstacked_tensor)
+                if size is None:
+                    size = len(unstacked_tensor)
+                    dim_type = tensor.shape.get_type(dim)
+                else:
+                    assert size == len(unstacked_tensor)
+                    assert dim_type == tensor.shape.get_type(dim)
+                if item_names is None:
+                    item_names = tensor.shape.get_item_names(dim)
+            else:
+                unstacked.append(tensor)
+        result_unstacked = []
+        for i in range(size):
+            gathered = [t[i] if isinstance(t, tuple) else t for t in unstacked]
+            result_unstacked.append(broadcast_op(operation, gathered, iter_dims=set(iter_dims) - {dim}))
+        if not no_return:
+            return TensorStack(result_unstacked, Shape((size,), (dim,), (dim_type,), (item_names,)))
+
+
+def where(condition: Union[Tensor, float, int], value_true: Union[Tensor, float, int], value_false: Union[Tensor, float, int]):
+    """
+    Builds a tensor by choosing either values from `value_true` or `value_false` depending on `condition`.
+    If `condition` is not of type boolean, non-zero values are interpreted as True.
+    
+    This function requires non-None values for `value_true` and `value_false`.
+    To get the indices of True / non-zero values, use :func:`nonzero`.
+
+    Args:
+      condition: determines where to choose values from value_true or from value_false
+      value_true: Values to pick where `condition != 0 / True`
+      value_false: Values to pick where `condition == 0 / False`
+
+    Returns:
+        `Tensor` containing dimensions of all inputs.
+    """
+    condition = wrap(condition)
+    value_true = wrap(value_true)
+    value_false = wrap(value_false)
+
+    def inner_where(c: Tensor, vt: Tensor, vf: Tensor):
+        if vt._is_tracer or vf._is_tracer or c._is_tracer:
+            return c * vt + (1 - c) * vf  # ToDo this does not take NaN into account
+        if is_sparse(vt) or is_sparse(vf):
+            if same_sparsity_pattern(vt, vf, allow_const=True) and same_sparsity_pattern(c, vt, allow_const=True):
+                c_values = c._values if is_sparse(c) else c
+                vt_values = vt._values if is_sparse(vt) else vt
+                vf_values = vf._values if is_sparse(vf) else vf
+                result_values = where(c_values, vt_values, vf_values)
+                return c._with_values(result_values)
+            raise NotImplementedError
+        shape, (c, vt, vf) = broadcastable_native_tensors(c, vt, vf)
+        result = choose_backend(c, vt, vf).where(c, vt, vf)
+        return NativeTensor(result, shape)
+
+    return broadcast_op(inner_where, [condition, value_true, value_false])
+
+
+def nonzero(value: Tensor, list_dim: Union[Shape, str] = instance('nonzero'), index_dim: Shape = channel('vector')):
+    """
+    Get spatial indices of non-zero / True values.
+    
+    Batch dimensions are preserved by this operation.
+    If channel dimensions are present, this method returns the indices where any component is nonzero.
+
+    Implementations:
+
+    * NumPy: [`numpy.argwhere`](https://numpy.org/doc/stable/reference/generated/numpy.argwhere.html)
+    * PyTorch: [`torch.nonzero`](https://pytorch.org/docs/stable/generated/torch.nonzero.html)
+    * TensorFlow: [`tf.where(tf.not_equal(values, 0))`](https://www.tensorflow.org/api_docs/python/tf/where)
+    * Jax: [`jax.numpy.nonzero`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.nonzero.html)
+
+    Args:
+        value: spatial tensor to find non-zero / True values in.
+        list_dim: Dimension listing non-zero values.
+        index_dim: Index dimension.
+
+    Returns:
+        `Tensor` of shape (batch dims..., `list_dim`=#non-zero, `index_dim`=value.shape.spatial_rank)
+
+    """
+    if value.shape.channel_rank > 0:
+        value = sum_(abs(value), value.shape.channel)
+    if isinstance(list_dim, str):
+        list_dim = instance(list_dim)
+    def unbatched_nonzero(value: Tensor):
+        if isinstance(value, CompressedSparseMatrix):
+            value = value.decompress()
+        if isinstance(value, SparseCoordinateTensor):
+            nonzero_values = nonzero(value._values)
+            nonzero_indices = value._indices[nonzero_values]
+            return nonzero_indices
+        else:
+            dims = value.shape.non_channel
+            native = reshaped_native(value, [*dims])
+            backend = choose_backend(native)
+            indices = backend.nonzero(native)
+            indices_shape = Shape(backend.staticshape(indices), (list_dim.name, index_dim.name), (list_dim.type, index_dim.type), (None, dims.names))
+            return NativeTensor(indices, indices_shape)
+    return broadcast_op(unbatched_nonzero, [value], iter_dims=value.shape.batch.names)
+
+
+def reduce_(f, value, dims, require_all_dims_present=False, required_kind: type = None):
+    if not dims:
+        return value
+    else:
+        if isinstance(value, (tuple, list)):
+            values = [wrap(v) for v in value]
+            value = stack_tensors(values, instance('0'))
+            dims = value.shape.only(dims)
+            assert '0' in dims, "When passing a sequence of tensors to be reduced, the sequence dimension '0' must be reduced."
+        elif isinstance(value, Layout):
+            if not value.shape.without(dims):  # reduce all
+                dims = batch('_flat_layout')
+                values = value._as_list()
+                if required_kind is not None:
+                    values = [required_kind(v) for v in values]
+                value = wrap(values, dims)
+        else:
+            value = wrap(value)
+        dims = value.shape.only(dims)
+        if require_all_dims_present and any(d not in value.shape for d in dims):
+            raise ValueError(f"Cannot sum dimensions {dims} because tensor {value.shape} is missing at least one of them")
+        return f(value._simplify(), dims)
+
+
+def sum_(value: Union[Tensor, list, tuple, Number, bool], dim: DimFilter = non_batch) -> Tensor:
+    """
+    Sums `values` along the specified dimensions.
+
+    Args:
+        value: `Tensor` or `list` / `tuple` of Tensors.
+        dim: Dimension or dimensions to be reduced. One of
+
+            * `None` to reduce all non-batch dimensions
+            * `str` containing single dimension or comma-separated list of dimensions
+            * `Tuple[str]` or `List[str]`
+            * `Shape`
+            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
+            * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors
+
+    Returns:
+        `Tensor` without the reduced dimensions.
+    """
+    return reduce_(_sum, bool_to_int(value), dim, require_all_dims_present=True)
+
+
+def _sum(value: Tensor, dims: Shape) -> Tensor:
+    if not dims:
+        return value
+    if isinstance(value, NativeTensor):
+        result = value.default_backend.sum(value._native, value._native_shape.indices(dims)) * value.collapsed_dims.only(dims).volume
+        return NativeTensor(result, value._native_shape.without(dims), value.shape.without(dims))
+    elif isinstance(value, TensorStack):
+        reduced_inners = [_sum(t, dims.without(value._stack_dim)) for t in value._tensors]
+        return functools.reduce(lambda x, y: x + y, reduced_inners) if value._stack_dim in dims else TensorStack(reduced_inners, value._stack_dim)
+    elif isinstance(value, (CompressedSparseMatrix, SparseCoordinateTensor)):
+        if value.sparse_dims in dims:  # reduce all sparse dims
+            return _sum(value._values, dims.without(value.sparse_dims) & instance(value._values))
+        value_only_dims = dims.only(value._values.shape).without(value.sparsity_batch)
+        if value_only_dims:
+            value = value._with_values(_sum(value._values, value_only_dims))
+        dims = dims.without(value_only_dims)
+        if not dims:
+            return value
+        if isinstance(value, CompressedSparseMatrix):
+            if value._compressed_dims in dims and value._uncompressed_dims.isdisjoint(dims):  # We can ignore the pointers
+                result_base = zeros(value.shape.without(value._compressed_dims))
+                return scatter(result_base, value._indices, value._values, mode='add', outside_handling='undefined')
+            elif value.sparse_dims.only(dims):  # reduce some sparse dims
+                return dot(value, dims, ones(dims), dims)  # this is what SciPy does in both axes, actually.
+            return value
+            # first sum value dims that are not part of indices
+        else:
+            assert isinstance(value, SparseCoordinateTensor)
+            if value._dense_shape in dims:  # sum all sparse dims
+                v_dims = dims.without(value._dense_shape) & instance(value._values)
+                return _sum(value._values, v_dims)
+            else:
+                result_base = zeros(value.shape.without(dims))
+                remaining_sparse_dims = value._dense_shape.without(dims)
+                indices = value._indices.vector[remaining_sparse_dims.names]
+                if remaining_sparse_dims.rank == 1:  # return dense result
+                    result = scatter(result_base, indices, value._values, mode='add', outside_handling='undefined')
+                    return result
+                else:  # return sparse result
+                    raise NotImplementedError
+    else:
+        raise ValueError(type(value))
+
+
+def prod(value: Union[Tensor, list, tuple, Number, bool], dim: DimFilter = non_batch) -> Tensor:
+    """
+    Multiplies `values` along the specified dimensions.
+
+    Args:
+        value: `Tensor` or `list` / `tuple` of Tensors.
+        dim: Dimension or dimensions to be reduced. One of
+
+            * `None` to reduce all non-batch dimensions
+            * `str` containing single dimension or comma-separated list of dimensions
+            * `Tuple[str]` or `List[str]`
+            * `Shape`
+            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
+            * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors
+
+    Returns:
+        `Tensor` without the reduced dimensions.
+    """
+    return reduce_(_prod, value, dim, require_all_dims_present=True)
+
+
+def _prod(value: Tensor, dims: Shape) -> Tensor:
+    if isinstance(value, NativeTensor):
+        result = value.default_backend.prod(value._native, value._native_shape.indices(dims)) ** value.collapsed_dims.only(dims).volume
+        return NativeTensor(result, value._native_shape.without(dims), value.shape.without(dims))
+    elif isinstance(value, TensorStack):
+        reduced_inners = [_prod(t, dims.without(value._stack_dim)) for t in value._tensors]
+        return functools.reduce(lambda x, y: x * y, reduced_inners) if value._stack_dim in dims else TensorStack(reduced_inners, value._stack_dim)
+    else:
+        raise ValueError(type(value))
+
+
+def mean(value: Union[Tensor, list, tuple, Number, bool], dim: DimFilter = non_batch) -> Tensor:
+    """
+    Computes the mean over `values` along the specified dimensions.
+
+    Args:
+        value: `Tensor` or `list` / `tuple` of Tensors.
+        dim: Dimension or dimensions to be reduced. One of
+
+            * `None` to reduce all non-batch dimensions
+            * `str` containing single dimension or comma-separated list of dimensions
+            * `Tuple[str]` or `List[str]`
+            * `Shape`
+            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
+            * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors
+
+    Returns:
+        `Tensor` without the reduced dimensions.
+    """
+    return reduce_(_mean, value, dim)
+
+
+def _mean(value: Tensor, dims: Shape) -> Tensor:
+    if not dims:
+        return value
+    if isinstance(value, NativeTensor):
+        result = value.default_backend.mean(value._native, value._native_shape.indices(dims))
+        return NativeTensor(result, value._native_shape.without(dims), value.shape.without(dims))
+    elif isinstance(value, TensorStack):
+        reduced_inners = [_mean(t, dims.without(value._stack_dim)) for t in value._tensors]
+        return functools.reduce(lambda x, y: x + y, reduced_inners) / len(reduced_inners) if value._stack_dim in dims else TensorStack(reduced_inners, value._stack_dim)
+    else:
+        raise ValueError(type(value))
+
+
+def std(value: Union[Tensor, list, tuple, Number, bool], dim: DimFilter = non_batch) -> Tensor:
+    """
+    Computes the standard deviation over `values` along the specified dimensions.
+
+    *Warning*: The standard deviation of non-uniform tensors along the stack dimension is undefined.
+
+    Args:
+        value: `Tensor` or `list` / `tuple` of Tensors.
+        dim: Dimension or dimensions to be reduced. One of
+
+            * `None` to reduce all non-batch dimensions
+            * `str` containing single dimension or comma-separated list of dimensions
+            * `Tuple[str]` or `List[str]`
+            * `Shape`
+            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
+            * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors
+
+    Returns:
+        `Tensor` without the reduced dimensions.
+    """
+    if not dim:
+        warnings.warn("std along empty shape returns 0", RuntimeWarning, stacklevel=2)
+        return zeros_like(value)
+    if not callable(dim) and set(parse_dim_order(dim)) - set(value.shape.names):
+        return zeros_like(value)  # std along constant dim is 0
+    return reduce_(_std, value, dim)
+
+
+def _std(value: Tensor, dims: Shape) -> Tensor:
+    if value.shape.is_uniform:
+        result = value.default_backend.std(value.native(value.shape), value.shape.indices(dims))
+        return NativeTensor(result, value.shape.without(dims))
+    else:
+        non_uniform_dim = value.shape.shape.without('dims')
+        assert non_uniform_dim.only(dims).is_empty, f"Cannot compute std along non-uniform dims {dims}. shape={value.shape}"
+        return stack([_std(t, dims) for t in value.unstack(non_uniform_dim.name)], non_uniform_dim)
+
+
+def any_(boolean_tensor: Union[Tensor, list, tuple], dim: DimFilter = non_batch) -> Tensor:
+    """
+    Tests whether any entry of `boolean_tensor` is `True` along the specified dimensions.
+
+    Args:
+        boolean_tensor: `Tensor` or `list` / `tuple` of Tensors.
+        dim: Dimension or dimensions to be reduced. One of
+
+            * `None` to reduce all non-batch dimensions
+            * `str` containing single dimension or comma-separated list of dimensions
+            * `Tuple[str]` or `List[str]`
+            * `Shape`
+            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
+            * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors
+
+    Returns:
+        `Tensor` without the reduced dimensions.
+    """
+    return reduce_(_any, boolean_tensor, dim)
+
+
+def _any(value: Tensor, dims: Shape) -> Tensor:
+    if isinstance(value, NativeTensor):
+        result = value.default_backend.any(value._native, value._native_shape.indices(dims))
+        return NativeTensor(result, value._native_shape.without(dims), value.shape.without(dims))
+    elif isinstance(value, TensorStack):
+        reduced_inners = [_any(t, dims.without(value._stack_dim)) for t in value._tensors]
+        return functools.reduce(lambda x, y: x | y, reduced_inners) if value._stack_dim in dims else TensorStack(reduced_inners, value._stack_dim)
+    else:
+        raise ValueError(type(value))
+
+
+def all_(boolean_tensor: Union[Tensor, list, tuple, Number, bool], dim: DimFilter = non_batch) -> Tensor:
+    """
+    Tests whether all entries of `boolean_tensor` are `True` along the specified dimensions.
+
+    Args:
+        boolean_tensor: `Tensor` or `list` / `tuple` of Tensors.
+        dim: Dimension or dimensions to be reduced. One of
+
+            * `None` to reduce all non-batch dimensions
+            * `str` containing single dimension or comma-separated list of dimensions
+            * `Tuple[str]` or `List[str]`
+            * `Shape`
+            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
+            * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors
+
+    Returns:
+        `Tensor` without the reduced dimensions.
+    """
+    return reduce_(_all, boolean_tensor, dim)
+
+
+def _all(value: Tensor, dims: Shape) -> Tensor:
+    if isinstance(value, NativeTensor):
+        result = value.default_backend.all(value.native(value.shape), value.shape.indices(dims))
+        return NativeTensor(result, value.shape.without(dims))
+    elif isinstance(value, TensorStack):
+        reduced_inners = [_all(t, dims.without(value._stack_dim)) for t in value._tensors]
+        return functools.reduce(lambda x, y: x & y, reduced_inners) if value._stack_dim in dims else TensorStack(reduced_inners, value._stack_dim)
+    elif isinstance(value, (SparseCoordinateTensor, CompressedSparseMatrix)):
+        if sparse_dims(value) in dims:
+            values_all = _all(value._values, dims.without(sparse_dims(value)) & instance(value._values))
+            return all_([values_all, value._default], '0') if value._default is not None else values_all
+    raise ValueError(type(value))
+
+
+def max_(value: Union[Tensor, list, tuple, Number, bool], dim: DimFilter = non_batch) -> Tensor:
+    """
+    Determines the maximum value of `values` along the specified dimensions.
+
+    Args:
+        value: `Tensor` or `list` / `tuple` of Tensors.
+        dim: Dimension or dimensions to be reduced. One of
+
+            * `None` to reduce all non-batch dimensions
+            * `str` containing single dimension or comma-separated list of dimensions
+            * `Tuple[str]` or `List[str]`
+            * `Shape`
+            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
+            * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors
+
+    Returns:
+        `Tensor` without the reduced dimensions.
+    """
+    return reduce_(_max, value, dim)
+
+
+def _max(value: Tensor, dims: Shape) -> Tensor:
+    if isinstance(value, NativeTensor):
+        result = value.default_backend.max(value.native(value.shape), value.shape.indices(dims))
+        return NativeTensor(result, value.shape.without(dims))
+    elif isinstance(value, TensorStack):
+        reduced_inners = [_max(t, dims.without(value._stack_dim)) for t in value._tensors]
+        return functools.reduce(lambda x, y: maximum(x, y), reduced_inners) if value._stack_dim in dims else TensorStack(reduced_inners, value._stack_dim)
+    elif isinstance(value, (SparseCoordinateTensor, CompressedSparseMatrix)):
+        if sparse_dims(value) in dims:
+            values_max = _max(value._values, dims.without(sparse_dims(value)) & instance(value._values))
+            return maximum(values_max, value._default) if value._default is not None else values_max
+    raise ValueError(type(value))
+
+
+def min_(value: Union[Tensor, list, tuple, Number, bool], dim: DimFilter = non_batch) -> Tensor:
+    """
+    Determines the minimum value of `values` along the specified dimensions.
+
+    Args:
+        value: `Tensor` or `list` / `tuple` of Tensors.
+        dim: Dimension or dimensions to be reduced. One of
+
+            * `None` to reduce all non-batch dimensions
+            * `str` containing single dimension or comma-separated list of dimensions
+            * `Tuple[str]` or `List[str]`
+            * `Shape`
+            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
+            * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors
+
+    Returns:
+        `Tensor` without the reduced dimensions.
+    """
+    return reduce_(_min, value, dim)
+
+
+def _min(value: Tensor, dims: Shape) -> Tensor:
+    if isinstance(value, NativeTensor):
+        result = value.default_backend.min(value.native(value.shape), value.shape.indices(dims))
+        return NativeTensor(result, value.shape.without(dims))
+    elif isinstance(value, TensorStack):
+        reduced_inners = [_min(t, dims.without(value._stack_dim)) for t in value._tensors]
+        return functools.reduce(lambda x, y: minimum(x, y), reduced_inners) if value._stack_dim in dims else TensorStack(reduced_inners, value._stack_dim)
+    elif isinstance(value, (SparseCoordinateTensor, CompressedSparseMatrix)):
+        if sparse_dims(value) in dims:
+            values_min = _min(value._values, dims.without(sparse_dims(value)) & instance(value._values))
+            return minimum(values_min, value._default) if value._default is not None else values_min
+    raise ValueError(type(value))
+
+
+def finite_min(value, dim: DimFilter = non_batch, default: Union[complex, float] = float('NaN')):
+    """
+    Finds the minimum along `dim` ignoring all non-finite values.
+
+    Args:
+        value: `Tensor` or `list` / `tuple` of Tensors.
+        dim: Dimension or dimensions to be reduced. One of
+
+            * `None` to reduce all non-batch dimensions
+            * `str` containing single dimension or comma-separated list of dimensions
+            * `Tuple[str]` or `List[str]`
+            * `Shape`
+            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
+            * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors
+
+        default: Value to use where no finite value was encountered.
+
+    Returns:
+        `Tensor` without the reduced dimensions.
+    """
+    value_inf = where(is_finite(value), value, float('inf'))
+    result_inf = min_(value_inf, dim)
+    return where(is_finite(result_inf), result_inf, default)
+
+
+def finite_max(value, dim: DimFilter = non_batch, default: Union[complex, float] = float('NaN')):
+    """
+    Finds the maximum along `dim` ignoring all non-finite values.
+
+    Args:
+        value: `Tensor` or `list` / `tuple` of Tensors.
+        dim: Dimension or dimensions to be reduced. One of
+
+            * `None` to reduce all non-batch dimensions
+            * `str` containing single dimension or comma-separated list of dimensions
+            * `Tuple[str]` or `List[str]`
+            * `Shape`
+            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
+            * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors
+
+        default: Value to use where no finite value was encountered.
+
+    Returns:
+        `Tensor` without the reduced dimensions.
+    """
+    value_inf = where(is_finite(value), value, float('-inf'))
+    result_inf = max_(value_inf, dim)
+    return where(is_finite(result_inf), result_inf, default)
+
+
+def finite_sum(value, dim: DimFilter = non_batch, default: Union[complex, float] = float('NaN')):
+    """
+    Sums all finite values in `value` along `dim`.
+
+    Args:
+        value: `Tensor` or `list` / `tuple` of Tensors.
+        dim: Dimension or dimensions to be reduced. One of
+
+            * `None` to reduce all non-batch dimensions
+            * `str` containing single dimension or comma-separated list of dimensions
+            * `Tuple[str]` or `List[str]`
+            * `Shape`
+            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
+            * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors
+
+        default: Value to use where no finite value was encountered.
+
+    Returns:
+        `Tensor` without the reduced dimensions.
+    """
+    finite = is_finite(value)
+    summed = sum_(where(finite, value, 0), dim)
+    return where(any_(finite, dim), summed, default)
+
+
+def finite_mean(value, dim: DimFilter = non_batch, default: Union[complex, float] = float('NaN')):
+    """
+    Computes the mean value of all finite values in `value` along `dim`.
+
+    Args:
+        value: `Tensor` or `list` / `tuple` of Tensors.
+        dim: Dimension or dimensions to be reduced. One of
+
+            * `None` to reduce all non-batch dimensions
+            * `str` containing single dimension or comma-separated list of dimensions
+            * `Tuple[str]` or `List[str]`
+            * `Shape`
+            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
+            * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors
+
+        default: Value to use where no finite value was encountered.
+
+    Returns:
+        `Tensor` without the reduced dimensions.
+    """
+    finite = is_finite(value)
+    summed = sum_(where(finite, value, 0), dim)
+    count = sum_(finite, dim)
+    mean_nan = summed / count
+    return where(is_finite(mean_nan), mean_nan, default)
+
+
+def quantile(value: Tensor,
+             quantiles: Union[float, tuple, list, Tensor],
+             dim: DimFilter = non_batch):
+    """
+    Compute the q-th quantile of `value` along `dim` for each q in `quantiles`.
+
+    Implementations:
+
+    * NumPy: [`quantile`](https://numpy.org/doc/stable/reference/generated/numpy.quantile.html)
+    * PyTorch: [`quantile`](https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile)
+    * TensorFlow: [`tfp.stats.percentile`](https://www.tensorflow.org/probability/api_docs/python/tfp/stats/percentile)
+    * Jax: [`quantile`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.quantile.html)
+
+    Args:
+        value: `Tensor`
+        quantiles: Single quantile or tensor of quantiles to compute.
+            Must be of type `float`, `tuple`, `list` or `Tensor`.
+        dim: Dimension or dimensions to be reduced. One of
+
+            * `None` to reduce all non-batch dimensions
+            * `str` containing single dimension or comma-separated list of dimensions
+            * `Tuple[str]` or `List[str]`
+            * `Shape`
+            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
+            * `'0'` when `isinstance(value, (tuple, list))` to reduce the sequence of Tensors
+
+    Returns:
+        `Tensor` with dimensions of `quantiles` and non-reduced dimensions of `value`.
+    """
+    dims = value.shape.only(dim)
+    native_values = reshaped_native(value, [*value.shape.without(dims), value.shape.only(dims)])
+    backend = choose_backend(native_values)
+    q = tensor(quantiles, default_list_dim=instance('quantiles'))
+    native_quantiles = reshaped_native(q, [q.shape])
+    native_result = backend.quantile(native_values, native_quantiles)
+    return reshaped_tensor(native_result, [q.shape, *value.shape.without(dims)])
+
+
+def median(value, dim: DimFilter = non_batch):
+    """
+    Reduces `dim` of `value` by picking the median value.
+    For odd dimension sizes (ambigous choice), the linear average of the two median values is computed.
+
+    Currently implemented via `quantile()`.
+
+    Args:
+        value: `Tensor`
+        dim: Dimension or dimensions to be reduced. One of
+
+            * `None` to reduce all non-batch dimensions
+            * `str` containing single dimension or comma-separated list of dimensions
+            * `Tuple[str]` or `List[str]`
+            * `Shape`
+            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
+            * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors
+
+    Returns:
+        `Tensor`
+    """
+    return quantile(value, 0.5, dim)
+
+
+def dot(x: Tensor,
+        x_dims: DimFilter,
+        y: Tensor,
+        y_dims: DimFilter) -> Tensor:
+    """
+    Computes the dot product along the specified dimensions.
+    Contracts `x_dims` with `y_dims` by first multiplying the elements and then summing them up.
+
+    For one dimension, this is equal to matrix-matrix or matrix-vector multiplication.
+
+    The function replaces the traditional `dot` / `tensordot` / `matmul` / `einsum` functions.
+
+    * NumPy: [`numpy.tensordot`](https://numpy.org/doc/stable/reference/generated/numpy.tensordot.html), [`numpy.einsum`](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html)
+    * PyTorch: [`torch.tensordot`](https://pytorch.org/docs/stable/generated/torch.tensordot.html#torch.tensordot), [`torch.einsum`](https://pytorch.org/docs/stable/generated/torch.einsum.html)
+    * TensorFlow: [`tf.tensordot`](https://www.tensorflow.org/api_docs/python/tf/tensordot), [`tf.einsum`](https://www.tensorflow.org/api_docs/python/tf/einsum)
+    * Jax: [`jax.numpy.tensordot`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.tensordot.html), [`jax.numpy.einsum`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.einsum.html)
+
+    Args:
+        x: First `Tensor`
+        x_dims: Dimensions of `x` to reduce against `y`
+        y: Second `Tensor`
+        y_dims: Dimensions of `y` to reduce against `x`.
+
+    Returns:
+        Dot product as `Tensor`.
+    """
+    x_dims = x.shape.only(x_dims)
+    y_dims = y.shape.only(y_dims)
+    if not x_dims:
+        assert y_dims.volume == 1, f"Cannot compute dot product between dimensions {x_dims} on {x.shape} and {y_dims} on {y.shape}"
+        y = y[{d: 0 for d in y_dims.names}]
+        return x * y
+    if not y_dims:
+        assert x_dims.volume == 1, f"Cannot compute dot product between dimensions {x_dims} on {x.shape} and {y_dims} on {y.shape}"
+        x = x[{d: 0 for d in x_dims.names}]
+        return x * y
+    if isinstance(x, CompressedSparseMatrix):
+        if isinstance(y, (CompressedSparseMatrix, SparseCoordinateTensor)):
+            if x_dims.isdisjoint(sparse_dims(x)) and y_dims.isdisjoint(sparse_dims(y)):
+                return x._op2(y, lambda vx, vy: dot(vx, x_dims, vy, y_dims), None, 'dot', '@')
+            if x_dims.only(sparse_dims(x)) and y_dims.only(sparse_dims(y)):
+                raise NotImplementedError("sparse-sparse multiplication not yet supported")
+            raise NotImplementedError
+        return dot_compressed_dense(x, x_dims, y, y_dims)
+    elif isinstance(y, CompressedSparseMatrix):
+        if isinstance(x, (CompressedSparseMatrix, SparseCoordinateTensor)):
+            raise NotImplementedError("sparse-sparse multiplication not yet supported")
+        return dot_compressed_dense(y, y_dims, x, x_dims)
+    if isinstance(x, SparseCoordinateTensor):
+        if isinstance(y, (CompressedSparseMatrix, SparseCoordinateTensor)):
+            if x_dims.isdisjoint(sparse_dims(x)) and y_dims.isdisjoint(sparse_dims(y)):
+                return x._op2(y, lambda vx, vy: dot(vx, x_dims, vy, y_dims), None, 'dot', '@')
+            raise NotImplementedError("sparse-sparse multiplication not yet supported")
+        return dot_coordinate_dense(x, x_dims, y, y_dims)
+    elif isinstance(y, SparseCoordinateTensor):
+        if isinstance(x, (CompressedSparseMatrix, SparseCoordinateTensor)):
+            raise NotImplementedError("sparse-sparse multiplication not yet supported")
+        return dot_coordinate_dense(y, y_dims, x, x_dims)
+    x_native = x.native(x.shape)
+    y_native = y.native(y.shape)
+    backend = choose_backend(x_native, y_native)
+    remaining_shape_x = x.shape.without(x_dims)
+    remaining_shape_y = y.shape.without(y_dims)
+    assert x_dims.volume == y_dims.volume, f"Failed to reduce {x_dims} against {y_dims} in dot product of {x.shape} and {y.shape}. Sizes do not match."
+    if remaining_shape_y.isdisjoint(remaining_shape_x):  # no shared batch dimensions -> tensordot
+        result_native = backend.tensordot(x_native, x.shape.indices(x_dims), y_native, y.shape.indices(y_dims))
+        result_shape = concat_shapes(remaining_shape_x, remaining_shape_y)
+    else:  # shared batch dimensions -> einsum
+        result_shape = merge_shapes(x.shape.without(x_dims), y.shape.without(y_dims))
+        REDUCE_LETTERS = list('ijklmn')
+        KEEP_LETTERS = list('abcdefgh')
+        x_letters = [(REDUCE_LETTERS if dim in x_dims else KEEP_LETTERS).pop(0) for dim in x.shape.names]
+        letter_map = {dim: letter for dim, letter in zip(x.shape.names, x_letters)}
+        REDUCE_LETTERS = list('ijklmn')
+        y_letters = []
+        for dim in y.shape.names:
+            if dim in y_dims:
+                y_letters.append(REDUCE_LETTERS.pop(0))
+            else:
+                if dim in x.shape and dim not in x_dims:
+                    y_letters.append(letter_map[dim])
+                else:
+                    next_letter = KEEP_LETTERS.pop(0)
+                    letter_map[dim] = next_letter
+                    y_letters.append(next_letter)
+        keep_letters = [letter_map[dim] for dim in result_shape.names]
+        subscripts = f'{"".join(x_letters)},{"".join(y_letters)}->{"".join(keep_letters)}'
+        result_native = backend.einsum(subscripts, x_native, y_native)
+    return NativeTensor(result_native, result_shape)
+
+
+def _backend_op1(x, unbound_method) -> Union[Tensor, PhiTreeNode]:
+    if isinstance(x, Tensor):
+        def apply_op(native_tensor):
+            backend = choose_backend(native_tensor)
+            return getattr(backend, unbound_method.__name__)(backend.auto_cast(native_tensor)[0])
+        apply_op.__name__ = unbound_method.__name__
+        return x._op1(apply_op)
+    elif isinstance(x, PhiTreeNode):
+        return copy_with(x, **{a: _backend_op1(getattr(x, a), unbound_method) for a in value_attributes(x)})
+    else:
+        backend = choose_backend(x)
+        y = getattr(backend, unbound_method.__name__)(backend.auto_cast(x)[0])
+        return y
+
+
+def abs_(x) -> Union[Tensor, PhiTreeNode]:
+    """
+    Computes *||x||<sub>1</sub>*.
+    Complex `x` result in matching precision float values.
+
+    *Note*: The gradient of this operation is undefined for *x=0*.
+    TensorFlow and PyTorch return 0 while Jax returns 1.
+
+    Args:
+        x: `Tensor` or `phi.math.magic.PhiTreeNode`
+
+    Returns:
+        Absolute value of `x` of same type as `x`.
+    """
+    return _backend_op1(x, Backend.abs)
+
+
+def sign(x) -> Union[Tensor, PhiTreeNode]:
+    """
+    The sign of positive numbers is 1 and -1 for negative numbers.
+    The sign of 0 is undefined.
+
+    Args:
+        x: `Tensor` or `phi.math.magic.PhiTreeNode`
+
+    Returns:
+        `Tensor` or `phi.math.magic.PhiTreeNode` matching `x`.
+    """
+    return _backend_op1(x, Backend.sign)
+
+
+def round_(x) -> Union[Tensor, PhiTreeNode]:
+    """ Rounds the `Tensor` or `phi.math.magic.PhiTreeNode` `x` to the closest integer. """
+    return _backend_op1(x, Backend.round)
+
+
+def ceil(x) -> Union[Tensor, PhiTreeNode]:
+    """ Computes *⌈x⌉* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
+    return _backend_op1(x, Backend.ceil)
+
+
+def floor(x) -> Union[Tensor, PhiTreeNode]:
+    """ Computes *⌊x⌋* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
+    return _backend_op1(x, Backend.floor)
+
+
+def sqrt(x) -> Union[Tensor, PhiTreeNode]:
+    """ Computes *sqrt(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
+    return _backend_op1(x, Backend.sqrt)
+
+
+def exp(x) -> Union[Tensor, PhiTreeNode]:
+    """ Computes *exp(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
+    return _backend_op1(x, Backend.exp)
+
+
+def soft_plus(x) -> Union[Tensor, PhiTreeNode]:
+    """ Computes *softplus(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
+    return _backend_op1(x, Backend.softplus)
+
+
+def factorial(x) -> Union[Tensor, PhiTreeNode]:
+    """
+    Computes *factorial(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`.
+    For floating-point numbers computes the continuous factorial using the gamma function.
+    For integer numbers computes the exact factorial and returns the same integer type.
+    However, this results in integer overflow for inputs larger than 12 (int32) or 19 (int64).
+    """
+    return _backend_op1(x, Backend.factorial)
+
+
+def log_gamma(x) -> Union[Tensor, PhiTreeNode]:
+    """ Computes *log(gamma(x))* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
+    return _backend_op1(x, Backend.log_gamma)
+
+
+def to_float(x) -> Union[Tensor, PhiTreeNode]:
+    """
+    Converts the given tensor to floating point format with the currently specified precision.
+    
+    The precision can be set globally using `math.set_global_precision()` and locally using `with math.precision()`.
+    
+    See the `phi.math` module documentation at https://tum-pbs.github.io/PhiFlow/Math.html
+
+    See Also:
+        `cast()`.
+
+    Args:
+        x: `Tensor` or `phi.math.magic.PhiTreeNode` to convert
+
+    Returns:
+        `Tensor` or `phi.math.magic.PhiTreeNode` matching `x`.
+    """
+    return _backend_op1(x, Backend.to_float)
+
+
+def to_int32(x) -> Union[Tensor, PhiTreeNode]:
+    """ Converts the `Tensor` or `phi.math.magic.PhiTreeNode` `x` to 32-bit integer. """
+    return _backend_op1(x, Backend.to_int32)
+
+
+def to_int64(x) -> Union[Tensor, PhiTreeNode]:
+    """ Converts the `Tensor` or `phi.math.magic.PhiTreeNode` `x` to 64-bit integer. """
+    return _backend_op1(x, Backend.to_int64)
+
+
+def to_complex(x) -> Union[Tensor, PhiTreeNode]:
+    """
+    Converts the given tensor to complex floating point format with the currently specified precision.
+
+    The precision can be set globally using `math.set_global_precision()` and locally using `with math.precision()`.
+
+    See the `phi.math` module documentation at https://tum-pbs.github.io/PhiFlow/Math.html
+
+    See Also:
+        `cast()`.
+
+    Args:
+        x: values to convert
+
+    Returns:
+        `Tensor` of same shape as `x`
+    """
+    return _backend_op1(x, Backend.to_complex)
+
+
+def is_finite(x) -> Union[Tensor, PhiTreeNode]:
+    """ Returns a `Tensor` or `phi.math.magic.PhiTreeNode` matching `x` with values `True` where `x` has a finite value and `False` otherwise. """
+    return _backend_op1(x, Backend.isfinite)
+
+
+def is_nan(x) -> Union[Tensor, PhiTreeNode]:
+    """ Returns a `Tensor` or `phi.math.magic.PhiTreeNode` matching `x` with values `True` where `x` is `NaN` and `False` otherwise. """
+    return _backend_op1(x, Backend.isnan)
+
+
+def is_inf(x) -> Union[Tensor, PhiTreeNode]:
+    """ Returns a `Tensor` or `phi.math.magic.PhiTreeNode` matching `x` with values `True` where `x` is `+inf` or `-inf` and `False` otherwise. """
+    return _backend_op1(x, Backend.isnan)
+
+
+def real(x) -> Union[Tensor, PhiTreeNode]:
+    """
+    See Also:
+        `imag()`, `conjugate()`.
+
+    Args:
+        x: `Tensor` or `phi.math.magic.PhiTreeNode` or native tensor.
+
+    Returns:
+        Real component of `x`.
+    """
+    return _backend_op1(x, Backend.real)
+
+
+def imag(x) -> Union[Tensor, PhiTreeNode]:
+    """
+    Returns the imaginary part of `x`.
+    If `x` does not store complex numbers, returns a zero tensor with the same shape and dtype as this tensor.
+
+    See Also:
+        `real()`, `conjugate()`.
+
+    Args:
+        x: `Tensor` or `phi.math.magic.PhiTreeNode` or native tensor.
+
+    Returns:
+        Imaginary component of `x` if `x` is complex, zeros otherwise.
+    """
+    return _backend_op1(x, Backend.imag)
+
+
+def conjugate(x) -> Union[Tensor, PhiTreeNode]:
+    """
+    See Also:
+        `imag()`, `real()`.
+
+    Args:
+        x: Real or complex `Tensor` or `phi.math.magic.PhiTreeNode` or native tensor.
+
+    Returns:
+        Complex conjugate of `x` if `x` is complex, else `x`.
+    """
+    return _backend_op1(x, Backend.conj)
+
+
+def degrees(deg):
+    """ Convert degrees to radians. """
+    return deg * (3.1415 / 180.)
+
+
+def sin(x) -> Union[Tensor, PhiTreeNode]:
+    """ Computes *sin(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
+    return _backend_op1(x, Backend.sin)
+
+
+def arcsin(x) -> Union[Tensor, PhiTreeNode]:
+    """ Computes the inverse of *sin(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`.
+    For real arguments, the result lies in the range [-π/2, π/2].
+    """
+    return _backend_op1(x, Backend.arcsin)
+
+
+def cos(x) -> Union[Tensor, PhiTreeNode]:
+    """ Computes *cos(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
+    return _backend_op1(x, Backend.cos)
+
+
+def arccos(x) -> Union[Tensor, PhiTreeNode]:
+    """ Computes the inverse of *cos(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`.
+    For real arguments, the result lies in the range [0, π].
+    """
+    return _backend_op1(x, Backend.cos)
+
+
+def tan(x) -> Union[Tensor, PhiTreeNode]:
+    """ Computes *tan(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
+    return _backend_op1(x, Backend.tan)
+
+
+def arctan(x, divide_by=None) -> Union[Tensor, PhiTreeNode]:
+    """
+    Computes the inverse of *tan(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`.
+
+    Args:
+        x: Input. The single-argument `arctan` function cannot output π/2 or -π/2 since tan(π/2) is infinite.
+        divide_by: If specified, computes `arctan(x/divide_by)` so that it can return π/2 and -π/2.
+            This is equivalent to the common `arctan2` function.
+    """
+    if divide_by is None:
+        return _backend_op1(x, Backend.arctan)
+    else:
+        divide_by = to_float(divide_by)
+        return custom_op2(x, divide_by, arctan, lambda a, b: choose_backend(a, b).arctan2(a, b), 'arctan')
+
+
+def sinh(x) -> Union[Tensor, PhiTreeNode]:
+    """ Computes *sinh(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
+    return _backend_op1(x, Backend.sinh)
+
+
+def arcsinh(x) -> Union[Tensor, PhiTreeNode]:
+    """ Computes the inverse of *sinh(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
+    return _backend_op1(x, Backend.arcsinh)
+
+
+def cosh(x) -> Union[Tensor, PhiTreeNode]:
+    """ Computes *cosh(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
+    return _backend_op1(x, Backend.cosh)
+
+
+def arccosh(x) -> Union[Tensor, PhiTreeNode]:
+    """ Computes the inverse of *cosh(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
+    return _backend_op1(x, Backend.arccosh)
+
+
+def tanh(x) -> Union[Tensor, PhiTreeNode]:
+    """ Computes *tanh(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
+    return _backend_op1(x, Backend.tanh)
+
+
+def arctanh(x) -> Union[Tensor, PhiTreeNode]:
+    """ Computes the inverse of *tanh(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
+    return _backend_op1(x, Backend.arctanh)
+
+
+def log(x) -> Union[Tensor, PhiTreeNode]:
+    """ Computes the natural logarithm of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
+    return _backend_op1(x, Backend.log)
+
+
+def log2(x) -> Union[Tensor, PhiTreeNode]:
+    """ Computes *log(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x` with base 2. """
+    return _backend_op1(x, Backend.log2)
+
+
+def log10(x) -> Union[Tensor, PhiTreeNode]:
+    """ Computes *log(x)* of the `Tensor` or `phi.math.magic.PhiTreeNode` `x` with base 10. """
+    return _backend_op1(x, Backend.log10)
+
+
+def sigmoid(x) -> Union[Tensor, PhiTreeNode]:
+    """ Computes the sigmoid function of the `Tensor` or `phi.math.magic.PhiTreeNode` `x`. """
+    return _backend_op1(x, Backend.sigmoid)
+
+
+def cast_same(*values: Tensor) -> Tuple[Tensor]:
+    """
+    Casts all tensors to the same `DType`.
+    If all data types are of the same kind, returns the largest occurring data type.
+    Otherwise casts `bool` &rarr; `int` &rarr; `float` &rarr; `complex`.
+
+    Args:
+        *values: tensors to cast
+
+    Returns:
+        Tuple of Tensors with same data type.
+    """
+    assert all(isinstance(v, Tensor) for v in values), f"Only Tensor arguments allowed but got {values}"
+    dtypes = [v.dtype for v in values]
+    if any(dt != dtypes[0] for dt in dtypes):
+        common_type = combine_types(*dtypes, fp_precision=get_precision())
+        return tuple([cast(v, common_type) for v in values])
+    else:
+        return values
+
+
+def safe_div(x: Union[float, Tensor], y: Union[float, Tensor]):
+    """ Computes *x/y* with the `Tensor`s `x` and `y` but returns 0 where *y=0*. """
+    return custom_op2(x, y,
+                      l_operator=safe_div,
+                      l_native_function=lambda x_, y_: choose_backend(x_, y_).divide_no_nan(x_, y_),
+                      r_operator=lambda y_, x_: safe_div(x_, y_),
+                      r_native_function=lambda y_, x_: choose_backend(x_, y_).divide_no_nan(x_, y_),
+                      op_name='divide_no_nan')
+
+
+def maximum(x: Union[Tensor, float], y: Union[Tensor, float]):
+    """ Computes the element-wise maximum of `x` and `y`. """
+    return custom_op2(x, y, maximum, lambda x_, y_: choose_backend(x_, y_).maximum(x_, y_), op_name='maximum')
+
+
+def minimum(x: Union[Tensor, float], y: Union[Tensor, float]):
+    """ Computes the element-wise minimum of `x` and `y`. """
+    return custom_op2(x, y, minimum, lambda x_, y_: choose_backend(x_, y_).minimum(x_, y_), op_name='minimum')
+
+
+def clip(x: Tensor, lower_limit: Union[float, Tensor], upper_limit: Union[float, Tensor]):
+    """ Limits the values of the `Tensor` `x` to lie between `lower_limit` and `upper_limit` (inclusive). """
+    if isinstance(lower_limit, Number) and isinstance(upper_limit, Number):
+
+        def clip_(x):
+            return x._op1(lambda native: choose_backend(native).clip(native, lower_limit, upper_limit))
+
+        return broadcast_op(clip_, [x])
+    else:
+        return maximum(lower_limit, minimum(x, upper_limit))
+
+
+def convolve(value: Tensor,
+             kernel: Tensor,
+             extrapolation: 'e_.Extrapolation' = None) -> Tensor:
+    """
+    Computes the convolution of `value` and `kernel` along the spatial axes of `kernel`.
+
+    The channel dimensions of `value` are reduced against the equally named dimensions of `kernel`.
+    The result will have the non-reduced channel dimensions of `kernel`.
+
+    Args:
+        value: `Tensor` whose shape includes all spatial dimensions of `kernel`.
+        kernel: `Tensor` used as convolutional filter.
+        extrapolation: If not None, pads `value` so that the result has the same shape as `value`.
+
+    Returns:
+        `Tensor`
+    """
+    assert all(dim in value.shape for dim in kernel.shape.spatial.names), f"Value must have all spatial dimensions of kernel but got value {value} kernel {kernel}"
+    conv_shape = kernel.shape.spatial
+    in_channels = value.shape.channel
+    out_channels = kernel.shape.channel.without(in_channels)
+    batch = value.shape.batch & kernel.shape.batch
+    if extrapolation is not None and extrapolation != e_.ZERO:
+        value = pad(value, {dim: (kernel.shape.get_size(dim) // 2, (kernel.shape.get_size(dim) - 1) // 2) for dim in conv_shape.names}, extrapolation)
+    native_kernel = reshaped_native(kernel, (batch, out_channels, in_channels, *conv_shape.names), force_expand=in_channels)
+    native_value = reshaped_native(value, (batch, in_channels, *conv_shape.names), force_expand=batch)
+    backend = choose_backend(native_value, native_kernel)
+    native_result = backend.conv(native_value, native_kernel, zero_padding=extrapolation == e_.ZERO)
+    result = reshaped_tensor(native_result, (batch, out_channels, *conv_shape))
+    return result
+
+
+def boolean_mask(x: Tensor, dim: DimFilter, mask: Tensor):
+    """
+    Discards values `x.dim[i]` where `mask.dim[i]=False`.
+    All dimensions of `mask` that are not `dim` are treated as batch dimensions.
+
+    Alternative syntax: `x.dim[mask]`.
+
+    Implementations:
+
+    * NumPy: Slicing
+    * PyTorch: [`masked_select`](https://pytorch.org/docs/stable/generated/torch.masked_select.html)
+    * TensorFlow: [`tf.boolean_mask`](https://www.tensorflow.org/api_docs/python/tf/boolean_mask)
+    * Jax: Slicing
+
+    Args:
+        x: `Tensor` of values.
+        dim: Dimension of `x` to along which to discard slices.
+        mask: Boolean `Tensor` marking which values to keep. Must have the dimension `dim` matching `x´.
+
+    Returns:
+        Selected values of `x` as `Tensor` with dimensions from `x` and `mask`.
+    """
+    dim, original_dim = mask.shape.only(dim), dim  # ToDo
+    assert dim, f"mask dimension '{original_dim}' must be present on the mask {mask.shape}"
+    assert dim.rank == 1, f"boolean mask only supports 1D selection"
+    
+    def uniform_boolean_mask(x: Tensor, mask_1d: Tensor):
+        if dim in x.shape:
+            x_native = x.native(x.shape.names)  # order does not matter
+            mask_native = mask_1d.native()  # only has 1 dim
+            backend = choose_backend(x_native, mask_native)
+            result_native = backend.boolean_mask(x_native, mask_native, axis=x.shape.index(dim))
+            new_shape = x.shape.with_sizes(backend.staticshape(result_native))
+            return NativeTensor(result_native, new_shape)
+        else:
+            total = int(sum_(to_int64(mask_1d), mask_1d.shape))
+            new_shape = mask_1d.shape.with_sizes([total])
+            return expand(x, new_shape)
+
+    return broadcast_op(uniform_boolean_mask, [x, mask], iter_dims=mask.shape.without(dim))
+
+
+def gather(values: Tensor, indices: Tensor, dims: Union[DimFilter, None] = None):
+    """
+    Gathers the entries of `values` at positions described by `indices`.
+    All non-channel dimensions of `indices` that are part of `values` but not indexed are treated as batch dimensions.
+
+    See Also:
+        `scatter()`.
+
+    Args:
+        values: `Tensor` containing values to gather.
+        indices: `int` `Tensor`. Multidimensional position references in `values`.
+            Must contain a single channel dimension for the index vector matching the number of dimensons to index.
+            This channel dimension should list the dimension names to index as item names unless explicitly specified as `dims`.
+        dims: (Optional) Dimensions indexed by `indices`.
+            Alternatively, the dimensions can be specified as the item names of the channel dimension of `indices`.
+            If `None` and no index item names are specified, will default to all spatial dimensions or all instance dimensions, depending on which ones are present (but not both).
+
+    Returns:
+        `Tensor` with combined batch dimensions, channel dimensions of `values` and spatial/instance dimensions of `indices`.
+    """
+    assert channel(indices).rank < 2, f"indices can at most have one channel dimension but got {indices.shape}"
+    if dims is None:
+        if channel(indices) and channel(indices).item_names[0]:
+            dims = channel(indices).item_names[0]
+        else:  # Fallback to spatial / instance
+            warnings.warn(f"Indexing without item names is not recommended. Got indices {indices.shape}", SyntaxWarning, stacklevel=2)
+            assert values.shape.instance.is_empty or values.shape.spatial.is_empty, f"Specify gather dimensions for values with both instance and spatial dimensions. Got {values.shape}"
+            dims = values.shape.instance if values.shape.spatial.is_empty else values.shape.spatial
+    if indices.dtype.kind == bool:
+        indices = to_int32(indices)
+    dims = parse_dim_order(dims)
+    assert dims in values.shape, f"Trying to index non-existant dimensions with indices {indices.shape} into values {values.shape}"
+    treat_as_batch = non_channel(indices).only(values.shape).without(dims)
+    batch_ = (values.shape.batch & indices.shape.batch).without(dims) & treat_as_batch
+    channel_ = values.shape.without(dims).without(batch_)
+    index_list_dims = indices.shape.non_channel.without(batch_)
+    squeeze_index_list = False
+    if not index_list_dims:
+        index_list_dims = instance('_single_index')
+        squeeze_index_list = True
+    native_values = reshaped_native(values, [batch_, *dims, channel_])
+    native_indices = reshaped_native(indices, [batch_, *index_list_dims, channel(indices)])
+    backend = choose_backend(native_values, native_indices)
+    native_result = backend.batched_gather_nd(native_values, native_indices)
+    result = reshaped_tensor(native_result, [batch_, *index_list_dims, channel_], convert=False)
+    if squeeze_index_list:
+        result = result[{'_single_index': 0}]
+    return result
+
+
+def scatter(base_grid: Union[Tensor, Shape],
+            indices: Union[Tensor, dict],
+            values: Union[Tensor, float],
+            mode: str = 'update',
+            outside_handling: str = 'discard',
+            indices_gradient=False):
+    """
+    Scatters `values` into `base_grid` at `indices`.
+    instance dimensions of `indices` and/or `values` are reduced during scattering.
+    Depending on `mode`, this method has one of the following effects:
+
+    * `mode='update'`: Replaces the values of `base_grid` at `indices` by `values`. The result is undefined if `indices` contains duplicates.
+    * `mode='add'`: Adds `values` to `base_grid` at `indices`. The values corresponding to duplicate indices are accumulated.
+    * `mode='mean'`: Replaces the values of `base_grid` at `indices` by the mean of all `values` with the same index.
+
+    Implementations:
+
+    * NumPy: Slice assignment / `numpy.add.at`
+    * PyTorch: [`torch.scatter`](https://pytorch.org/docs/stable/generated/torch.scatter.html), [`torch.scatter_add`](https://pytorch.org/docs/stable/generated/torch.scatter_add.html)
+    * TensorFlow: [`tf.tensor_scatter_nd_add`](https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_add), [`tf.tensor_scatter_nd_update`](https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update)
+    * Jax: [`jax.lax.scatter_add`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scatter_add.html), [`jax.lax.scatter`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scatter.html)
+
+    See Also:
+        `gather()`.
+
+    Args:
+        base_grid: `Tensor` into which `values` are scattered.
+        indices: `Tensor` of n-dimensional indices at which to place `values`.
+            Must have a single channel dimension with size matching the number of spatial dimensions of `base_grid`.
+            This dimension is optional if the spatial rank is 1.
+            Must also contain all `scatter_dims`.
+        values: `Tensor` of values to scatter at `indices`.
+        mode: Scatter mode as `str`. One of ('add', 'mean', 'update')
+        outside_handling: Defines how indices lying outside the bounds of `base_grid` are handled.
+
+            * `'discard'`: outside indices are ignored.
+            * `'clamp'`: outside indices are projected onto the closest point inside the grid.
+            * `'undefined'`: All points are expected to lie inside the grid. Otherwise an error may be thrown or an undefined tensor may be returned.
+        indices_gradient: Whether to allow the gradient of this operation to be backpropagated through `indices`.
+
+    Returns:
+        Copy of `base_grid` with updated values at `indices`.
+    """
+    assert mode in ('update', 'add', 'mean')
+    assert outside_handling in ('discard', 'clamp', 'undefined')
+    assert isinstance(indices_gradient, bool)
+    if isinstance(indices, dict):  # update a slice
+        if len(indices) == 1 and isinstance(next(iter(indices.values())), (str, int, slice)):  # update a range
+            dim, sel = next(iter(indices.items()))
+            full_dim = base_grid.shape[dim]
+            if isinstance(sel, str):
+                sel = full_dim.item_names[0].index(sel)
+            if isinstance(sel, int):
+                sel = slice(sel, sel+1)
+            assert isinstance(sel, slice), f"Selection must be a str, int or slice but got {type(sel)}"
+            values = expand(values, full_dim.after_gather({dim: sel}))
+            parts = [
+                base_grid[{dim: slice(sel.start)}],
+                values,
+                base_grid[{dim: slice(sel.stop, None)}]
+            ]
+            return concat(parts, dim)
+        else:
+            raise NotImplementedError("scattering into non-continuous values not yet supported by dimension")
+    grid_shape = base_grid if isinstance(base_grid, Shape) else base_grid.shape
+    assert channel(indices).rank < 2
+    if channel(indices) and channel(indices).item_names[0]:
+        indexed_dims = channel(indices).item_names[0]
+        assert indexed_dims in grid_shape, f"Scatter indices {indices.shape} point to missing dimensions in grid {grid_shape}"
+        if indexed_dims != grid_shape.only(indexed_dims).names:
+            indices = indices.vector[grid_shape.only(indexed_dims).names]
+        indexed_dims = grid_shape.only(indexed_dims)
+    else:
+        assert channel(indices).rank == 1 or (grid_shape.spatial_rank + grid_shape.instance_rank == 1 and indices.shape.channel_rank == 0)
+        indexed_dims = grid_shape.spatial or grid_shape.instance
+        assert channel(indices).volume == indexed_dims.rank
+    values = wrap(values)
+    batches = values.shape.non_channel.non_instance & indices.shape.non_channel.non_instance
+    channels = grid_shape.without(indexed_dims).without(batches) & values.shape.channel
+    # --- Set up grid ---
+    if isinstance(base_grid, Shape):
+        with choose_backend_t(indices, values):
+            base_grid = zeros(base_grid & batches & values.shape.channel, dtype=values.dtype)
+        if mode != 'add':
+            base_grid += math.nan
+    # --- Handle outside indices ---
+    if outside_handling == 'clamp':
+        indices = clip(indices, 0, tensor(indexed_dims, channel('vector')) - 1)
+    elif outside_handling == 'discard':
+        indices_linear = pack_dims(indices, instance, instance(_scatter_instance=1))
+        indices_inside = min_((round_(indices_linear) >= 0) & (round_(indices_linear) < tensor(indexed_dims, channel('vector'))), 'vector')
+        indices_linear = boolean_mask(indices_linear, '_scatter_instance', indices_inside)
+        if instance(values).rank > 0:
+            values_linear = pack_dims(values, instance, instance(_scatter_instance=1))
+            values_linear = boolean_mask(values_linear, '_scatter_instance', indices_inside)
+            values = unpack_dim(values_linear, '_scatter_instance', instance(values))
+        indices = unpack_dim(indices_linear, '_scatter_instance', instance(indices))
+        if indices.shape.is_non_uniform:
+            raise NotImplementedError()
+    lists = indices.shape.instance & values.shape.instance
+
+    def scatter_forward(base_grid, indices, values):
+        indices = to_int32(round_(indices))
+        native_grid = reshaped_native(base_grid, [batches, *indexed_dims, channels])
+        native_values = reshaped_native(values, [batches, lists, channels])
+        native_indices = reshaped_native(indices, [batches, lists, 'vector'])
+        backend = choose_backend(native_indices, native_values, native_grid)
+        if mode in ('add', 'update'):
+            native_result = backend.scatter(native_grid, native_indices, native_values, mode=mode)
+        else:  # mean
+            zero_grid = backend.zeros_like(native_grid)
+            summed = backend.scatter(zero_grid, native_indices, native_values, mode='add')
+            count = backend.scatter(zero_grid, native_indices, backend.ones_like(native_values), mode='add')
+            native_result = summed / backend.maximum(count, 1)
+            native_result = backend.where(count == 0, native_grid, native_result)
+        return reshaped_tensor(native_result, [batches, *indexed_dims, channels], check_sizes=True)
+
+    def scatter_backward(args: dict, _output, d_output):
+        from ._nd import spatial_gradient
+        values_grad = gather(d_output, args['indices'])
+        spatial_gradient_indices = gather(spatial_gradient(d_output, dims=indexed_dims), args['indices'])
+        indices_grad = mean(spatial_gradient_indices * args['values'], 'vector_')
+        return None, indices_grad, values_grad
+
+    from ._functional import custom_gradient
+    scatter_function = custom_gradient(scatter_forward, scatter_backward) if indices_gradient else scatter_forward
+    result = scatter_function(base_grid, indices, values)
+    return result
+
+
+def histogram(values: Tensor, bins: Shape or Tensor = spatial(bins=30), weights=1, same_bins: DimFilter = None):
+    """
+    Compute a histogram of a distribution of values.
+
+    *Important Note:* In its current implementation, values outside the range of bins may or may not be added to the outermost bins.
+
+    Args:
+        values: `Tensor` listing the values to be binned along spatial or instance dimensions.
+            `values´ may not contain channel or dual dimensions.
+        bins: Either `Shape` specifying the number of equally-spaced bins to use or bin edge positions as `Tensor` with a spatial or instance dimension.
+        weights: `Tensor` assigning a weight to every value in `values` that will be added to the bin, default 1.
+        same_bins: Only used if `bins` is given as a `Shape`.
+            Use the same bin sizes and positions across these batch dimensions.
+            By default, bins will be chosen independently for each example.
+
+    Returns:
+        hist: `Tensor` containing all batch dimensions and the `bins` dimension with dtype matching `weights`.
+        bin_edges: `Tensor`
+        bin_center: `Tensor`
+    """
+    assert isinstance(values, Tensor), f"values must be a Tensor but got {type(values)}"
+    assert channel(values).is_empty, f"Only 1D histograms supported but values have a channel dimension: {values.shape}"
+    assert dual(values).is_empty, f"values cannot contain dual dimensions but got shape {values.shape}"
+    weights = wrap(weights)
+    if isinstance(bins, Shape):
+        def equal_bins(v):
+            return linspace(finite_min(v, shape), finite_max(v, shape), bins.with_size(bins.size + 1))
+        bins = broadcast_op(equal_bins, [values], iter_dims=(batch(values) & batch(weights)).without(same_bins))
+    assert isinstance(bins, Tensor), f"bins must be a Tensor but got {type(bins)}"
+    assert non_batch(bins).rank == 1, f"bins must contain exactly one spatial or instance dimension listing the bin edges but got shape {bins.shape}"
+    assert channel(bins).rank == dual(bins).rank == 0, f"bins cannot have any channel or dual dimensions but got shape {bins.shape}"
+    tensors = [values, bins] if weights is None else [values, weights, bins]
+    backend = choose_backend_t(*tensors)
+
+    def histogram_uniform(values: Tensor, bin_edges: Tensor, weights):
+        batch_dims = batch(values) & batch(bin_edges) & batch(weights)
+        value_dims = non_batch(values) & non_batch(weights)
+        values_native = reshaped_native(values, [batch_dims, value_dims])
+        weights_native = reshaped_native(weights, [batch_dims, value_dims])
+        bin_edges_native = reshaped_native(bin_edges, [batch_dims, non_batch(bin_edges)])
+        hist_native = backend.histogram1d(values_native, weights_native, bin_edges_native)
+        hist = reshaped_tensor(hist_native, [batch_dims, non_batch(bin_edges).with_size(non_batch(bin_edges).size - 1)])
+        return hist
+        # return stack_tensors([bin_edges, hist], channel(vector=[bin_edges.shape.name, 'hist']))
+
+    bin_center = (bins[{non_batch(bins).name: slice(1, None)}] + bins[{non_batch(bins).name: slice(0, -1)}]) / 2
+    bin_center = expand(bin_center, channel(vector=non_batch(bins).names))
+    bin_edges = stack_tensors([bins], channel(values)) if channel(values) else bins
+    return broadcast_op(histogram_uniform, [values, bins, weights]), bin_edges, bin_center
+
+
+def fft(x: Tensor, dims: DimFilter = spatial) -> Tensor:
+    """
+    Performs a fast Fourier transform (FFT) on all spatial dimensions of x.
+    
+    The inverse operation is `ifft()`.
+
+    Implementations:
+
+    * NumPy: [`np.fft.fft`](https://numpy.org/doc/stable/reference/generated/numpy.fft.fft.html),
+      [`numpy.fft.fft2`](https://numpy.org/doc/stable/reference/generated/numpy.fft.fft2.html),
+      [`numpy.fft.fftn`](https://numpy.org/doc/stable/reference/generated/numpy.fft.fftn.html)
+    * PyTorch: [`torch.fft.fft`](https://pytorch.org/docs/stable/fft.html)
+    * TensorFlow: [`tf.signal.fft`](https://www.tensorflow.org/api_docs/python/tf/signal/fft),
+      [`tf.signal.fft2d`](https://www.tensorflow.org/api_docs/python/tf/signal/fft2d),
+      [`tf.signal.fft3d`](https://www.tensorflow.org/api_docs/python/tf/signal/fft3d)
+    * Jax: [`jax.numpy.fft.fft`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.fft.fft.html),
+      [`jax.numpy.fft.fft2`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.fft.fft2.html)
+      [`jax.numpy.fft.fft`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.fft.fftn.html)
+
+    Args:
+        x: Uniform complex or float `Tensor` with at least one spatial dimension.
+        dims: Dimensions along which to perform the FFT.
+            If `None`, performs the FFT along all spatial dimensions of `x`.
+
+    Returns:
+        *Ƒ(x)* as complex `Tensor`
+    """
+    dims = x.shape.only(dims)
+    x_native = x.native(x.shape)
+    result_native = choose_backend(x_native).fft(x_native, x.shape.indices(dims))
+    return NativeTensor(result_native, x.shape)
+
+
+def ifft(k: Tensor, dims: DimFilter = spatial):
+    """
+    Inverse of `fft()`.
+
+    Args:
+        k: Complex or float `Tensor` with at least one spatial dimension.
+        dims: Dimensions along which to perform the inverse FFT.
+            If `None`, performs the inverse FFT along all spatial dimensions of `k`.
+
+    Returns:
+        *Ƒ<sup>-1</sup>(k)* as complex `Tensor`
+    """
+    dims = k.shape.only(dims)
+    k_native = k.native(k.shape)
+    result_native = choose_backend(k_native).ifft(k_native, k.shape.indices(dims))
+    return NativeTensor(result_native, k.shape)
+
+
+def dtype(x) -> DType:
+    """
+    Returns the data type of `x`.
+
+    Args:
+        x: `Tensor` or native tensor.
+
+    Returns:
+        `DType`
+    """
+    if isinstance(x, Tensor):
+        return x.dtype
+    else:
+        return choose_backend(x).dtype(x)
+
+
+def close(*tensors, rel_tolerance=1e-5, abs_tolerance=0) -> bool:
+    """
+    Checks whether all tensors have equal values within the specified tolerance.
+    
+    Does not check that the shapes exactly match.
+    Tensors with different shapes are reshaped before comparing.
+
+    Args:
+        *tensors: `Tensor` or tensor-like (constant) each
+        rel_tolerance: relative tolerance (Default value = 1e-5)
+        abs_tolerance: absolute tolerance (Default value = 0)
+
+    Returns:
+        Whether all given tensors are equal to the first tensor within the specified tolerance.
+    """
+    tensors = [wrap(t) for t in tensors]
+    for other in tensors[1:]:
+        if not _close(tensors[0], other, rel_tolerance=rel_tolerance, abs_tolerance=abs_tolerance):
+            return False
+    return True
+
+
+def _close(tensor1, tensor2, rel_tolerance=1e-5, abs_tolerance=0):
+    if tensor2 is tensor1:
+        return True
+    new_shape, (native1, native2) = broadcastable_native_tensors(tensor1, tensor2)
+    np1 = choose_backend(native1).numpy(native1)
+    np2 = choose_backend(native2).numpy(native2)
+    return np.allclose(np1, np2, rel_tolerance, abs_tolerance)
+
+
+def assert_close(*values,
+                 rel_tolerance: float = 1e-5,
+                 abs_tolerance: float = 0,
+                 msg: str = "",
+                 verbose: bool = True):
+    """
+    Checks that all given tensors have equal values within the specified tolerance.
+    Raises an AssertionError if the values of this tensor are not within tolerance of any of the other tensors.
+    
+    Does not check that the shapes match as long as they can be broadcast to a common shape.
+
+    Args:
+      values: Tensors or native tensors or numbers or sequences of numbers.
+      rel_tolerance: Relative tolerance.
+      abs_tolerance: Absolute tolerance.
+      msg: Optional error message.
+      verbose: Whether to print conflicting values.
+    """
+    if not values:
+        return
+    phi_tensors = [t for t in values if isinstance(t, Tensor)]
+    if phi_tensors:
+        values = [compatible_tensor(t, phi_tensors[0].shape)._simplify() for t in values]  # use Tensor to infer dimensions
+        for other in values[1:]:
+            _assert_close(values[0], other, rel_tolerance, abs_tolerance, msg, verbose)
+    elif all(isinstance(v, PhiTreeNode) for v in values):
+        tree0, tensors0 = disassemble_tree(values[0])
+        for value in values[1:]:
+            tree, tensors_ = disassemble_tree(value)
+            assert tree0 == tree, f"Tree structures do not match: {tree0} and {tree}"
+            for t0, t in zip(tensors0, tensors_):
+                _assert_close(t0, t, rel_tolerance, abs_tolerance, msg, verbose)
+    else:
+        np_values = [choose_backend(t).numpy(t) for t in values]
+        for other in np_values[1:]:
+            np.testing.assert_allclose(np_values[0], other, rel_tolerance, abs_tolerance, err_msg=msg, verbose=verbose)
+
+
+def _assert_close(tensor1: Tensor, tensor2: Tensor, rel_tolerance: float, abs_tolerance: float, msg: str, verbose: bool):
+    if tensor2 is tensor1:
+        return
+    # if isinstance(tensor2, (int, float, bool)):
+    #     np.testing.assert_allclose(tensor1.numpy(), tensor2, rel_tolerance, abs_tolerance)
+    if isinstance(tensor1, Layout):
+        tensor1._assert_close(tensor2, rel_tolerance, abs_tolerance, msg, verbose)
+    elif isinstance(tensor2, Layout):
+        tensor2._assert_close(tensor1, rel_tolerance, abs_tolerance, msg, verbose)
+    elif isinstance(tensor1, CompressedSparseMatrix):
+        if isinstance(tensor2, CompressedSparseMatrix):
+            _assert_close(tensor1._values, tensor2._values, rel_tolerance, abs_tolerance, msg, verbose)
+            _assert_close(tensor1._indices, tensor2._indices, 0, 0, msg, verbose)
+            _assert_close(tensor1._pointers, tensor2._pointers, 0, 0, msg, verbose)
+        elif tensor1._compressed_dims.only(tensor2.shape):
+            _assert_close(dense(tensor1), tensor2, rel_tolerance, abs_tolerance, msg, verbose)
+        else:
+            _assert_close(tensor1._values, tensor2._values, rel_tolerance, abs_tolerance, msg, verbose)
+    elif isinstance(tensor2, CompressedSparseMatrix):
+        return _assert_close(tensor2, tensor1, rel_tolerance, abs_tolerance, msg, verbose)
+    else:
+        def inner_assert_close(tensor1, tensor2):
+            new_shape, (native1, native2) = broadcastable_native_tensors(tensor1, tensor2)
+            np1 = choose_backend(native1).numpy(native1)
+            np2 = choose_backend(native2).numpy(native2)
+            if not np.allclose(np1, np2, rel_tolerance, abs_tolerance):
+                np.testing.assert_allclose(np1, np2, rel_tolerance, abs_tolerance, err_msg=msg, verbose=verbose)
+
+        broadcast_op(inner_assert_close, [tensor1, tensor2], no_return=True)
+
+
+def _native_wrapper(tensor_function: Callable, create_native_function: Callable, persistent_refs=False):
+    INPUT_TENSORS = []
+    OUTPUT_TENSORS = []
+
+    def native_function(*natives):
+        natives = list(natives)
+        values = [t._op1(lambda _: natives.pop(0)) for t in INPUT_TENSORS]
+        assert len(natives) == 0, "Not all arguments were converted"
+        result = tensor_function(*values)
+        results = [result] if not isinstance(result, (tuple, list)) else result
+        OUTPUT_TENSORS.clear()
+        OUTPUT_TENSORS.extend(results)
+        return sum([v._natives() for v in results], ())
+
+    backend = default_backend()
+    traced = create_native_function(native_function, backend)
+    if traced is NotImplemented:
+        warnings.warn(f"Backend '{backend}' not supported. Returning original function.", RuntimeWarning)
+        return tensor_function, None, INPUT_TENSORS, OUTPUT_TENSORS
+
+    def wrapper(*values: Tensor):
+        INPUT_TENSORS.clear()
+        INPUT_TENSORS.extend(values)
+        for v in values:
+            v._expand()
+        natives = sum([v._natives() for v in values], ())
+        results_native = list(traced(*natives))
+        results = [t._with_natives_replaced(results_native) for t in OUTPUT_TENSORS]
+        if not persistent_refs:
+            INPUT_TENSORS.clear()
+            # OUTPUT_TENSORS.clear()  outputs need to be saved because native_function may be called only the first time. Will get garbage collected once the function is not referenced anymore.
+        assert len(results_native) == 0
+        return results[0] if len(results) == 1 else results
+
+    return wrapper, traced, INPUT_TENSORS, OUTPUT_TENSORS
+
+
+def stop_gradient(x):
+    """
+    Disables gradients for the given tensor.
+    This may switch off the gradients for `x` itself or create a copy of `x` with disabled gradients.
+
+    Implementations:
+
+    * PyTorch: [`x.detach()`](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach)
+    * TensorFlow: [`tf.stop_gradient`](https://www.tensorflow.org/api_docs/python/tf/stop_gradient)
+    * Jax: [`jax.lax.stop_gradient`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.stop_gradient.html)
+
+    Args:
+        x: `Tensor` or `phi.math.magic.PhiTreeNode` for which gradients should be disabled.
+
+    Returns:
+        Copy of `x`.
+    """
+    if isinstance(x, Tensor):
+        return x._op1(lambda native: choose_backend(native).stop_gradient(native))
+    elif isinstance(x, PhiTreeNode):
+        nest, values = disassemble_tree(x)
+        new_values = [stop_gradient(v) for v in values]
+        return assemble_tree(nest, new_values)
+    else:
+        return wrap(choose_backend(x).stop_gradient(x))
+
+
+def pairwise_distances(positions: Tensor,
+                       max_distance: Union[float, Tensor] = None,
+                       format: str = 'dense',
+                       default: Optional[float] = None,
+                       method: str = 'sparse') -> Tensor:
+    """
+    Computes the distance matrix containing the pairwise position differences between each pair of points.
+    Points that are further apart than `max_distance` (if specified) are assigned a distance value of `0`.
+    The diagonal of the matrix (self-distance) also consists purely of zero-vectors and may or may not be stored explicitly.
+
+    Args:
+        positions: `Tensor`.
+            Channel dimensions are interpreted as position components.
+            Instance and spatial dimensions list nodes.
+        max_distance: Scalar or `Tensor` specifying a max_radius for each point separately.
+            Can contain additional batch dimensions but spatial/instance dimensions must match `positions` if present.
+            If not specified, uses an infinite cutoff radius, i.e. all points will be considered neighbors.
+        format: Matrix format as `str` or concrete sparsity pattern as `Tensor`.
+            Allowed strings are `'dense', `'csr'`, `'coo'`, `'csc'`.
+            When a `Tensor` is passed, it needs to have all instance and spatial dims as `positions` as well as corresponding dual dimensions.
+            The distances will be evaluated at all stored entries of the `format` tensor.
+        default: Value the sparse tensor returns for non-stored values. Must be `0` or `None`.
+
+    Returns:
+        Distance matrix as sparse or dense `Tensor`, depending on `format`.
+        For each spatial/instance dimension in `positions`, the matrix also contains a dual dimension of the same name and size.
+        The matrix also contains all batch dimensions of `positions` and one channel dimension called `vector`.
+
+    Examples:
+        >>> pos = vec(x=0, y=tensor([0, 1, 2.5], instance('particles')))
+        >>> dx = pairwise_distances(pos, format='dense', max_distance=2)
+        >>> dx.particles[0]
+        (x=0.000, y=0.000); (x=0.000, y=1.000); (x=0.000, y=0.000) (~particlesᵈ=3, vectorᶜ=x,y)
+    """
+    assert isinstance(positions, Tensor), f"positions must be a Tensor but got {type(positions)}"
+    assert default in [0, None], f"default value must be either 0 or None but got '{default}'"
+    primal_dims = positions.shape.non_batch.non_channel.non_dual
+    dual_dims = dual(**primal_dims.untyped_dict)
+    if isinstance(format, Tensor):  # sparse connectivity specified, no neighborhood search required
+        assert max_distance is None, "max_distance not allowed when connectivity is specified (passing a Tensor for format)"
+        return map_pairs(lambda p1, p2: p2 - p1, positions, format)
+    # --- Dense ---
+    elif format == 'dense':
+        dx = unpack_dim(pack_dims(positions, non_batch(positions).non_channel.non_dual, instance('_tmp')), '_tmp', dual_dims) - positions
+        if max_distance is not None:
+            neighbors = sum_(dx ** 2, channel) <= max_distance ** 2
+            default = float('nan') if default is None else default
+            dx = where(neighbors, dx, default)
+        return dx
+    # --- Sparse neighbor search from here on ---
+    assert max_distance is not None, "max_distance must be specified when computing distance in sparse format"
+    max_distance = wrap(max_distance)
+    index_dtype = DType(int, 32)
+    backend = choose_backend_t(positions, max_distance)
+    batch_shape = batch(positions) & batch(max_distance)
+    if not dual_dims.well_defined:
+        assert dual_dims.rank == 1, f"others_dims sizes must be specified when passing more then one dimension but got {dual_dims}"
+        dual_dims = dual_dims.with_size(primal_dims.volume)
+    # --- Determine mode ---
+    tmp_pair_count = None
+    pair_count = None
+    table_len = None
+    mode = 'vectorize' if batch_shape.volume > 1 and batch_shape.is_uniform else 'loop'
+    if backend.is_available(positions):
+        if mode == 'vectorize':
+            # ToDo determine limits from positions? build_cells+bincount would be enough
+            pair_count = 7
+    else:  # tracing
+        if backend.requires_fixed_shapes_when_tracing():
+            # ToDo use fixed limits (set by user)
+            pair_count = 7
+            mode = 'vectorize'
+    # --- Run neighborhood search ---
+    from .backend._partition import find_neighbors, find_neighbors_matscipy, find_neighbors_sklearn
+    if mode == 'loop':
+        indices = []
+        values = []
+        for b in batch_shape.meshgrid():
+            native_positions = reshaped_native(positions[b], [primal_dims, channel(positions)])
+            native_max_dist = max_distance[b].native()
+            if method == 'sparse':
+                nat_rows, nat_cols, nat_vals = find_neighbors(native_positions, native_max_dist, None, periodic=False, default=default)
+            elif method == 'matscipy':
+                assert positions.available, f"Cannot jit-compile matscipy neighborhood search"
+                nat_rows, nat_cols, nat_vals = find_neighbors_matscipy(native_positions, native_max_dist, None, periodic=False)
+            elif method == 'sklearn':
+                assert positions.available, f"Cannot jit-compile matscipy neighborhood search"
+                nat_rows, nat_cols, nat_vals = find_neighbors_sklearn(native_positions, native_max_dist)
+            else:
+                raise ValueError(method)
+            nat_indices = backend.stack([nat_rows, nat_cols], -1)
+            indices.append(reshaped_tensor(nat_indices, [instance('pairs'), channel(vector=primal_dims.names + dual_dims.names)], convert=False))
+            values.append(reshaped_tensor(nat_vals, [instance('pairs'), channel(positions)]))
+        indices = stack(indices, batch_shape)
+        values = stack(values, batch_shape)
+    elif mode == 'vectorize':
+        raise NotImplementedError
+        # native_positions = reshaped_native(positions, [batch_shape, primal_dims, channel(positions)])
+        # native_max_dist = reshaped_native(max_distance, [batch_shape, primal_dims], force_expand=False)
+        # def single_search(pos, r):
+        #     return find_neighbors(pos, r, None, periodic=False, pair_count=pair_count, default=default)
+        # nat_rows, nat_cols, nat_vals = backend.vectorized_call(single_search, native_positions, native_max_dist, output_dtypes=(index_dtype, index_dtype, positions.dtype))
+        # nat_indices = backend.stack([nat_rows, nat_cols], -1)
+        # indices = reshaped_tensor(nat_indices, [batch_shape, instance('pairs'), channel(vector=primal_dims.names + dual_dims.names)], convert=False)
+        # values = reshaped_tensor(nat_vals, [batch_shape, instance('pairs'), channel(positions)])
+    else:
+        raise RuntimeError
+    # --- Assemble sparse matrix ---
+    dense_shape = primal_dims & dual_dims
+    coo = SparseCoordinateTensor(indices, values, dense_shape, can_contain_double_entries=False, indices_sorted=False, default=default)
+    return to_format(coo, format)
+
+
+def map_pairs(map_function: Callable, values: Tensor, connections: Tensor):
+    """
+    Evaluates `map_function` on all pairs of elements present in the sparsity pattern of `connections`.
+
+    Args:
+        map_function: Function with signature `(Tensor, Tensor) -> Tensor`.
+        values: Values to evaluate `map_function` on.
+            Needs to have a spatial or instance dimension but must not have a dual dimension.
+        connections: Sparse tensor.
+
+    Returns:
+        `Tensor` with the sparse dimensions of `connections` and all non-instance dimensions returned by `map_function`.
+    """
+    assert dual(values).is_empty, f"values must not have a dual dimension but got {values.shape}"
+    inst_dim = non_batch(values).non_channel.non_dual.name
+    indices = stored_indices(connections, invalid='clamp')
+    origin = values[{inst_dim: indices[inst_dim]}]
+    target = values[{inst_dim: indices['~' + inst_dim]}]
+    result = map_function(origin, target)
+    return tensor_like(connections, result, value_order='as existing')
```

### Comparing `phiflow-2.3.4/phi/math/_optimize.py` & `phiflow-2.4.0/phi/math/_optimize.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,661 +1,807 @@
-import time
-import uuid
-import warnings
-from typing import Callable, Generic, List, TypeVar, Any, Tuple, Union
-
-import numpy
-import numpy as np
-
-from .backend import get_precision
-from ._shape import EMPTY_SHAPE, Shape, merge_shapes, batch, non_batch, shape, dual, channel, non_dual
-from ._magic_ops import stack, copy_with, rename_dims, unpack_dim
-from ._sparse import native_matrix, SparseCoordinateTensor, CompressedSparseMatrix
-from ._tensors import Tensor, disassemble_tree, assemble_tree, wrap, cached, NativeTensor, layout
-from . import _ops as math
-from ._ops import choose_backend_t, zeros_like, all_available, reshaped_native, reshaped_tensor, to_float, reshaped_numpy
-from ._functional import custom_gradient, LinearFunction, f_name
-from .backend import Backend
-from .backend._backend import SolveResult, PHI_LOGGER
-
-
-X = TypeVar('X')
-Y = TypeVar('Y')
-
-
-class Solve(Generic[X, Y]):
-    """
-    Specifies parameters and stopping criteria for solving a minimization problem or system of equations.
-    """
-
-    def __init__(self,
-                 method: Union[str, None] = 'auto',
-                 rel_tol: Union[float, Tensor] = None,
-                 abs_tol: Union[float, Tensor] = None,
-                 x0: Union[X, Any] = None,
-                 max_iterations: Union[int, Tensor] = 1000,
-                 suppress: Union[tuple, list] = (),
-                 preprocess_y: Callable = None,
-                 preprocess_y_args: tuple = (),
-                 gradient_solve: Union['Solve[Y, X]', None] = None):
-        method = method or 'auto'
-        assert isinstance(method, str)
-        self.method: str = method
-        """ Optimization method to use. Available solvers depend on the solve function that is used to perform the solve. """
-        self.rel_tol: Tensor = math.to_float(wrap(rel_tol)) if rel_tol is not None else None
-        """Relative tolerance for linear solves only, defaults to 1e-5 for singe precision solves and 1e-12 for double precision solves.
-        This must be unset or `0` for minimization problems.
-        For systems of equations *f(x)=y*, the final tolerance is `max(rel_tol * norm(y), abs_tol)`. """
-        self.abs_tol: Tensor = math.to_float(wrap(abs_tol)) if abs_tol is not None else None
-        """ Absolut tolerance for optimization problems and linear solves.
-        Defaults to 1e-5 for singe precision solves and 1e-12 for double precision solves.
-        For systems of equations *f(x)=y*, the final tolerance is `max(rel_tol * norm(y), abs_tol)`. """
-        self.max_iterations: Tensor = math.to_int32(wrap(max_iterations))
-        """ Maximum number of iterations to perform before raising a `NotConverged` error is raised. """
-        self.x0 = x0
-        """ Initial guess for the method, of same type and dimensionality as the solve result.
-         This property must be set to a value compatible with the solution `x` before running a method. """
-        self.preprocess_y: Callable = preprocess_y
-        """ Function to be applied to the right-hand-side vector of an equation system before solving the system.
-        This property is propagated to gradient solves by default. """
-        self.preprocess_y_args: tuple = preprocess_y_args
-        assert all(issubclass(err, ConvergenceException) for err in suppress)
-        self.suppress: tuple = tuple(suppress)
-        """ Error types to suppress; `tuple` of `ConvergenceException` types. For these errors, the solve function will instead return the partial result without raising the error. """
-        self._gradient_solve: Solve[Y, X] = gradient_solve
-        self.id = str(uuid.uuid4())  # not altered by copy_with(), so that the lookup SolveTape[Solve] works after solve has been copied
-
-    @property
-    def gradient_solve(self) -> 'Solve[Y, X]':
-        """
-        Parameters to use for the gradient pass when an implicit gradient is computed.
-        If `None`, a duplicate of this `Solve` is created for the gradient solve.
-
-        In any case, the gradient solve information will be stored in `gradient_solve.result`.
-        """
-        if self._gradient_solve is None:
-            self._gradient_solve = Solve(self.method, self.rel_tol, self.abs_tol, None, self.max_iterations, self.suppress, self.preprocess_y, self.preprocess_y_args)
-        return self._gradient_solve
-
-    def __repr__(self):
-        return f"{self.method} with tolerance {self.rel_tol} (rel), {self.abs_tol} (abs), max_iterations={self.max_iterations}" + (" including preprocessing" if self.preprocess_y else "")
-
-    def __eq__(self, other):
-        if not isinstance(other, Solve):
-            return False
-        if self.method != other.method \
-                or (self.abs_tol != other.abs_tol).any \
-                or (self.rel_tol != other.rel_tol).any \
-                or (self.max_iterations != other.max_iterations).any \
-                or self.preprocess_y is not other.preprocess_y \
-                or self.suppress != other.suppress:
-            return False
-        return self.x0 == other.x0
-
-    def __variable_attrs__(self):
-        return 'x0', 'preprocess_y_args'
-
-    def with_defaults(self, mode: str):
-        assert mode in ('solve', 'optimization')
-        result = self
-        if result.rel_tol is None:
-            result = copy_with(result, rel_tol=_default_tolerance() if mode == 'solve' else wrap(0.))
-        if result.abs_tol is None:
-            result = copy_with(result, abs_tol=_default_tolerance())
-        return result
-
-    def with_preprocessing(self, preprocess_y: Callable, *args) -> 'Solve':
-        """
-        Adds preprocessing to this `Solve` and all corresponding gradient solves.
-
-        Args:
-            preprocess_y: Preprocessing function.
-            *args: Arguments for the preprocessing function.
-
-        Returns:
-            Copy of this `Solve` with given preprocessing.
-        """
-        assert self.preprocess_y is None, f"preprocessing for linear solve '{self}' already set"
-        gradient_solve = self._gradient_solve.with_preprocessing(preprocess_y, *args) if self._gradient_solve is not None else None
-        return copy_with(self, preprocess_y=preprocess_y, preprocess_y_args=args, _gradient_solve=gradient_solve)
-
-
-def _default_tolerance():
-    if get_precision() == 64:
-        return wrap(1e-12)
-    elif get_precision() == 32:
-        return wrap(1e-5)
-    else:
-        return wrap(1e-2)
-
-
-class SolveInfo(Generic[X, Y]):
-    """
-    Stores information about the solution or trajectory of a solve.
-
-    When representing the full optimization trajectory, all tracked quantities will have an additional `trajectory` batch dimension.
-    """
-
-    def __init__(self,
-                 solve: Solve,
-                 x: X,
-                 residual: Union[Y, None],
-                 iterations: Union[Tensor, None],
-                 function_evaluations: Union[Tensor, None],
-                 converged: Tensor,
-                 diverged: Tensor,
-                 method: str,
-                 msg: Tensor,
-                 solve_time: float):
-        # tuple.__new__(SolveInfo, (x, residual, iterations, function_evaluations, converged, diverged))
-        self.solve: Solve[X, Y] = solve
-        """ `Solve`, Parameters specified for the solve. """
-        self.x: X = x
-        """ `Tensor` or `phi.math.magic.PhiTreeNode`, solution estimate. """
-        self.residual: Y = residual
-        """ `Tensor` or `phi.math.magic.PhiTreeNode`, residual vector for systems of equations or function value for minimization problems. """
-        self.iterations: Tensor = iterations
-        """ `Tensor`, number of performed iterations to reach this state. """
-        self.function_evaluations: Tensor = function_evaluations
-        """ `Tensor`, how often the function (or its gradient function) was called. """
-        self.converged: Tensor = converged
-        """ `Tensor`, whether the residual is within the specified tolerance. """
-        self.diverged: Tensor = diverged
-        """ `Tensor`, whether the solve has diverged at this point. """
-        self.method = method
-        """ `str`, which method and implementation that was used. """
-        if all_available(diverged, converged, iterations):
-            msg = math.map_(_default_solve_info_msg, msg, converged.trajectory[-1], diverged.trajectory[-1], iterations.trajectory[-1], solve=solve, method=method, residual=residual)
-        self.msg = msg
-        """ `str`, termination message """
-        self.solve_time = solve_time
-        """ Time spent in Backend solve function (in seconds) """
-
-    def __repr__(self):
-        return f"{self.method}: {self.converged.trajectory[-1].sum} converged, {self.diverged.trajectory[-1].sum} diverged"
-
-    def snapshot(self, index):
-        return SolveInfo(self.solve, self.x.trajectory[index], self.residual.trajectory[index], self.iterations.trajectory[index], self.function_evaluations.trajectory[index],
-                         self.converged.trajectory[index], self.diverged.trajectory[index], self.method, self.msg, self.solve_time)
-
-    def convergence_check(self, only_warn: bool):
-        if not all_available(self.diverged, self.converged):
-            return
-        if self.diverged.any:
-            if Diverged not in self.solve.suppress:
-                if only_warn:
-                    warnings.warn(self.msg, ConvergenceWarning)
-                else:
-                    raise Diverged(self)
-        if not self.converged.trajectory[-1].all:
-            if NotConverged not in self.solve.suppress:
-                if only_warn:
-                    warnings.warn(self.msg, ConvergenceWarning)
-                else:
-                    raise NotConverged(self)
-
-
-def _default_solve_info_msg(msg, converged, diverged, iterations, solve: Solve, method, residual):
-    if msg:
-        return msg
-    if diverged:
-        return f"Solve diverged within {iterations if iterations is not None else '?'} iterations using {method}."
-    elif not converged:
-        max_res = [f"{math.max_(t.trajectory[-1]):no-color:no-dtype}" for t in disassemble_tree(residual)[1]]
-        return f"{method} did not converge to rel_tol={float(solve.rel_tol):.0e}, abs_tol={float(solve.abs_tol):.0e} within {int(solve.max_iterations)} iterations. Max residual: {', '.join(max_res)}"
-    else:
-        return f"Converged within {iterations if iterations is not None else '?'} iterations."
-
-
-class ConvergenceException(RuntimeError):
-    """
-    Base class for exceptions raised when a solve does not converge.
-
-    See Also:
-        `Diverged`, `NotConverged`.
-    """
-
-    def __init__(self, result: SolveInfo):
-        RuntimeError.__init__(self, result.msg)
-        self.result: SolveInfo = result
-        """ `SolveInfo` holding information about the solve. """
-
-
-class ConvergenceWarning(RuntimeWarning):
-    pass
-
-
-class NotConverged(ConvergenceException):
-    """
-    Raised during optimization if the desired accuracy was not reached within the maximum number of iterations.
-
-    This exception inherits from `ConvergenceException`.
-
-    See Also:
-        `Diverged`.
-    """
-
-    def __init__(self, result: SolveInfo):
-        ConvergenceException.__init__(self, result)
-
-
-class Diverged(ConvergenceException):
-    """
-    Raised if the optimization was stopped prematurely and cannot continue.
-    This may indicate that no solution exists.
-
-    The values of the last estimate `x` may or may not be finite.
-
-    This exception inherits from `ConvergenceException`.
-
-    See Also:
-        `NotConverged`.
-    """
-
-    def __init__(self, result: SolveInfo):
-        ConvergenceException.__init__(self, result)
-
-
-class SolveTape:
-    """
-    Used to record additional information about solves invoked via `solve_linear()`, `solve_nonlinear()` or `minimize()`.
-    While a `SolveTape` is active, certain performance optimizations and algorithm implementations may be disabled.
-
-    To access a `SolveInfo` of a recorded solve, use
-    >>> solve = Solve(method, ...)
-    >>> with SolveTape() as solves:
-    >>>     x = math.solve_linear(f, y, solve)
-    >>> result: SolveInfo = solves[solve]  # get by Solve
-    >>> result: SolveInfo = solves[0]  # get by index
-    """
-
-    def __init__(self, record_trajectories=False):
-        """
-        Args:
-            record_trajectories: When enabled, the entries of `SolveInfo` will contain an additional batch dimension named `trajectory`.
-        """
-        self.record_trajectories = record_trajectories
-        self.solves: List[SolveInfo] = []
-        self.solve_ids: List[str] = []
-
-    def __enter__(self):
-        _SOLVE_TAPES.append(self)
-        return self
-
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        _SOLVE_TAPES.remove(self)
-
-    def _add(self, solve: Solve, trj: bool, result: SolveInfo):
-        if any(s.solve.id == solve.id for s in self.solves):
-            warnings.warn("SolveTape contains two results for the same solve settings. SolveTape[solve] will return the first solve result.", RuntimeWarning)
-        if self.record_trajectories:
-            assert trj, "Solve did not record a trajectory."
-            self.solves.append(result)
-        elif trj:
-            self.solves.append(result.snapshot(-1))
-        else:
-            self.solves.append(result)
-        self.solve_ids.append(solve.id)
-
-    def __getitem__(self, item) -> SolveInfo:
-        if isinstance(item, int):
-            return self.solves[item]
-        else:
-            assert isinstance(item, Solve)
-            solves = [s for s in self.solves if s.solve.id == item.id]
-            if len(solves) == 0:
-                raise KeyError(f"No solve recorded with key '{item}'.")
-            assert len(solves) == 1
-            return solves[0]
-
-    def __iter__(self):
-        return iter(self.solves)
-
-    def __len__(self):
-        return len(self.solves)
-
-
-_SOLVE_TAPES: List[SolveTape] = []
-
-
-def minimize(f: Callable[[X], Y], solve: Solve[X, Y]) -> X:
-    """
-    Finds a minimum of the scalar function *f(x)*.
-    The `method` argument of `solve` determines which optimizer is used.
-    All optimizers supported by `scipy.optimize.minimize` are supported,
-    see https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html .
-    Additionally a gradient descent solver with adaptive step size can be used with `method='GD'`.
-
-    `math.minimize()` is limited to backends that support `jacobian()`, i.e. PyTorch, TensorFlow and Jax.
-
-    To obtain additional information about the performed solve, use a `SolveTape`.
-
-    See Also:
-        `solve_nonlinear()`.
-
-    Args:
-        f: Function whose output is subject to minimization.
-            All positional arguments of `f` are optimized and must be `Tensor` or `phi.math.magic.PhiTreeNode`.
-            If `solve.x0` is a `tuple` or `list`, it will be passed to *f* as varargs, `f(*x0)`.
-            To minimize a subset of the positional arguments, define a new (lambda) function depending only on those.
-            The first return value of `f` must be a scalar float `Tensor` or `phi.math.magic.PhiTreeNode`.
-        solve: `Solve` object to specify method type, parameters and initial guess for `x`.
-
-    Returns:
-        x: solution, the minimum point `x`.
-
-    Raises:
-        NotConverged: If the desired accuracy was not be reached within the maximum number of iterations.
-        Diverged: If the optimization failed prematurely.
-    """
-    solve = solve.with_defaults('optimization')
-    assert (solve.rel_tol == 0).all, f"rel_tol must be zero for minimize() but got {solve.rel_tol}"
-    assert solve.preprocess_y is None, "minimize() does not allow preprocess_y"
-    x0_nest, x0_tensors = disassemble_tree(solve.x0)
-    x0_tensors = [to_float(t) for t in x0_tensors]
-    backend = choose_backend_t(*x0_tensors, prefer_default=True)
-    batch_dims = merge_shapes(*[t.shape for t in x0_tensors]).batch
-    x0_natives = []
-    for t in x0_tensors:
-        t._expand()
-        assert t.shape.is_uniform
-        x0_natives.append(reshaped_native(t, [batch_dims, t.shape.non_batch], force_expand=True))
-    x0_flat = backend.concat(x0_natives, -1)
-
-    def unflatten_assemble(x_flat, additional_dims: Shape = EMPTY_SHAPE, convert=True):
-        i = 0
-        x_tensors = []
-        for x0_native, x0_tensor in zip(x0_natives, x0_tensors):
-            vol = backend.shape(x0_native)[-1]
-            flat_native = x_flat[..., i:i + vol]
-            x_tensors.append(reshaped_tensor(flat_native, [*additional_dims, batch_dims, x0_tensor.shape.non_batch], convert=convert))
-            i += vol
-        x = assemble_tree(x0_nest, x_tensors)
-        return x
-
-    def native_function(x_flat):
-        x = unflatten_assemble(x_flat)
-        if isinstance(x, (tuple, list)):
-            y = f(*x)
-        else:
-            y = f(x)
-        _, y_tensors = disassemble_tree(y)
-        assert not non_batch(y_tensors[0]), f"Failed to minimize '{f.__name__}' because it returned a non-scalar output {shape(y_tensors[0])}. Reduce all non-batch dimensions, e.g. using math.l2_loss()"
-        try:
-            loss_native = reshaped_native(y_tensors[0], [batch_dims])
-        except AssertionError:
-            raise AssertionError(f"Failed to minimize '{f.__name__}' because its output loss {shape(y_tensors[0])} has more batch dimensions than the initial guess {batch_dims}.")
-        return y_tensors[0].sum, (loss_native,)
-
-    atol = backend.to_float(reshaped_native(solve.abs_tol, [batch_dims], force_expand=True))
-    maxi = reshaped_numpy(solve.max_iterations, [batch_dims], force_expand=True)
-    trj = _SOLVE_TAPES and any(t.record_trajectories for t in _SOLVE_TAPES)
-    t = time.perf_counter()
-    ret = backend.minimize(solve.method, native_function, x0_flat, atol, maxi, trj)
-    t = time.perf_counter() - t
-    if not trj:
-        assert isinstance(ret, SolveResult)
-        converged = reshaped_tensor(ret.converged, [batch_dims])
-        diverged = reshaped_tensor(ret.diverged, [batch_dims])
-        x = unflatten_assemble(ret.x)
-        iterations = reshaped_tensor(ret.iterations, [batch_dims])
-        function_evaluations = reshaped_tensor(ret.function_evaluations, [batch_dims])
-        residual = reshaped_tensor(ret.residual, [batch_dims])
-        result = SolveInfo(solve, x, residual, iterations, function_evaluations, converged, diverged, ret.method, ret.message, t)
-    else:  # trajectory
-        assert isinstance(ret, (tuple, list)) and all(isinstance(r, SolveResult) for r in ret)
-        converged = reshaped_tensor(ret[-1].converged, [batch_dims])
-        diverged = reshaped_tensor(ret[-1].diverged, [batch_dims])
-        x = unflatten_assemble(ret[-1].x)
-        x_ = unflatten_assemble(numpy.stack([r.x for r in ret]), additional_dims=batch('trajectory'), convert=False)
-        residual = stack([reshaped_tensor(r.residual, [batch_dims]) for r in ret], batch('trajectory'))
-        iterations = reshaped_tensor(ret[-1].iterations, [batch_dims])
-        function_evaluations = stack([reshaped_tensor(r.function_evaluations, [batch_dims]) for r in ret], batch('trajectory'))
-        result = SolveInfo(solve, x_, residual, iterations, function_evaluations, converged, diverged, ret[-1].method, ret[-1].message, t)
-    for tape in _SOLVE_TAPES:
-        tape._add(solve, trj, result)
-    result.convergence_check(False)  # raises ConvergenceException
-    return x
-
-
-def solve_nonlinear(f: Callable, y, solve: Solve) -> Tensor:
-    """
-    Solves the non-linear equation *f(x) = y* by minimizing the norm of the residual.
-
-    This method is limited to backends that support `jacobian()`, currently PyTorch, TensorFlow and Jax.
-
-    To obtain additional information about the performed solve, use a `SolveTape`.
-
-    See Also:
-        `minimize()`, `solve_linear()`.
-
-    Args:
-        f: Function whose output is optimized to match `y`.
-            All positional arguments of `f` are optimized and must be `Tensor` or `phi.math.magic.PhiTreeNode`.
-            The output of `f` must match `y`.
-        y: Desired output of `f(x)` as `Tensor` or `phi.math.magic.PhiTreeNode`.
-        solve: `Solve` object specifying optimization method, parameters and initial guess for `x`.
-
-    Returns:
-        x: Solution fulfilling `f(x) = y` within specified tolerance as `Tensor` or `phi.math.magic.PhiTreeNode`.
-
-    Raises:
-        NotConverged: If the desired accuracy was not be reached within the maximum number of iterations.
-        Diverged: If the solve failed prematurely.
-    """
-    def min_func(x):
-        diff = f(x) - y
-        l2 = l2_loss(diff)
-        return l2
-    if solve.preprocess_y is not None:
-        y = solve.preprocess_y(y)
-    from ._nd import l2_loss
-    solve = solve.with_defaults('solve')
-    tol = math.maximum(solve.rel_tol * l2_loss(y), solve.abs_tol)
-    min_solve = copy_with(solve, abs_tol=tol, rel_tol=0, preprocess_y=None)
-    return minimize(min_func, min_solve)
-
-
-def solve_linear(f: Union[Callable[[X], Y], Tensor],
-                 y: Y,
-                 solve: Solve[X, Y],
-                 *f_args,
-                 grad_for_f=False,
-                 f_kwargs: dict = None,
-                 **f_kwargs_) -> X:
-    """
-    Solves the system of linear equations *f(x) = y* and returns *x*.
-    This method will use the solver specified in `solve`.
-    The following method identifiers are supported by all backends:
-
-    * `'auto'`: Automatically choose a solver
-    * `'CG'`: Conjugate gradient, only for symmetric and positive definite matrices.
-    * `'CG-adaptive'`: Conjugate gradient with adaptive step size, only for symmetric and positive definite matrices.
-    * `'biCG'`: Biconjugate gradient
-    * `'biCGstab'`: Biconjugate gradient stabilized, first order
-    * `'biCGstab(2)'`: Biconjugate gradient stabilized, second order
-
-    For maximum performance, compile `f` using `jit_compile_linear()` beforehand.
-    Then, an optimized representation of `f` (such as a sparse matrix) will be used to solve the linear system.
-
-    To obtain additional information about the performed solve, perform the solve within a `SolveTape` context.
-    The used implementation can be obtained as `SolveInfo.method`.
-
-    The gradient of this operation will perform another linear solve with the parameters specified by `Solve.gradient_solve`.
-
-    See Also:
-        `solve_nonlinear()`, `jit_compile_linear()`.
-
-    Args:
-        f: One of the following:
-
-            * Linear function with `Tensor` or `phi.math.magic.PhiTreeNode` first parameter and return value. `f` can have additional auxiliary arguments and return auxiliary values.
-            * Dense matrix (`Tensor` with at least one dual dimension)
-            * Sparse matrix (Sparse `Tensor` with at least one dual dimension)
-            * Native tensor (not yet supported)
-
-        y: Desired output of `f(x)` as `Tensor` or `phi.math.magic.PhiTreeNode`.
-        solve: `Solve` object specifying optimization method, parameters and initial guess for `x`.
-        *f_args: Positional arguments to be passed to `f` after `solve.x0`. These arguments will not be solved for.
-            Supports vararg mode or pass all arguments as a `tuple`.
-        f_kwargs: Additional keyword arguments to be passed to `f`.
-            These arguments are treated as auxiliary arguments and can be of any type.
-
-    Returns:
-        x: solution of the linear system of equations `f(x) = y` as `Tensor` or `phi.math.magic.PhiTreeNode`.
-
-    Raises:
-        NotConverged: If the desired accuracy was not be reached within the maximum number of iterations.
-        Diverged: If the solve failed prematurely.
-    """
-    # --- Handle parameters ---
-    f_kwargs = f_kwargs or {}
-    f_kwargs.update(f_kwargs_)
-    f_args = f_args[0] if len(f_args) == 1 and isinstance(f_args[0], tuple) else f_args
-    # --- Get input and output tensors ---
-    y_tree, y_tensors = disassemble_tree(y)
-    x0_tree, x0_tensors = disassemble_tree(solve.x0)
-    assert solve.x0 is not None, "Please specify the initial guess as Solve(..., x0=initial_guess)"
-    assert len(x0_tensors) == len(y_tensors) == 1, "Only single-tensor linear solves are currently supported"
-    backend = choose_backend_t(*y_tensors, *x0_tensors)
-    prefer_explicit = backend.supports(Backend.sparse_coo_tensor) or backend.supports(Backend.csr_matrix) or grad_for_f
-
-    if isinstance(f, Tensor) or (isinstance(f, LinearFunction) and prefer_explicit):  # Matrix solve
-        if isinstance(f, LinearFunction):
-            matrix, bias = f.sparse_matrix_and_bias(solve.x0, *f_args, **f_kwargs)
-        else:
-            matrix = f
-            bias = 0
-
-        def _matrix_solve_forward(y, solve: Solve, matrix: Tensor, is_backprop=False):
-            backend_matrix = native_matrix(matrix)
-            pattern_dims_in = channel(**dual(matrix).untyped_dict).names
-            pattern_dims_out = non_dual(matrix).names  # batch dims can be sparse or batched matrices
-            result = _linear_solve_forward(y, solve, backend_matrix, pattern_dims_in, pattern_dims_out, backend, is_backprop)
-            return result  # must return exactly `x` so gradient isn't computed w.r.t. other quantities
-
-        _matrix_solve = attach_gradient_solve(_matrix_solve_forward, auxiliary_args='is_backprop,solve', matrix_adjoint=grad_for_f)
-        return _matrix_solve(y - bias, solve, matrix)
-    else:  # Matrix-free solve
-        f_args = cached(f_args)
-        solve = cached(solve)
-        assert not grad_for_f, f"grad_for_f=True can only be used for math.jit_compile_linear functions but got '{f_name(f)}'. Please decorate the linear function with @jit_compile_linear"
-
-        def _function_solve_forward(y, solve: Solve, f_args: tuple, f_kwargs: dict = None, is_backprop=False):
-            y_nest, (y_tensor,) = disassemble_tree(y)
-            x0_nest, (x0_tensor,) = disassemble_tree(solve.x0)
-            # active_dims = (y_tensor.shape & x0_tensor.shape).non_batch  # assumes batch dimensions are not active
-            batches = (y_tensor.shape & x0_tensor.shape).batch
-
-            def native_lin_f(native_x, batch_index=None):
-                if batch_index is not None and batches.volume > 1:
-                    native_x = backend.tile(backend.expand_dims(native_x), [batches.volume, 1])
-                x = assemble_tree(x0_nest, [reshaped_tensor(native_x, [batches, non_batch(x0_tensor)] if backend.ndims(native_x) >= 2 else [non_batch(x0_tensor)], convert=False)])
-                y = f(x, *f_args, **f_kwargs)
-                _, (y_tensor,) = disassemble_tree(y)
-                y_native = reshaped_native(y_tensor, [batches, non_batch(y_tensor)] if backend.ndims(native_x) >= 2 else [non_batch(y_tensor)])
-                if batch_index is not None and batches.volume > 1:
-                    y_native = y_native[batch_index]
-                return y_native
-
-            result = _linear_solve_forward(y, solve, native_lin_f, pattern_dims_in=non_batch(x0_tensor).names, pattern_dims_out=non_batch(y_tensor).names, backend=backend, is_backprop=is_backprop)
-            return result  # must return exactly `x` so gradient isn't computed w.r.t. other quantities
-
-        _function_solve = attach_gradient_solve(_function_solve_forward, auxiliary_args='is_backprop,f_kwargs,solve', matrix_adjoint=grad_for_f)
-        return _function_solve(y, solve, f_args, f_kwargs=f_kwargs)
-
-
-def _linear_solve_forward(y,
-                          solve: Solve,
-                          native_lin_op,
-                          pattern_dims_in: Tuple[str, ...],
-                          pattern_dims_out: Tuple[str, ...],
-                          backend: Backend,
-                          is_backprop: bool) -> Any:
-    solve = solve.with_defaults('solve')
-    PHI_LOGGER.debug(f"Performing linear solve {solve} with backend {backend}")
-    if solve.preprocess_y is not None:
-        y = solve.preprocess_y(y, *solve.preprocess_y_args)
-    y_nest, (y_tensor,) = disassemble_tree(y)
-    x0_nest, (x0_tensor,) = disassemble_tree(solve.x0)
-    pattern_dims_in = x0_tensor.shape.only(pattern_dims_in, reorder=True)
-    pattern_dims_out = y_tensor.shape.only(pattern_dims_out, reorder=True)
-    batch_dims = merge_shapes(y_tensor.shape.without(pattern_dims_out), x0_tensor.shape.without(pattern_dims_in))
-    x0_native = backend.as_tensor(reshaped_native(x0_tensor, [batch_dims, pattern_dims_in], force_expand=True))
-    y_native = backend.as_tensor(reshaped_native(y_tensor, [batch_dims, y_tensor.shape.only(pattern_dims_out)], force_expand=True))
-    rtol = backend.as_tensor(reshaped_native(math.to_float(solve.rel_tol), [batch_dims], force_expand=True))
-    atol = backend.as_tensor(reshaped_native(solve.abs_tol, [batch_dims], force_expand=True))
-    tol_sq = backend.maximum(rtol ** 2 * backend.sum(y_native ** 2, -1), atol ** 2)
-    trj = _SOLVE_TAPES and any(t.record_trajectories for t in _SOLVE_TAPES)
-    if trj:
-        assert all_available(y_tensor, x0_tensor), "Cannot record linear solve in jit mode"
-        max_iter = np.expand_dims(np.arange(int(solve.max_iterations)+1), -1)
-    else:
-        max_iter = reshaped_numpy(solve.max_iterations, [shape(solve.max_iterations).without(batch_dims), batch_dims], force_expand=True)
-    t = time.perf_counter()
-    ret = backend.linear_solve(solve.method, native_lin_op, y_native, x0_native, tol_sq, max_iter)
-    t = time.perf_counter() - t
-    trj_dims = [batch(trajectory=len(max_iter))] if trj else []
-    assert isinstance(ret, SolveResult)
-    converged = reshaped_tensor(ret.converged, [*trj_dims, batch_dims])
-    diverged = reshaped_tensor(ret.diverged, [*trj_dims, batch_dims])
-    x = assemble_tree(x0_nest, [reshaped_tensor(ret.x, [*trj_dims, batch_dims, pattern_dims_out])])
-    iterations = reshaped_tensor(ret.iterations, [*trj_dims, batch_dims])
-    function_evaluations = reshaped_tensor(ret.function_evaluations, [*trj_dims, batch_dims])
-    if ret.residual is not None:
-        residual = assemble_tree(y_nest, [reshaped_tensor(ret.residual, [*trj_dims, batch_dims, pattern_dims_out])])
-    elif _SOLVE_TAPES:
-        residual = backend.linear(native_lin_op, ret.x) - y_native
-        residual = assemble_tree(y_nest, [reshaped_tensor(residual, [*trj_dims, batch_dims, pattern_dims_out])])
-    else:
-        residual = None
-    msg = unpack_dim(layout(ret.message, batch('_all')), '_all', batch_dims)
-    result = SolveInfo(solve, x, residual, iterations, function_evaluations, converged, diverged, ret.method, msg, t)
-    # else:  # trajectory
-    #     converged = reshaped_tensor(ret[-1].converged, [batch_dims])
-    #     diverged = reshaped_tensor(ret[-1].diverged, [batch_dims])
-    #     x = assemble_tree(x0_nest, [reshaped_tensor(ret[-1].x, [batch_dims, pattern_dims_in])])
-    #     x_ = assemble_tree(x0_nest, [stack([reshaped_tensor(r.x, [batch_dims, pattern_dims_in]) for r in ret], )])
-    #     residual = assemble_tree(y_nest, [stack([reshaped_tensor(r.residual, [batch_dims, pattern_dims_out]) for r in ret], batch('trajectory'))])
-    #     iterations = reshaped_tensor(ret[-1].iterations, [batch_dims])
-    #     function_evaluations = stack([reshaped_tensor(r.function_evaluations, [batch_dims]) for r in ret], batch('trajectory'))
-    #     result = SolveInfo(solve, x_, residual, iterations, function_evaluations, converged, diverged, ret[-1].method, ret[-1].message, t)
-    for tape in _SOLVE_TAPES:
-        tape._add(solve, trj, result)
-    result.convergence_check(is_backprop and 'TensorFlow' in backend.name)  # raises ConvergenceException
-    return x[{'trajectory': -1}] if isinstance(x, Tensor) else x
-
-
-def attach_gradient_solve(forward_solve: Callable, auxiliary_args: str, matrix_adjoint: bool):
-    def implicit_gradient_solve(fwd_args: dict, x, dx):
-        solve = fwd_args['solve']
-        matrix = (fwd_args['matrix'],) if 'matrix' in fwd_args else ()
-        if matrix_adjoint:
-            assert matrix, "No matrix given but matrix_gradient=True"
-        grad_solve = solve.gradient_solve
-        x0 = grad_solve.x0 if grad_solve.x0 is not None else zeros_like(solve.x0)
-        grad_solve_ = copy_with(solve.gradient_solve, x0=x0)
-        if 'is_backprop' in fwd_args:
-            del fwd_args['is_backprop']
-        dy = solve_with_grad(dx, grad_solve_, *matrix, is_backprop=True, **fwd_args)  # this should hopefully result in implicit gradients for higher orders as well
-        if matrix_adjoint:  # matrix adjoint = dy * x^T sampled at indices
-            matrix = matrix[0]
-            if isinstance(matrix, CompressedSparseMatrix):
-                matrix = matrix.decompress()
-            if isinstance(matrix, SparseCoordinateTensor):
-                col = matrix.dual_indices(to_primal=True)
-                row = matrix.primal_indices()
-                dm_values = dy[col] * x[row]
-                dm = matrix._with_values(dm_values)
-            elif isinstance(matrix, NativeTensor):
-                dy_dual = rename_dims(dy, shape(dy), dual(**shape(dy).untyped_dict))
-                dm = dy_dual * x  # outer product
-                raise NotImplementedError("Matrix adjoint not yet supported for dense matrices")
-            else:
-                raise AssertionError
-            return {'y': dy, 'matrix': dm}
-        else:
-            return {'y': dy}
-
-    solve_with_grad = custom_gradient(forward_solve, implicit_gradient_solve, auxiliary_args=auxiliary_args)
-    return solve_with_grad
-
+import time
+import uuid
+import warnings
+from typing import Callable, Generic, List, TypeVar, Any, Tuple, Union, Optional
+
+import numpy
+import numpy as np
+
+from .backend import get_precision, NUMPY
+from ._shape import EMPTY_SHAPE, Shape, merge_shapes, batch, non_batch, shape, dual, channel, non_dual, instance, spatial
+from ._magic_ops import stack, copy_with, rename_dims, unpack_dim
+from ._sparse import native_matrix, SparseCoordinateTensor, CompressedSparseMatrix, stored_values, is_sparse
+from ._tensors import Tensor, disassemble_tree, assemble_tree, wrap, cached, NativeTensor, layout
+from . import _ops as math
+from ._ops import choose_backend_t, zeros_like, all_available, reshaped_native, reshaped_tensor, to_float, reshaped_numpy
+from ._functional import custom_gradient, LinearFunction, f_name, _TRACING_JIT
+from .backend import Backend
+from .backend._backend import SolveResult, PHI_LOGGER, choose_backend, default_backend, convert, Preconditioner
+from .backend._linalg import IncompleteLU, incomplete_lu_dense, incomplete_lu_coo, coarse_explicit_preconditioner_coo
+
+X = TypeVar('X')
+Y = TypeVar('Y')
+
+
+class Solve(Generic[X, Y]):
+    """
+    Specifies parameters and stopping criteria for solving a minimization problem or system of equations.
+    """
+
+    def __init__(self,
+                 method: Union[str, None] = 'auto',
+                 rel_tol: Union[float, Tensor] = None,
+                 abs_tol: Union[float, Tensor] = None,
+                 x0: Union[X, Any] = None,
+                 max_iterations: Union[int, Tensor] = 1000,
+                 suppress: Union[tuple, list] = (),
+                 preprocess_y: Callable = None,
+                 preprocess_y_args: tuple = (),
+                 preconditioner: Optional[str] = None,
+                 gradient_solve: Union['Solve[Y, X]', None] = None):
+        method = method or 'auto'
+        assert isinstance(method, str)
+        self.method: str = method
+        """ Optimization method to use. Available solvers depend on the solve function that is used to perform the solve. """
+        self.rel_tol: Tensor = math.to_float(wrap(rel_tol)) if rel_tol is not None else None
+        """Relative tolerance for linear solves only, defaults to 1e-5 for singe precision solves and 1e-12 for double precision solves.
+        This must be unset or `0` for minimization problems.
+        For systems of equations *f(x)=y*, the final tolerance is `max(rel_tol * norm(y), abs_tol)`. """
+        self.abs_tol: Tensor = math.to_float(wrap(abs_tol)) if abs_tol is not None else None
+        """ Absolut tolerance for optimization problems and linear solves.
+        Defaults to 1e-5 for singe precision solves and 1e-12 for double precision solves.
+        For systems of equations *f(x)=y*, the final tolerance is `max(rel_tol * norm(y), abs_tol)`. """
+        self.max_iterations: Tensor = math.to_int32(wrap(max_iterations))
+        """ Maximum number of iterations to perform before raising a `NotConverged` error is raised. """
+        self.x0 = x0
+        """ Initial guess for the method, of same type and dimensionality as the solve result.
+         This property must be set to a value compatible with the solution `x` before running a method. """
+        self.preprocess_y: Callable = preprocess_y
+        """ Function to be applied to the right-hand-side vector of an equation system before solving the system.
+        This property is propagated to gradient solves by default. """
+        self.preprocess_y_args: tuple = preprocess_y_args
+        assert all(issubclass(err, ConvergenceException) for err in suppress)
+        self.suppress: tuple = tuple(suppress)
+        """ Error types to suppress; `tuple` of `ConvergenceException` types. For these errors, the solve function will instead return the partial result without raising the error. """
+        self.preconditioner = preconditioner
+        self._gradient_solve: Solve[Y, X] = gradient_solve
+        self.id = str(uuid.uuid4())  # not altered by copy_with(), so that the lookup SolveTape[Solve] works after solve has been copied
+
+    @property
+    def gradient_solve(self) -> 'Solve[Y, X]':
+        """
+        Parameters to use for the gradient pass when an implicit gradient is computed.
+        If `None`, a duplicate of this `Solve` is created for the gradient solve.
+
+        In any case, the gradient solve information will be stored in `gradient_solve.result`.
+        """
+        if self._gradient_solve is None:
+            self._gradient_solve = Solve(self.method, self.rel_tol, self.abs_tol, None, self.max_iterations, self.suppress, self.preprocess_y, self.preprocess_y_args)
+        return self._gradient_solve
+
+    def __repr__(self):
+        return f"{self.method} with tolerance {self.rel_tol} (rel), {self.abs_tol} (abs), max_iterations={self.max_iterations}" + (" including preprocessing" if self.preprocess_y else "")
+
+    def __eq__(self, other):
+        if not isinstance(other, Solve):
+            return False
+        if self.method != other.method \
+                or (self.abs_tol != other.abs_tol).any \
+                or (self.rel_tol != other.rel_tol).any \
+                or (self.max_iterations != other.max_iterations).any \
+                or self.preprocess_y is not other.preprocess_y \
+                or self.suppress != other.suppress:
+            return False
+        return self.x0 == other.x0
+
+    def __variable_attrs__(self):
+        return 'x0', 'preprocess_y_args'
+
+    def with_defaults(self, mode: str):
+        assert mode in ('solve', 'optimization')
+        result = self
+        if result.rel_tol is None:
+            result = copy_with(result, rel_tol=_default_tolerance() if mode == 'solve' else wrap(0.))
+        if result.abs_tol is None:
+            result = copy_with(result, abs_tol=_default_tolerance())
+        return result
+
+    def with_preprocessing(self, preprocess_y: Callable, *args) -> 'Solve':
+        """
+        Adds preprocessing to this `Solve` and all corresponding gradient solves.
+
+        Args:
+            preprocess_y: Preprocessing function.
+            *args: Arguments for the preprocessing function.
+
+        Returns:
+            Copy of this `Solve` with given preprocessing.
+        """
+        assert self.preprocess_y is None, f"preprocessing for linear solve '{self}' already set"
+        gradient_solve = self._gradient_solve.with_preprocessing(preprocess_y, *args) if self._gradient_solve is not None else None
+        return copy_with(self, preprocess_y=preprocess_y, preprocess_y_args=args, _gradient_solve=gradient_solve)
+
+
+def _default_tolerance():
+    if get_precision() == 64:
+        return wrap(1e-12)
+    elif get_precision() == 32:
+        return wrap(1e-5)
+    else:
+        return wrap(1e-2)
+
+
+class SolveInfo(Generic[X, Y]):
+    """
+    Stores information about the solution or trajectory of a solve.
+
+    When representing the full optimization trajectory, all tracked quantities will have an additional `trajectory` batch dimension.
+    """
+
+    def __init__(self,
+                 solve: Solve,
+                 x: X,
+                 residual: Union[Y, None],
+                 iterations: Union[Tensor, None],
+                 function_evaluations: Union[Tensor, None],
+                 converged: Tensor,
+                 diverged: Tensor,
+                 method: str,
+                 msg: Tensor,
+                 solve_time: float):
+        # tuple.__new__(SolveInfo, (x, residual, iterations, function_evaluations, converged, diverged))
+        self.solve: Solve[X, Y] = solve
+        """ `Solve`, Parameters specified for the solve. """
+        self.x: X = x
+        """ `Tensor` or `phi.math.magic.PhiTreeNode`, solution estimate. """
+        self.residual: Y = residual
+        """ `Tensor` or `phi.math.magic.PhiTreeNode`, residual vector for systems of equations or function value for minimization problems. """
+        self.iterations: Tensor = iterations
+        """ `Tensor`, number of performed iterations to reach this state. """
+        self.function_evaluations: Tensor = function_evaluations
+        """ `Tensor`, how often the function (or its gradient function) was called. """
+        self.converged: Tensor = converged
+        """ `Tensor`, whether the residual is within the specified tolerance. """
+        self.diverged: Tensor = diverged
+        """ `Tensor`, whether the solve has diverged at this point. """
+        self.method = method
+        """ `str`, which method and implementation that was used. """
+        if all_available(diverged, converged, iterations):
+            msg = math.map_(_default_solve_info_msg, msg, converged.trajectory[-1], diverged.trajectory[-1], iterations.trajectory[-1], solve=solve, method=method, residual=residual)
+        self.msg = msg
+        """ `str`, termination message """
+        self.solve_time = solve_time
+        """ Time spent in Backend solve function (in seconds) """
+
+    def __repr__(self):
+        return f"{self.method}: {self.converged.trajectory[-1].sum} converged, {self.diverged.trajectory[-1].sum} diverged"
+
+    def snapshot(self, index):
+        return SolveInfo(self.solve, self.x.trajectory[index], self.residual.trajectory[index], self.iterations.trajectory[index], self.function_evaluations.trajectory[index],
+                         self.converged.trajectory[index], self.diverged.trajectory[index], self.method, self.msg, self.solve_time)
+
+    def convergence_check(self, only_warn: bool):
+        if not all_available(self.diverged, self.converged):
+            return
+        if self.diverged.any:
+            if Diverged not in self.solve.suppress:
+                if only_warn:
+                    warnings.warn(self.msg, ConvergenceWarning)
+                else:
+                    raise Diverged(self)
+        if not self.converged.trajectory[-1].all:
+            if NotConverged not in self.solve.suppress:
+                if only_warn:
+                    warnings.warn(self.msg, ConvergenceWarning)
+                else:
+                    raise NotConverged(self)
+
+
+def _default_solve_info_msg(msg, converged, diverged, iterations, solve: Solve, method, residual):
+    if msg:
+        return msg
+    if diverged:
+        return f"Solve diverged within {iterations if iterations is not None else '?'} iterations using {method}."
+    elif not converged:
+        max_res = [f"{math.max_(t.trajectory[-1]):no-color:no-dtype}" for t in disassemble_tree(residual)[1]]
+        return f"{method} did not converge to rel_tol={float(solve.rel_tol):.0e}, abs_tol={float(solve.abs_tol):.0e} within {int(solve.max_iterations)} iterations. Max residual: {', '.join(max_res)}"
+    else:
+        return f"Converged within {iterations if iterations is not None else '?'} iterations."
+
+
+class ConvergenceException(RuntimeError):
+    """
+    Base class for exceptions raised when a solve does not converge.
+
+    See Also:
+        `Diverged`, `NotConverged`.
+    """
+
+    def __init__(self, result: SolveInfo):
+        RuntimeError.__init__(self, result.msg)
+        self.result: SolveInfo = result
+        """ `SolveInfo` holding information about the solve. """
+
+
+class ConvergenceWarning(RuntimeWarning):
+    pass
+
+
+class NotConverged(ConvergenceException):
+    """
+    Raised during optimization if the desired accuracy was not reached within the maximum number of iterations.
+
+    This exception inherits from `ConvergenceException`.
+
+    See Also:
+        `Diverged`.
+    """
+
+    def __init__(self, result: SolveInfo):
+        ConvergenceException.__init__(self, result)
+
+
+class Diverged(ConvergenceException):
+    """
+    Raised if the optimization was stopped prematurely and cannot continue.
+    This may indicate that no solution exists.
+
+    The values of the last estimate `x` may or may not be finite.
+
+    This exception inherits from `ConvergenceException`.
+
+    See Also:
+        `NotConverged`.
+    """
+
+    def __init__(self, result: SolveInfo):
+        ConvergenceException.__init__(self, result)
+
+
+class SolveTape:
+    """
+    Used to record additional information about solves invoked via `solve_linear()`, `solve_nonlinear()` or `minimize()`.
+    While a `SolveTape` is active, certain performance optimizations and algorithm implementations may be disabled.
+
+    To access a `SolveInfo` of a recorded solve, use
+    >>> solve = Solve(method, ...)
+    >>> with SolveTape() as solves:
+    >>>     x = math.solve_linear(f, y, solve)
+    >>> result: SolveInfo = solves[solve]  # get by Solve
+    >>> result: SolveInfo = solves[0]  # get by index
+    """
+
+    def __init__(self, *solves: Solve, record_trajectories=False):
+        """
+        Args:
+            *solves: (Optional) Select specific `solves` to be recorded.
+                If none is given, records all solves that occur within the scope of this `SolveTape`.
+            record_trajectories: When enabled, the entries of `SolveInfo` will contain an additional batch dimension named `trajectory`.
+        """
+        self.record_only_ids = [s.id for s in solves]
+        self.record_trajectories = record_trajectories
+        self.solves: List[SolveInfo] = []
+
+    def should_record_trajectory_for(self, solve: Solve):
+        if not self.record_trajectories:
+            return False
+        if not self.record_only_ids:
+            return True
+        return solve.id in self.record_only_ids
+
+    def __enter__(self):
+        _SOLVE_TAPES.append(self)
+        return self
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        _SOLVE_TAPES.remove(self)
+
+    def _add(self, solve: Solve, trj: bool, result: SolveInfo):
+        if any(s.solve.id == solve.id for s in self.solves):
+            warnings.warn("SolveTape contains two results for the same solve settings. SolveTape[solve] will return the first solve result.", RuntimeWarning)
+        if self.record_only_ids and solve.id not in self.record_only_ids:
+            return  # this solve should not be recorded
+        if self.record_trajectories:
+            assert trj, "Solve did not record a trajectory."
+            self.solves.append(result)
+        elif trj:
+            self.solves.append(result.snapshot(-1))
+        else:
+            self.solves.append(result)
+
+    def __getitem__(self, item) -> SolveInfo:
+        if isinstance(item, int):
+            return self.solves[item]
+        else:
+            assert isinstance(item, Solve)
+            solves = [s for s in self.solves if s.solve.id == item.id]
+            if len(solves) == 0:
+                raise KeyError(f"No solve recorded with key '{item}'.")
+            assert len(solves) == 1
+            return solves[0]
+
+    def __iter__(self):
+        return iter(self.solves)
+
+    def __len__(self):
+        return len(self.solves)
+
+
+_SOLVE_TAPES: List[SolveTape] = []
+
+
+def minimize(f: Callable[[X], Y], solve: Solve[X, Y]) -> X:
+    """
+    Finds a minimum of the scalar function *f(x)*.
+    The `method` argument of `solve` determines which optimizer is used.
+    All optimizers supported by `scipy.optimize.minimize` are supported,
+    see https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html .
+    Additionally a gradient descent solver with adaptive step size can be used with `method='GD'`.
+
+    `math.minimize()` is limited to backends that support `jacobian()`, i.e. PyTorch, TensorFlow and Jax.
+
+    To obtain additional information about the performed solve, use a `SolveTape`.
+
+    See Also:
+        `solve_nonlinear()`.
+
+    Args:
+        f: Function whose output is subject to minimization.
+            All positional arguments of `f` are optimized and must be `Tensor` or `phi.math.magic.PhiTreeNode`.
+            If `solve.x0` is a `tuple` or `list`, it will be passed to *f* as varargs, `f(*x0)`.
+            To minimize a subset of the positional arguments, define a new (lambda) function depending only on those.
+            The first return value of `f` must be a scalar float `Tensor` or `phi.math.magic.PhiTreeNode`.
+        solve: `Solve` object to specify method type, parameters and initial guess for `x`.
+
+    Returns:
+        x: solution, the minimum point `x`.
+
+    Raises:
+        NotConverged: If the desired accuracy was not be reached within the maximum number of iterations.
+        Diverged: If the optimization failed prematurely.
+    """
+    solve = solve.with_defaults('optimization')
+    assert (solve.rel_tol == 0).all, f"rel_tol must be zero for minimize() but got {solve.rel_tol}"
+    assert solve.preprocess_y is None, "minimize() does not allow preprocess_y"
+    x0_nest, x0_tensors = disassemble_tree(solve.x0)
+    x0_tensors = [to_float(t) for t in x0_tensors]
+    backend = choose_backend_t(*x0_tensors, prefer_default=True)
+    batch_dims = merge_shapes(*[t.shape for t in x0_tensors]).batch
+    x0_natives = []
+    for t in x0_tensors:
+        t._expand()
+        assert t.shape.is_uniform
+        x0_natives.append(reshaped_native(t, [batch_dims, t.shape.non_batch]))
+    x0_flat = backend.concat(x0_natives, -1)
+
+    def unflatten_assemble(x_flat, additional_dims: Shape = EMPTY_SHAPE, convert=True):
+        i = 0
+        x_tensors = []
+        for x0_native, x0_tensor in zip(x0_natives, x0_tensors):
+            vol = backend.shape(x0_native)[-1]
+            flat_native = x_flat[..., i:i + vol]
+            x_tensors.append(reshaped_tensor(flat_native, [*additional_dims, batch_dims, x0_tensor.shape.non_batch], convert=convert))
+            i += vol
+        x = assemble_tree(x0_nest, x_tensors)
+        return x
+
+    def native_function(x_flat):
+        x = unflatten_assemble(x_flat)
+        if isinstance(x, (tuple, list)):
+            y = f(*x)
+        else:
+            y = f(x)
+        _, y_tensors = disassemble_tree(y)
+        assert not non_batch(y_tensors[0]), f"Failed to minimize '{f.__name__}' because it returned a non-scalar output {shape(y_tensors[0])}. Reduce all non-batch dimensions, e.g. using math.l2_loss()"
+        try:
+            loss_native = reshaped_native(y_tensors[0], [batch_dims], force_expand=False)
+        except AssertionError:
+            raise AssertionError(f"Failed to minimize '{f.__name__}' because its output loss {shape(y_tensors[0])} has more batch dimensions than the initial guess {batch_dims}.")
+        return y_tensors[0].sum, (loss_native,)
+
+    atol = backend.to_float(reshaped_native(solve.abs_tol, [batch_dims]))
+    maxi = reshaped_numpy(solve.max_iterations, [batch_dims])
+    trj = _SOLVE_TAPES and any(t.should_record_trajectory_for(solve) for t in _SOLVE_TAPES)
+    t = time.perf_counter()
+    ret = backend.minimize(solve.method, native_function, x0_flat, atol, maxi, trj)
+    t = time.perf_counter() - t
+    if not trj:
+        assert isinstance(ret, SolveResult)
+        converged = reshaped_tensor(ret.converged, [batch_dims])
+        diverged = reshaped_tensor(ret.diverged, [batch_dims])
+        x = unflatten_assemble(ret.x)
+        iterations = reshaped_tensor(ret.iterations, [batch_dims])
+        function_evaluations = reshaped_tensor(ret.function_evaluations, [batch_dims])
+        residual = reshaped_tensor(ret.residual, [batch_dims])
+        result = SolveInfo(solve, x, residual, iterations, function_evaluations, converged, diverged, ret.method, ret.message, t)
+    else:  # trajectory
+        assert isinstance(ret, (tuple, list)) and all(isinstance(r, SolveResult) for r in ret)
+        converged = reshaped_tensor(ret[-1].converged, [batch_dims])
+        diverged = reshaped_tensor(ret[-1].diverged, [batch_dims])
+        x = unflatten_assemble(ret[-1].x)
+        x_ = unflatten_assemble(numpy.stack([r.x for r in ret]), additional_dims=batch('trajectory'), convert=False)
+        residual = stack([reshaped_tensor(r.residual, [batch_dims]) for r in ret], batch('trajectory'))
+        iterations = reshaped_tensor(ret[-1].iterations, [batch_dims])
+        function_evaluations = stack([reshaped_tensor(r.function_evaluations, [batch_dims]) for r in ret], batch('trajectory'))
+        result = SolveInfo(solve, x_, residual, iterations, function_evaluations, converged, diverged, ret[-1].method, ret[-1].message, t)
+    for tape in _SOLVE_TAPES:
+        tape._add(solve, trj, result)
+    result.convergence_check(False)  # raises ConvergenceException
+    return x
+
+
+def solve_nonlinear(f: Callable, y, solve: Solve) -> Tensor:
+    """
+    Solves the non-linear equation *f(x) = y* by minimizing the norm of the residual.
+
+    This method is limited to backends that support `jacobian()`, currently PyTorch, TensorFlow and Jax.
+
+    To obtain additional information about the performed solve, use a `SolveTape`.
+
+    See Also:
+        `minimize()`, `solve_linear()`.
+
+    Args:
+        f: Function whose output is optimized to match `y`.
+            All positional arguments of `f` are optimized and must be `Tensor` or `phi.math.magic.PhiTreeNode`.
+            The output of `f` must match `y`.
+        y: Desired output of `f(x)` as `Tensor` or `phi.math.magic.PhiTreeNode`.
+        solve: `Solve` object specifying optimization method, parameters and initial guess for `x`.
+
+    Returns:
+        x: Solution fulfilling `f(x) = y` within specified tolerance as `Tensor` or `phi.math.magic.PhiTreeNode`.
+
+    Raises:
+        NotConverged: If the desired accuracy was not be reached within the maximum number of iterations.
+        Diverged: If the solve failed prematurely.
+    """
+    def min_func(x):
+        diff = f(x) - y
+        l2 = l2_loss(diff)
+        return l2
+    if solve.preprocess_y is not None:
+        y = solve.preprocess_y(y)
+    from ._nd import l2_loss
+    solve = solve.with_defaults('solve')
+    tol = math.maximum(solve.rel_tol * l2_loss(y), solve.abs_tol)
+    min_solve = copy_with(solve, abs_tol=tol, rel_tol=0, preprocess_y=None)
+    return minimize(min_func, min_solve)
+
+
+def solve_linear(f: Union[Callable[[X], Y], Tensor],
+                 y: Y,
+                 solve: Solve[X, Y],
+                 *f_args,
+                 grad_for_f=False,
+                 f_kwargs: dict = None,
+                 **f_kwargs_) -> X:
+    """
+    Solves the system of linear equations *f(x) = y* and returns *x*.
+    This method will use the solver specified in `solve`.
+    The following method identifiers are supported by all backends:
+
+    * `'auto'`: Automatically choose a solver
+    * `'CG'`: Conjugate gradient, only for symmetric and positive definite matrices.
+    * `'CG-adaptive'`: Conjugate gradient with adaptive step size, only for symmetric and positive definite matrices.
+    * `'biCG'` or `'biCG-stab(0)'`: Biconjugate gradient
+    * `'biCG-stab'` or `'biCG-stab(1)'`: Biconjugate gradient stabilized, first order
+    * `'biCG-stab(2)'`, `'biCG-stab(4)'`, ...: Biconjugate gradient stabilized, second or higher order
+    * `'scipy-direct'`: SciPy direct solve always run oh the CPU using `scipy.sparse.linalg.spsolve`.
+    * `'scipy-CG'`, `'scipy-GMres'`, `'scipy-biCG'`, `'scipy-biCG-stab'`, `'scipy-CGS'`, `'scipy-QMR'`, `'scipy-GCrotMK'`: SciPy iterative solvers always run oh the CPU.
+
+    **Caution**: SciPy solvers cannot be jit-compiled and should only be used for debugging purposes.
+
+    For maximum performance, compile `f` using `jit_compile_linear()` beforehand.
+    Then, an optimized representation of `f` (such as a sparse matrix) will be used to solve the linear system.
+
+    To obtain additional information about the performed solve, perform the solve within a `SolveTape` context.
+    The used implementation can be obtained as `SolveInfo.method`.
+
+    The gradient of this operation will perform another linear solve with the parameters specified by `Solve.gradient_solve`.
+
+    See Also:
+        `solve_nonlinear()`, `jit_compile_linear()`.
+
+    Args:
+        f: One of the following:
+
+            * Linear function with `Tensor` or `phi.math.magic.PhiTreeNode` first parameter and return value. `f` can have additional auxiliary arguments and return auxiliary values.
+            * Dense matrix (`Tensor` with at least one dual dimension)
+            * Sparse matrix (Sparse `Tensor` with at least one dual dimension)
+            * Native tensor (not yet supported)
+
+        y: Desired output of `f(x)` as `Tensor` or `phi.math.magic.PhiTreeNode`.
+        solve: `Solve` object specifying optimization method, parameters and initial guess for `x`.
+        *f_args: Positional arguments to be passed to `f` after `solve.x0`. These arguments will not be solved for.
+            Supports vararg mode or pass all arguments as a `tuple`.
+        f_kwargs: Additional keyword arguments to be passed to `f`.
+            These arguments are treated as auxiliary arguments and can be of any type.
+
+    Returns:
+        x: solution of the linear system of equations `f(x) = y` as `Tensor` or `phi.math.magic.PhiTreeNode`.
+
+    Raises:
+        NotConverged: If the desired accuracy was not be reached within the maximum number of iterations.
+        Diverged: If the solve failed prematurely.
+    """
+    # --- Handle parameters ---
+    f_kwargs = f_kwargs or {}
+    f_kwargs.update(f_kwargs_)
+    f_args = f_args[0] if len(f_args) == 1 and isinstance(f_args[0], tuple) else f_args
+    # --- Get input and output tensors ---
+    y_tree, y_tensors = disassemble_tree(y)
+    x0_tree, x0_tensors = disassemble_tree(solve.x0)
+    assert solve.x0 is not None, "Please specify the initial guess as Solve(..., x0=initial_guess)"
+    assert len(x0_tensors) == len(y_tensors) == 1, "Only single-tensor linear solves are currently supported"
+    backend = choose_backend_t(*y_tensors, *x0_tensors)
+    prefer_explicit = backend.supports(Backend.sparse_coo_tensor) or backend.supports(Backend.csr_matrix) or grad_for_f
+
+    if isinstance(f, Tensor) or (isinstance(f, LinearFunction) and prefer_explicit):  # Matrix solve
+        if isinstance(f, LinearFunction):
+            matrix, bias = f.sparse_matrix_and_bias(solve.x0, *f_args, **f_kwargs)
+        else:
+            matrix = f
+            bias = 0
+        preconditioner = compute_preconditioner(solve.preconditioner, matrix, safe=False, target_backend=NUMPY if solve.method.startswith('scipy-') else backend, solver=solve.method) if solve.preconditioner is not None else None
+
+        def _matrix_solve_forward(y, solve: Solve, matrix: Tensor, is_backprop=False):
+            backend_matrix = native_matrix(matrix, choose_backend_t(*y_tensors, matrix))
+            pattern_dims_in = channel(**dual(matrix).untyped_dict).names
+            pattern_dims_out = non_dual(matrix).names  # batch dims can be sparse or batched matrices
+            result = _linear_solve_forward(y, solve, backend_matrix, pattern_dims_in, pattern_dims_out, preconditioner, backend, is_backprop)
+            return result  # must return exactly `x` so gradient isn't computed w.r.t. other quantities
+
+        _matrix_solve = attach_gradient_solve(_matrix_solve_forward, auxiliary_args=f'is_backprop,solve{",matrix" if matrix.default_backend == NUMPY else ""}', matrix_adjoint=grad_for_f)
+        return _matrix_solve(y - bias, solve, matrix)
+    else:  # Matrix-free solve
+        f_args = cached(f_args)
+        solve = cached(solve)
+        assert not grad_for_f, f"grad_for_f=True can only be used for math.jit_compile_linear functions but got '{f_name(f)}'. Please decorate the linear function with @jit_compile_linear"
+        assert solve.preconditioner is None, f"Preconditioners not currently supported for matrix-free solves. Decorate '{f_name(f)}' with @math.jit_compile_linear to perform a matrix solve."
+
+        def _function_solve_forward(y, solve: Solve, f_args: tuple, f_kwargs: dict = None, is_backprop=False):
+            y_nest, (y_tensor,) = disassemble_tree(y)
+            x0_nest, (x0_tensor,) = disassemble_tree(solve.x0)
+            # active_dims = (y_tensor.shape & x0_tensor.shape).non_batch  # assumes batch dimensions are not active
+            batches = (y_tensor.shape & x0_tensor.shape).batch
+
+            def native_lin_f(native_x, batch_index=None):
+                if batch_index is not None and batches.volume > 1:
+                    native_x = backend.tile(backend.expand_dims(native_x), [batches.volume, 1])
+                x = assemble_tree(x0_nest, [reshaped_tensor(native_x, [batches, non_batch(x0_tensor)] if backend.ndims(native_x) >= 2 else [non_batch(x0_tensor)], convert=False)])
+                y = f(x, *f_args, **f_kwargs)
+                _, (y_tensor,) = disassemble_tree(y)
+                y_native = reshaped_native(y_tensor, [batches, non_batch(y_tensor)] if backend.ndims(native_x) >= 2 else [non_batch(y_tensor)])
+                if batch_index is not None and batches.volume > 1:
+                    y_native = y_native[batch_index]
+                return y_native
+
+            result = _linear_solve_forward(y, solve, native_lin_f, pattern_dims_in=non_batch(x0_tensor).names, pattern_dims_out=non_batch(y_tensor).names, preconditioner=None, backend=backend, is_backprop=is_backprop)
+            return result  # must return exactly `x` so gradient isn't computed w.r.t. other quantities
+
+        _function_solve = attach_gradient_solve(_function_solve_forward, auxiliary_args='is_backprop,f_kwargs,solve', matrix_adjoint=grad_for_f)
+        return _function_solve(y, solve, f_args, f_kwargs=f_kwargs)
+
+
+def _linear_solve_forward(y,
+                          solve: Solve,
+                          native_lin_op: Union[Callable, Any],  # native function or native matrix
+                          pattern_dims_in: Tuple[str, ...],
+                          pattern_dims_out: Tuple[str, ...],
+                          preconditioner: Optional[Callable],
+                          backend: Backend,
+                          is_backprop: bool) -> Any:
+    solve = solve.with_defaults('solve')
+    PHI_LOGGER.debug(f"Performing linear solve {solve} with backend {backend}")
+    if solve.preprocess_y is not None:
+        y = solve.preprocess_y(y, *solve.preprocess_y_args)
+    y_nest, (y_tensor,) = disassemble_tree(y)
+    x0_nest, (x0_tensor,) = disassemble_tree(solve.x0)
+    pattern_dims_in = x0_tensor.shape.only(pattern_dims_in, reorder=True)
+    pattern_dims_out = y_tensor.shape.only(pattern_dims_out, reorder=True)
+    batch_dims = merge_shapes(y_tensor.shape.without(pattern_dims_out), x0_tensor.shape.without(pattern_dims_in))
+    x0_native = backend.as_tensor(reshaped_native(x0_tensor, [batch_dims, pattern_dims_in]))
+    y_native = backend.as_tensor(reshaped_native(y_tensor, [batch_dims, y_tensor.shape.only(pattern_dims_out)]))
+    rtol = backend.as_tensor(reshaped_native(math.to_float(solve.rel_tol), [batch_dims]))
+    atol = backend.as_tensor(reshaped_native(solve.abs_tol, [batch_dims]))
+    trj = _SOLVE_TAPES and any(t.should_record_trajectory_for(solve) for t in _SOLVE_TAPES)
+    if trj:
+        max_iter = np.expand_dims(np.arange(int(solve.max_iterations)+1), -1)
+    else:
+        max_iter = reshaped_numpy(solve.max_iterations, [shape(solve.max_iterations).without(batch_dims), batch_dims])
+    t = time.perf_counter()
+    ret = backend.linear_solve(solve.method, native_lin_op, y_native, x0_native, rtol, atol, max_iter, preconditioner)
+    t = time.perf_counter() - t
+    trj_dims = [batch(trajectory=len(max_iter))] if trj else []
+    assert isinstance(ret, SolveResult)
+    converged = reshaped_tensor(ret.converged, [*trj_dims, batch_dims])
+    diverged = reshaped_tensor(ret.diverged, [*trj_dims, batch_dims])
+    x = assemble_tree(x0_nest, [reshaped_tensor(ret.x, [*trj_dims, batch_dims, pattern_dims_out])])
+    iterations = reshaped_tensor(ret.iterations, [*trj_dims, batch_dims])
+    function_evaluations = reshaped_tensor(ret.function_evaluations, [*trj_dims, batch_dims])
+    if ret.residual is not None:
+        residual = assemble_tree(y_nest, [reshaped_tensor(ret.residual, [*trj_dims, batch_dims, pattern_dims_out])])
+    elif _SOLVE_TAPES:
+        residual = backend.linear(native_lin_op, ret.x) - y_native
+        residual = assemble_tree(y_nest, [reshaped_tensor(residual, [*trj_dims, batch_dims, pattern_dims_out])])
+    else:
+        residual = None
+    msg = unpack_dim(layout(ret.message, batch('_all')), '_all', batch_dims)
+    result = SolveInfo(solve, x, residual, iterations, function_evaluations, converged, diverged, ret.method, msg, t)
+    # else:  # trajectory
+    #     converged = reshaped_tensor(ret[-1].converged, [batch_dims])
+    #     diverged = reshaped_tensor(ret[-1].diverged, [batch_dims])
+    #     x = assemble_tree(x0_nest, [reshaped_tensor(ret[-1].x, [batch_dims, pattern_dims_in])])
+    #     x_ = assemble_tree(x0_nest, [stack([reshaped_tensor(r.x, [batch_dims, pattern_dims_in]) for r in ret], )])
+    #     residual = assemble_tree(y_nest, [stack([reshaped_tensor(r.residual, [batch_dims, pattern_dims_out]) for r in ret], batch('trajectory'))])
+    #     iterations = reshaped_tensor(ret[-1].iterations, [batch_dims])
+    #     function_evaluations = stack([reshaped_tensor(r.function_evaluations, [batch_dims]) for r in ret], batch('trajectory'))
+    #     result = SolveInfo(solve, x_, residual, iterations, function_evaluations, converged, diverged, ret[-1].method, ret[-1].message, t)
+    for tape in _SOLVE_TAPES:
+        tape._add(solve, trj, result)
+    result.convergence_check(is_backprop and 'TensorFlow' in backend.name)  # raises ConvergenceException
+    return x[{'trajectory': -1}] if isinstance(x, Tensor) else x
+
+
+def attach_gradient_solve(forward_solve: Callable, auxiliary_args: str, matrix_adjoint: bool):
+    def implicit_gradient_solve(fwd_args: dict, x, dx):
+        solve = fwd_args['solve']
+        matrix = (fwd_args['matrix'],) if 'matrix' in fwd_args else ()
+        if matrix_adjoint:
+            assert matrix, "No matrix given but matrix_gradient=True"
+        grad_solve = solve.gradient_solve
+        x0 = grad_solve.x0 if grad_solve.x0 is not None else zeros_like(solve.x0)
+        grad_solve_ = copy_with(solve.gradient_solve, x0=x0)
+        if 'is_backprop' in fwd_args:
+            del fwd_args['is_backprop']
+        dy = solve_with_grad(dx, grad_solve_, *matrix, is_backprop=True, **fwd_args)  # this should hopefully result in implicit gradients for higher orders as well
+        if matrix_adjoint:  # matrix adjoint = dy * x^T sampled at indices
+            matrix = matrix[0]
+            if isinstance(matrix, CompressedSparseMatrix):
+                matrix = matrix.decompress()
+            if isinstance(matrix, SparseCoordinateTensor):
+                col = matrix.dual_indices(to_primal=True)
+                row = matrix.primal_indices()
+                dm_values = dy[col] * x[row]
+                dm = matrix._with_values(dm_values)
+            elif isinstance(matrix, NativeTensor):
+                dy_dual = rename_dims(dy, shape(dy), dual(**shape(dy).untyped_dict))
+                dm = dy_dual * x  # outer product
+                raise NotImplementedError("Matrix adjoint not yet supported for dense matrices")
+            else:
+                raise AssertionError
+            return {'y': dy, 'matrix': dm}
+        else:
+            return {'y': dy}
+
+    solve_with_grad = custom_gradient(forward_solve, implicit_gradient_solve, auxiliary_args=auxiliary_args)
+    return solve_with_grad
+
+
+def compute_preconditioner(method: str, matrix: Tensor, safe=False, target_backend: Backend = None, solver: str = None) -> Optional[Preconditioner]:
+    if method == 'auto':
+        target_backend = target_backend or default_backend()
+        # is_cpu = target_backend.get_default_device().device_type == 'CPU'
+        # if tracing and not Backend.supports(Backend.python_call) -> cannot use ILU
+        native_triangular = target_backend.supports(Backend.solve_triangular_sparse) if is_sparse(matrix) else target_backend.supports(Backend.solve_triangular_dense)
+        if solver in ['direct', 'scipy-direct']:
+            method = None
+        elif native_triangular:
+            method = 'ilu'
+        elif spatial(matrix):
+            method = 'cluster'
+        else:
+            method = None
+        PHI_LOGGER.info(f"Auto-selecting preconditioner '{method}' for '{solver}' on {target_backend}")
+    if method == 'ilu':
+        n = dual(matrix).volume
+        entry_count = stored_values(matrix).shape.volume
+        avg_entries_per_element = entry_count / n
+        d = (avg_entries_per_element - 1) / 2
+        if _TRACING_JIT and matrix.available:
+            iterations = int(math.ceil(n ** (1 / d)))  # high-quality preconditioner when jit-compiling with constant matrix
+            PHI_LOGGER.debug(f"factor_ilu: auto-selecting iterations={iterations} (constant matrix) for matrix {matrix}")
+        else:
+            iterations = int(math.ceil(math.sqrt(d * n ** (1 / d))))  # in 1D take sqrt(n), in 2D take sqrt(2*n**1/2)
+            PHI_LOGGER.debug(f"factor_ilu: auto-selecting iterations={iterations} ({'variable matrix' if _TRACING_JIT else 'eager mode'}) for matrix {matrix}")
+        lower, upper = factor_ilu(matrix, iterations, safe=safe)
+        native_lower = native_matrix(lower, target_backend)
+        native_upper = native_matrix(upper, target_backend)
+        native_lower = convert(native_lower, target_backend)
+        native_upper = convert(native_upper, target_backend)
+        return IncompleteLU(native_lower, True, native_upper, False, rank_deficiency=0, source=f"iter={iterations}")  # ToDo rank deficiency
+    elif method == 'cluster':
+        return explicit_coarse(matrix, target_backend)
+    elif method is None:
+        return None
+    raise NotImplementedError
+
+
+def factor_ilu(matrix: Tensor, iterations: int, safe=False):
+    """
+    Incomplete LU factorization for dense or sparse matrices.
+
+    For sparse matrices, keeps the sparsity pattern of `matrix`.
+    L and U will be trimmed to the respective areas, i.e. stored upper elements in L will be dropped,
+     unless this would lead to varying numbers of stored elements along a batch dimension.
+
+    Args:
+        matrix: Dense or sparse matrix to factor.
+            Currently, compressed sparse matrices are decompressed before running the ILU algorithm.
+        iterations: (Optional) Number of fixed-point iterations to perform.
+            If not given, will be automatically determined from matrix size and sparsity.
+        safe: If `False` (default), only matrices with a rank deficiency of up to 1 can be factored as all values of L and U are uniquely determined.
+            For matrices with higher rank deficiencies, the result includes `NaN` values.
+            If `True`, the algorithm runs slightly slower but can factor highly rank-deficient matrices as well.
+            However, then L is undeterdetermined and unused values of L are set to 0.
+            Rank deficiencies of 1 occur frequently in periodic settings but higher ones are rare.
+
+    Returns:
+        L: Lower-triangular matrix as `Tensor` with all diagonal elements equal to 1.
+        U: Upper-triangular matrix as `Tensor`.
+
+    Examples:
+        >>> matrix = wrap([[-2, 1, 0],
+        >>>                [1, -2, 1],
+        >>>                [0, 1, -2]], channel('row'), dual('col'))
+        >>> L, U = math.factor_ilu(matrix)
+        >>> math.print(L)
+        row=0      1.          0.          0.         along ~col
+        row=1     -0.5         1.          0.         along ~col
+        row=2      0.         -0.6666667   1.         along ~col
+        >>> math.print(L @ U, "L @ U")
+                    L @ U
+        row=0     -2.   1.   0.  along ~col
+        row=1      1.  -2.   1.  along ~col
+        row=2      0.   1.  -2.  along ~col
+    """
+    if isinstance(matrix, CompressedSparseMatrix):
+        matrix = matrix.decompress()
+    if isinstance(matrix, SparseCoordinateTensor):
+        ind_batch, channels, indices, values, shape = matrix._native_coo_components(dual, matrix=True)
+        (l_idx_nat, l_val_nat), (u_idx_nat, u_val_nat) = incomplete_lu_coo(indices, values, shape, iterations, safe)
+        col_dims = matrix._shape.only(dual)
+        row_dims = matrix._dense_shape.without(col_dims)
+        l_indices = matrix._unpack_indices(l_idx_nat[..., 0], l_idx_nat[..., 1], row_dims, col_dims, ind_batch)
+        u_indices = matrix._unpack_indices(u_idx_nat[..., 0], u_idx_nat[..., 1], row_dims, col_dims, ind_batch)
+        l_values = reshaped_tensor(l_val_nat, [ind_batch, instance(matrix._values), channels], convert=False)
+        u_values = reshaped_tensor(u_val_nat, [ind_batch, instance(matrix._values), channels], convert=False)
+        lower = SparseCoordinateTensor(l_indices, l_values, matrix._dense_shape, matrix._can_contain_double_entries, matrix._indices_sorted, matrix._default)
+        upper = SparseCoordinateTensor(u_indices, u_values, matrix._dense_shape, matrix._can_contain_double_entries, matrix._indices_sorted, matrix._default)
+    else:  # dense matrix
+        native_matrix = reshaped_native(matrix, [batch, non_batch(matrix).non_dual, dual, EMPTY_SHAPE])
+        l_native, u_native = incomplete_lu_dense(native_matrix, iterations, safe)
+        lower = reshaped_tensor(l_native, [batch(matrix), non_batch(matrix).non_dual, dual(matrix), EMPTY_SHAPE])
+        upper = reshaped_tensor(u_native, [batch(matrix), non_batch(matrix).non_dual, dual(matrix), EMPTY_SHAPE])
+    return lower, upper
+
+
+def explicit_coarse(matrix: Tensor,
+                    target_backend: Backend,
+                    cluster_count=3 ** 6,
+                    cluster_hint=None):
+    b0 = matrix.default_backend
+    cols = dual(matrix).volume
+    # --- cluster entries ---
+    if cluster_count >= cols:  # 1 cluster per element
+        cluster_count = cols
+        clusters = b0.to_int32(b0.linspace_without_last(0, cluster_count, cols))[None, :]
+    elif spatial(matrix) and not instance(matrix):  # cell clusters
+        axes = spatial(matrix)
+        with matrix.default_backend:
+            clusters_by_axis = np.round(np.asarray(axes.sizes) * (cluster_count / axes.volume) ** (1/axes.rank)).astype(np.int32)
+            cluster_count = int(np.prod(clusters_by_axis))
+            clusters_nd = math.meshgrid(axes) / axes * clusters_by_axis
+            clusters_nd = math.to_int32(clusters_nd)
+            clusters = math.reshaped_native(clusters_nd, [batch, spatial(matrix), 'vector'])
+            clusters = b0.ravel_multi_index(clusters, clusters_by_axis)
+    else:  # arbitrary clusters
+        assert cluster_hint is not None
+        raise NotImplementedError(f"Clustering currently only supported for grids but got matrix with shape {matrix.shape}")
+    # --- build preconditioner ---
+    if isinstance(matrix, CompressedSparseMatrix):
+        matrix = matrix.decompress()
+    if isinstance(matrix, SparseCoordinateTensor):
+        ind_batch, channels, indices, values, shape = matrix._native_coo_components(dual, matrix=True)
+        return coarse_explicit_preconditioner_coo(target_backend, indices, values, shape, clusters, cluster_count)
+    else:  # dense matrix
+        raise NotImplementedError
```

### Comparing `phiflow-2.3.4/phi/math/_shape.py` & `phiflow-2.4.0/phi/math/_shape.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,1799 +1,1873 @@
-import re
-import warnings
-from numbers import Number
-from typing import Tuple, Callable, List, Union, Any
-
-from phi import math
-
-
-BATCH_DIM = 'batch'
-SPATIAL_DIM = 'spatial'
-CHANNEL_DIM = 'channel'
-INSTANCE_DIM = 'înstance'
-DUAL_DIM = 'dual'
-
-TYPE_ABBR = {SPATIAL_DIM: "ˢ", CHANNEL_DIM: "ᶜ", INSTANCE_DIM: "ⁱ", BATCH_DIM: "ᵇ", DUAL_DIM: "ᵈ", None: "⁻"}  # ᵃᵇᶜᵈᵉᶠᵍʰⁱʲᵏˡᵐⁿᵒᵖʳˢᵗᵘᵛʷˣʸᶻ
-
-DEBUG_CHECKS = False
-
-
-def enable_debug_checks():
-    """
-    Once called, additional type checks are enabled.
-    This may result in a noticeable drop in performance.
-    """
-    global DEBUG_CHECKS
-    DEBUG_CHECKS = True
-
-
-class Shape:
-    """
-    Shapes enumerate dimensions, each consisting of a name, size and type.
-
-    There are five types of dimensions: `batch`, `dual`, `spatial`, `channel`, and `instance`.
-    """
-
-    def __init__(self, sizes: tuple, names: tuple, types: tuple, item_names: tuple):
-        """
-        To construct a `Shape`, use `batch`, `dual`, `spatial`, `channel` or `instance`, depending on the desired dimension type.
-        To create a shape with multiple types, use `merge_shapes()`, `concat_shapes()` or the syntax `shape1 & shape2`.
-
-        The `__init__` constructor is for internal use only.
-        """
-        if len(sizes) > 0 and any(s is not None and not isinstance(s, int) for s in sizes):
-            from ._tensors import Tensor
-            sizes = tuple([s if isinstance(s, Tensor) or s is None else int(s) for s in sizes])  # TODO replace this by an assert
-        self.sizes: tuple = sizes
-        """
-        Ordered dimension sizes as `tuple`.
-        The size of a dimension can be an `int` or a `Tensor` for [non-uniform shapes](https://tum-pbs.github.io/PhiFlow/Math.html#non-uniform-tensors).
-        
-        See Also:
-            `Shape.get_size()`, `Shape.size`, `Shape.shape`.
-        """
-        self.names: Tuple[str] = names
-        """
-        Ordered dimension names as `tuple[str]`.
-        
-        See Also:
-            `Shape.name`.
-        """
-        self.types: Tuple[str] = types  # undocumented, may be private
-        self.item_names: Tuple[Union[str, 'Shape']] = (None,) * len(sizes) if item_names is None else item_names  # undocumented
-        if DEBUG_CHECKS:
-            assert len(sizes) == len(names) == len(types) == len(item_names), f"sizes={sizes}, names={names}, types={types}, item_names={item_names}"
-            assert len(set(names)) == len(names), f"Duplicate dimension names: {names}"
-            assert all(isinstance(n, str) for n in names), f"All names must be of type string but got {names}"
-            assert isinstance(self.item_names, tuple)
-            assert all([items is None or isinstance(items, tuple) for items in self.item_names])
-            assert all([items is None or all([isinstance(n, str) for n in items]) for items in self.item_names])
-            from ._tensors import Tensor
-            for name, size in zip(names, sizes):
-                if isinstance(size, Tensor):
-                    assert size.rank > 0
-            for size, item_names in zip(self.sizes, self.item_names):
-                if item_names is not None:
-                    assert len(item_names) == size, f"Number of item names ({len(item_names)}) does not match size {size}"
-
-    def _check_is_valid_tensor_shape(self):
-        if DEBUG_CHECKS:
-            from ._tensors import Tensor
-            for name, size in zip(self.names, self.sizes):
-                if size is not None and isinstance(size, Tensor):
-                    assert size.rank > 0
-                    for dim in size.shape.names:
-                        assert dim in self.names, f"Dimension {name} varies along {dim} but {dim} is not part of the Shape {self}"
-
-    def _to_dict(self, include_sizes=True):
-        result = dict(names=self.names, types=self.types, item_names=self.item_names)
-        if include_sizes:
-            if not all([isinstance(s, int)] for s in self.sizes):
-                raise NotImplementedError()
-            result['sizes'] = self.sizes
-        return result
-
-    @staticmethod
-    def _from_dict(dict_: dict):
-        names = tuple(dict_['names'])
-        sizes = tuple(dict_['sizes']) if 'sizes' in dict_ else (None,) * len(names)
-        item_names = tuple([None if n is None else tuple(n) for n in dict_['item_names']])
-        return Shape(sizes, names, tuple(dict_['types']), item_names)
-
-    @property
-    def _named_sizes(self):
-        return zip(self.names, self.sizes)
-
-    @property
-    def _dimensions(self):
-        return zip(self.sizes, self.names, self.types, self.item_names)
-
-    @property
-    def untyped_dict(self):
-        """
-        Returns:
-            `dict` containing dimension names as keys.
-                The values are either the item names as `tuple` if available, otherwise the size.
-        """
-        return {name: self.get_item_names(i) or self.get_size(i) for i, name in enumerate(self.names)}
-
-    def __len__(self):
-        return len(self.sizes)
-
-    def __contains__(self, item):
-        if isinstance(item, (str, tuple, list)):
-            dims = parse_dim_order(item)
-            return all(dim in self.names for dim in dims)
-        elif isinstance(item, Shape):
-            return all([d in self.names for d in item.names])
-        else:
-            raise ValueError(item)
-
-    def isdisjoint(self, other: Union['Shape', tuple, list, str]):
-        """ Shapes are disjoint if all dimension names of one shape do not occur in the other shape. """
-        other = parse_dim_order(other)
-        return not any(dim in self.names for dim in other)
-
-    def __iter__(self):
-        return iter(self[i] for i in range(self.rank))
-
-    def index(self, dim: Union[str, 'Shape', None]) -> int:
-        """
-        Finds the index of the dimension within this `Shape`.
-
-        See Also:
-            `Shape.indices()`.
-
-        Args:
-            dim: Dimension name or single-dimension `Shape`.
-
-        Returns:
-            Index as `int`.
-        """
-        if dim is None:
-            return None
-        elif isinstance(dim, str):
-            if dim not in self.names:
-                raise ValueError(f"Shape {self} has no dimension '{dim}'")
-            return self.names.index(dim)
-        elif isinstance(dim, Shape):
-            assert dim.rank == 1, f"index() requires a single dimension as input but got {dim}. Use indices() for multiple dimensions."
-            return self.names.index(dim.name)
-        else:
-            raise ValueError(f"index() requires a single dimension as input but got {dim}")
-
-    def indices(self, dims: Union[tuple, list, 'Shape']) -> Tuple[int]:
-        """
-        Finds the indices of the given dimensions within this `Shape`.
-
-        See Also:
-            `Shape.index()`.
-
-        Args:
-            dims: Sequence of dimensions as `tuple`, `list` or `Shape`.
-
-        Returns:
-            Indices as `tuple[int]`.
-        """
-        if isinstance(dims, (list, tuple, set)):
-            return tuple([self.index(n) for n in dims])
-        elif isinstance(dims, Shape):
-            return tuple([self.index(n) for n in dims.names])
-        else:
-            raise ValueError(f"indices() requires a sequence of dimensions but got {dims}")
-
-    def get_size(self, dim: Union[str, 'Shape', int], default=None):
-        """
-        See Also:
-            `Shape.get_sizes()`, `Shape.size`
-
-        Args:
-            dim: Dimension, either as name `str` or single-dimension `Shape` or index `int`.
-            default: (Optional) If the dim does not exist, return this value instead of raising an error.
-
-        Returns:
-            Size associated with `dim` as `int` or `Tensor`.
-        """
-        if isinstance(dim, int):
-            assert default is None, "Cannot use a default value when passing an int for dim"
-            return self.sizes[dim]
-        if isinstance(dim, Shape):
-            assert dim.rank == 1, f"get_size() requires a single dimension but got {dim}. Use indices() to get multiple sizes."
-            dim = dim.name
-        if isinstance(dim, str):
-            if dim not in self.names:
-                if default is None:
-                    raise KeyError(f"get_size() failed because '{dim}' is not part of Shape {self} and no default value was provided")
-                else:
-                    return default
-            return self.sizes[self.names.index(dim)]
-        else:
-            raise ValueError(f"get_size() requires a single dimension but got {dim}. Use indices() to get multiple sizes.")
-
-    def get_sizes(self, dims: Union[tuple, list, 'Shape']) -> tuple:
-        """
-        See Also:
-            `Shape.get_size()`
-
-        Args:
-            dims: Dimensions as `tuple`, `list` or `Shape`.
-
-        Returns:
-            `tuple`
-        """
-        assert isinstance(dims, (tuple, list, Shape)), f"get_sizes() requires a sequence of dimensions but got {dims}"
-        return tuple([self.get_size(dim) for dim in dims])
-
-    def get_type(self, dim: Union[str, 'Shape']) -> str:
-        # undocumented, use get_dim_type() instead.
-        if isinstance(dim, str):
-            return self.types[self.names.index(dim)]
-        elif isinstance(dim, Shape):
-            assert dim.rank == 1, f"Shape.get_type() only accepts single-dimension Shapes but got {dim}"
-            return self.types[self.names.index(dim.name)]
-        else:
-            raise ValueError(dim)
-
-    def get_dim_type(self, dim: Union[str, 'Shape']) -> Callable:
-        """
-        Args:
-            dim: Dimension, either as name `str` or single-dimension `Shape`.
-
-        Returns:
-            Dimension type, one of `batch`, `spatial`, `instance`, `channel`.
-        """
-        return {BATCH_DIM: batch, SPATIAL_DIM: spatial, INSTANCE_DIM: instance, CHANNEL_DIM: channel}[self.get_type(dim)]
-
-    def get_types(self, dims: Union[tuple, list, 'Shape']) -> tuple:
-        # undocumented, do not use
-        if isinstance(dims, (tuple, list)):
-            return tuple(self.get_type(n) for n in dims)
-        elif isinstance(dims, Shape):
-            return tuple(self.get_type(n) for n in dims.names)
-        else:
-            raise ValueError(dims)
-
-    def get_item_names(self, dim: Union[str, 'Shape', int], fallback_spatial=False) -> Union[tuple, None]:
-        """
-        Args:
-            fallback_spatial: If `True` and no item names are defined for `dim` and `dim` is a channel dimension, the spatial dimension names are interpreted as item names along `dim` in the order they are listed in this `Shape`.
-            dim: Dimension, either as `int` index, `str` name or single-dimension `Shape`.
-
-        Returns:
-            Item names as `tuple` or `None` if not defined.
-        """
-        if isinstance(dim, int):
-            result = self.item_names[dim]
-        elif isinstance(dim, str):
-            result = self.item_names[self.index(dim)]
-        elif isinstance(dim, Shape):
-            assert dim.rank == 1, f"Shape.get_type() only accepts single-dimension Shapes but got {dim}"
-            result = self.item_names[self.names.index(dim.name)]
-        else:
-            raise ValueError(dim)
-        if result is not None:
-            return result
-        elif fallback_spatial and self.spatial_rank == self.get_size(dim) and self.get_type(dim) == CHANNEL_DIM:
-            return self.spatial.names
-        else:
-            return None
-
-    def flipped(self, dims: Union[List[str], Tuple[str]]):
-        item_names = list(self.item_names)
-        for dim in dims:
-            if dim in self.names:
-                dim_i_n = self.get_item_names(dim)
-                if dim_i_n is not None:
-                    item_names[self.index(dim)] = tuple(reversed(dim_i_n))
-        return Shape(self.sizes, self.names, self.types, tuple(item_names))
-
-    def __getitem__(self, selection):
-        if isinstance(selection, int):
-            return Shape((self.sizes[selection],), (self.names[selection],), (self.types[selection],), (self.item_names[selection],))
-        elif isinstance(selection, slice):
-            return Shape(self.sizes[selection], self.names[selection], self.types[selection], self.item_names[selection])
-        elif isinstance(selection, str):
-            if ',' in selection:
-                selection = [self.index(s.strip()) for s in selection.split(',')]
-            else:
-                selection = self.index(selection)
-            return self[selection]
-        elif isinstance(selection, (tuple, list)):
-            selection = [self.index(s) if isinstance(s, str) else s for s in selection]
-            return Shape(tuple([self.sizes[i] for i in selection]), tuple([self.names[i] for i in selection]), tuple([self.types[i] for i in selection]), tuple([self.item_names[i] for i in selection]))
-        raise AssertionError("Can only access shape elements as shape[int] or shape[slice]")
-
-    @property
-    def reversed(self):
-        return Shape(tuple(reversed(self.sizes)), tuple(reversed(self.names)), tuple(reversed(self.types)), tuple(reversed(self.item_names)))
-
-    @property
-    def batch(self) -> 'Shape':
-        """
-        Filters this shape, returning only the batch dimensions as a new `Shape` object.
-
-        See also:
-            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.
-
-        Returns:
-            New `Shape` object
-        """
-        return self[[i for i, t in enumerate(self.types) if t == BATCH_DIM]]
-
-    @property
-    def non_batch(self) -> 'Shape':
-        """
-        Filters this shape, returning only the non-batch dimensions as a new `Shape` object.
-
-        See also:
-            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.
-
-        Returns:
-            New `Shape` object
-        """
-        return self[[i for i, t in enumerate(self.types) if t != BATCH_DIM]]
-
-    @property
-    def spatial(self) -> 'Shape':
-        """
-        Filters this shape, returning only the spatial dimensions as a new `Shape` object.
-
-        See also:
-            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.
-
-        Returns:
-            New `Shape` object
-        """
-        return self[[i for i, t in enumerate(self.types) if t == SPATIAL_DIM]]
-
-    @property
-    def non_spatial(self) -> 'Shape':
-        """
-        Filters this shape, returning only the non-spatial dimensions as a new `Shape` object.
-
-        See also:
-            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.
-
-        Returns:
-            New `Shape` object
-        """
-        return self[[i for i, t in enumerate(self.types) if t != SPATIAL_DIM]]
-
-    @property
-    def instance(self) -> 'Shape':
-        """
-        Filters this shape, returning only the instance dimensions as a new `Shape` object.
-
-        See also:
-            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.
-
-        Returns:
-            New `Shape` object
-        """
-        return self[[i for i, t in enumerate(self.types) if t == INSTANCE_DIM]]
-
-    @property
-    def non_instance(self) -> 'Shape':
-        """
-        Filters this shape, returning only the non-instance dimensions as a new `Shape` object.
-
-        See also:
-            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.
-
-        Returns:
-            New `Shape` object
-        """
-        return self[[i for i, t in enumerate(self.types) if t != INSTANCE_DIM]]
-
-    @property
-    def channel(self) -> 'Shape':
-        """
-        Filters this shape, returning only the channel dimensions as a new `Shape` object.
-
-        See also:
-            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.
-
-        Returns:
-            New `Shape` object
-        """
-        return self[[i for i, t in enumerate(self.types) if t == CHANNEL_DIM]]
-
-    @property
-    def non_channel(self) -> 'Shape':
-        """
-        Filters this shape, returning only the non-channel dimensions as a new `Shape` object.
-
-        See also:
-            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.
-
-        Returns:
-            New `Shape` object
-        """
-        return self[[i for i, t in enumerate(self.types) if t != CHANNEL_DIM]]
-
-    @property
-    def dual(self) -> 'Shape':
-        """
-        Filters this shape, returning only the dual dimensions as a new `Shape` object.
-
-        See also:
-            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.
-
-        Returns:
-            New `Shape` object
-        """
-        return self[[i for i, t in enumerate(self.types) if t == DUAL_DIM]]
-
-    @property
-    def non_dual(self) -> 'Shape':
-        """
-        Filters this shape, returning only the non-dual dimensions as a new `Shape` object.
-
-        See also:
-            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.
-
-        Returns:
-            New `Shape` object
-        """
-        return self[[i for i, t in enumerate(self.types) if t != DUAL_DIM]]
-
-    @property
-    def non_singleton(self) -> 'Shape':
-        """
-        Filters this shape, returning only non-singleton dimensions as a new `Shape` object.
-        Dimensions are singleton if their size is exactly `1`.
-
-        Returns:
-            New `Shape` object
-        """
-        return self[[i for i, s in enumerate(self.sizes) if not _size_equal(s, 1)]]
-
-    @property
-    def singleton(self) -> 'Shape':
-        """
-        Filters this shape, returning only singleton dimensions as a new `Shape` object.
-        Dimensions are singleton if their size is exactly `1`.
-
-        Returns:
-            New `Shape` object
-        """
-        return self[[i for i, s in enumerate(self.sizes) if _size_equal(s, 1)]]
-
-    def unstack(self, dim='dims') -> Tuple['Shape']:
-        """
-        Slices this `Shape` along a dimension.
-        The dimension listing the sizes of the shape is referred to as `'dims'`.
-
-        Non-uniform tensor shapes may be unstacked along other dimensions as well, see
-        https://tum-pbs.github.io/PhiFlow/Math.html#non-uniform-tensors
-
-        Args:
-            dim: dimension to unstack
-
-        Returns:
-            slices of this shape
-        """
-        if dim == 'dims':
-            return tuple(Shape((self.sizes[i],), (self.names[i],), (self.types[i],), (self.item_names[i],)) for i in range(self.rank))
-        if dim not in self and self.is_uniform:
-            return tuple([self])
-        from ._tensors import Tensor
-        if dim in self:
-            inner = self.without(dim)
-            dim_size = self.get_size(dim)
-        else:
-            inner = self
-            dim_size = self.shape.get_size(dim)
-        sizes = []
-        for size in inner.sizes:
-            if isinstance(size, Tensor) and dim in size.shape:
-                sizes.append(size.unstack(dim))
-                dim_size = size.shape.get_size(dim)
-            else:
-                sizes.append(size)
-        assert isinstance(dim_size, int)
-        shapes = tuple(Shape(tuple([int(size[i]) if isinstance(size, tuple) else size for size in sizes]), inner.names, inner.types, inner.item_names) for i in range(dim_size))
-        return shapes
-
-    @property
-    def name(self) -> str:
-        """
-        Only for Shapes containing exactly one single dimension.
-        Returns the name of the dimension.
-
-        See Also:
-            `Shape.names`.
-        """
-        assert self.rank == 1, f"Shape.name is only defined for shapes of rank 1. shape={self}"
-        return self.names[0]
-
-    @property
-    def size(self) -> int:
-        """
-        Only for Shapes containing exactly one single dimension.
-        Returns the size of the dimension.
-
-        See Also:
-            `Shape.sizes`, `Shape.get_size()`.
-        """
-        assert self.rank == 1, "Shape.size is only defined for shapes of rank 1."
-        return self.sizes[0]
-
-    @property
-    def type(self) -> int:
-        """
-        Only for Shapes containing exactly one single dimension.
-        Returns the type of the dimension.
-
-        See Also:
-            `Shape.get_type()`.
-        """
-        assert self.rank == 1, "Shape.type is only defined for shapes of rank 1."
-        return self.types[0]
-
-    def __int__(self):
-        assert self.rank == 1, "int(Shape) is only defined for shapes of rank 1."
-        return self.sizes[0]
-
-    def mask(self, names: Union[tuple, list, set, 'Shape']):
-        """
-        Returns a binary sequence corresponding to the names of this Shape.
-        A value of 1 means that a dimension of this Shape is contained in `names`.
-
-        Args:
-          names: instance of dimension
-          names: tuple or list or set: 
-
-        Returns:
-          binary sequence
-
-        """
-        if isinstance(names, str):
-            names = [names]
-        elif isinstance(names, Shape):
-            names = names.names
-        mask = [1 if name in names else 0 for name in self.names]
-        return tuple(mask)
-
-    def __repr__(self):
-        def size_repr(size, items):
-            if items is not None:
-                if len(items) <= 4:
-                    return ",".join(items)
-                else:
-                    return f"{size}:{items[0]}..{items[-1]}"
-            else:
-                return size
-
-        strings = [f"{name}{TYPE_ABBR.get(dim_type, '?')}={size_repr(size, items)}" for size, name, dim_type, items in self._dimensions]
-        return '(' + ', '.join(strings) + ')'
-
-    def __eq__(self, other):
-        if not isinstance(other, Shape):
-            return False
-        if self.names != other.names or self.types != other.types:
-            return False
-        for size1, size2 in zip(self.sizes, other.sizes):
-            equal = size1 == size2
-            assert isinstance(equal, (bool, math.Tensor))
-            if isinstance(equal, math.Tensor):
-                equal = equal.all
-            if not equal:
-                return False
-        for names1, names2 in zip(self.item_names, other.item_names):
-            if names1 != names2:
-                return False
-        return True
-
-    def __ne__(self, other):
-        return not self == other
-
-    def __bool__(self):
-        return self.rank > 0
-
-    def _reorder(self, names: Union[tuple, list, 'Shape']) -> 'Shape':
-        assert len(names) == self.rank
-        if isinstance(names, Shape):
-            names = names.names
-        order = [self.index(n) for n in names]
-        return self[order]
-
-    def _order_group(self, names: Union[tuple, list, 'Shape']) -> list:
-        """ Reorders the dimensions of this `Shape` so that `names` are clustered together and occur in the specified order. """
-        if isinstance(names, Shape):
-            names = names.names
-        result = []
-        for dim in self.names:
-            if dim not in result:
-                if dim in names:
-                    result.extend(names)
-                else:
-                    result.append(dim)
-        return result
-
-    def __and__(self, other):
-        return merge_shapes(self, other)
-
-    def _expand(self, dim: 'Shape', pos=None) -> 'Shape':
-        """**Deprecated.** Use `phi.math.merge_shapes()` or `phi.math.concat_shapes()` instead. """
-        warnings.warn("Shape.expand() is deprecated. Use merge_shapes() or concat_shapes() instead.", DeprecationWarning)
-        if not dim:
-            return self
-        assert dim.name not in self, f"Cannot expand shape {self} by {dim} because dimension already exists."
-        assert isinstance(dim, Shape) and dim.rank == 1, f"Shape.expand() requires a single dimension as a Shape but got {dim}"
-        if pos is None:
-            same_type_dims = self[[i for i, t in enumerate(self.types) if t == dim.type]]
-            if len(same_type_dims) > 0:
-                pos = self.index(same_type_dims.names[0])
-            else:
-                pos = {BATCH_DIM: 0, INSTANCE_DIM: self.batch_rank, SPATIAL_DIM: self.batch.rank + self.instance_rank, CHANNEL_DIM: self.rank + 1}[dim.type]
-        elif pos < 0:
-            pos += self.rank + 1
-        sizes = list(self.sizes)
-        names = list(self.names)
-        types = list(self.types)
-        item_names = list(self.item_names)
-        sizes.insert(pos, dim.size)
-        names.insert(pos, dim.name)
-        types.insert(pos, dim.type)
-        item_names.insert(pos, dim.item_names[0])
-        return Shape(tuple(sizes), tuple(names), tuple(types), tuple(item_names))
-
-    def without(self, dims: 'DimFilter') -> 'Shape':
-        """
-        Builds a new shape from this one that is missing all given dimensions.
-        Dimensions in `dims` that are not part of this Shape are ignored.
-        
-        The complementary operation is `Shape.only()`.
-
-        Args:
-          dims: Single dimension (str) or instance of dimensions (tuple, list, Shape)
-          dims: Dimensions to exclude as `str` or `tuple` or `list` or `Shape`. Dimensions that are not included in this shape are ignored.
-
-        Returns:
-          Shape without specified dimensions
-        """
-        if callable(dims):
-            dims = dims(self)
-        if isinstance(dims, str):
-            dims = parse_dim_order(dims)
-        if isinstance(dims, (tuple, list, set)):
-            return self[[i for i in range(self.rank) if self.names[i] not in dims]]
-        elif isinstance(dims, Shape):
-            return self[[i for i in range(self.rank) if self.names[i] not in dims.names]]
-        elif dims is None:  # subtract none
-            return self
-        else:
-            raise ValueError(dims)
-
-    def only(self, dims: 'DimFilter', reorder=False):
-        """
-        Builds a new shape from this one that only contains the given dimensions.
-        Dimensions in `dims` that are not part of this Shape are ignored.
-        
-        The complementary operation is :func:`Shape.without`.
-
-        Args:
-          dims: comma-separated dimension names (str) or instance of dimensions (tuple, list, Shape) or filter function.
-          reorder: If `False`, keeps the dimension order as defined in this shape.
-            If `True`, reorders the dimensions of this shape to match the order of `dims`.
-
-        Returns:
-          Shape containing only specified dimensions
-
-        """
-        if dims is None:  # keep none
-            return EMPTY_SHAPE
-        if callable(dims):
-            dims = dims(self)
-        if isinstance(dims, str):
-            dims = parse_dim_order(dims)
-        if isinstance(dims, Shape):
-            dims = dims.names
-        if not isinstance(dims, (tuple, list, set)):
-            raise ValueError(dims)
-        if reorder:
-            return self[[self.names.index(d) for d in dims if d in self.names]]
-        else:
-            return self[[i for i in range(self.rank) if self.names[i] in dims]]
-
-    @property
-    def rank(self) -> int:
-        """
-        Returns the number of dimensions.
-        Equal to `len(shape)`.
-
-        See `Shape.is_empty`, `Shape.batch_rank`, `Shape.spatial_rank`, `Shape.channel_rank`.
-        """
-        return len(self.sizes)
-
-    @property
-    def batch_rank(self) -> int:
-        """ Number of batch dimensions """
-        return sum([1 for ty in self.types if ty == BATCH_DIM])
-
-    @property
-    def instance_rank(self) -> int:
-        return sum([1 for ty in self.types if ty == INSTANCE_DIM])
-
-    @property
-    def spatial_rank(self) -> int:
-        """ Number of spatial dimensions """
-        return sum([1 for ty in self.types if ty == SPATIAL_DIM])
-
-    @property
-    def dual_rank(self) -> int:
-        """ Number of spatial dimensions """
-        return sum([1 for ty in self.types if ty == DUAL_DIM])
-
-    @property
-    def channel_rank(self) -> int:
-        """ Number of channel dimensions """
-        return sum([1 for ty in self.types if ty == CHANNEL_DIM])
-
-    @property
-    def well_defined(self):
-        """
-        Returns `True` if no dimension size is `None`.
-
-        Shapes with undefined sizes may be used in `phi.math.tensor()`, `phi.math.wrap()`, `phi.math.stack()` or `phi.math.concat()`.
-
-        To create an undefined size, call a constructor function (`batch()`, `spatial()`, `channel()`, `instance()`)
-        with positional `str` arguments, e.g. `spatial('x')`.
-        """
-        for size in self.sizes:
-            if size is None:
-                return False
-        return True
-
-    @property
-    def shape(self) -> 'Shape':
-        """
-        Higher-order `Shape`.
-        The returned shape will always contain the channel dimension `dims` with a size equal to the `Shape.rank` of this shape.
-
-        For uniform shapes, `Shape.shape` will only contain the dimension `dims` but the shapes of [non-uniform shapes](https://tum-pbs.github.io/PhiFlow/Math.html#non-uniform-tensors)
-        may contain additional dimensions.
-
-        See Also:
-            `Shape.is_uniform`.
-
-        Returns:
-            `Shape`.
-        """
-        from phi.math import Tensor
-        shape = Shape((self.rank,), ('dims',), (CHANNEL_DIM,), (self.names,))
-        for size in self.sizes:
-            if isinstance(size, Tensor):
-                shape = shape & size.shape
-        return shape
-
-    @property
-    def is_uniform(self) -> bool:
-        """
-        A shape is uniform if it all sizes have a single integer value.
-
-        See Also:
-            `Shape.is_non_uniform`, `Shape.shape`.
-        """
-        return all(isinstance(s, int) for s in self.sizes)
-
-    @property
-    def is_non_uniform(self) -> bool:
-        """
-        A shape is non-uniform if the size of any dimension varies along another dimension.
-
-        See Also:
-            `Shape.is_uniform`, `Shape.shape`.
-        """
-        return not self.is_uniform
-
-    @property
-    def non_uniform(self) -> 'Shape':
-        """
-        Returns only the non-uniform dimensions of this shape, i.e. the dimensions whose size varies along another dimension.
-        """
-        from phi.math import Tensor
-        indices = [i for i, size in enumerate(self.sizes) if isinstance(size, Tensor) and size.rank > 0]
-        return self[indices]
-
-    def with_size(self, size: Union[int, None]):
-        """
-        Only for single-dimension shapes.
-        Returns a `Shape` representing this dimension but with a different size.
-
-        See Also:
-            `Shape.with_sizes()`.
-
-        Args:
-            size: Replacement size for this dimension.
-
-        Returns:
-            `Shape`
-        """
-        assert self.rank == 1, "Shape.with_size() is only defined for shapes of rank 1."
-        return self.with_sizes([size])
-
-    def with_sizes(self, sizes: Union[tuple, list, 'Shape', int], keep_item_names=True):
-        """
-        Returns a new `Shape` matching the dimension names and types of `self` but with different sizes.
-
-        See Also:
-            `Shape.with_size()`.
-
-        Args:
-            sizes: One of
-
-                * `tuple` / `list` of same length as `self` containing replacement sizes.
-                * `Shape` of any rank. Replaces sizes for dimensions shared by `sizes` and `self`.
-
-            keep_item_names: If `False`, forgets all item names.
-                If `True`, keeps item names where the size does not change.
-
-        Returns:
-            `Shape` with same names and types as `self`.
-        """
-        if isinstance(sizes, int):
-            sizes = [sizes] * len(self.sizes)
-        if isinstance(sizes, Shape):
-            item_names = [sizes.get_item_names(dim) if dim in sizes else self.get_item_names(dim) for dim in self.names]
-            sizes = [sizes.get_size(dim) if dim in sizes else s for dim, s in self._named_sizes]
-            return Shape(tuple(sizes), self.names, self.types, tuple(item_names))
-        else:
-            assert len(sizes) == len(self.sizes), f"Cannot create shape from {self} with sizes {sizes}"
-            sizes_ = []
-            item_names = []
-            for i, obj in enumerate(sizes):
-                new_size, new_item_names = Shape._size_and_item_names_from_obj(obj, self.sizes[i], self.item_names[i], keep_item_names)
-                sizes_.append(new_size)
-                item_names.append(new_item_names)
-            return Shape(tuple(sizes_), self.names, self.types, tuple(item_names))
-
-    @staticmethod
-    def _size_and_item_names_from_obj(obj, prev_size, prev_item_names, keep_item_names=True):
-        if isinstance(obj, str):
-            obj = [s.strip() for s in obj.split(',')]
-        if isinstance(obj, (tuple, list)):
-            return len(obj), tuple(obj)
-        elif isinstance(obj, Number):
-            return obj, prev_item_names if keep_item_names and (prev_size is None or _size_equal(obj, prev_size)) else None
-        elif isinstance(obj, math.Tensor) or obj is None:
-            return obj, None
-        else:
-            raise ValueError(f"sizes can only contain int, str or Tensor but got {type(obj)}")
-
-    def without_sizes(self):
-        """
-        Returns:
-            `Shape` with all sizes undefined (`None`)
-        """
-        return Shape((None,) * self.rank, self.names, self.types, (None,) * self.rank)
-
-    def _replace_single_size(self, dim: str, size: int, keep_item_names: bool = False):
-        new_sizes = list(self.sizes)
-        new_sizes[self.index(dim)] = size
-        return self.with_sizes(new_sizes, keep_item_names=keep_item_names)
-
-    def with_dim_size(self, dim: Union[str, 'Shape'], size: Union[int, 'math.Tensor', str, tuple, list], keep_item_names=True):
-        """
-        Returns a new `Shape` that has a different size for `dim`.
-
-        Args:
-            dim: Dimension for which to replace the size, `Shape` or `str`.
-            size: New size, `int` or `Tensor`
-
-        Returns:
-            `Shape` with same names and types as `self`.
-        """
-        if isinstance(dim, Shape):
-            dim = dim.name
-        assert isinstance(dim, str)
-        new_size, new_item_names = Shape._size_and_item_names_from_obj(size, self.get_size(dim), self.get_item_names(dim), keep_item_names)
-        return self.replace(dim, Shape((new_size,), (dim,), (self.get_type(dim),), (new_item_names,)))
-
-    def _with_names(self, names: Union[str, tuple, list]):
-        if isinstance(names, str):
-            names = parse_dim_names(names, self.rank)
-            names = [n if n is not None else o for n, o in zip(names, self.names)]
-        return Shape(self.sizes, tuple(names), self.types, self.item_names)
-
-    def _replace_names_and_types(self,
-                                 dims: Union['Shape', str, tuple, list],
-                                 new: Union['Shape', str, tuple, list]) -> 'Shape':
-        """
-        Returns a copy of `self` with `dims` replaced by `new`.
-        Dimensions that are not present in `self` are ignored.
-
-        The dimension order is preserved.
-
-        Args:
-            dims: Dimensions to replace.
-            new: New dimensions, must have same length as `dims`.
-                If a `Shape` is given, replaces the dimension types and item names as well.
-
-        Returns:
-            `Shape` with same rank and dimension order as `self`.
-        """
-        dims = parse_dim_order(dims)
-        sizes = [math.rename_dims(s, dims, new) if isinstance(s, math.Tensor) else s for s in self.sizes]
-        new = parse_dim_order(new) if isinstance(new, str) else new
-        names = list(self.names)
-        types = list(self.types)
-        item_names = list(self.item_names)
-        for old_name, new_dim in zip(dims, new):
-            if old_name in self:
-                if isinstance(new_dim, Shape):
-                    names[self.index(old_name)] = new_dim.name
-                    types[self.index(old_name)] = new_dim.type
-                    item_names[self.index(old_name)] = new_dim.item_names[0]
-                else:
-                    names[self.index(old_name)] = new_dim
-        return Shape(tuple(sizes), tuple(names), tuple(types), tuple(item_names))
-
-    def replace(self, dims: Union['Shape', str, tuple, list], new: 'Shape') -> 'Shape':
-        """
-        Returns a copy of `self` with `dims` replaced by `new`.
-        Dimensions that are not present in `self` are ignored.
-
-        The dimension order is preserved.
-
-        Args:
-            dims: Dimensions to replace.
-            new: New dimensions, must have same length as `dims`.
-                If a `Shape` is given, replaces the dimension types and item names as well.
-
-        Returns:
-            `Shape` with same rank and dimension order as `self`.
-        """
-        dims = parse_dim_order(dims)
-        assert isinstance(new, Shape), f"new must be a Shape but got {new}"
-        names = list(self.names)
-        sizes = list(self.sizes)
-        types = list(self.types)
-        item_names = list(self.item_names)
-        if len(new) > len(dims):  # Put all in one spot
-            assert len(dims) == 1, "Cannot replace 2+ dims by more replacements"
-            index = self.index(dims[0])
-            return concat_shapes(self[:index], new, self[index+1:])
-        for old_name, new_dim in zip(dims, new):
-            if old_name in self:
-                names[self.index(old_name)] = new_dim.name
-                types[self.index(old_name)] = new_dim.type
-                item_names[self.index(old_name)] = new_dim.item_names[0]
-                sizes[self.index(old_name)] = new_dim.size
-        replaced = Shape(tuple(sizes), tuple(names), tuple(types), tuple(item_names))
-        if len(new) == len(dims):
-            return replaced
-        to_remove = dims[-(len(dims) - len(new)):]
-        return replaced.without(to_remove)
-
-    def _with_types(self, types: Union['Shape', str]):
-        """
-        Only for internal use.
-        Note: This method does not rename dimensions to comply with type requirements (e.g. ~ for dual dims).
-        """
-        if isinstance(types, Shape):
-            return Shape(self.sizes, self.names, tuple([types.get_type(name) if name in types else self_type for name, self_type in zip(self.names, self.types)]), self.item_names)
-        elif isinstance(types, str):
-            return Shape(self.sizes, self.names, (types,) * self.rank, self.item_names)
-        else:
-            raise ValueError(types)
-
-    def _with_item_names(self, item_names: tuple):
-        return Shape(self.sizes, self.names, self.types, item_names)
-
-    def _with_item_name(self, dim: str, item_name: tuple):
-        if dim not in self:
-            return self
-        item_names = list(self.item_names)
-        item_names[self.index(dim)] = item_name
-        return Shape(self.sizes, self.names, self.types, tuple(item_names))
-
-    def _perm(self, names: Tuple[str]):
-        assert len(set(names)) == len(names), f"No duplicates allowed but got {names}"
-        assert len(names) >= len(self.names), f"Cannot find permutation for {self} given {names} because names {set(self.names) - set(names)} are missing"
-        assert len(names) <= len(self.names), f"Cannot find permutation for {self} given {names} because too many names were passed: {names}"
-        perm = [self.names.index(name) for name in names]
-        return perm
-
-    @property
-    def volume(self) -> Union[int, None]:
-        """
-        Returns the total number of values contained in a tensor of this shape.
-        This is the product of all dimension sizes.
-
-        Returns:
-            volume as `int` or `Tensor` or `None` if the shape is not `Shape.well_defined`
-        """
-        from phi.math import Tensor
-        for dim, size in self._named_sizes:
-            if isinstance(size, Tensor) and size.rank > 0:
-                non_uniform_dim = size.shape.names[0]
-                shapes = self.unstack(non_uniform_dim)
-                return sum(s.volume for s in shapes)
-        result = 1
-        for size in self.sizes:
-            if size is None:
-                return None
-            result *= size
-        return int(result)
-
-    @property
-    def is_empty(self) -> bool:
-        """ True if this shape has no dimensions. Equivalent to `Shape.rank` `== 0`. """
-        return len(self.sizes) == 0
-
-    def after_pad(self, widths: dict) -> 'Shape':
-        sizes = list(self.sizes)
-        item_names = list(self.item_names)
-        for dim, (lo, up) in widths.items():
-            sizes[self.index(dim)] += lo + up
-            item_names[self.index(dim)] = None
-        return Shape(tuple(sizes), self.names, self.types, tuple(item_names))
-
-    def prepare_gather(self, dim: str, selection):
-        if isinstance(selection, Shape):
-            selection = selection.name if selection.rank == 1 else selection.names
-        if isinstance(selection, str) and ',' in selection:
-            selection = parse_dim_order(selection)
-        if isinstance(selection, str):  # single item name
-            item_names = self.get_item_names(dim, fallback_spatial=True)
-            assert item_names is not None, f"No item names defined for dim '{dim}' in tensor {self.shape} and dimension size does not match spatial rank."
-            assert selection in item_names, f"Accessing tensor.{dim}['{selection}'] failed. Item names are {item_names}."
-            selection = item_names.index(selection)
-        if isinstance(selection, (tuple, list)):
-            selection = list(selection)
-            if any([isinstance(s, str) for s in selection]):
-                item_names = self.get_item_names(dim, fallback_spatial=True)
-                for i, s in enumerate(selection):
-                    if isinstance(s, str):
-                        assert item_names is not None, f"Accessing tensor.{dim}['{s}'] failed because no item names are present on tensor {self.shape}"
-                        assert s in item_names, f"Accessing tensor.{dim}['{s}'] failed. Item names are {item_names}."
-                        selection[i] = item_names.index(s)
-            if not selection:  # empty
-                selection = slice(0, 0)
-        return selection
-
-    def after_gather(self, selection: dict) -> 'Shape':
-        result = self
-        for sel_dim, selection in selection.items():
-            if sel_dim not in self.names:
-                continue
-            selection = self.prepare_gather(sel_dim, selection)
-            if isinstance(selection, int):
-                if result.is_uniform:
-                    result = result.without(sel_dim)
-                else:
-                    from phi.math import Tensor
-                    gathered_sizes = [(s[{sel_dim: selection}] if isinstance(s, Tensor) else s) for s in result.sizes]
-                    gathered_sizes = [(int(s) if isinstance(s, Tensor) and s.rank == 0 else s) for s in gathered_sizes]
-                    result = result.with_sizes(gathered_sizes, keep_item_names=True).without(sel_dim)
-            elif isinstance(selection, slice):
-                assert isinstance(selection.step, int) or selection.step is None, f"slice step must be an int or None but got {type(selection.step).__name__}"
-                assert isinstance(selection.start, int) or selection.start is None, f"slice start must be an int or None but got {type(selection.start).__name__}"
-                assert isinstance(selection.stop, int) or selection.stop is None, f"slice stop must be an int or None but got {type(selection.stop).__name__}"
-                step = selection.step or 1
-                start = selection.start if isinstance(selection.start, int) else (0 if step > 0 else self.get_size(sel_dim)-1)
-                stop = selection.stop if isinstance(selection.stop, int) else (self.get_size(sel_dim) if step > 0 else -1)
-                if stop < 0 and step > 0:
-                    stop += self.get_size(sel_dim)
-                    assert stop >= 0
-                if start < 0 and step > 0:
-                    start += self.get_size(sel_dim)
-                    assert start >= 0
-                stop = min(stop, self.get_size(sel_dim))
-                new_size = math.to_int64(math.ceil(math.wrap((stop - start) / step)))
-                if new_size.rank == 0:
-                    new_size = int(new_size)  # NumPy array not allowed because not hashable
-                result = result._replace_single_size(sel_dim, new_size, keep_item_names=True)
-                if step < 0:
-                    result = result.flipped([sel_dim])
-                if self.get_item_names(sel_dim) is not None:
-                    result = result._with_item_name(sel_dim, tuple(self.get_item_names(sel_dim)[selection]))
-            elif isinstance(selection, (tuple, list)):
-                result = result._replace_single_size(sel_dim, len(selection))
-                if self.get_item_names(sel_dim) is not None:
-                    result = result._with_item_name(sel_dim, tuple([self.get_item_names(sel_dim)[i] for i in selection]))
-            else:
-                raise NotImplementedError(f"{type(selection)} not supported. Only (int, slice) allowed.")
-        return result
-
-    def meshgrid(self, names=False):
-        """
-        Builds a sequence containing all multi-indices within a tensor of this shape.
-        All indices are returned as `dict` mapping dimension names to `int` indices.
-
-        The corresponding values can be retrieved from Tensors and other Sliceables using `tensor[index]`.
-
-        This function currently only supports uniform tensors.
-
-        Args:
-            names: If `True`, replace indices by their item names if available.
-
-        Returns:
-            `dict` iterator.
-        """
-        assert self.is_uniform, f"Shape.meshgrid() is currently not supported for non-uniform tensors, {self}"
-        indices = [0] * self.rank
-        while True:
-            if names:
-                yield {dim: (names[index] if names is not None else index) for dim, index, names in zip(self.names, indices, self.item_names)}
-            else:
-                yield {dim: index for dim, index in zip(self.names, indices)}
-            for i in range(self.rank-1, -1, -1):
-                indices[i] = (indices[i] + 1) % self.sizes[i]
-                if indices[i] != 0:
-                    break
-            else:
-                return
-
-    def first_index(self, names=False):
-        return next(iter(self.meshgrid(names=names)))
-
-    def are_adjacent(self, dims: Union[str, tuple, list, set, 'Shape']):
-        indices = self.indices(dims)
-        return (max(indices) - min(indices)) == len(dims) - 1
-
-    def __add__(self, other):
-        return self._op2(other, lambda s, o: s + o, 0)
-
-    def __radd__(self, other):
-        return self._op2(other, lambda s, o: o + s, 0)
-
-    def __sub__(self, other):
-        return self._op2(other, lambda s, o: s - o, 0)
-
-    def __rsub__(self, other):
-        return self._op2(other, lambda s, o: o - s, 0)
-
-    def __mul__(self, other):
-        return self._op2(other, lambda s, o: s * o, 1)
-
-    def __rmul__(self, other):
-        return self._op2(other, lambda s, o: o * s, 1)
-
-    def _op2(self, other, fun, default: int):
-        if isinstance(other, int):
-            return Shape(tuple([fun(s, other) for s in self.sizes]), self.names, self.types, (None,) * self.rank)
-        elif isinstance(other, Shape):
-            merged = self.without_sizes() & other.without_sizes()
-            sizes = ()
-            for dim in merged.names:
-                self_val = self.get_size(dim) if dim in self else default
-                other_val = other.get_size(dim) if dim in other else default
-                sizes += (fun(self_val, other_val),)
-            return merged.with_sizes(sizes)
-        else:
-            return NotImplemented
-
-    def __hash__(self):
-        return hash(self.names)
-
-
-EMPTY_SHAPE = Shape((), (), (), ())
-""" Empty shape, `()` """
-
-DimFilter = Union[str, tuple, list, set, Shape, Callable]
-try:
-    DimFilter.__doc__ = """Dimension filters can be used with `Shape.only()` and `Shype.without()`, making them the standard tool for specifying sets of dimensions.
-    
-    The following types can be used as dimension filters:
-    
-    * `Shape` instances
-    * `tuple` or `list` objects containing dimension names as `str`
-    * Single `str` listing comma-separated dimension names
-    * Any function `filter(Shape) -> Shape`, such as `math.batch()`, `math.non_batch()`, `math.spatial()`, etc.
-    """  # docstring must be set explicitly
-except AttributeError:  # on older Python versions, this is not possible
-    pass
-
-
-class IncompatibleShapes(Exception):
-    """
-    Raised when the shape of a tensor does not match the other arguments.
-    """
-    def __init__(self, message, *shapes: Shape):
-        Exception.__init__(self, message)
-        self.shapes = shapes
-
-
-def parse_dim_names(obj: Union[str, tuple, list, Shape], count: int) -> tuple:
-    if isinstance(obj, str):
-        parts = obj.split(',')
-        result = []
-        for part in parts:
-            part = part.strip()
-            if part == '...':
-                result.extend([None] * (count - len(parts) + 1))
-            elif part == ':':
-                result.append(None)
-            else:
-                result.append(part)
-        assert len(result) == count, f"Number of specified names in '{obj}' does not match number of dimensions ({count})"
-        return tuple(result)
-    elif isinstance(obj, Shape):
-        assert len(obj) == count, f"Number of specified names in {obj} does not match number of dimensions ({count})"
-        return obj.names
-    elif isinstance(obj, (tuple, list)):
-        assert len(obj) == count, f"Number of specified names in {obj} does not match number of dimensions ({count})"
-        return tuple(obj)
-    raise ValueError(obj)
-
-
-def parse_dim_order(order: Union[str, tuple, list, Shape, None], check_rank: int = None) -> Union[tuple, None]:
-    if order is None:
-        if check_rank is not None:
-            assert check_rank <= 1, "When calling Tensor.native() or Tensor.numpy(), the dimension order must be specified for Tensors with more than one dimension. The listed default dimension order can vary depending on the chosen backend. Consider using math.reshaped_native(Tensor) instead."
-        return None
-    elif isinstance(order, Shape):
-        return order.names
-    if isinstance(order, list):
-        return tuple(order)
-    elif isinstance(order, tuple):
-        return order
-    elif isinstance(order, str):
-        parts = order.split(',')
-        parts = [p.strip() for p in parts if p]
-        return tuple(parts)
-    raise ValueError(order)
-
-
-def _construct_shape(dim_type: str, prefix: str, *args, **dims):
-    sizes = ()
-    names = []
-    item_names = ()
-    for arg in args:
-        parts = [s.strip() for s in arg.split(',')]
-        for name in parts:
-            assert name not in names, f"Duplicate dimension name {name}"
-            sizes += (None,)
-            names.append(name)
-            item_names += (None,)
-    for name, size in dims.items():
-        assert name not in names, f"Duplicate dimension name {name}"
-        if isinstance(size, str):
-            items = tuple([i.strip() for i in size.split(',')])
-            size = len(items)
-        elif isinstance(size, (tuple, list)):
-            assert all(isinstance(s, str) for s in size), f"Item names must all be of type 'str' but got '{size}'"
-            items = tuple(size)
-            size = len(items)
-        elif isinstance(size, Shape):
-            items = size.names
-            size = size.rank
-        elif size is None or isinstance(size, int):
-            # keep size
-            items = None
-        else:
-            items = None
-            from ._tensors import Tensor
-            if isinstance(size, Tensor):
-                size = int(size) if size.shape.volume == 1 else size
-            else:
-                try:
-                    size = int(size)
-                except ValueError:
-                    raise ValueError(f"Cannot construct dimension from {type(size).__name__}. Only int, tuple, list, str or Shape allowed. Got {size}")
-        names.append(name)
-        sizes += (size,)
-        item_names += (items,)
-    names = tuple(_apply_prefix(name, prefix) for name in names)
-    return math.Shape(sizes, names, (dim_type,) * len(sizes), item_names)
-
-
-def _apply_prefix(name: str, prefix: str):
-    match = re.search("\\w", name)
-    assert match, f"Dimension name must contain at least one letter or underscore but got '{name}'"
-    proper_name_index = match.start()
-    return prefix + name[proper_name_index:]
-
-
-def shape(obj) -> Shape:
-    """
-    If `obj` is a `Tensor` or `phi.math.magic.Shaped`, returns its shape.
-    If `obj` is a `Shape`, returns `obj`.
-
-    This function can be passed as a `dim` argument to an operation to specify that it should act upon all dimensions.
-
-    Args:
-        obj: `Tensor` or `Shape` or `Shaped`
-
-    Returns:
-        `Shape`
-    """
-    from phi.math.magic import PhiTreeNode
-    if isinstance(obj, Shape):
-        return obj
-    elif hasattr(obj, '__shape__'):
-        return obj.__shape__()
-    elif hasattr(obj, 'shape') and isinstance(obj.shape, Shape):
-        return obj.shape
-    elif isinstance(obj, (int, float, complex, bool)):
-        return EMPTY_SHAPE
-    elif isinstance(obj, (tuple, list)) and all(isinstance(item, (int, float, complex, bool)) for item in obj):
-        return channel('vector')
-    elif isinstance(obj, (Number, bool)):
-        return EMPTY_SHAPE
-    elif isinstance(obj, (tuple, list)) and all(isinstance(item, PhiTreeNode) for item in obj):
-        return merge_shapes(*obj, allow_varying_sizes=True)
-    elif isinstance(obj, PhiTreeNode):
-        from phi.math._magic_ops import all_attributes
-        return merge_shapes(*[getattr(obj, a) for a in all_attributes(obj, assert_any=True)], allow_varying_sizes=True)
-    else:
-        from .backend import choose_backend, NoBackendFound
-        try:
-            backend = choose_backend(obj)
-            shape_tuple = backend.staticshape(obj)
-            if len(shape_tuple) == 0:
-                return EMPTY_SHAPE
-            elif len(shape_tuple) == 1:
-                return channel('vector')
-            else:
-                raise ValueError(f"Cannot auto-complete shape of {backend} tensor with shape {shape_tuple}. Only 0D and 1D tensors have a Φ-Flow shape by default.")
-        except NoBackendFound:
-            raise ValueError(f'shape() requires Shaped or Shape argument but got {type(obj)}')
-
-
-def spatial(*args, **dims: Union[int, str, tuple, list, Shape]) -> Shape:
-    """
-    Returns the spatial dimensions of an existing `Shape` or creates a new `Shape` with only spatial dimensions.
-
-    Usage for filtering spatial dimensions:
-    >>> spatial_dims = spatial(shape)
-    >>> spatial_dims = spatial(tensor)
-
-    Usage for creating a `Shape` with only spatial dimensions:
-    >>> spatial_shape = spatial('undef', x=2, y=3)
-    (x=2, y=3, undef=None)
-
-    Here, the dimension `undef` is created with an undefined size of `None`.
-    Undefined sizes are automatically filled in by `tensor`, `wrap`, `stack` and `concat`.
-
-    To create a shape with multiple types, use `merge_shapes()`, `concat_shapes()` or the syntax `shape1 & shape2`.
-
-    See Also:
-        `channel`, `batch`, `instance`
-
-    Args:
-        *args: Either
-
-            * `Shape` or `Tensor` to filter or
-            * Names of dimensions with undefined sizes as `str`.
-
-        **dims: Dimension sizes and names. Must be empty when used as a filter operation.
-
-    Returns:
-        `Shape` containing only dimensions of type spatial.
-    """
-    from .magic import Shaped
-    if all(isinstance(arg, str) for arg in args) or dims:
-        return _construct_shape(SPATIAL_DIM, '', *args, **dims)
-    elif len(args) == 1 and isinstance(args[0], Shape):
-        return args[0].spatial
-    elif len(args) == 1 and isinstance(args[0], Shaped):
-        return shape(args[0]).spatial
-    else:
-        raise AssertionError(f"spatial() must be called either as a selector spatial(Shape) or spatial(Tensor) or as a constructor spatial(*names, **dims). Got *args={args}, **dims={dims}")
-
-
-def channel(*args, **dims: Union[int, str, tuple, list, Shape]) -> Shape:
-    """
-    Returns the channel dimensions of an existing `Shape` or creates a new `Shape` with only channel dimensions.
-
-    Usage for filtering channel dimensions:
-    >>> channel_dims = channel(shape)
-    >>> channel_dims = channel(tensor)
-
-    Usage for creating a `Shape` with only channel dimensions:
-    >>> channel_shape = channel('undef', vector=2)
-    (vector=2, undef=None)
-
-    Here, the dimension `undef` is created with an undefined size of `None`.
-    Undefined sizes are automatically filled in by `tensor`, `wrap`, `stack` and `concat`.
-
-    To create a shape with multiple types, use `merge_shapes()`, `concat_shapes()` or the syntax `shape1 & shape2`.
-
-    See Also:
-        `spatial`, `batch`, `instance`
-
-    Args:
-        *args: Either
-
-            * `Shape` or `Tensor` to filter or
-            * Names of dimensions with undefined sizes as `str`.
-
-        **dims: Dimension sizes and names. Must be empty when used as a filter operation.
-
-    Returns:
-        `Shape` containing only dimensions of type channel.
-    """
-    from .magic import Shaped
-    if all(isinstance(arg, str) for arg in args) or dims:
-        return _construct_shape(CHANNEL_DIM, '', *args, **dims)
-    elif len(args) == 1 and isinstance(args[0], Shape):
-        return args[0].channel
-    elif len(args) == 1 and isinstance(args[0], Shaped):
-        return shape(args[0]).channel
-    else:
-        raise AssertionError(f"channel() must be called either as a selector channel(Shape) or channel(Tensor) or as a constructor channel(*names, **dims). Got *args={args}, **dims={dims}")
-
-
-def batch(*args, **dims: Union[int, str, tuple, list, Shape]) -> Shape:
-    """
-    Returns the batch dimensions of an existing `Shape` or creates a new `Shape` with only batch dimensions.
-
-    Usage for filtering batch dimensions:
-    >>> batch_dims = batch(shape)
-    >>> batch_dims = batch(tensor)
-
-    Usage for creating a `Shape` with only batch dimensions:
-    >>> batch_shape = batch('undef', batch=2)
-    (batch=2, undef=None)
-
-    Here, the dimension `undef` is created with an undefined size of `None`.
-    Undefined sizes are automatically filled in by `tensor`, `wrap`, `stack` and `concat`.
-
-    To create a shape with multiple types, use `merge_shapes()`, `concat_shapes()` or the syntax `shape1 & shape2`.
-
-    See Also:
-        `channel`, `spatial`, `instance`
-
-    Args:
-        *args: Either
-
-            * `Shape` or `Tensor` to filter or
-            * Names of dimensions with undefined sizes as `str`.
-
-        **dims: Dimension sizes and names. Must be empty when used as a filter operation.
-
-    Returns:
-        `Shape` containing only dimensions of type batch.
-    """
-    from .magic import Shaped
-    if all(isinstance(arg, str) for arg in args) or dims:
-        return _construct_shape(BATCH_DIM, '', *args, **dims)
-    elif len(args) == 1 and isinstance(args[0], Shape):
-        return args[0].batch
-    elif len(args) == 1 and isinstance(args[0], Shaped):
-        return shape(args[0]).batch
-    else:
-        raise AssertionError(f"batch() must be called either as a selector batch(Shape) or batch(Tensor) or as a constructor batch(*names, **dims). Got *args={args}, **dims={dims}")
-
-
-def instance(*args, **dims: Union[int, str, tuple, list, Shape]) -> Shape:
-    """
-    Returns the instance dimensions of an existing `Shape` or creates a new `Shape` with only instance dimensions.
-
-    Usage for filtering instance dimensions:
-    >>> instance_dims = instance(shape)
-    >>> instance_dims = instance(tensor)
-
-    Usage for creating a `Shape` with only instance dimensions:
-    >>> instance_shape = instance('undef', points=2)
-    (points=2, undef=None)
-
-    Here, the dimension `undef` is created with an undefined size of `None`.
-    Undefined sizes are automatically filled in by `tensor`, `wrap`, `stack` and `concat`.
-
-    To create a shape with multiple types, use `merge_shapes()`, `concat_shapes()` or the syntax `shape1 & shape2`.
-
-    See Also:
-        `channel`, `batch`, `spatial`
-
-    Args:
-        *args: Either
-
-            * `Shape` or `Tensor` to filter or
-            * Names of dimensions with undefined sizes as `str`.
-
-        **dims: Dimension sizes and names. Must be empty when used as a filter operation.
-
-    Returns:
-        `Shape` containing only dimensions of type instance.
-    """
-    from .magic import Shaped
-    if all(isinstance(arg, str) for arg in args) or dims:
-        return _construct_shape(INSTANCE_DIM, '', *args, **dims)
-    elif len(args) == 1 and isinstance(args[0], Shape):
-        return args[0].instance
-    elif len(args) == 1 and isinstance(args[0], Shaped):
-        return shape(args[0]).instance
-    else:
-        raise AssertionError(f"instance() must be called either as a selector instance(Shape) or instance(Tensor) or as a constructor instance(*names, **dims). Got *args={args}, **dims={dims}")
-
-
-def dual(*args, **dims: Union[int, str, tuple, list, Shape]) -> Shape:
-    """
-    Returns the dual dimensions of an existing `Shape` or creates a new `Shape` with only dual dimensions.
-
-    Dual dimensions are assigned the prefix `~` to distinguish them from regular dimensions.
-    This way, a regular and dual dimension of the same name can exist in one `Shape`.
-
-    Dual dimensions represent the input space and are typically only present on matrices or higher-order matrices.
-    Dual dimensions behave like batch dimensions in regular operations, if supported.
-    During matrix multiplication, they are matched against their regular counterparts by name (ignoring the `~` prefix).
-
-    Usage for filtering dual dimensions:
-
-    >>> dual_dims = dual(shape)
-    >>> dual_dims = dual(tensor)
-
-    Usage for creating a `Shape` with only dual dimensions:
-
-    >>> dual('undef', points=2)
-    (~undefᵈ=None, ~pointsᵈ=2)
-
-    Here, the dimension `undef` is created with an undefined size of `None`.
-    Undefined sizes are automatically filled in by `tensor`, `wrap`, `stack` and `concat`.
-
-    To create a shape with multiple types, use `merge_shapes()`, `concat_shapes()` or the syntax `shape1 & shape2`.
-
-    See Also:
-        `channel`, `batch`, `spatial`
-
-    Args:
-        *args: Either
-
-            * `Shape` or `Tensor` to filter or
-            * Names of dimensions with undefined sizes as `str`.
-
-        **dims: Dimension sizes and names. Must be empty when used as a filter operation.
-
-    Returns:
-        `Shape` containing only dimensions of type dual.
-    """
-    from .magic import Shaped
-    if all(isinstance(arg, str) for arg in args) or dims:
-        return _construct_shape(DUAL_DIM, '~', *args, **dims)
-    elif len(args) == 1 and isinstance(args[0], Shape):
-        return args[0].dual
-    elif len(args) == 1 and isinstance(args[0], Shaped):
-        return shape(args[0]).dual
-    else:
-        raise AssertionError(f"dual() must be called either as a selector dual(Shape) or dual(Tensor) or as a constructor dual(*names, **dims). Got *args={args}, **dims={dims}")
-
-
-def merge_shapes(*objs: Union[Shape, Any], order=(batch, dual, instance, spatial, channel), allow_varying_sizes=False):
-    """
-    Combines `shapes` into a single `Shape`, grouping dimensions by type.
-    If dimensions with equal names are present in multiple shapes, their types and sizes must match.
-
-    The shorthand `shape1 & shape2` merges shapes with `check_exact=[spatial]`.
-
-    See Also:
-        `concat_shapes()`.
-
-    Args:
-        *objs: `Shape` or `Shaped` objects to combine.
-        order: Dimension type order as `tuple` of type filters (`channel`, `batch`, `spatial` or `instance`). Dimensions are grouped by type while merging.
-
-    Returns:
-        Merged `Shape`
-
-    Raises:
-        IncompatibleShapes if the shapes are not compatible
-    """
-    if not objs:
-        return EMPTY_SHAPE
-    shapes = [obj if isinstance(obj, Shape) else shape(obj) for obj in objs]
-    merged = []
-    for dim_type in order:
-        type_group = dim_type(shapes[0])
-        for sh in shapes[1:]:
-            sh = dim_type(sh)
-            for dim in sh:
-                if dim not in type_group:
-                    type_group = type_group._expand(dim, pos=-1)
-                else:  # check size match
-                    sizes_match = _size_equal(dim.size, type_group.get_size(dim.name))
-                    if allow_varying_sizes:
-                        if not sizes_match:
-                            type_group = type_group.with_dim_size(dim, None)
-                    else:
-                        if not sizes_match:
-                            raise IncompatibleShapes(f"Cannot merge shapes {shapes} because dimension '{dim.name}' exists with different sizes.", *shapes)
-                        names1 = type_group.get_item_names(dim)
-                        names2 = sh.get_item_names(dim)
-                        if names1 is not None and names2 is not None and len(names1) > 1:
-                            if names1 != names2:
-                                if set(names1) == set(names2):
-                                    raise IncompatibleShapes(f"Inconsistent component order: '{','.join(names1)}' vs '{','.join(names2)}' in dimension '{dim.name}'. Failed to merge shapes {shapes}", *shapes)
-                                else:
-                                    raise IncompatibleShapes(f"Cannot merge shapes {shapes} because dimension '{dim.name}' exists with different item names.", *shapes)
-                        elif names1 is None and names2 is not None:
-                            type_group = type_group._with_item_name(dim, tuple(names2))
-        merged.append(type_group)
-    return concat_shapes(*merged)
-
-
-def non_batch(obj) -> Shape:
-    """
-    Returns the non-batch dimensions of an object.
-
-    Args:
-        obj: `Shape` or object with a valid `shape` property.
-
-    Returns:
-        `Shape`
-    """
-    from .magic import Shaped
-    if isinstance(obj, Shape):
-        return obj.non_batch
-    elif isinstance(obj, Shaped):
-        return shape(obj).non_batch
-    else:
-        raise AssertionError(f"non_batch() must be called either on a Shape or an object with a 'shape' property but got {obj}")
-
-
-def non_spatial(obj) -> Shape:
-    """
-    Returns the non-spatial dimensions of an object.
-
-    Args:
-        obj: `Shape` or object with a valid `shape` property.
-
-    Returns:
-        `Shape`
-    """
-    from .magic import Shaped
-    if isinstance(obj, Shape):
-        return obj.non_spatial
-    elif isinstance(obj, Shaped):
-        return shape(obj).non_spatial
-    else:
-        raise AssertionError(f"non_spatial() must be called either on a Shape or an object with a 'shape' property but got {obj}")
-
-
-def non_instance(obj) -> Shape:
-    """
-    Returns the non-instance dimensions of an object.
-
-    Args:
-        obj: `Shape` or object with a valid `shape` property.
-
-    Returns:
-        `Shape`
-    """
-    from .magic import Shaped
-    if isinstance(obj, Shape):
-        return obj.non_instance
-    elif isinstance(obj, Shaped):
-        return shape(obj).non_instance
-    else:
-        raise AssertionError(f"non_instance() must be called either on a Shape or an object with a 'shape' property but got {obj}")
-
-
-def non_channel(obj) -> Shape:
-    """
-    Returns the non-channel dimensions of an object.
-
-    Args:
-        obj: `Shape` or object with a valid `shape` property.
-
-    Returns:
-        `Shape`
-    """
-    from .magic import Shaped
-    if isinstance(obj, Shape):
-        return obj.non_channel
-    elif isinstance(obj, Shaped):
-        return shape(obj).non_channel
-    else:
-        raise AssertionError(f"non_channel() must be called either on a Shape or an object with a 'shape' property but got {obj}")
-
-
-def non_dual(obj) -> Shape:
-    """
-    Returns the non-dual dimensions of an object.
-
-    Args:
-        obj: `Shape` or object with a valid `shape` property.
-
-    Returns:
-        `Shape`
-    """
-    from .magic import Shaped
-    if isinstance(obj, Shape):
-        return obj.non_dual
-    elif isinstance(obj, Shaped):
-        return shape(obj).non_dual
-    else:
-        raise AssertionError(f"non_dual() must be called either on a Shape or an object with a 'shape' property but got {obj}")
-
-
-
-def _size_equal(s1, s2):
-    if s1 is None:
-        return s2 is None
-    if isinstance(s1, int):
-        return isinstance(s2, int) and s2 == s1
-    else:
-        return math.close(s1, s2)
-
-
-def concat_shapes(*shapes: Union[Shape, Any]) -> Shape:
-    """
-    Creates a `Shape` listing the dimensions of all `shapes` in the given order.
-
-    See Also:
-        `merge_shapes()`.
-
-    Args:
-        *shapes: Shapes to concatenate. No two shapes must contain a dimension with the same name.
-
-    Returns:
-        Combined `Shape`.
-    """
-    shapes = [obj if isinstance(obj, Shape) else shape(obj) for obj in shapes]
-    names = sum([s.names for s in shapes], ())
-    if len(set(names)) != len(names):
-        raise IncompatibleShapes(f"Cannot concatenate shapes {list(shapes)}. Duplicate dimension names are not allowed.")
-    sizes = sum([s.sizes for s in shapes], ())
-    types = sum([s.types for s in shapes], ())
-    item_names = sum([s.item_names for s in shapes], ())
-    return Shape(sizes, names, types, item_names)
-
-
-def shape_stack(stack_dim: Shape, *shapes: Shape):
-    """ Returns the shape of a tensor created by stacking tensors with `shapes`. """
-    names = list(stack_dim.names)
-    types = list(stack_dim.types)
-    item_names = list(stack_dim.item_names)
-    for other in shapes:
-        for size, name, type, items in other._dimensions:
-            if name not in names:
-                if type in types:
-                    index = len(types) - types[::-1].index(type)
-                elif type == BATCH_DIM:
-                    index = 0
-                elif type == DUAL_DIM:
-                    index = min([len(names), *[i for i in range(len(names)) if types[i] == DUAL_DIM]])
-                elif type == CHANNEL_DIM:
-                    index = len(names)
-                elif type == SPATIAL_DIM:
-                    index = min([len(names), *[i for i in range(len(names)) if types[i] == CHANNEL_DIM]])
-                elif type == INSTANCE_DIM:
-                    index = min([len(names), *[i for i in range(len(names)) if types[i] == INSTANCE_DIM]])
-                else:
-                    raise ValueError(type)
-                names.insert(index, name)
-                types.insert(index, type)
-                item_names.insert(index, items)
-            else:
-                index = names.index(name)
-                if items != item_names[index]:
-                    if item_names[index] is None:
-                        item_names[index] = items
-                    else:
-                        warnings.warn(f"Stacking shapes with incompatible item names will result in item names being lost. Got {item_names[index]} and {items}", RuntimeWarning)
-                        item_names[index] = None
-    sizes = []
-    for name in names:
-        if name == stack_dim.name:
-            size = len(shapes)
-        else:
-            dim_sizes = [(shape.get_size(name) if name in shape else 1) for shape in shapes]
-            if all([math.close(s, dim_sizes[0]) for s in dim_sizes[1:]]):
-                size = dim_sizes[0]
-            else:
-                from ._magic_ops import stack
-                from ._tensors import wrap
-                dim_sizes = [wrap(d) for d in dim_sizes]
-                size = stack(dim_sizes, stack_dim)
-        sizes.append(size)
-    return Shape(tuple(sizes), tuple(names), tuple(types), tuple(item_names))
-
-
-def vector_add(*shapes: Shape):
-    if not shapes:
-        return EMPTY_SHAPE
-    names = shapes[0].names
-    types = shapes[0].types
-    item_names = shapes[0].item_names
-    for shape in shapes[1:]:
-        for name in shape.names:
-            if name not in names:
-                names += (name,)
-                types += (shape.get_type(name),)
-                item_names += (shape.get_item_names(name),)
-    sizes = [sum(sh.get_size(dim) if dim in sh else 0 for sh in shapes) for dim in names]
-    return Shape(tuple(sizes), names, types, item_names)
+import re
+import warnings
+from numbers import Number
+from typing import Tuple, Callable, List, Union, Any, Sequence, Optional
+
+from phi import math
+
+
+BATCH_DIM = 'batch'
+SPATIAL_DIM = 'spatial'
+CHANNEL_DIM = 'channel'
+INSTANCE_DIM = 'înstance'
+DUAL_DIM = 'dual'
+
+TYPE_ABBR = {SPATIAL_DIM: "ˢ", CHANNEL_DIM: "ᶜ", INSTANCE_DIM: "ⁱ", BATCH_DIM: "ᵇ", DUAL_DIM: "ᵈ", None: "⁻"}  # ᵃᵇᶜᵈᵉᶠᵍʰⁱʲᵏˡᵐⁿᵒᵖʳˢᵗᵘᵛʷˣʸᶻ
+
+DEBUG_CHECKS = []
+
+
+def enable_debug_checks():
+    """
+    Once called, additional type checks are enabled.
+    This may result in a noticeable drop in performance.
+    """
+    DEBUG_CHECKS.append(True)
+
+
+class Shape:
+    """
+    Shapes enumerate dimensions, each consisting of a name, size and type.
+
+    There are five types of dimensions: `batch`, `dual`, `spatial`, `channel`, and `instance`.
+    """
+
+    def __init__(self, sizes: tuple, names: tuple, types: tuple, item_names: tuple):
+        """
+        To construct a `Shape`, use `batch`, `dual`, `spatial`, `channel` or `instance`, depending on the desired dimension type.
+        To create a shape with multiple types, use `merge_shapes()`, `concat_shapes()` or the syntax `shape1 & shape2`.
+
+        The `__init__` constructor is for internal use only.
+        """
+        if len(sizes) > 0 and any(s is not None and not isinstance(s, int) for s in sizes):
+            from ._tensors import Tensor
+            sizes = tuple([s if isinstance(s, Tensor) or s is None else int(s) for s in sizes])  # TODO replace this by an assert
+        self.sizes: tuple = sizes
+        """
+        Ordered dimension sizes as `tuple`.
+        The size of a dimension can be an `int` or a `Tensor` for [non-uniform shapes](https://tum-pbs.github.io/PhiFlow/Math.html#non-uniform-tensors).
+        
+        See Also:
+            `Shape.get_size()`, `Shape.size`, `Shape.shape`.
+        """
+        self.names: Tuple[str] = names
+        """
+        Ordered dimension names as `tuple[str]`.
+        
+        See Also:
+            `Shape.name`.
+        """
+        self.types: Tuple[str] = types  # undocumented, may be private
+        self.item_names: Tuple[Optional[Tuple[str, ...]]] = (None,) * len(sizes) if item_names is None else item_names  # undocumented
+        if DEBUG_CHECKS:
+            assert len(sizes) == len(names) == len(types) == len(item_names), f"sizes={sizes}, names={names}, types={types}, item_names={item_names}"
+            assert len(set(names)) == len(names), f"Duplicate dimension names: {names}"
+            assert all(isinstance(n, str) for n in names), f"All names must be of type string but got {names}"
+            assert isinstance(self.item_names, tuple)
+            assert all([items is None or isinstance(items, tuple) for items in self.item_names])
+            assert all([items is None or all([isinstance(n, str) for n in items]) for items in self.item_names])
+            from ._tensors import Tensor
+            for name, size in zip(names, sizes):
+                if isinstance(size, Tensor):
+                    assert size.rank > 0
+            for size, item_names in zip(self.sizes, self.item_names):
+                if item_names is not None:
+                    assert len(item_names) == size, f"Number of item names ({len(item_names)}) does not match size {size}"
+
+    def _check_is_valid_tensor_shape(self):
+        if DEBUG_CHECKS:
+            from ._tensors import Tensor
+            for name, size in zip(self.names, self.sizes):
+                if size is not None and isinstance(size, Tensor):
+                    assert size.rank > 0
+                    for dim in size.shape.names:
+                        assert dim in self.names, f"Dimension {name} varies along {dim} but {dim} is not part of the Shape {self}"
+
+    def _to_dict(self, include_sizes=True):
+        result = dict(names=self.names, types=self.types, item_names=self.item_names)
+        if include_sizes:
+            if not all([isinstance(s, int)] for s in self.sizes):
+                raise NotImplementedError()
+            result['sizes'] = self.sizes
+        return result
+
+    @staticmethod
+    def _from_dict(dict_: dict):
+        names = tuple(dict_['names'])
+        sizes = tuple(dict_['sizes']) if 'sizes' in dict_ else (None,) * len(names)
+        item_names = tuple([None if n is None else tuple(n) for n in dict_['item_names']])
+        return Shape(sizes, names, tuple(dict_['types']), item_names)
+
+    @property
+    def _named_sizes(self):
+        return zip(self.names, self.sizes)
+
+    @property
+    def _dimensions(self):
+        return zip(self.sizes, self.names, self.types, self.item_names)
+
+    @property
+    def untyped_dict(self):
+        """
+        Returns:
+            `dict` containing dimension names as keys.
+                The values are either the item names as `tuple` if available, otherwise the size.
+        """
+        return {name: self.get_item_names(i) or self.get_size(i) for i, name in enumerate(self.names)}
+
+    def __len__(self):
+        return len(self.sizes)
+
+    def __contains__(self, item):
+        if isinstance(item, (str, tuple, list)):
+            dims = parse_dim_order(item)
+            return all(dim in self.names for dim in dims)
+        elif isinstance(item, Shape):
+            return all([d in self.names for d in item.names])
+        else:
+            raise ValueError(item)
+
+    def isdisjoint(self, other: Union['Shape', tuple, list, str]):
+        """ Shapes are disjoint if all dimension names of one shape do not occur in the other shape. """
+        other = parse_dim_order(other)
+        return not any(dim in self.names for dim in other)
+
+    def __iter__(self):
+        return iter(self[i] for i in range(self.rank))
+
+    def index(self, dim: Union[str, 'Shape', None]) -> int:
+        """
+        Finds the index of the dimension within this `Shape`.
+
+        See Also:
+            `Shape.indices()`.
+
+        Args:
+            dim: Dimension name or single-dimension `Shape`.
+
+        Returns:
+            Index as `int`.
+        """
+        if dim is None:
+            return None
+        elif isinstance(dim, str):
+            if dim not in self.names:
+                raise ValueError(f"Shape {self} has no dimension '{dim}'")
+            return self.names.index(dim)
+        elif isinstance(dim, Shape):
+            assert dim.rank == 1, f"index() requires a single dimension as input but got {dim}. Use indices() for multiple dimensions."
+            return self.names.index(dim.name)
+        else:
+            raise ValueError(f"index() requires a single dimension as input but got {dim}")
+
+    def indices(self, dims: Union[tuple, list, 'Shape']) -> Tuple[int]:
+        """
+        Finds the indices of the given dimensions within this `Shape`.
+
+        See Also:
+            `Shape.index()`.
+
+        Args:
+            dims: Sequence of dimensions as `tuple`, `list` or `Shape`.
+
+        Returns:
+            Indices as `tuple[int]`.
+        """
+        if isinstance(dims, (list, tuple, set)):
+            return tuple([self.index(n) for n in dims if n in self.names])
+        elif isinstance(dims, Shape):
+            return tuple([self.index(n) for n in dims.names if n in self.names])
+        else:
+            raise ValueError(f"indices() requires a sequence of dimensions but got {dims}")
+
+    def get_size(self, dim: Union[str, 'Shape', int], default=None):
+        """
+        See Also:
+            `Shape.get_sizes()`, `Shape.size`
+
+        Args:
+            dim: Dimension, either as name `str` or single-dimension `Shape` or index `int`.
+            default: (Optional) If the dim does not exist, return this value instead of raising an error.
+
+        Returns:
+            Size associated with `dim` as `int` or `Tensor`.
+        """
+        if isinstance(dim, int):
+            assert default is None, "Cannot use a default value when passing an int for dim"
+            return self.sizes[dim]
+        if isinstance(dim, Shape):
+            assert dim.rank == 1, f"get_size() requires a single dimension but got {dim}. Use indices() to get multiple sizes."
+            dim = dim.name
+        if isinstance(dim, str):
+            if dim not in self.names:
+                if default is None:
+                    raise KeyError(f"get_size() failed because '{dim}' is not part of Shape {self} and no default value was provided")
+                else:
+                    return default
+            return self.sizes[self.names.index(dim)]
+        else:
+            raise ValueError(f"get_size() requires a single dimension but got {dim}. Use indices() to get multiple sizes.")
+
+    def get_sizes(self, dims: Union[tuple, list, 'Shape']) -> tuple:
+        """
+        See Also:
+            `Shape.get_size()`
+
+        Args:
+            dims: Dimensions as `tuple`, `list` or `Shape`.
+
+        Returns:
+            `tuple`
+        """
+        assert isinstance(dims, (tuple, list, Shape)), f"get_sizes() requires a sequence of dimensions but got {dims}"
+        return tuple([self.get_size(dim) for dim in dims])
+
+    def get_type(self, dim: Union[str, 'Shape']) -> str:
+        # undocumented, use get_dim_type() instead.
+        if isinstance(dim, str):
+            return self.types[self.names.index(dim)]
+        elif isinstance(dim, Shape):
+            assert dim.rank == 1, f"Shape.get_type() only accepts single-dimension Shapes but got {dim}"
+            return self.types[self.names.index(dim.name)]
+        else:
+            raise ValueError(dim)
+
+    def get_dim_type(self, dim: Union[str, 'Shape']) -> Callable:
+        """
+        Args:
+            dim: Dimension, either as name `str` or single-dimension `Shape`.
+
+        Returns:
+            Dimension type, one of `batch`, `spatial`, `instance`, `channel`.
+        """
+        return DIM_FUNCTIONS[self.get_type(dim)]
+
+    def get_types(self, dims: Union[tuple, list, 'Shape']) -> tuple:
+        # undocumented, do not use
+        if isinstance(dims, (tuple, list)):
+            return tuple(self.get_type(n) for n in dims)
+        elif isinstance(dims, Shape):
+            return tuple(self.get_type(n) for n in dims.names)
+        else:
+            raise ValueError(dims)
+
+    def get_item_names(self, dim: Union[str, 'Shape', int], fallback_spatial=False) -> Union[tuple, None]:
+        """
+        Args:
+            fallback_spatial: If `True` and no item names are defined for `dim` and `dim` is a channel dimension, the spatial dimension names are interpreted as item names along `dim` in the order they are listed in this `Shape`.
+            dim: Dimension, either as `int` index, `str` name or single-dimension `Shape`.
+
+        Returns:
+            Item names as `tuple` or `None` if not defined.
+        """
+        if isinstance(dim, int):
+            result = self.item_names[dim]
+        elif isinstance(dim, str):
+            result = self.item_names[self.index(dim)]
+        elif isinstance(dim, Shape):
+            assert dim.rank == 1, f"Shape.get_type() only accepts single-dimension Shapes but got {dim}"
+            result = self.item_names[self.names.index(dim.name)]
+        else:
+            raise ValueError(dim)
+        if result is not None:
+            return result
+        elif fallback_spatial and self.spatial_rank == self.get_size(dim) and self.get_type(dim) == CHANNEL_DIM:
+            return self.spatial.names
+        else:
+            return None
+
+    def flipped(self, dims: Union[List[str], Tuple[str]]):
+        item_names = list(self.item_names)
+        for dim in dims:
+            if dim in self.names:
+                dim_i_n = self.get_item_names(dim)
+                if dim_i_n is not None:
+                    item_names[self.index(dim)] = tuple(reversed(dim_i_n))
+        return Shape(self.sizes, self.names, self.types, tuple(item_names))
+
+    def __getitem__(self, selection):
+        if isinstance(selection, int):
+            return Shape((self.sizes[selection],), (self.names[selection],), (self.types[selection],), (self.item_names[selection],))
+        elif isinstance(selection, slice):
+            return Shape(self.sizes[selection], self.names[selection], self.types[selection], self.item_names[selection])
+        elif isinstance(selection, str):
+            if ',' in selection:
+                selection = [self.index(s.strip()) for s in selection.split(',')]
+            else:
+                selection = self.index(selection)
+            return self[selection]
+        elif isinstance(selection, (tuple, list)):
+            selection = [self.index(s) if isinstance(s, str) else s for s in selection]
+            return Shape(tuple([self.sizes[i] for i in selection]), tuple([self.names[i] for i in selection]), tuple([self.types[i] for i in selection]), tuple([self.item_names[i] for i in selection]))
+        raise AssertionError("Can only access shape elements as shape[int] or shape[slice]")
+
+    @property
+    def reversed(self):
+        return Shape(tuple(reversed(self.sizes)), tuple(reversed(self.names)), tuple(reversed(self.types)), tuple(reversed(self.item_names)))
+
+    @property
+    def batch(self) -> 'Shape':
+        """
+        Filters this shape, returning only the batch dimensions as a new `Shape` object.
+
+        See also:
+            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.
+
+        Returns:
+            New `Shape` object
+        """
+        return self[[i for i, t in enumerate(self.types) if t == BATCH_DIM]]
+
+    @property
+    def non_batch(self) -> 'Shape':
+        """
+        Filters this shape, returning only the non-batch dimensions as a new `Shape` object.
+
+        See also:
+            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.
+
+        Returns:
+            New `Shape` object
+        """
+        return self[[i for i, t in enumerate(self.types) if t != BATCH_DIM]]
+
+    @property
+    def spatial(self) -> 'Shape':
+        """
+        Filters this shape, returning only the spatial dimensions as a new `Shape` object.
+
+        See also:
+            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.
+
+        Returns:
+            New `Shape` object
+        """
+        return self[[i for i, t in enumerate(self.types) if t == SPATIAL_DIM]]
+
+    @property
+    def non_spatial(self) -> 'Shape':
+        """
+        Filters this shape, returning only the non-spatial dimensions as a new `Shape` object.
+
+        See also:
+            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.
+
+        Returns:
+            New `Shape` object
+        """
+        return self[[i for i, t in enumerate(self.types) if t != SPATIAL_DIM]]
+
+    @property
+    def instance(self) -> 'Shape':
+        """
+        Filters this shape, returning only the instance dimensions as a new `Shape` object.
+
+        See also:
+            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.
+
+        Returns:
+            New `Shape` object
+        """
+        return self[[i for i, t in enumerate(self.types) if t == INSTANCE_DIM]]
+
+    @property
+    def non_instance(self) -> 'Shape':
+        """
+        Filters this shape, returning only the non-instance dimensions as a new `Shape` object.
+
+        See also:
+            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.
+
+        Returns:
+            New `Shape` object
+        """
+        return self[[i for i, t in enumerate(self.types) if t != INSTANCE_DIM]]
+
+    @property
+    def channel(self) -> 'Shape':
+        """
+        Filters this shape, returning only the channel dimensions as a new `Shape` object.
+
+        See also:
+            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.
+
+        Returns:
+            New `Shape` object
+        """
+        return self[[i for i, t in enumerate(self.types) if t == CHANNEL_DIM]]
+
+    @property
+    def non_channel(self) -> 'Shape':
+        """
+        Filters this shape, returning only the non-channel dimensions as a new `Shape` object.
+
+        See also:
+            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.
+
+        Returns:
+            New `Shape` object
+        """
+        return self[[i for i, t in enumerate(self.types) if t != CHANNEL_DIM]]
+
+    @property
+    def dual(self) -> 'Shape':
+        """
+        Filters this shape, returning only the dual dimensions as a new `Shape` object.
+
+        See also:
+            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.
+
+        Returns:
+            New `Shape` object
+        """
+        return self[[i for i, t in enumerate(self.types) if t == DUAL_DIM]]
+
+    @property
+    def non_dual(self) -> 'Shape':
+        """
+        Filters this shape, returning only the non-dual dimensions as a new `Shape` object.
+
+        See also:
+            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.
+
+        Returns:
+            New `Shape` object
+        """
+        return self[[i for i, t in enumerate(self.types) if t != DUAL_DIM]]
+
+    @property
+    def primal(self) -> 'Shape':
+        """
+        Filters this shape, returning only the dual dimensions as a new `Shape` object.
+
+        See also:
+            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.
+
+        Returns:
+            New `Shape` object
+        """
+        return self[[i for i, t in enumerate(self.types) if t not in [DUAL_DIM, BATCH_DIM]]]
+
+    @property
+    def non_primal(self) -> 'Shape':
+        """
+        Filters this shape, returning only batch and dual dimensions as a new `Shape` object.
+
+        See also:
+            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.
+
+        Returns:
+            New `Shape` object
+        """
+        return self[[i for i, t in enumerate(self.types) if t in [DUAL_DIM, BATCH_DIM]]]
+
+    @property
+    def non_singleton(self) -> 'Shape':
+        """
+        Filters this shape, returning only non-singleton dimensions as a new `Shape` object.
+        Dimensions are singleton if their size is exactly `1`.
+
+        Returns:
+            New `Shape` object
+        """
+        return self[[i for i, s in enumerate(self.sizes) if not _size_equal(s, 1)]]
+
+    @property
+    def singleton(self) -> 'Shape':
+        """
+        Filters this shape, returning only singleton dimensions as a new `Shape` object.
+        Dimensions are singleton if their size is exactly `1`.
+
+        Returns:
+            New `Shape` object
+        """
+        return self[[i for i, s in enumerate(self.sizes) if _size_equal(s, 1)]]
+
+    def unstack(self, dim='dims') -> Tuple['Shape']:
+        """
+        Slices this `Shape` along a dimension.
+        The dimension listing the sizes of the shape is referred to as `'dims'`.
+
+        Non-uniform tensor shapes may be unstacked along other dimensions as well, see
+        https://tum-pbs.github.io/PhiFlow/Math.html#non-uniform-tensors
+
+        Args:
+            dim: dimension to unstack
+
+        Returns:
+            slices of this shape
+        """
+        if dim == 'dims':
+            return tuple(Shape((self.sizes[i],), (self.names[i],), (self.types[i],), (self.item_names[i],)) for i in range(self.rank))
+        if dim not in self and self.is_uniform:
+            return tuple([self])
+        from ._tensors import Tensor
+        if dim in self:
+            inner = self.without(dim)
+            dim_size = self.get_size(dim)
+        else:
+            inner = self
+            dim_size = self.shape.get_size(dim)
+        sizes = []
+        for size in inner.sizes:
+            if isinstance(size, Tensor) and dim in size.shape:
+                sizes.append(size.unstack(dim))
+                dim_size = size.shape.get_size(dim)
+            else:
+                sizes.append(size)
+        assert isinstance(dim_size, int)
+        shapes = tuple(Shape(tuple([int(size[i]) if isinstance(size, tuple) else size for size in sizes]), inner.names, inner.types, inner.item_names) for i in range(dim_size))
+        return shapes
+
+    @property
+    def name(self) -> str:
+        """
+        Only for Shapes containing exactly one single dimension.
+        Returns the name of the dimension.
+
+        See Also:
+            `Shape.names`.
+        """
+        assert self.rank == 1, f"Shape.name is only defined for shapes of rank 1. shape={self}"
+        return self.names[0]
+
+    @property
+    def size(self) -> int:
+        """
+        Only for Shapes containing exactly one single dimension.
+        Returns the size of the dimension.
+
+        See Also:
+            `Shape.sizes`, `Shape.get_size()`.
+        """
+        assert self.rank == 1, f"Shape.size is only defined for shapes of rank 1 but has dims {self}"
+        return self.sizes[0]
+
+    @property
+    def type(self) -> str:
+        """
+        Only for Shapes containing exactly one single dimension.
+        Returns the type of the dimension.
+
+        See Also:
+            `Shape.get_type()`.
+        """
+        assert self.rank == 1, "Shape.type is only defined for shapes of rank 1."
+        return self.types[0]
+
+    @property
+    def dim_type(self):
+        types = set(self.types)
+        assert len(types) == 1, f"Shape contains multiple types: {self}"
+        return DIM_FUNCTIONS[next(iter(types))]
+
+    def __int__(self):
+        assert self.rank == 1, "int(Shape) is only defined for shapes of rank 1."
+        return self.sizes[0]
+
+    def mask(self, names: Union[tuple, list, set, 'Shape']):
+        """
+        Returns a binary sequence corresponding to the names of this Shape.
+        A value of 1 means that a dimension of this Shape is contained in `names`.
+
+        Args:
+          names: instance of dimension
+          names: tuple or list or set: 
+
+        Returns:
+          binary sequence
+
+        """
+        if isinstance(names, str):
+            names = [names]
+        elif isinstance(names, Shape):
+            names = names.names
+        mask = [1 if name in names else 0 for name in self.names]
+        return tuple(mask)
+
+    def __repr__(self):
+        def size_repr(size, items):
+            if items is not None:
+                items_str = ",".join(items)
+                return items_str if len(items_str) <= 20 else f"{size}:{items[0]}..{items[-1]}"
+            return size
+
+        strings = [f"{name}{TYPE_ABBR.get(dim_type, '?')}={size_repr(size, items)}" for size, name, dim_type, items in self._dimensions]
+        return '(' + ', '.join(strings) + ')'
+
+    def __eq__(self, other):
+        if not isinstance(other, Shape):
+            return False
+        if self.names != other.names or self.types != other.types:
+            return False
+        for size1, size2 in zip(self.sizes, other.sizes):
+            equal = size1 == size2
+            assert isinstance(equal, (bool, math.Tensor))
+            if isinstance(equal, math.Tensor):
+                equal = equal.all
+            if not equal:
+                return False
+        for names1, names2 in zip(self.item_names, other.item_names):
+            if names1 != names2:
+                return False
+        return True
+
+    def __ne__(self, other):
+        return not self == other
+
+    def __bool__(self):
+        return self.rank > 0
+
+    def _reorder(self, names: Union[tuple, list, 'Shape']) -> 'Shape':
+        assert len(names) == self.rank
+        if isinstance(names, Shape):
+            names = names.names
+        order = [self.index(n) for n in names]
+        return self[order]
+
+    def _order_group(self, names: Union[tuple, list, 'Shape']) -> list:
+        """ Reorders the dimensions of this `Shape` so that `names` are clustered together and occur in the specified order. """
+        if isinstance(names, Shape):
+            names = names.names
+        result = []
+        for dim in self.names:
+            if dim not in result:
+                if dim in names:
+                    result.extend(names)
+                else:
+                    result.append(dim)
+        return result
+
+    def __and__(self, other):
+        return merge_shapes(self, other)
+
+    def _expand(self, dim: 'Shape', pos=None) -> 'Shape':
+        """**Deprecated.** Use `phi.math.merge_shapes()` or `phi.math.concat_shapes()` instead. """
+        warnings.warn("Shape.expand() is deprecated. Use merge_shapes() or concat_shapes() instead.", DeprecationWarning)
+        if not dim:
+            return self
+        assert dim.name not in self, f"Cannot expand shape {self} by {dim} because dimension already exists."
+        assert isinstance(dim, Shape) and dim.rank == 1, f"Shape.expand() requires a single dimension as a Shape but got {dim}"
+        if pos is None:
+            same_type_dims = self[[i for i, t in enumerate(self.types) if t == dim.type]]
+            if len(same_type_dims) > 0:
+                pos = self.index(same_type_dims.names[0])
+            else:
+                pos = {BATCH_DIM: 0, INSTANCE_DIM: self.batch_rank, SPATIAL_DIM: self.batch.rank + self.instance_rank, CHANNEL_DIM: self.rank + 1}[dim.type]
+        elif pos < 0:
+            pos += self.rank + 1
+        sizes = list(self.sizes)
+        names = list(self.names)
+        types = list(self.types)
+        item_names = list(self.item_names)
+        sizes.insert(pos, dim.size)
+        names.insert(pos, dim.name)
+        types.insert(pos, dim.type)
+        item_names.insert(pos, dim.item_names[0])
+        return Shape(tuple(sizes), tuple(names), tuple(types), tuple(item_names))
+
+    def without(self, dims: 'DimFilter') -> 'Shape':
+        """
+        Builds a new shape from this one that is missing all given dimensions.
+        Dimensions in `dims` that are not part of this Shape are ignored.
+        
+        The complementary operation is `Shape.only()`.
+
+        Args:
+          dims: Single dimension (str) or instance of dimensions (tuple, list, Shape)
+          dims: Dimensions to exclude as `str` or `tuple` or `list` or `Shape`. Dimensions that are not included in this shape are ignored.
+
+        Returns:
+          Shape without specified dimensions
+        """
+        if callable(dims):
+            dims = dims(self)
+        if isinstance(dims, str):
+            dims = parse_dim_order(dims)
+        if isinstance(dims, (tuple, list, set)):
+            return self[[i for i in range(self.rank) if self.names[i] not in dims]]
+        elif isinstance(dims, Shape):
+            return self[[i for i in range(self.rank) if self.names[i] not in dims.names]]
+        elif dims is None:  # subtract none
+            return self
+        else:
+            raise ValueError(dims)
+
+    def only(self, dims: 'DimFilter', reorder=False):
+        """
+        Builds a new shape from this one that only contains the given dimensions.
+        Dimensions in `dims` that are not part of this Shape are ignored.
+        
+        The complementary operation is :func:`Shape.without`.
+
+        Args:
+          dims: comma-separated dimension names (str) or instance of dimensions (tuple, list, Shape) or filter function.
+          reorder: If `False`, keeps the dimension order as defined in this shape.
+            If `True`, reorders the dimensions of this shape to match the order of `dims`.
+
+        Returns:
+          Shape containing only specified dimensions
+
+        """
+        if dims is None:  # keep none
+            return EMPTY_SHAPE
+        if callable(dims):
+            dims = dims(self)
+        if isinstance(dims, str):
+            dims = parse_dim_order(dims)
+        if isinstance(dims, Shape):
+            dims = dims.names
+        if not isinstance(dims, (tuple, list, set)):
+            raise ValueError(dims)
+        if reorder:
+            return self[[self.names.index(d) for d in dims if d in self.names]]
+        else:
+            return self[[i for i in range(self.rank) if self.names[i] in dims]]
+
+    @property
+    def rank(self) -> int:
+        """
+        Returns the number of dimensions.
+        Equal to `len(shape)`.
+
+        See `Shape.is_empty`, `Shape.batch_rank`, `Shape.spatial_rank`, `Shape.channel_rank`.
+        """
+        return len(self.sizes)
+
+    @property
+    def batch_rank(self) -> int:
+        """ Number of batch dimensions """
+        return sum([1 for ty in self.types if ty == BATCH_DIM])
+
+    @property
+    def instance_rank(self) -> int:
+        return sum([1 for ty in self.types if ty == INSTANCE_DIM])
+
+    @property
+    def spatial_rank(self) -> int:
+        """ Number of spatial dimensions """
+        return sum([1 for ty in self.types if ty == SPATIAL_DIM])
+
+    @property
+    def dual_rank(self) -> int:
+        """ Number of spatial dimensions """
+        return sum([1 for ty in self.types if ty == DUAL_DIM])
+
+    @property
+    def channel_rank(self) -> int:
+        """ Number of channel dimensions """
+        return sum([1 for ty in self.types if ty == CHANNEL_DIM])
+
+    @property
+    def well_defined(self):
+        """
+        Returns `True` if no dimension size is `None`.
+
+        Shapes with undefined sizes may be used in `phi.math.tensor()`, `phi.math.wrap()`, `phi.math.stack()` or `phi.math.concat()`.
+
+        To create an undefined size, call a constructor function (`batch()`, `spatial()`, `channel()`, `instance()`)
+        with positional `str` arguments, e.g. `spatial('x')`.
+        """
+        for size in self.sizes:
+            if size is None:
+                return False
+        return True
+
+    @property
+    def shape(self) -> 'Shape':
+        """
+        Higher-order `Shape`.
+        The returned shape will always contain the channel dimension `dims` with a size equal to the `Shape.rank` of this shape.
+
+        For uniform shapes, `Shape.shape` will only contain the dimension `dims` but the shapes of [non-uniform shapes](https://tum-pbs.github.io/PhiFlow/Math.html#non-uniform-tensors)
+        may contain additional dimensions.
+
+        See Also:
+            `Shape.is_uniform`.
+
+        Returns:
+            `Shape`.
+        """
+        from phi.math import Tensor
+        shape = Shape((self.rank,), ('dims',), (CHANNEL_DIM,), (self.names,))
+        for size in self.sizes:
+            if isinstance(size, Tensor):
+                shape = shape & size.shape
+        return shape
+
+    @property
+    def is_uniform(self) -> bool:
+        """
+        A shape is uniform if it all sizes have a single integer value.
+
+        See Also:
+            `Shape.is_non_uniform`, `Shape.shape`.
+        """
+        return all(isinstance(s, int) for s in self.sizes)
+
+    @property
+    def is_non_uniform(self) -> bool:
+        """
+        A shape is non-uniform if the size of any dimension varies along another dimension.
+
+        See Also:
+            `Shape.is_uniform`, `Shape.shape`.
+        """
+        return not self.is_uniform
+
+    @property
+    def non_uniform(self) -> 'Shape':
+        """
+        Returns only the non-uniform dimensions of this shape, i.e. the dimensions whose size varies along another dimension.
+        """
+        from phi.math import Tensor
+        indices = [i for i, size in enumerate(self.sizes) if isinstance(size, Tensor) and size.rank > 0]
+        return self[indices]
+
+    def with_size(self, size: Union[int, Tuple[str, ...]]):
+        """
+        Only for single-dimension shapes.
+        Returns a `Shape` representing this dimension but with a different size.
+
+        See Also:
+            `Shape.with_sizes()`.
+
+        Args:
+            size: Replacement size for this dimension.
+
+        Returns:
+            `Shape`
+        """
+        assert self.rank == 1, "Shape.with_size() is only defined for shapes of rank 1."
+        return self.with_sizes([size])
+
+    def with_sizes(self, sizes: Union[Sequence[int], Sequence[Tuple[str, ...]], 'Shape', int], keep_item_names=True):
+        """
+        Returns a new `Shape` matching the dimension names and types of `self` but with different sizes.
+
+        See Also:
+            `Shape.with_size()`.
+
+        Args:
+            sizes: One of
+
+                * `tuple` / `list` of same length as `self` containing replacement sizes or replacement item names.
+                * `Shape` of any rank. Replaces sizes for dimensions shared by `sizes` and `self`.
+                * `int`: new size for all dimensions
+
+            keep_item_names: If `False`, forgets all item names.
+                If `True`, keeps item names where the size does not change.
+
+        Returns:
+            `Shape` with same names and types as `self`.
+        """
+        if isinstance(sizes, int):
+            sizes = [sizes] * len(self.sizes)
+        if isinstance(sizes, Shape):
+            item_names = [sizes.get_item_names(dim) if dim in sizes else self.get_item_names(dim) for dim in self.names]
+            sizes = [sizes.get_size(dim) if dim in sizes else s for dim, s in self._named_sizes]
+            return Shape(tuple(sizes), self.names, self.types, tuple(item_names))
+        else:
+            assert len(sizes) == len(self.sizes), f"Cannot create shape from {self} with sizes {sizes}"
+            sizes_ = []
+            item_names = []
+            for i, obj in enumerate(sizes):
+                new_size, new_item_names = Shape._size_and_item_names_from_obj(obj, self.sizes[i], self.item_names[i], keep_item_names)
+                sizes_.append(new_size)
+                item_names.append(new_item_names)
+            return Shape(tuple(sizes_), self.names, self.types, tuple(item_names))
+
+    @staticmethod
+    def _size_and_item_names_from_obj(obj, prev_size, prev_item_names, keep_item_names=True):
+        if isinstance(obj, str):
+            obj = [s.strip() for s in obj.split(',')]
+        if isinstance(obj, (tuple, list)):
+            return len(obj), tuple(obj)
+        elif isinstance(obj, Number):
+            return obj, prev_item_names if keep_item_names and (prev_size is None or _size_equal(obj, prev_size)) else None
+        elif isinstance(obj, math.Tensor) or obj is None:
+            return obj, None
+        else:
+            raise ValueError(f"sizes can only contain int, str or Tensor but got {type(obj)}")
+
+    def without_sizes(self):
+        """
+        Returns:
+            `Shape` with all sizes undefined (`None`)
+        """
+        return Shape((None,) * self.rank, self.names, self.types, (None,) * self.rank)
+
+    def _replace_single_size(self, dim: str, size: int, keep_item_names: bool = False):
+        new_sizes = list(self.sizes)
+        new_sizes[self.index(dim)] = size
+        return self.with_sizes(new_sizes, keep_item_names=keep_item_names)
+
+    def with_dim_size(self, dim: Union[str, 'Shape'], size: Union[int, 'math.Tensor', str, tuple, list], keep_item_names=True):
+        """
+        Returns a new `Shape` that has a different size for `dim`.
+
+        Args:
+            dim: Dimension for which to replace the size, `Shape` or `str`.
+            size: New size, `int` or `Tensor`
+
+        Returns:
+            `Shape` with same names and types as `self`.
+        """
+        if isinstance(dim, Shape):
+            dim = dim.name
+        assert isinstance(dim, str)
+        new_size, new_item_names = Shape._size_and_item_names_from_obj(size, self.get_size(dim), self.get_item_names(dim), keep_item_names)
+        return self.replace(dim, Shape((new_size,), (dim,), (self.get_type(dim),), (new_item_names,)))
+
+    def _with_names(self, names: Union[str, tuple, list]):
+        if isinstance(names, str):
+            names = parse_dim_names(names, self.rank)
+            names = [n if n is not None else o for n, o in zip(names, self.names)]
+        return Shape(self.sizes, tuple(names), self.types, self.item_names)
+
+    def _replace_names_and_types(self,
+                                 dims: Union['Shape', str, tuple, list],
+                                 new: Union['Shape', str, tuple, list]) -> 'Shape':
+        """
+        Returns a copy of `self` with `dims` replaced by `new`.
+        Dimensions that are not present in `self` are ignored.
+
+        The dimension order is preserved.
+
+        Args:
+            dims: Dimensions to replace.
+            new: New dimensions, must have same length as `dims`.
+                If a `Shape` is given, replaces the dimension types and item names as well.
+
+        Returns:
+            `Shape` with same rank and dimension order as `self`.
+        """
+        dims = parse_dim_order(dims)
+        sizes = [math.rename_dims(s, dims, new) if isinstance(s, math.Tensor) else s for s in self.sizes]
+        new = parse_dim_order(new) if isinstance(new, str) else new
+        names = list(self.names)
+        types = list(self.types)
+        item_names = list(self.item_names)
+        for old_name, new_dim in zip(dims, new):
+            if old_name in self:
+                if isinstance(new_dim, Shape):
+                    names[self.index(old_name)] = new_dim.name
+                    types[self.index(old_name)] = new_dim.type
+                    item_names[self.index(old_name)] = new_dim.item_names[0]
+                else:
+                    names[self.index(old_name)] = new_dim
+        return Shape(tuple(sizes), tuple(names), tuple(types), tuple(item_names))
+
+    def replace(self, dims: Union['Shape', str, tuple, list], new: 'Shape') -> 'Shape':
+        """
+        Returns a copy of `self` with `dims` replaced by `new`.
+        Dimensions that are not present in `self` are ignored.
+
+        The dimension order is preserved.
+
+        Args:
+            dims: Dimensions to replace.
+            new: New dimensions, must have same length as `dims`.
+                If a `Shape` is given, replaces the dimension types and item names as well.
+
+        Returns:
+            `Shape` with same rank and dimension order as `self`.
+        """
+        dims = parse_dim_order(dims)
+        assert isinstance(new, Shape), f"new must be a Shape but got {new}"
+        names = list(self.names)
+        sizes = list(self.sizes)
+        types = list(self.types)
+        item_names = list(self.item_names)
+        if len(new) > len(dims):  # Put all in one spot
+            assert len(dims) == 1, "Cannot replace 2+ dims by more replacements"
+            index = self.index(dims[0])
+            return concat_shapes(self[:index], new, self[index+1:])
+        for old_name, new_dim in zip(dims, new):
+            if old_name in self:
+                names[self.index(old_name)] = new_dim.name
+                types[self.index(old_name)] = new_dim.type
+                item_names[self.index(old_name)] = new_dim.item_names[0]
+                sizes[self.index(old_name)] = new_dim.size
+        replaced = Shape(tuple(sizes), tuple(names), tuple(types), tuple(item_names))
+        if len(new) == len(dims):
+            return replaced
+        to_remove = dims[-(len(dims) - len(new)):]
+        return replaced.without(to_remove)
+
+    def _with_types(self, types: Union['Shape', str]):
+        """
+        Only for internal use.
+        Note: This method does not rename dimensions to comply with type requirements (e.g. ~ for dual dims).
+        """
+        if isinstance(types, Shape):
+            return Shape(self.sizes, self.names, tuple([types.get_type(name) if name in types else self_type for name, self_type in zip(self.names, self.types)]), self.item_names)
+        elif isinstance(types, str):
+            return Shape(self.sizes, self.names, (types,) * self.rank, self.item_names)
+        else:
+            raise ValueError(types)
+
+    def _with_item_names(self, item_names: tuple):
+        return Shape(self.sizes, self.names, self.types, item_names)
+
+    def _with_item_name(self, dim: str, item_name: tuple):
+        if dim not in self:
+            return self
+        item_names = list(self.item_names)
+        item_names[self.index(dim)] = item_name
+        return Shape(self.sizes, self.names, self.types, tuple(item_names))
+
+    def _perm(self, names: Tuple[str]) -> List[int]:
+        assert len(set(names)) == len(names), f"No duplicates allowed but got {names}"
+        assert len(names) >= len(self.names), f"Cannot find permutation for {self} given {names} because names {set(self.names) - set(names)} are missing"
+        assert len(names) <= len(self.names), f"Cannot find permutation for {self} given {names} because too many names were passed: {names}"
+        perm = [self.names.index(name) for name in names]
+        return perm
+
+    @property
+    def volume(self) -> Union[int, None]:
+        """
+        Returns the total number of values contained in a tensor of this shape.
+        This is the product of all dimension sizes.
+
+        Returns:
+            volume as `int` or `Tensor` or `None` if the shape is not `Shape.well_defined`
+        """
+        from phi.math import Tensor
+        for dim, size in self._named_sizes:
+            if isinstance(size, Tensor) and size.rank > 0:
+                non_uniform_dim = size.shape.names[0]
+                shapes = self.unstack(non_uniform_dim)
+                return sum(s.volume for s in shapes)
+        result = 1
+        for size in self.sizes:
+            if size is None:
+                return None
+            result *= size
+        return int(result)
+
+    @property
+    def is_empty(self) -> bool:
+        """ True if this shape has no dimensions. Equivalent to `Shape.rank` `== 0`. """
+        return len(self.sizes) == 0
+
+    def after_pad(self, widths: dict) -> 'Shape':
+        sizes = list(self.sizes)
+        item_names = list(self.item_names)
+        for dim, (lo, up) in widths.items():
+            if dim in self.names:
+                sizes[self.index(dim)] += lo + up
+                item_names[self.index(dim)] = None
+        return Shape(tuple(sizes), self.names, self.types, tuple(item_names))
+
+    def prepare_gather(self, dim: str, selection):
+        if isinstance(selection, Shape):
+            selection = selection.name if selection.rank == 1 else selection.names
+        if isinstance(selection, str) and ',' in selection:
+            selection = parse_dim_order(selection)
+        if isinstance(selection, str):  # single item name
+            item_names = self.get_item_names(dim, fallback_spatial=True)
+            assert item_names is not None, f"No item names defined for dim '{dim}' in tensor {self.shape} and dimension size does not match spatial rank."
+            assert selection in item_names, f"Accessing tensor.{dim}['{selection}'] failed. Item names are {item_names}."
+            selection = item_names.index(selection)
+        if isinstance(selection, (tuple, list)):
+            selection = list(selection)
+            if any([isinstance(s, str) for s in selection]):
+                item_names = self.get_item_names(dim, fallback_spatial=True)
+                for i, s in enumerate(selection):
+                    if isinstance(s, str):
+                        assert item_names is not None, f"Accessing tensor.{dim}['{s}'] failed because no item names are present on tensor {self.shape}"
+                        assert s in item_names, f"Accessing tensor.{dim}['{s}'] failed. Item names are {item_names}."
+                        selection[i] = item_names.index(s)
+            if not selection:  # empty
+                selection = slice(0, 0)
+        return selection
+
+    def after_gather(self, selection: dict) -> 'Shape':
+        result = self
+        for sel_dim, selection in selection.items():
+            if sel_dim not in self.names:
+                continue
+            selection = self.prepare_gather(sel_dim, selection)
+            if isinstance(selection, int):
+                if result.is_uniform:
+                    result = result.without(sel_dim)
+                else:
+                    from phi.math import Tensor
+                    gathered_sizes = [(s[{sel_dim: selection}] if isinstance(s, Tensor) else s) for s in result.sizes]
+                    gathered_sizes = [(int(s) if isinstance(s, Tensor) and s.rank == 0 else s) for s in gathered_sizes]
+                    result = result.with_sizes(gathered_sizes, keep_item_names=True).without(sel_dim)
+            elif isinstance(selection, slice):
+                assert isinstance(selection.step, int) or selection.step is None, f"slice step must be an int or None but got {type(selection.step).__name__}"
+                assert isinstance(selection.start, int) or selection.start is None, f"slice start must be an int or None but got {type(selection.start).__name__}"
+                assert isinstance(selection.stop, int) or selection.stop is None, f"slice stop must be an int or None but got {type(selection.stop).__name__}"
+                step = selection.step or 1
+                start = selection.start if isinstance(selection.start, int) else (0 if step > 0 else self.get_size(sel_dim)-1)
+                stop = selection.stop if isinstance(selection.stop, int) else (self.get_size(sel_dim) if step > 0 else -1)
+                if stop < 0 and step > 0:
+                    stop += self.get_size(sel_dim)
+                    assert stop >= 0
+                if start < 0 and step > 0:
+                    start += self.get_size(sel_dim)
+                    assert start >= 0
+                stop = min(stop, self.get_size(sel_dim))
+                new_size = math.to_int64(math.ceil(math.wrap((stop - start) / step)))
+                if new_size.rank == 0:
+                    new_size = int(new_size)  # NumPy array not allowed because not hashable
+                result = result._replace_single_size(sel_dim, new_size, keep_item_names=True)
+                if step < 0:
+                    result = result.flipped([sel_dim])
+                if self.get_item_names(sel_dim) is not None:
+                    result = result._with_item_name(sel_dim, tuple(self.get_item_names(sel_dim)[selection]))
+            elif isinstance(selection, (tuple, list)):
+                result = result._replace_single_size(sel_dim, len(selection))
+                if self.get_item_names(sel_dim) is not None:
+                    result = result._with_item_name(sel_dim, tuple([self.get_item_names(sel_dim)[i] for i in selection]))
+            else:
+                raise NotImplementedError(f"{type(selection)} not supported. Only (int, slice) allowed.")
+        return result
+
+    def meshgrid(self, names=False):
+        """
+        Builds a sequence containing all multi-indices within a tensor of this shape.
+        All indices are returned as `dict` mapping dimension names to `int` indices.
+
+        The corresponding values can be retrieved from Tensors and other Sliceables using `tensor[index]`.
+
+        This function currently only supports uniform tensors.
+
+        Args:
+            names: If `True`, replace indices by their item names if available.
+
+        Returns:
+            `dict` iterator.
+        """
+        assert self.is_uniform, f"Shape.meshgrid() is currently not supported for non-uniform tensors, {self}"
+        indices = [0] * self.rank
+        while True:
+            if names:
+                yield {dim: (names[index] if names is not None else index) for dim, index, names in zip(self.names, indices, self.item_names)}
+            else:
+                yield {dim: index for dim, index in zip(self.names, indices)}
+            for i in range(self.rank-1, -1, -1):
+                indices[i] = (indices[i] + 1) % self.sizes[i]
+                if indices[i] != 0:
+                    break
+            else:
+                return
+
+    def first_index(self, names=False):
+        return next(iter(self.meshgrid(names=names)))
+
+    def are_adjacent(self, dims: Union[str, tuple, list, set, 'Shape']):
+        indices = self.indices(dims)
+        return (max(indices) - min(indices)) == len(dims) - 1
+
+    def __add__(self, other):
+        return self._op2(other, lambda s, o: s + o, 0)
+
+    def __radd__(self, other):
+        return self._op2(other, lambda s, o: o + s, 0)
+
+    def __sub__(self, other):
+        return self._op2(other, lambda s, o: s - o, 0)
+
+    def __rsub__(self, other):
+        return self._op2(other, lambda s, o: o - s, 0)
+
+    def __mul__(self, other):
+        return self._op2(other, lambda s, o: s * o, 1)
+
+    def __rmul__(self, other):
+        return self._op2(other, lambda s, o: o * s, 1)
+
+    def _op2(self, other, fun, default: int):
+        if isinstance(other, int):
+            return Shape(tuple([fun(s, other) for s in self.sizes]), self.names, self.types, (None,) * self.rank)
+        elif isinstance(other, Shape):
+            merged = self.without_sizes() & other.without_sizes()
+            sizes = ()
+            for dim in merged.names:
+                self_val = self.get_size(dim) if dim in self else default
+                other_val = other.get_size(dim) if dim in other else default
+                sizes += (fun(self_val, other_val),)
+            return merged.with_sizes(sizes)
+        else:
+            return NotImplemented
+
+    def __hash__(self):
+        return hash(self.names)
+
+
+EMPTY_SHAPE = Shape((), (), (), ())
+""" Empty shape, `()` """
+
+DimFilter = Union[str, tuple, list, set, Shape, Callable]
+try:
+    DimFilter.__doc__ = """Dimension filters can be used with `Shape.only()` and `Shype.without()`, making them the standard tool for specifying sets of dimensions.
+    
+    The following types can be used as dimension filters:
+    
+    * `Shape` instances
+    * `tuple` or `list` objects containing dimension names as `str`
+    * Single `str` listing comma-separated dimension names
+    * Any function `filter(Shape) -> Shape`, such as `math.batch()`, `math.non_batch()`, `math.spatial()`, etc.
+    """  # docstring must be set explicitly
+except AttributeError:  # on older Python versions, this is not possible
+    pass
+
+
+class IncompatibleShapes(Exception):
+    """
+    Raised when the shape of a tensor does not match the other arguments.
+    """
+    def __init__(self, message, *shapes: Shape):
+        Exception.__init__(self, message)
+        self.shapes = shapes
+
+
+def parse_dim_names(obj: Union[str, tuple, list, Shape], count: int) -> tuple:
+    if isinstance(obj, str):
+        parts = obj.split(',')
+        result = []
+        for part in parts:
+            part = part.strip()
+            if part == '...':
+                result.extend([None] * (count - len(parts) + 1))
+            elif part == ':':
+                result.append(None)
+            else:
+                result.append(part)
+        assert len(result) == count, f"Number of specified names in '{obj}' does not match number of dimensions ({count})"
+        return tuple(result)
+    elif isinstance(obj, Shape):
+        assert len(obj) == count, f"Number of specified names in {obj} does not match number of dimensions ({count})"
+        return obj.names
+    elif isinstance(obj, (tuple, list)):
+        assert len(obj) == count, f"Number of specified names in {obj} does not match number of dimensions ({count})"
+        return tuple(obj)
+    raise ValueError(obj)
+
+
+def parse_dim_order(order: Union[str, tuple, list, Shape, None], check_rank: int = None) -> Union[tuple, None]:
+    if order is None:
+        if check_rank is not None:
+            assert check_rank <= 1, "When calling Tensor.native() or Tensor.numpy(), the dimension order must be specified for Tensors with more than one dimension. The listed default dimension order can vary depending on the chosen backend. Consider using math.reshaped_native(Tensor) instead."
+        return None
+    elif isinstance(order, Shape):
+        return order.names
+    if isinstance(order, list):
+        return tuple(order)
+    elif isinstance(order, tuple):
+        return order
+    elif isinstance(order, str):
+        parts = order.split(',')
+        parts = [p.strip() for p in parts if p]
+        return tuple(parts)
+    raise ValueError(order)
+
+
+def _construct_shape(dim_type: str, prefix: str, *args, **dims):
+    sizes = ()
+    names = []
+    item_names = ()
+    for arg in args:
+        parts = [s.strip() for s in arg.split(',')]
+        for name in parts:
+            assert name not in names, f"Duplicate dimension name {name}"
+            sizes += (None,)
+            names.append(name)
+            item_names += (None,)
+    for name, size in dims.items():
+        assert name not in names, f"Duplicate dimension name {name}"
+        if isinstance(size, str):
+            items = tuple([i.strip() for i in size.split(',')])
+            size = len(items)
+        elif isinstance(size, (tuple, list)):
+            assert all(isinstance(s, str) for s in size), f"Item names must all be of type 'str' but got '{size}'"
+            items = tuple(size)
+            size = len(items)
+        elif isinstance(size, Shape):
+            items = size.names
+            size = size.rank
+        elif size is None or isinstance(size, int):
+            # keep size
+            items = None
+        else:
+            items = None
+            from ._tensors import Tensor
+            if isinstance(size, Tensor):
+                size = int(size) if size.shape.volume == 1 else size
+            else:
+                try:
+                    size = int(size)
+                except ValueError:
+                    raise ValueError(f"Cannot construct dimension from {type(size).__name__}. Only int, tuple, list, str or Shape allowed. Got {size}")
+        names.append(name)
+        sizes += (size,)
+        item_names += (items,)
+    names = tuple(_apply_prefix(name, prefix) for name in names)
+    return math.Shape(sizes, names, (dim_type,) * len(sizes), item_names)
+
+
+def _apply_prefix(name: str, prefix: str):
+    match = re.search("\\w", name)
+    assert match, f"Dimension name must contain at least one letter or underscore but got '{name}'"
+    proper_name_index = match.start()
+    return prefix + name[proper_name_index:]
+
+
+def shape(obj) -> Shape:
+    """
+    If `obj` is a `Tensor` or `phi.math.magic.Shaped`, returns its shape.
+    If `obj` is a `Shape`, returns `obj`.
+
+    This function can be passed as a `dim` argument to an operation to specify that it should act upon all dimensions.
+
+    Args:
+        obj: `Tensor` or `Shape` or `Shaped`
+
+    Returns:
+        `Shape`
+    """
+    from phi.math.magic import PhiTreeNode, Shaped
+    if isinstance(obj, Shape):
+        return obj
+    elif hasattr(obj, '__shape__'):
+        return obj.__shape__()
+    elif hasattr(obj, 'shape') and isinstance(obj.shape, Shape):
+        return obj.shape
+    elif isinstance(obj, (int, float, complex, bool)):
+        return EMPTY_SHAPE
+    elif isinstance(obj, (tuple, list)) and all(isinstance(item, (int, float, complex, bool)) for item in obj):
+        return channel('vector')
+    elif isinstance(obj, (Number, bool)):
+        return EMPTY_SHAPE
+    elif isinstance(obj, (tuple, list)) and all(isinstance(item, (PhiTreeNode, Shaped)) for item in obj):
+        return merge_shapes(*obj, allow_varying_sizes=True)
+    if isinstance(obj, dict) and all(isinstance(item, (PhiTreeNode, Shaped)) for item in obj):
+        return merge_shapes(*obj.values(), allow_varying_sizes=True)
+    elif isinstance(obj, PhiTreeNode):
+        from phi.math._magic_ops import all_attributes
+        return merge_shapes(*[getattr(obj, a) for a in all_attributes(obj, assert_any=True)], allow_varying_sizes=True)
+    else:
+        from .backend import choose_backend, NoBackendFound
+        try:
+            backend = choose_backend(obj)
+            shape_tuple = backend.staticshape(obj)
+            if len(shape_tuple) == 0:
+                return EMPTY_SHAPE
+            elif len(shape_tuple) == 1:
+                return channel('vector')
+            else:
+                raise ValueError(f"Cannot auto-complete shape of {backend} tensor with shape {shape_tuple}. Only 0D and 1D tensors have a Φ-Flow shape by default.")
+        except NoBackendFound:
+            raise ValueError(f'shape() requires Shaped or Shape argument but got {type(obj)}')
+
+
+def spatial(*args, **dims: Union[int, str, tuple, list, Shape]) -> Shape:
+    """
+    Returns the spatial dimensions of an existing `Shape` or creates a new `Shape` with only spatial dimensions.
+
+    Usage for filtering spatial dimensions:
+    >>> spatial_dims = spatial(shape)
+    >>> spatial_dims = spatial(tensor)
+
+    Usage for creating a `Shape` with only spatial dimensions:
+    >>> spatial_shape = spatial('undef', x=2, y=3)
+    (x=2, y=3, undef=None)
+
+    Here, the dimension `undef` is created with an undefined size of `None`.
+    Undefined sizes are automatically filled in by `tensor`, `wrap`, `stack` and `concat`.
+
+    To create a shape with multiple types, use `merge_shapes()`, `concat_shapes()` or the syntax `shape1 & shape2`.
+
+    See Also:
+        `channel`, `batch`, `instance`
+
+    Args:
+        *args: Either
+
+            * `Shape` or `Tensor` to filter or
+            * Names of dimensions with undefined sizes as `str`.
+
+        **dims: Dimension sizes and names. Must be empty when used as a filter operation.
+
+    Returns:
+        `Shape` containing only dimensions of type spatial.
+    """
+    from .magic import Shaped
+    if all(isinstance(arg, str) for arg in args) or dims:
+        return _construct_shape(SPATIAL_DIM, '', *args, **dims)
+    elif len(args) == 1 and isinstance(args[0], Shape):
+        return args[0].spatial
+    elif len(args) == 1 and isinstance(args[0], Shaped):
+        return shape(args[0]).spatial
+    else:
+        raise AssertionError(f"spatial() must be called either as a selector spatial(Shape) or spatial(Tensor) or as a constructor spatial(*names, **dims). Got *args={args}, **dims={dims}")
+
+
+def channel(*args, **dims: Union[int, str, tuple, list, Shape]) -> Shape:
+    """
+    Returns the channel dimensions of an existing `Shape` or creates a new `Shape` with only channel dimensions.
+
+    Usage for filtering channel dimensions:
+    >>> channel_dims = channel(shape)
+    >>> channel_dims = channel(tensor)
+
+    Usage for creating a `Shape` with only channel dimensions:
+    >>> channel_shape = channel('undef', vector=2)
+    (vector=2, undef=None)
+
+    Here, the dimension `undef` is created with an undefined size of `None`.
+    Undefined sizes are automatically filled in by `tensor`, `wrap`, `stack` and `concat`.
+
+    To create a shape with multiple types, use `merge_shapes()`, `concat_shapes()` or the syntax `shape1 & shape2`.
+
+    See Also:
+        `spatial`, `batch`, `instance`
+
+    Args:
+        *args: Either
+
+            * `Shape` or `Tensor` to filter or
+            * Names of dimensions with undefined sizes as `str`.
+
+        **dims: Dimension sizes and names. Must be empty when used as a filter operation.
+
+    Returns:
+        `Shape` containing only dimensions of type channel.
+    """
+    from .magic import Shaped
+    if all(isinstance(arg, str) for arg in args) or dims:
+        return _construct_shape(CHANNEL_DIM, '', *args, **dims)
+    elif len(args) == 1 and isinstance(args[0], Shape):
+        return args[0].channel
+    elif len(args) == 1 and isinstance(args[0], Shaped):
+        return shape(args[0]).channel
+    else:
+        raise AssertionError(f"channel() must be called either as a selector channel(Shape) or channel(Tensor) or as a constructor channel(*names, **dims). Got *args={args}, **dims={dims}")
+
+
+def batch(*args, **dims: Union[int, str, tuple, list, Shape]) -> Shape:
+    """
+    Returns the batch dimensions of an existing `Shape` or creates a new `Shape` with only batch dimensions.
+
+    Usage for filtering batch dimensions:
+    >>> batch_dims = batch(shape)
+    >>> batch_dims = batch(tensor)
+
+    Usage for creating a `Shape` with only batch dimensions:
+    >>> batch_shape = batch('undef', batch=2)
+    (batch=2, undef=None)
+
+    Here, the dimension `undef` is created with an undefined size of `None`.
+    Undefined sizes are automatically filled in by `tensor`, `wrap`, `stack` and `concat`.
+
+    To create a shape with multiple types, use `merge_shapes()`, `concat_shapes()` or the syntax `shape1 & shape2`.
+
+    See Also:
+        `channel`, `spatial`, `instance`
+
+    Args:
+        *args: Either
+
+            * `Shape` or `Tensor` to filter or
+            * Names of dimensions with undefined sizes as `str`.
+
+        **dims: Dimension sizes and names. Must be empty when used as a filter operation.
+
+    Returns:
+        `Shape` containing only dimensions of type batch.
+    """
+    from .magic import Shaped
+    if all(isinstance(arg, str) for arg in args) or dims:
+        return _construct_shape(BATCH_DIM, '', *args, **dims)
+    elif len(args) == 1 and isinstance(args[0], Shape):
+        return args[0].batch
+    elif len(args) == 1 and isinstance(args[0], Shaped):
+        return shape(args[0]).batch
+    else:
+        raise AssertionError(f"batch() must be called either as a selector batch(Shape) or batch(Tensor) or as a constructor batch(*names, **dims). Got *args={args}, **dims={dims}")
+
+
+def instance(*args, **dims: Union[int, str, tuple, list, Shape]) -> Shape:
+    """
+    Returns the instance dimensions of an existing `Shape` or creates a new `Shape` with only instance dimensions.
+
+    Usage for filtering instance dimensions:
+    >>> instance_dims = instance(shape)
+    >>> instance_dims = instance(tensor)
+
+    Usage for creating a `Shape` with only instance dimensions:
+    >>> instance_shape = instance('undef', points=2)
+    (points=2, undef=None)
+
+    Here, the dimension `undef` is created with an undefined size of `None`.
+    Undefined sizes are automatically filled in by `tensor`, `wrap`, `stack` and `concat`.
+
+    To create a shape with multiple types, use `merge_shapes()`, `concat_shapes()` or the syntax `shape1 & shape2`.
+
+    See Also:
+        `channel`, `batch`, `spatial`
+
+    Args:
+        *args: Either
+
+            * `Shape` or `Tensor` to filter or
+            * Names of dimensions with undefined sizes as `str`.
+
+        **dims: Dimension sizes and names. Must be empty when used as a filter operation.
+
+    Returns:
+        `Shape` containing only dimensions of type instance.
+    """
+    from .magic import Shaped
+    if all(isinstance(arg, str) for arg in args) or dims:
+        return _construct_shape(INSTANCE_DIM, '', *args, **dims)
+    elif len(args) == 1 and isinstance(args[0], Shape):
+        return args[0].instance
+    elif len(args) == 1 and isinstance(args[0], Shaped):
+        return shape(args[0]).instance
+    else:
+        raise AssertionError(f"instance() must be called either as a selector instance(Shape) or instance(Tensor) or as a constructor instance(*names, **dims). Got *args={args}, **dims={dims}")
+
+
+def dual(*args, **dims: Union[int, str, tuple, list, Shape]) -> Shape:
+    """
+    Returns the dual dimensions of an existing `Shape` or creates a new `Shape` with only dual dimensions.
+
+    Dual dimensions are assigned the prefix `~` to distinguish them from regular dimensions.
+    This way, a regular and dual dimension of the same name can exist in one `Shape`.
+
+    Dual dimensions represent the input space and are typically only present on matrices or higher-order matrices.
+    Dual dimensions behave like batch dimensions in regular operations, if supported.
+    During matrix multiplication, they are matched against their regular counterparts by name (ignoring the `~` prefix).
+
+    Usage for filtering dual dimensions:
+
+    >>> dual_dims = dual(shape)
+    >>> dual_dims = dual(tensor)
+
+    Usage for creating a `Shape` with only dual dimensions:
+
+    >>> dual('undef', points=2)
+    (~undefᵈ=None, ~pointsᵈ=2)
+
+    Here, the dimension `undef` is created with an undefined size of `None`.
+    Undefined sizes are automatically filled in by `tensor`, `wrap`, `stack` and `concat`.
+
+    To create a shape with multiple types, use `merge_shapes()`, `concat_shapes()` or the syntax `shape1 & shape2`.
+
+    See Also:
+        `channel`, `batch`, `spatial`
+
+    Args:
+        *args: Either
+
+            * `Shape` or `Tensor` to filter or
+            * Names of dimensions with undefined sizes as `str`.
+
+        **dims: Dimension sizes and names. Must be empty when used as a filter operation.
+
+    Returns:
+        `Shape` containing only dimensions of type dual.
+    """
+    from .magic import Shaped
+    if all(isinstance(arg, str) for arg in args) or dims:
+        return _construct_shape(DUAL_DIM, '~', *args, **dims)
+    elif len(args) == 1 and isinstance(args[0], Shape):
+        return args[0].dual
+    elif len(args) == 1 and isinstance(args[0], Shaped):
+        return shape(args[0]).dual
+    else:
+        raise AssertionError(f"dual() must be called either as a selector dual(Shape) or dual(Tensor) or as a constructor dual(*names, **dims). Got *args={args}, **dims={dims}")
+
+
+DIM_FUNCTIONS = {BATCH_DIM: batch, SPATIAL_DIM: spatial, INSTANCE_DIM: instance, CHANNEL_DIM: channel, DUAL_DIM: dual}
+
+
+def merge_shapes(*objs: Union[Shape, Any], order=(batch, dual, instance, spatial, channel), allow_varying_sizes=False):
+    """
+    Combines `shapes` into a single `Shape`, grouping dimensions by type.
+    If dimensions with equal names are present in multiple shapes, their types and sizes must match.
+
+    The shorthand `shape1 & shape2` merges shapes with `check_exact=[spatial]`.
+
+    See Also:
+        `concat_shapes()`.
+
+    Args:
+        *objs: `Shape` or `Shaped` objects to combine.
+        order: Dimension type order as `tuple` of type filters (`channel`, `batch`, `spatial` or `instance`). Dimensions are grouped by type while merging.
+
+    Returns:
+        Merged `Shape`
+
+    Raises:
+        IncompatibleShapes if the shapes are not compatible
+    """
+    if not objs:
+        return EMPTY_SHAPE
+    shapes = [obj if isinstance(obj, Shape) else shape(obj) for obj in objs]
+    merged = []
+    for dim_type in order:
+        type_group = dim_type(shapes[0])
+        for sh in shapes[1:]:
+            sh = dim_type(sh)
+            for dim in sh:
+                if dim not in type_group:
+                    type_group = type_group._expand(dim, pos=-1)
+                else:  # check size match
+                    sizes_match = _size_equal(dim.size, type_group.get_size(dim.name))
+                    if allow_varying_sizes:
+                        if not sizes_match:
+                            type_group = type_group.with_dim_size(dim, None)
+                    else:
+                        if not sizes_match:
+                            raise IncompatibleShapes(f"Cannot merge shapes {shapes} because dimension '{dim.name}' exists with different sizes.", *shapes)
+                        names1 = type_group.get_item_names(dim)
+                        names2 = sh.get_item_names(dim)
+                        if names1 is not None and names2 is not None and len(names1) > 1:
+                            if names1 != names2:
+                                if set(names1) == set(names2):
+                                    raise IncompatibleShapes(f"Inconsistent component order: '{','.join(names1)}' vs '{','.join(names2)}' in dimension '{dim.name}'. Failed to merge shapes {shapes}", *shapes)
+                                else:
+                                    raise IncompatibleShapes(f"Cannot merge shapes {shapes} because dimension '{dim.name}' exists with different item names.", *shapes)
+                        elif names1 is None and names2 is not None:
+                            type_group = type_group._with_item_name(dim, tuple(names2))
+        merged.append(type_group)
+    return concat_shapes(*merged)
+
+
+def non_batch(obj) -> Shape:
+    """
+    Returns the non-batch dimensions of an object.
+
+    Args:
+        obj: `Shape` or object with a valid `shape` property.
+
+    Returns:
+        `Shape`
+    """
+    from .magic import Shaped
+    if isinstance(obj, Shape):
+        return obj.non_batch
+    elif isinstance(obj, Shaped):
+        return shape(obj).non_batch
+    else:
+        raise AssertionError(f"non_batch() must be called either on a Shape or an object with a 'shape' property but got {obj}")
+
+
+def non_spatial(obj) -> Shape:
+    """
+    Returns the non-spatial dimensions of an object.
+
+    Args:
+        obj: `Shape` or object with a valid `shape` property.
+
+    Returns:
+        `Shape`
+    """
+    from .magic import Shaped
+    if isinstance(obj, Shape):
+        return obj.non_spatial
+    elif isinstance(obj, Shaped):
+        return shape(obj).non_spatial
+    else:
+        raise AssertionError(f"non_spatial() must be called either on a Shape or an object with a 'shape' property but got {obj}")
+
+
+def non_instance(obj) -> Shape:
+    """
+    Returns the non-instance dimensions of an object.
+
+    Args:
+        obj: `Shape` or object with a valid `shape` property.
+
+    Returns:
+        `Shape`
+    """
+    from .magic import Shaped
+    if isinstance(obj, Shape):
+        return obj.non_instance
+    elif isinstance(obj, Shaped):
+        return shape(obj).non_instance
+    else:
+        raise AssertionError(f"non_instance() must be called either on a Shape or an object with a 'shape' property but got {obj}")
+
+
+def non_channel(obj) -> Shape:
+    """
+    Returns the non-channel dimensions of an object.
+
+    Args:
+        obj: `Shape` or object with a valid `shape` property.
+
+    Returns:
+        `Shape`
+    """
+    from .magic import Shaped
+    if isinstance(obj, Shape):
+        return obj.non_channel
+    elif isinstance(obj, Shaped):
+        return shape(obj).non_channel
+    else:
+        raise AssertionError(f"non_channel() must be called either on a Shape or an object with a 'shape' property but got {obj}")
+
+
+def non_dual(obj) -> Shape:
+    """
+    Returns the non-dual dimensions of an object.
+
+    Args:
+        obj: `Shape` or object with a valid `shape` property.
+
+    Returns:
+        `Shape`
+    """
+    from .magic import Shaped
+    if isinstance(obj, Shape):
+        return obj.non_dual
+    elif isinstance(obj, Shaped):
+        return shape(obj).non_dual
+    else:
+        raise AssertionError(f"non_dual() must be called either on a Shape or an object with a 'shape' property but got {obj}")
+
+
+def non_primal(obj) -> Shape:
+    """
+    Returns the batch and dual dimensions of an object.
+
+    Args:
+        obj: `Shape` or object with a valid `shape` property.
+
+    Returns:
+        `Shape`
+    """
+    from .magic import Shaped
+    if isinstance(obj, Shape):
+        return obj.non_primal
+    elif isinstance(obj, Shaped):
+        return shape(obj).non_primal
+    else:
+        raise AssertionError(f"non_dual() must be called either on a Shape or an object with a 'shape' property but got {obj}")
+
+
+def primal(obj) -> Shape:
+    """
+    Returns the instance, spatial and channel dimensions of an object.
+
+    Args:
+        obj: `Shape` or object with a valid `shape` property.
+
+    Returns:
+        `Shape`
+    """
+    from .magic import Shaped
+    if isinstance(obj, Shape):
+        return obj.primal
+    elif isinstance(obj, Shaped):
+        return shape(obj).primal
+    else:
+        raise AssertionError(f"primal() must be called either on a Shape or an object with a 'shape' property but got {obj}")
+
+
+def _size_equal(s1, s2):
+    if s1 is None:
+        return s2 is None
+    if s2 is None:
+        return False
+    if isinstance(s1, int):
+        return isinstance(s2, int) and s2 == s1
+    else:
+        return math.close(s1, s2)
+
+
+def concat_shapes(*shapes: Union[Shape, Any]) -> Shape:
+    """
+    Creates a `Shape` listing the dimensions of all `shapes` in the given order.
+
+    See Also:
+        `merge_shapes()`.
+
+    Args:
+        *shapes: Shapes to concatenate. No two shapes must contain a dimension with the same name.
+
+    Returns:
+        Combined `Shape`.
+    """
+    shapes = [obj if isinstance(obj, Shape) else shape(obj) for obj in shapes]
+    names = sum([s.names for s in shapes], ())
+    if len(set(names)) != len(names):
+        raise IncompatibleShapes(f"Cannot concatenate shapes {list(shapes)}. Duplicate dimension names are not allowed.")
+    sizes = sum([s.sizes for s in shapes], ())
+    types = sum([s.types for s in shapes], ())
+    item_names = sum([s.item_names for s in shapes], ())
+    return Shape(sizes, names, types, item_names)
+
+
+def shape_stack(stack_dim: Shape, *shapes: Shape):
+    """ Returns the shape of a tensor created by stacking tensors with `shapes`. """
+    names = list(stack_dim.names)
+    types = list(stack_dim.types)
+    item_names = list(stack_dim.item_names)
+    for other in shapes:
+        for size, name, type, items in other._dimensions:
+            if name not in names:
+                if type in types:
+                    index = len(types) - types[::-1].index(type)
+                elif type == BATCH_DIM:
+                    index = 0
+                elif type == DUAL_DIM:
+                    index = min([len(names), *[i for i in range(len(names)) if types[i] == DUAL_DIM]])
+                elif type == CHANNEL_DIM:
+                    index = len(names)
+                elif type == SPATIAL_DIM:
+                    index = min([len(names), *[i for i in range(len(names)) if types[i] == CHANNEL_DIM]])
+                elif type == INSTANCE_DIM:
+                    index = min([len(names), *[i for i in range(len(names)) if types[i] == INSTANCE_DIM]])
+                else:
+                    raise ValueError(type)
+                names.insert(index, name)
+                types.insert(index, type)
+                item_names.insert(index, items)
+            else:
+                index = names.index(name)
+                if items != item_names[index]:
+                    if item_names[index] is None:
+                        item_names[index] = items
+                    else:
+                        warnings.warn(f"Stacking shapes with incompatible item names will result in item names being lost. Got {item_names[index]} and {items}", RuntimeWarning)
+                        item_names[index] = None
+    sizes = []
+    for name in names:
+        if name == stack_dim.name:
+            size = len(shapes)
+        else:
+            dim_sizes = [(shape.get_size(name) if name in shape else 1) for shape in shapes]
+            if all([math.close(s, dim_sizes[0]) for s in dim_sizes[1:]]):
+                size = dim_sizes[0]
+            else:
+                from ._magic_ops import stack
+                from ._tensors import wrap
+                dim_sizes = [wrap(d) for d in dim_sizes]
+                size = stack(dim_sizes, stack_dim)
+        sizes.append(size)
+    return Shape(tuple(sizes), tuple(names), tuple(types), tuple(item_names))
+
+
+def vector_add(*shapes: Shape):
+    if not shapes:
+        return EMPTY_SHAPE
+    names = shapes[0].names
+    types = shapes[0].types
+    item_names = shapes[0].item_names
+    for shape in shapes[1:]:
+        for name in shape.names:
+            if name not in names:
+                names += (name,)
+                types += (shape.get_type(name),)
+                item_names += (shape.get_item_names(name),)
+    sizes = [sum(sh.get_size(dim) if dim in sh else 0 for sh in shapes) for dim in names]
+    return Shape(tuple(sizes), names, types, item_names)
```

### Comparing `phiflow-2.3.4/phi/math/_sparse.py` & `phiflow-2.4.0/phi/math/_sparse.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,660 +1,949 @@
-import warnings
-from numbers import Number
-from typing import List, Callable, Tuple, Union
-
-import numpy as np
-import scipy.sparse
-
-from ._shape import Shape, non_batch, merge_shapes, instance, batch, non_instance, shape, channel, spatial, DimFilter, concat_shapes, EMPTY_SHAPE, dual, DUAL_DIM, SPATIAL_DIM
-from ._magic_ops import concat, pack_dims, expand, rename_dims
-from ._tensors import Tensor, TensorStack, CollapsedTensor, NativeTensor, cached, wrap
-from .backend import choose_backend, NUMPY
-from .backend._dtype import DType
-
-
-class SparseCoordinateTensor(Tensor):
-
-    def __init__(self, indices: Tensor, values: Tensor, dense_shape: Shape, can_contain_double_entries: bool, indices_sorted: bool):
-        """
-        Construct a sparse tensor with any number of sparse, dense and batch dimensions.
-
-        Args:
-            indices: `Tensor` encoding the positions of stored values. It has the following dimensions:
-
-                * One instance dimension exactly matching the instance dimension on `values`.
-                  It enumerates the positions of stored entries.
-                * One channel dimension called `vector`.
-                  Its item names must match the dimension names of `dense_shape` but the order can be arbitrary.
-                * Any number of batch dimensions
-
-            values: `Tensor` containing the stored values at positions given by `indices`. It has the following dimensions:
-
-                * One instance dimension exactly matching the instance dimension on `indices`.
-                  It enumerates the values of stored entries.
-                * Any number of channel dimensions if multiple values are stored at each index.
-                * Any number of batch dimensions
-
-            dense_shape: Dimensions listed in `indices`.
-                The order can differ from the item names of `indices`.
-            can_contain_double_entries: Whether some indices might occur more than once.
-                If so, values at the same index will be summed.
-            indices_sorted: Whether the indices are sorted in ascending order given the dimension order of the item names of `indices`.
-        """
-        assert instance(indices), "indices must have an instance dimension"
-        assert 'vector' in indices.shape, "indices must have a vector dimension"
-        assert set(indices.vector.item_names) == set(dense_shape.names), "The 'vector' dimension of indices must list the dense dimensions as item names"
-        assert indices.dtype.kind == int, f"indices must have dtype=int but got {indices.dtype}"
-        self._shape = merge_shapes(dense_shape, batch(indices), non_instance(values))
-        self._dense_shape = dense_shape
-        self._indices = indices
-        self._values = values
-        self._can_contain_double_entries = can_contain_double_entries
-        self._indices_sorted = indices_sorted
-
-    @property
-    def shape(self) -> Shape:
-        return self._shape
-
-    @property
-    def dtype(self) -> DType:
-        return self._values.dtype
-
-    def native(self, order: Union[str, tuple, list, Shape] = None):
-        raise RuntimeError("Sparse tensors do not have a native representation. Use math.dense(tensor).native() instead")
-
-    @property
-    def _is_tracer(self) -> bool:
-        return self._indices._is_tracer or self._values._is_tracer
-
-    def _with_values(self, new_values: Tensor):
-        return SparseCoordinateTensor(self._indices, new_values, self._dense_shape, self._can_contain_double_entries, self._indices_sorted)
-
-    def _natives(self) -> tuple:
-        indices_const = self._indices.default_backend is not self._values.default_backend
-        if indices_const:
-            return self._values._natives()  # If we return NumPy arrays, they might get converted in function transformations
-        else:
-            return self._values._natives() + self._indices._natives()
-
-    def _spec_dict(self) -> dict:
-        indices_const = self._indices.default_backend is not self._values.default_backend
-        return {'type': SparseCoordinateTensor,
-                'shape': self._shape,
-                'dense_shape': self._dense_shape,
-                'indices': self._indices if indices_const else self._indices._spec_dict(),
-                'values': self._values._spec_dict(),
-                'can_contain_double_entries': self._can_contain_double_entries,
-                'indices_sorted': self._indices_sorted}
-
-    @classmethod
-    def _from_spec_and_natives(cls, spec: dict, natives: list):
-        values = spec['values']['type']._from_spec_and_natives(spec['values'], natives)
-        indices_or_spec = spec['indices']
-        if isinstance(indices_or_spec, Tensor):
-            indices = indices_or_spec
-        else:
-            indices = spec['indices']['type']._from_spec_and_natives(spec['indices'], natives)
-        return SparseCoordinateTensor(indices, values, spec['dense_shape'], spec['can_contain_double_entries'], spec['indices_sorted'])
-
-    def _native_coo_components(self, col_dims: DimFilter, matrix=False):
-        col_dims = self._shape.only(col_dims)
-        row_dims = self._dense_shape.without(col_dims)
-        row_idx_packed, col_idx_packed = self._pack_indices(row_dims, col_dims)
-        from phi.math import reshaped_native
-        ind_batch = batch(self._indices)
-        channels = non_instance(self._values).without(ind_batch)
-        if matrix:
-            native_indices = choose_backend(row_idx_packed, col_idx_packed).stack([row_idx_packed, col_idx_packed], -1)
-            native_shape = (row_dims.volume, col_dims.volume)
-        else:
-            native_indices = reshaped_native(self._indices, [ind_batch, instance, 'vector'], force_expand=True)
-            native_shape = self._dense_shape.sizes
-        native_values = reshaped_native(self._values, [ind_batch, instance, channels])
-        return ind_batch, channels, native_indices, native_values, native_shape
-
-    def dual_indices(self, to_primal=False):
-        """ Unpacked column indices """
-        idx = self._indices[self._dense_shape.dual]
-        if to_primal:
-            dual_names = idx.shape.get_item_names('vector')
-            primal_names = spatial(*dual_names).names
-            idx = rename_dims(idx, 'vector', channel(vector=primal_names))
-        return idx
-
-    def primal_indices(self):
-        """ Unpacked row indices """
-        return self._indices[self._dense_shape.non_dual]
-
-    def _pack_indices(self, row_dims: Shape, col_dims: Shape):
-        assert self._indices.default_backend is NUMPY, "Can only compress NumPy indices as of yet"
-        assert row_dims in self._dense_shape, f"Can only compress sparse dims but got {row_dims} which contains non-sparse dims"
-        from ._ops import reshaped_native
-        row_idx = self._indices[row_dims.names]
-        col_idx = self._indices[self._dense_shape.without(row_dims).names]
-        # ToDo if not row_dims: idx = [0]
-        row_idx_packed = np.ravel_multi_index(reshaped_native(row_idx, [channel, batch, instance]), row_dims.sizes)
-        col_idx_packed = np.ravel_multi_index(reshaped_native(col_idx, [channel, batch, instance]), col_dims.sizes)
-        return row_idx_packed, col_idx_packed
-
-    def _unpack_indices(self, row_idx_packed, col_idx_packed, row_dims: Shape, col_dims: Shape, ind_batch: Shape):
-        row_idx = np.stack(np.unravel_index(row_idx_packed, row_dims.sizes), -1)
-        col_idx = np.stack(np.unravel_index(col_idx_packed, col_dims.sizes), -1)
-        np_indices = np.concatenate([row_idx, col_idx], -1)
-        from ._ops import reshaped_tensor
-        idx_dim = channel(**{channel(self._indices).name: row_dims.names + col_dims.names})
-        indices = reshaped_tensor(np_indices, [ind_batch, instance(self._indices), idx_dim], convert=False)
-        return indices
-
-    def compress_rows(self):
-        return self.compress(self._dense_shape.non_dual)
-
-    def compress_cols(self):
-        return self.compress(self._dense_shape.dual)
-
-    def compress(self, dims: DimFilter):
-        c_dims = self._shape.only(dims, reorder=True)
-        u_dims = self._dense_shape.without(c_dims)
-        c_idx_packed, u_idx_packed = self._pack_indices(c_dims, u_dims)
-        # --- Use scipy.sparse.csr_matrix to reorder values ---
-        idx = np.arange(1, c_idx_packed.shape[-1] + 1)  # start indexing at 1 since 0 might get removed
-        scipy_csr = scipy.sparse.csr_matrix((idx, (c_idx_packed[0], u_idx_packed[0])), shape=(c_dims.volume, u_dims.volume))
-        assert c_idx_packed.shape[1] == len(scipy_csr.data), "Failed to create CSR matrix because the CSR matrix contains fewer non-zero values than COO. This can happen when the `x` tensor is too small for the stencil."
-        # --- Construct CompressedSparseMatrix ---
-        entries_dim = instance(self._values).name
-        perm = {entries_dim: wrap(scipy_csr.data - 1, instance(entries_dim))}
-        values = self._values[perm]  # Change order accordingly
-        indices = wrap(scipy_csr.indices, instance(entries_dim))
-        pointers = wrap(scipy_csr.indptr, instance('pointers'))
-        return CompressedSparseMatrix(indices, pointers, values, u_dims, c_dims, uncompressed_indices=self._indices, uncompressed_indices_perm=perm)
-
-    def __pack_dims__(self, dims: Tuple[str, ...], packed_dim: Shape, pos: Union[int, None], **kwargs) -> 'Tensor':
-        dims = self._shape.only(dims)
-        assert dims in self._dense_shape, f"Can only pack sparse dimensions on SparseCoordinateTensor but got {dims} of which {dims.without(self._dense_shape)} are not sparse"
-        assert self._indices.default_backend is NUMPY, "Can only pack NumPy indices as of yet"
-        from ._ops import reshaped_native
-        idx = self._indices.vector[dims.names]
-        idx_packed = np.ravel_multi_index(reshaped_native(idx, [channel, instance]), dims.sizes)
-        idx_packed = expand(wrap(idx_packed, instance(self._indices)), channel(vector=packed_dim.name))
-        indices = concat([self._indices.vector[self._dense_shape.without(dims).names], idx_packed], 'vector')
-        dense_shape = concat_shapes(self._dense_shape.without(dims), packed_dim.with_size(dims.volume))
-        idx_sorted = self._indices_sorted and False  # ToDo still sorted if dims are ordered correctly and no other dim in between and inserted at right point
-        return SparseCoordinateTensor(indices, self._values, dense_shape, self._can_contain_double_entries, idx_sorted)
-
-    def _with_shape_replaced(self, new_shape: Shape):
-        assert self._shape.rank == new_shape.rank
-        dense_shape = new_shape[self._shape.indices(self._dense_shape)]
-        new_item_names = new_shape[self._shape.indices(self._indices.shape.get_item_names('vector'))].names
-        indices = self._indices._with_shape_replaced(self._indices.shape.replace(self._shape, new_shape).with_dim_size('vector', new_item_names))
-        values = self._values._with_shape_replaced(self._values.shape.replace(self._shape, new_shape))
-        return SparseCoordinateTensor(indices, values, dense_shape, self._can_contain_double_entries, self._indices_sorted)
-
-
-class CompressedSparseMatrix(Tensor):
-
-    def __init__(self,
-                 indices: Tensor,
-                 pointers: Tensor,
-                 values: Tensor,
-                 uncompressed_dims: Shape,
-                 compressed_dims: Shape,
-                 uncompressed_offset: int = None,
-                 uncompressed_indices: Tensor = None,
-                 uncompressed_indices_perm: Tensor = None):
-        """
-
-        Args:
-            indices: indices must be sorted in ascending order by compressed_dim and other sparse dims.
-                Must have one or multiple instance dimensions and can have any number of batch dimensions.
-                No spatial and channel dimensions allowed.
-            pointers:
-            values:
-            compressed_dims: Sparse dimensions with compressed pointer representation.
-                Only one pointer array is used per matrix, i.e. the dimensions are packed internally.
-                These dimensions are indexed by `pointers`.
-            uncompressed_dims: Sparse dimensions with full index storage.
-                These dimensions are indexed by `indices`.
-            uncompressed_offset: For sliced sparse tensors.
-                If `None`, indicates that all entries lie within bounds.
-                If an `int`, indicate that this is a slice of a larger compressed sparse matrix.
-                Indices actually refer to `indices - uncompressed_offset` within this matrix, i.e. they may reference phantom values to the left or right of the matrix.
-                The `values` corresponding to phantom entries must all be 0.
-                The size of the slice is given by `compressed_dims.volume`.
-        """
-        assert instance(indices), "indices must have an instance dimension"
-        assert instance(pointers), "pointers must have an instance dimension"
-        assert instance(values) == instance(indices), "Instance dimensions of values and indices must match exactly"
-        assert not channel(indices) and not spatial(indices), f"channel and spatial dimensions not allowed on indices but got {shape(indices)}"
-        assert not channel(pointers) and not spatial(pointers), f"channel and spatial dimensions not allowed on pointers but got {shape(pointers)}"
-        assert uncompressed_dims.isdisjoint(compressed_dims), f"Dimensions cannot be compressed and uncompressed at the same time but got compressed={compressed_dims}, uncompressed={uncompressed_dims}"
-        assert instance(pointers).size == compressed_dims.volume + 1
-        self._shape = merge_shapes(compressed_dims, uncompressed_dims, batch(indices), batch(pointers), non_instance(values))
-        self._indices = indices
-        self._pointers = pointers
-        self._values = values
-        self._uncompressed_dims = uncompressed_dims
-        self._compressed_dims = compressed_dims
-        self._uncompressed_offset = uncompressed_offset
-        self._uncompressed_indices = uncompressed_indices
-        self._uncompressed_indices_perm = uncompressed_indices_perm
-
-    @property
-    def shape(self) -> Shape:
-        return self._shape
-
-    @property
-    def sparse_dims(self):
-        return self._compressed_dims & self._uncompressed_dims
-
-    @property
-    def sparsity_batch(self):
-        return batch(self._indices) & batch(self._pointers)
-
-    @property
-    def dtype(self) -> DType:
-        return self._values.dtype
-
-    @property
-    def _is_tracer(self) -> bool:
-        return self._values._is_tracer or self._indices._is_tracer or self._pointers._is_tracer
-
-    def _natives(self) -> tuple:
-        indices_const = self._indices.default_backend is not self._values.default_backend
-        pointers_const = self._pointers.default_backend is not self._values.default_backend
-        result = self._values._natives()
-        if not indices_const:
-            result += self._indices._natives()
-        if not pointers_const:
-            result += self._pointers._natives()
-        return result
-
-    def _spec_dict(self) -> dict:
-        indices_const = self._indices.default_backend is not self._values.default_backend
-        pointers_const = self._pointers.default_backend is not self._values.default_backend
-        return {'type': CompressedSparseMatrix,
-                'shape': self._shape,
-                'values': self._values._spec_dict(),
-                'indices': self._indices if indices_const else self._indices._spec_dict(),
-                'pointers': self._pointers if pointers_const else self._pointers._spec_dict(),
-                'uncompressed_dims': self._uncompressed_dims,
-                'compressed_dims': self._compressed_dims,
-                'uncompressed_offset': self._uncompressed_offset,
-                'uncompressed_indices': self._uncompressed_indices,
-                'uncompressed_indices_perm': self._uncompressed_indices_perm,
-                }
-
-    @classmethod
-    def _from_spec_and_natives(cls, spec: dict, natives: list):
-        values = spec['values']['type']._from_spec_and_natives(spec['values'], natives)
-        indices_or_spec = spec['indices']
-        if isinstance(indices_or_spec, Tensor):
-            indices = indices_or_spec
-        else:
-            indices = spec['indices']['type']._from_spec_and_natives(spec['indices'], natives)
-        pointers_or_spec = spec['pointers']
-        if isinstance(pointers_or_spec, Tensor):
-            pointers = pointers_or_spec
-        else:
-            pointers = spec['pointers']['type']._from_spec_and_natives(spec['pointers'], natives)
-        return CompressedSparseMatrix(indices, pointers, values, spec['uncompressed_dims'], spec['compressed_dims'], spec['uncompressed_offset'], spec['uncompressed_indices'], spec['uncompressed_indices_perm'])
-
-    def _getitem(self, selection: dict) -> 'Tensor':
-        batch_selection = {dim: selection[dim] for dim in self._shape.only(tuple(selection)).names}
-        indices = self._indices[batch_selection]
-        pointers = self._pointers[batch_selection]
-        values = self._values[batch_selection]
-        uncompressed = self._uncompressed_dims
-        compressed = self._compressed_dims
-        uncompressed_offset = self._uncompressed_offset
-        if compressed.only(tuple(selection)):
-            if compressed.rank > 1:
-                raise NotImplementedError
-            ptr_sel = selection[compressed.name]
-            if isinstance(ptr_sel, int):
-                raise NotImplementedError(f"Slicing with int not yet supported for sparse tensors. Use a range instead, e.g. [{ptr_sel}:{ptr_sel+1}] instead of [{ptr_sel}]")
-            elif isinstance(ptr_sel, slice):
-                assert ptr_sel.step in (None, 1), f"Only step size 1 supported for sparse indexing but got {ptr_sel.step}"
-                if batch(indices):
-                    raise NotImplementedError("Slicing not yet supported for batched sparse tensors")
-                start = ptr_sel.start or 0
-                stop = uncompressed.volume if ptr_sel.stop is None else ptr_sel.stop
-                pointers = pointers[start:stop+1]
-                indices = indices[{instance(indices).name: slice(int(pointers[0]), int(pointers[-1]))}]
-                values = values[{instance(values).name: slice(int(pointers[0]), int(pointers[-1]))}]
-                pointers -= pointers[0]
-                compressed = compressed.after_gather({compressed.name: ptr_sel})
-            else:
-                raise NotImplementedError
-        if uncompressed.only(tuple(selection)):
-            if self._uncompressed_dims.rank > 1:
-                raise NotImplementedError
-            ind_sel = selection[uncompressed.name]
-            if isinstance(ind_sel, int):
-                raise NotImplementedError(f"Slicing with int not yet supported for sparse tensors. Use a range instead, e.g. [{ind_sel}:{ind_sel+1}] instead of [{ind_sel}]")
-            elif isinstance(ind_sel, slice):
-                assert ind_sel.step in (None, 1), f"Only step size 1 supported for sparse indexing but got {ind_sel.step}"
-                start = ind_sel.start or 0
-                stop = uncompressed.volume if ind_sel.stop is None else ind_sel.stop
-                keep = (start <= self._indices) & (self._indices < stop)
-                from phi.math import where
-                values = where(keep, values, 0)
-                uncompressed_offset = start
-                uncompressed = uncompressed.after_gather({uncompressed.name: ind_sel})
-            else:
-                raise NotImplementedError
-        return CompressedSparseMatrix(indices, pointers, values, uncompressed, compressed, uncompressed_offset)
-
-    def __concat__(self, tensors: tuple, dim: str, **kwargs) -> 'CompressedSparseMatrix':
-        if not all(isinstance(t, CompressedSparseMatrix) for t in tensors):
-            return NotImplemented
-        if dim == self._compressed_dims[0].name:
-            indices = concat([t._indices for t in tensors], instance(self._indices), **kwargs)
-            values = concat([t._values for t in tensors], instance(self._values), **kwargs)
-            pointers = []
-            pointer_offset = 0
-            for i, t in enumerate(tensors):
-                pointers.append((t._pointers[1:] if i else t._pointers) + pointer_offset)
-                pointer_offset += t._pointers[-1]
-            assert pointer_offset == instance(indices).volume
-            pointers = concat(pointers, instance(self._pointers))
-            compressed = self._compressed_dims.with_dim_size(dim, sum(t.shape.get_size(dim) for t in tensors))
-            return CompressedSparseMatrix(indices, pointers, values, self._uncompressed_dims, compressed, self._uncompressed_offset)
-        elif dim == self._uncompressed_dims[0].name:
-            if all(t._indices is self._indices and t._pointers is self._pointers for t in tensors):
-                # ToDo test if offsets match and ordered correctly
-                from ._ops import sum_
-                values = sum_([t._values for t in tensors], '0')
-                uncompressed = self._uncompressed_dims.with_dim_size(dim, sum(t.shape.get_size(dim) for t in tensors))
-                return CompressedSparseMatrix(self._indices, self._pointers, values, uncompressed, self._compressed_dims, uncompressed_offset=None)
-            else:
-                raise NotImplementedError("concatenating arbitrary compressed sparse tensors along uncompressed dim is not yet supported")
-        else:
-            raise NotImplementedError("concatenating compressed sparse tensors along non-sparse dims not yet supported")
-
-    def _op1(self, native_function):
-        return self._with_values(self._values._op1(native_function))
-
-    def _op2(self, other, operator: Callable, native_function: Callable, op_name: str = 'unknown', op_symbol: str = '?') -> 'Tensor':
-        other_shape = shape(other)
-        affects_only_values = self.sparse_dims not in other_shape and non_instance(self._indices).isdisjoint(other_shape)
-        if affects_only_values:
-            return self._with_values(operator(self._values, other))
-        elif isinstance(other, CompressedSparseMatrix):
-            if other._indices is self._indices and other._pointers is self._pointers:
-                return self._with_values(operator(self._values, other._values))
-            elif op_symbol == '+':
-                raise NotImplementedError("Compressed addition not yet implemented")
-            else:
-                # convert to COO, then perform operation
-                raise NotImplementedError
-        raise NotImplementedError
-
-    def _with_values(self, new_values: Tensor):
-        return CompressedSparseMatrix(self._indices, self._pointers, new_values, self._uncompressed_dims, self._compressed_dims)
-
-    def _with_shape_replaced(self, new_shape: Shape):
-        assert self._shape.rank == new_shape.rank
-        raise NotImplementedError
-
-    def _native_csr_components(self):
-        from phi.math import reshaped_native
-        ind_batch = batch(self._indices) & batch(self._pointers)
-        channels = non_instance(self._values).without(ind_batch)
-        native_indices = reshaped_native(self._indices, [ind_batch, instance], force_expand=True)
-        native_pointers = reshaped_native(self._pointers, [ind_batch, instance], force_expand=True)
-        native_values = reshaped_native(self._values, [ind_batch, instance, channels])
-        native_shape = self._compressed_dims.volume, self._uncompressed_dims.volume
-        if self._uncompressed_offset is not None:
-            native_indices -= self._uncompressed_offset
-            native_indices = choose_backend(native_indices).clip(native_indices, 0, self._uncompressed_dims.volume - 1)
-        return ind_batch, channels, native_indices, native_pointers, native_values, native_shape
-
-    def decompress(self):
-        if self._uncompressed_indices is None:
-            self._uncompressed_indices = None
-            raise NotImplementedError()
-        if self._uncompressed_indices_perm is not None:
-            self._uncompressed_indices = self._uncompressed_indices[self._uncompressed_indices_perm]
-            self._uncompressed_indices_perm = None
-        return SparseCoordinateTensor(self._uncompressed_indices, self._values, self._compressed_dims & self._uncompressed_dims, False, False)
-
-    def native(self, order: Union[str, tuple, list, Shape] = None):
-        raise RuntimeError("Sparse tensors do not have a native representation. Use math.dense(tensor).native() instead")
-
-    def __pack_dims__(self, dims: Tuple[str, ...], packed_dim: Shape, pos: Union[int, None], **kwargs) -> 'Tensor':
-        assert all(d in self._shape for d in dims)
-        dims = self._shape.only(dims, reorder=True)
-        if dims.only(self._compressed_dims).is_empty:  # pack cols
-            assert self._uncompressed_dims.are_adjacent(dims), f"Can only compress adjacent dims but got {dims} for matrix {self._shape}"
-            uncompressed_dims = self._uncompressed_dims.replace(dims, packed_dim.with_size(dims.volume))
-            return CompressedSparseMatrix(self._indices, self._pointers, self._values, uncompressed_dims, self._compressed_dims, self._uncompressed_offset)
-        elif dims.only(self._uncompressed_dims).is_empty:   # pack rows
-            assert self._compressed_dims.are_adjacent(dims), f"Can only compress adjacent dims but got {dims} for matrix {self._shape}"
-            compressed_dims = self._compressed_dims.replace(dims, packed_dim.with_size(dims.volume))
-            return CompressedSparseMatrix(self._indices, self._pointers, self._values, self._uncompressed_dims, compressed_dims, self._uncompressed_offset)
-        else:
-            raise NotImplementedError(f"Cannot pack dimensions from both columns and rows with compressed sparse matrices but got {dims}")
-
-
-def sparse_dims(x: Tensor) -> Shape:
-    """
-    Returns the dimensions of a `Tensor` that are explicitly stored in a sparse format.
-
-    Args:
-        x: Any `Tensor`
-
-    Returns:
-        `Shape`
-    """
-    if isinstance(x, SparseCoordinateTensor):
-        return x._dense_shape
-    elif isinstance(x, CompressedSparseMatrix):
-        return concat_shapes(x._compressed_dims, x._uncompressed_dims)
-    else:
-        return EMPTY_SHAPE
-
-
-def get_sparsity(x: Tensor):
-    """
-    Fraction of values currently stored on disk for the given `Tensor` `x`.
-    For sparse tensors, this is `nnz / shape`.
-
-    This is a lower limit on the number of values that will need to be processed for operations involving `x`.
-    The actual number is often higher since many operations require data be laid out in a certain format.
-    In these cases, missing values, such as zeros, are filled in before the operation.
-
-    The following operations may return tensors whose values are only partially stored:
-
-    * `phi.math.expand()`
-    * `phi.math.pairwise_distance()` with `max_distance` set.
-    * Tracers used in `phi.math.jit_compile_linear()`
-    * Stacking any of the above.
-
-    Args:
-        x: `Tensor`
-
-    Returns:
-        The number of values that are actually stored on disk.
-        This does not include additional information, such as position information / indices.
-        For sparse matrices, this is equal to the number of nonzero values.
-    """
-    # ToDo this does not give the correct result for linear tracers since the matrix shape is not taken into account
-    return sum([t.shape.volume for t in stored_values(x)]) / x.shape.volume
-
-
-def stored_values(x: Tensor) -> List[Tensor]:
-    """
-    Returns the values currently stored on disk for the given `Tensor` `x`.
-
-    Some operations may require non-stored values to be explicitly stored, or they may be filled in for performance reasons.
-
-    Args:
-        x: `Tensor`
-
-    Returns:
-        List of `Tensor`s representing all values stored to represent `x`.
-    """
-    if isinstance(x, NativeTensor):
-        return [x]
-    elif isinstance(x, CollapsedTensor):
-        return [cached(x)] if x.is_cached else stored_values(x._inner)
-    elif isinstance(x, TensorStack):
-        return [cached(x)] if x.is_cached else sum([stored_values(t) for t in x._tensors], [])
-    elif isinstance(x, CompressedSparseMatrix):
-        return [x._values]
-    elif isinstance(x, SparseCoordinateTensor):
-        if x._can_contain_double_entries:
-            warnings.warn(f"Sparsity of sparse tensor {x.shape} is unknown as multiple values can reference the same position.")
-        return [x._values]
-    else:
-        from phi.math._functional import ShiftLinTracer
-        if isinstance(x, ShiftLinTracer):
-            return sum([stored_values(v) for v in x.val.values()], [])
-        raise ValueError(x)
-
-
-def dense(x: Tensor) -> Tensor:
-    """
-    Convert a sparse tensor representation to an equivalent dense one in which all values are explicitly stored contiguously in memory.
-
-    Args:
-        x: Any `Tensor`.
-            Python primitives like `float`, `int` or `bool` will be converted to `Tensors` in the process.
-
-    Returns:
-        Dense tensor.
-    """
-    from phi.math import reshaped_tensor
-    if isinstance(x, SparseCoordinateTensor):
-        from ._ops import scatter
-        return scatter(x.shape, x._indices, x._values, mode='add', outside_handling='undefined')
-    elif isinstance(x, CompressedSparseMatrix):
-        ind_batch, channels, native_indices, native_pointers, native_values, native_shape = x._native_csr_components()
-        native_dense = x.default_backend.csr_to_dense(native_indices, native_pointers, native_values, native_shape)
-        return reshaped_tensor(native_dense, [ind_batch, x._compressed_dims, x._uncompressed_dims, channels])
-    elif isinstance(x, NativeTensor):
-        return x
-    elif isinstance(x, Tensor):
-        return cached(x)
-    elif isinstance(x, (Number, bool)):
-        return wrap(x)
-
-
-def dot_compressed_dense(compressed: CompressedSparseMatrix, cdims: Shape, dense: Tensor, ddims: Shape):
-    from phi.math import reshaped_native, reshaped_tensor
-    backend = choose_backend(*compressed._natives() + dense._natives())
-    if compressed._uncompressed_dims in cdims:  # proper matrix-vector multiplication
-        ind_batch, channels, native_indices, native_pointers, native_values, native_shape = compressed._native_csr_components()
-        rhs_channels = shape(dense).without(ddims).without(channels)
-        dense_native = reshaped_native(dense, [ind_batch, ddims, channels, rhs_channels], force_expand=True)
-        result_native = backend.mul_csr_dense(native_indices, native_pointers, native_values, native_shape, dense_native)
-        result = reshaped_tensor(result_native, [ind_batch, channels, compressed._compressed_dims, rhs_channels])
-        return result
-    else:  # transposed matrix vector multiplication. This is inefficient
-        raise NotImplementedError("Transposed sparse matrix multiplication not yet implemented")
-
-
-def dot_coordinate_dense(sparse: SparseCoordinateTensor, sdims: Shape, dense: Tensor, ddims: Shape):
-    from phi.math import reshaped_native, reshaped_tensor
-    backend = choose_backend(*sparse._natives() + dense._natives())
-    ind_batch, channels, native_indices, native_values, native_shape = sparse._native_coo_components(sdims, matrix=True)
-    rhs_channels = shape(dense).without(ddims).without(channels)
-    dense_native = reshaped_native(dense, [ind_batch, ddims, channels, rhs_channels], force_expand=True)
-    result_native = backend.mul_coo_dense(native_indices, native_values, native_shape, dense_native)
-    result = reshaped_tensor(result_native, [ind_batch, channels, sparse._dense_shape.without(sdims), rhs_channels])
-    return result
-
-
-def native_matrix(value: Tensor):
-    cols = dual(value)
-    rows = non_batch(value).non_dual
-    if isinstance(value, SparseCoordinateTensor):
-        ind_batch, channels, indices, values, shape = value._native_coo_components(dual, matrix=True)
-        if ind_batch.volume > 1 or channels.volume > 1:
-            return value.default_backend.sparse_coo_tensor_batched(indices, values, shape)
-        else:
-            return value.default_backend.sparse_coo_tensor(indices[0], values[0, :, 0], shape)
-    elif isinstance(value, CompressedSparseMatrix):
-        assert not non_instance(value._values), f"native_matrix does not support vector-valued matrices. Vector dims: {non_batch(value).without(sparse_dims(value))}"
-        ind_batch, channels, indices, pointers, values, shape = value._native_csr_components()
-        if dual(value._uncompressed_dims):  # CSR
-            assert not dual(value._compressed_dims), "Dual dimensions on both compressed and uncompressed dimensions"
-            if ind_batch.volume > 1 or channels.volume > 1:
-                return value.default_backend.csr_matrix_batched(indices, pointers, values, shape)
-            else:
-                return value.default_backend.csr_matrix(indices[0], pointers[0], values[0, :, 0], shape)
-        else:  # CSC
-            assert not dual(value._uncompressed_dims)
-            if ind_batch.volume > 1 or channels.volume > 1:
-                return value.default_backend.csc_matrix_batched(pointers, indices, values, shape)
-            else:
-                return value.default_backend.csc_matrix(pointers[0], indices[0], values[0, :, 0], shape)
-    else:
-        if batch(value):
-            raise NotImplementedError
-        v = pack_dims(value, rows, channel('_row'))
-        v = pack_dims(v, cols, channel('_col'))
-        from ._ops import reshaped_native
-        return reshaped_native(v, ['_row', '_col'])
-
-
-def factor_ilu(matrix: Tensor, iterations=None, safe=False):
-    """
-    Incomplete LU factorization for dense or sparse matrices.
-
-    For sparse matrices, keeps the sparsity pattern of `matrix`.
-    L and U will be trimmed to the respective areas, i.e. stored upper elements in L will be dropped,
-     unless this would lead to varying numbers of stored elements along a batch dimension.
-
-    Args:
-        matrix: Dense or sparse matrix to factor.
-            Currently, compressed sparse matrices are decompressed before running the ILU algorithm.
-        iterations: (Optional) Number of fixed-point iterations to perform.
-        safe: If `False` (default), only matrices with a rank deficiency of up to 1 can be factored as all values of L and U are uniquely determined.
-            For matrices with higher rank deficiencies, the result includes `NaN` values.
-            If `True`, the algorithm runs slightly slower but can factor highly rank-deficient matrices as well.
-            However, then L is undeterdetermined and unused values of L are set to 0.
-            Rank deficiencies of 1 occur frequently in periodic settings but higher ones are rare.
-
-    Returns:
-        L: Lower-triangular matrix as `Tensor` with all diagonal elements equal to 1.
-        U: Upper-triangular matrix as `Tensor`.
-
-    Examples:
-        >>> matrix = wrap([[-2, 1, 0],
-        >>>                [1, -2, 1],
-        >>>                [0, 1, -2]], channel('row'), dual('col'))
-        >>> L, U = math.factor_ilu(matrix)
-        >>> math.print(L)
-        row=0      1.          0.          0.         along ~col
-        row=1     -0.5         1.          0.         along ~col
-        row=2      0.         -0.6666667   1.         along ~col
-        >>> math.print(L @ U, "L @ U")
-                    L @ U
-        row=0     -2.   1.   0.  along ~col
-        row=1      1.  -2.   1.  along ~col
-        row=2      0.   1.  -2.  along ~col
-    """
-    if iterations is None:
-        cols = dual(matrix).volume
-        iterations = min(20, int(round(1.6 * cols)))
-    if isinstance(matrix, CompressedSparseMatrix):
-        matrix = matrix.decompress()
-    if isinstance(matrix, SparseCoordinateTensor):
-        ind_batch, channels, indices, values, shape = matrix._native_coo_components(dual, matrix=True)
-        (l_idx_nat, l_val_nat), (u_idx_nat, u_val_nat) = matrix.default_backend.ilu_coo(indices, values, shape, iterations, safe)
-        col_dims = matrix._shape.only(dual)
-        row_dims = matrix._dense_shape.without(col_dims)
-        l_indices = matrix._unpack_indices(l_idx_nat[..., 0], l_idx_nat[..., 1], row_dims, col_dims, ind_batch)
-        u_indices = matrix._unpack_indices(u_idx_nat[..., 0], u_idx_nat[..., 1], row_dims, col_dims, ind_batch)
-        from ._ops import reshaped_tensor
-        l_values = reshaped_tensor(l_val_nat, [ind_batch, instance(matrix._values), channels], convert=False)
-        u_values = reshaped_tensor(u_val_nat, [ind_batch, instance(matrix._values), channels], convert=False)
-        lower = SparseCoordinateTensor(l_indices, l_values, matrix._dense_shape, matrix._can_contain_double_entries, matrix._indices_sorted)
-        upper = SparseCoordinateTensor(u_indices, u_values, matrix._dense_shape, matrix._can_contain_double_entries, matrix._indices_sorted)
-    else:  # dense matrix
-        from ._ops import reshaped_native, reshaped_tensor
-        native_matrix = reshaped_native(matrix, [batch, non_batch(matrix).non_dual, dual, EMPTY_SHAPE])
-        l_native, u_native = choose_backend(native_matrix).ilu_dense(native_matrix, iterations, safe)
-        lower = reshaped_tensor(l_native, [batch(matrix), non_batch(matrix).non_dual, dual(matrix), EMPTY_SHAPE])
-        upper = reshaped_tensor(u_native, [batch(matrix), non_batch(matrix).non_dual, dual(matrix), EMPTY_SHAPE])
-    return lower, upper
+import warnings
+from numbers import Number
+from typing import List, Callable, Tuple, Union
+
+import numpy as np
+import scipy.sparse
+
+from ._shape import Shape, non_batch, merge_shapes, instance, batch, non_instance, shape, channel, spatial, DimFilter, concat_shapes, EMPTY_SHAPE, dual, DUAL_DIM, SPATIAL_DIM, \
+    non_channel
+from ._magic_ops import concat, pack_dims, expand, rename_dims, stack, unpack_dim
+from ._tensors import Tensor, TensorStack, NativeTensor, cached, wrap
+from .backend import choose_backend, NUMPY, Backend
+from .backend._dtype import DType
+
+
+def sparse_tensor(indices: Tensor,
+                  values: Tensor,
+                  dense_shape: Shape,
+                  can_contain_double_entries=True,
+                  indices_sorted=False,
+                  format='auto',
+                  default: Number or None = 0) -> Tensor:
+    """
+    Construct a sparse tensor that stores `values` at the corresponding `indices` and is 0 everywhere else.
+    In addition to the sparse dimensions indexed by `indices`, the tensor inherits all batch and channel dimensions from `values`.
+
+    indices: `Tensor` encoding the positions of stored values. It has the following dimensions:
+
+        * One instance dimension exactly matching the instance dimension on `values`.
+          It enumerates the positions of stored entries.
+        * One channel dimension called `vector`.
+          Its item names must match the dimension names of `dense_shape` but the order can be arbitrary.
+        * Any number of batch dimensions
+
+    values: `Tensor` containing the stored values at positions given by `indices`. It has the following dimensions:
+
+        * One instance dimension exactly matching the instance dimension on `indices`.
+          It enumerates the values of stored entries.
+        * Any number of channel dimensions if multiple values are stored at each index.
+        * Any number of batch dimensions
+
+    dense_shape: Dimensions listed in `indices`.
+        The order can differ from the item names of `indices`.
+    can_contain_double_entries: Whether some indices might occur more than once.
+        If so, values at the same index will be summed.
+    indices_sorted: Whether the indices are sorted in ascending order given the dimension order of the item names of `indices`.
+    format: Sparse format in which to store the data, such as `'coo'` or `'csr'`. See `phi.math.get_format`.
+    default: Value the sparse tensor returns for non-stored values. Must be `0` or `None`.
+
+    Returns:
+        Sparse `Tensor` with the specified `format`.
+    """
+    assert default in [0, None], f"default value must be either 0 or None but got '{default}'"
+    coo = SparseCoordinateTensor(indices, values, dense_shape, can_contain_double_entries, indices_sorted, default)
+    return to_format(coo, format)
+
+
+def tensor_like(existing_tensor: Tensor, values: Union[Tensor, Number, bool], value_order: str = None):
+    """
+    Creates a tensor with the same format and shape as `existing_tensor`.
+
+    Args:
+        existing_tensor: Any `Tensor`, sparse or dense.
+        values: New values to replace the existing values by.
+            If `existing_tensor` is sparse, `values` must have an instance dimension to list the stored values, matching the sparse indices.
+        value_order: Order of `values` compared to `existing_tensor`.
+            If `'original'`, the values are ordered like the values that was used to create the first tensor with this sparsity pattern.
+            If `'as existing'`, the values match the current order of `existing_tensor`.
+            Note that the order of values may be changed upon creating a sparse tensor.
+
+    Returns:
+        `Tensor`
+    """
+    assert value_order in ['original', 'as existing', None]
+    if isinstance(existing_tensor, (SparseCoordinateTensor, CompressedSparseMatrix)):
+        if value_order is None:
+            assert not instance(values), f"When creating a sparse tensor from a list of values, value_order must be specified."
+        if instance(values):
+            values = rename_dims(values, instance, instance(existing_tensor._values))
+        values = expand(values, instance(existing_tensor._values))
+        if value_order == 'original' and isinstance(existing_tensor, CompressedSparseMatrix) and existing_tensor._uncompressed_indices_perm is not None:
+            values = values[existing_tensor._uncompressed_indices_perm]
+        if isinstance(existing_tensor, CompressedSparseMatrix) and existing_tensor._uncompressed_offset is not None:
+            from ._ops import where
+            values = where(existing_tensor._valid_mask(), values, 0)
+        return existing_tensor._with_values(values)
+    if not is_sparse(existing_tensor):
+        return unpack_dim(values, instance, existing_tensor.shape.non_channel.non_batch)
+    raise NotImplementedError
+
+
+class SparseCoordinateTensor(Tensor):
+
+    def __init__(self, indices: Tensor, values: Tensor, dense_shape: Shape, can_contain_double_entries: bool, indices_sorted: bool, default: Number):
+        """
+        Construct a sparse tensor with any number of sparse, dense and batch dimensions.
+        """
+        assert isinstance(indices, Tensor), f"indices must be a Tensor but got {type(indices)}"
+        assert isinstance(values, Tensor), f"values must be a Tensor but got {type(values)}"
+        assert instance(indices), "indices must have an instance dimension"
+        assert 'vector' in indices.shape, "indices must have a vector dimension"
+        assert set(indices.vector.item_names) == set(dense_shape.names), "The 'vector' dimension of indices must list the dense dimensions as item names"
+        assert indices.dtype.kind == int, f"indices must have dtype=int but got {indices.dtype}"
+        self._shape = merge_shapes(dense_shape, batch(indices), non_instance(values))
+        self._dense_shape = dense_shape
+        self._indices = indices
+        self._values = values
+        self._can_contain_double_entries = can_contain_double_entries
+        self._indices_sorted = indices_sorted
+        self._default = default
+
+    @property
+    def shape(self) -> Shape:
+        return self._shape
+
+    @property
+    def dtype(self) -> DType:
+        return self._values.dtype
+
+    @property
+    def sparse_dims(self):
+        return self._dense_shape
+
+    @property
+    def sparsity_batch(self):
+        return batch(self._indices)
+
+    def native(self, order: Union[str, tuple, list, Shape] = None, singleton_for_const=False):
+        assert order is None, f"sparse matrices are always ordered (primal, dual). For custom ordering, use math.dense(tensor).native() instead."
+        return native_matrix(self, self.default_backend)
+
+    @property
+    def _is_tracer(self) -> bool:
+        return self._indices._is_tracer or self._values._is_tracer
+
+    def _with_values(self, new_values: Tensor):
+        return SparseCoordinateTensor(self._indices, new_values, self._dense_shape, self._can_contain_double_entries, self._indices_sorted, self._default)
+
+    def _natives(self) -> tuple:
+        indices_const = self._indices.default_backend is not self._values.default_backend
+        if indices_const:
+            return self._values._natives()  # If we return NumPy arrays, they might get converted in function transformations
+        else:
+            return self._values._natives() + self._indices._natives()
+
+    def _spec_dict(self) -> dict:
+        indices_const = self._indices.default_backend is not self._values.default_backend
+        return {'type': SparseCoordinateTensor,
+                'shape': self._shape,
+                'dense_shape': self._dense_shape,
+                'indices': self._indices if indices_const else self._indices._spec_dict(),
+                'values': self._values._spec_dict(),
+                'can_contain_double_entries': self._can_contain_double_entries,
+                'indices_sorted': self._indices_sorted,
+                'default': self._default}
+
+    @classmethod
+    def _from_spec_and_natives(cls, spec: dict, natives: list):
+        values = spec['values']['type']._from_spec_and_natives(spec['values'], natives)
+        indices_or_spec = spec['indices']
+        if isinstance(indices_or_spec, Tensor):
+            indices = indices_or_spec
+        else:
+            indices = spec['indices']['type']._from_spec_and_natives(spec['indices'], natives)
+        return SparseCoordinateTensor(indices, values, spec['dense_shape'], spec['can_contain_double_entries'], spec['indices_sorted'], spec['default'])
+
+    def _native_coo_components(self, col_dims: DimFilter, matrix=False):
+        col_dims = self._shape.only(col_dims)
+        row_dims = self._dense_shape.without(col_dims)
+        row_idx_packed, col_idx_packed = self._pack_indices(row_dims, col_dims)
+        from phi.math import reshaped_native
+        ind_batch = batch(self._indices)
+        channels = non_instance(self._values).without(ind_batch)
+        if matrix:
+            native_indices = choose_backend(row_idx_packed, col_idx_packed).stack([row_idx_packed, col_idx_packed], -1)
+            native_shape = (row_dims.volume, col_dims.volume)
+        else:
+            native_indices = reshaped_native(self._indices, [ind_batch, instance, 'vector'])
+            native_shape = self._dense_shape.sizes
+        native_values = reshaped_native(self._values, [ind_batch, instance, channels])
+        return ind_batch, channels, native_indices, native_values, native_shape
+
+    def dual_indices(self, to_primal=False):
+        """ Unpacked column indices """
+        idx = self._indices[self._dense_shape.dual]
+        if to_primal:
+            dual_names = idx.shape.get_item_names('vector')
+            primal_names = spatial(*dual_names).names
+            idx = rename_dims(idx, 'vector', channel(vector=primal_names))
+        return idx
+
+    def primal_indices(self):
+        """ Unpacked row indices """
+        return self._indices[self._dense_shape.non_dual]
+
+    def _pack_indices(self, row_dims: Shape, col_dims: Shape):
+        assert row_dims in self._dense_shape, f"Can only compress sparse dims but got {row_dims} which contains non-sparse dims"
+        from ._ops import reshaped_native
+        b = self._indices.default_backend
+        row_idx = self._indices[row_dims.names]
+        col_idx = self._indices[self._dense_shape.without(row_dims).names]
+        # ToDo if not row_dims: idx = [0]
+        row_idx_packed = b.ravel_multi_index(reshaped_native(row_idx, [batch, instance, channel]), row_dims.sizes)
+        col_idx_packed = b.ravel_multi_index(reshaped_native(col_idx, [batch, instance, channel]), col_dims.sizes)
+        return row_idx_packed, col_idx_packed
+
+    def _unpack_indices(self, row_idx_packed, col_idx_packed, row_dims: Shape, col_dims: Shape, ind_batch: Shape):
+        row_idx = np.stack(np.unravel_index(row_idx_packed, row_dims.sizes), -1)
+        col_idx = np.stack(np.unravel_index(col_idx_packed, col_dims.sizes), -1)
+        np_indices = np.concatenate([row_idx, col_idx], -1)
+        from ._ops import reshaped_tensor
+        idx_dim = channel(**{channel(self._indices).name: row_dims.names + col_dims.names})
+        indices = reshaped_tensor(np_indices, [ind_batch, instance(self._indices), idx_dim], convert=False)
+        return indices
+
+    def compress_rows(self):
+        return self.compress(self._dense_shape.non_dual)
+
+    def compress_cols(self):
+        return self.compress(self._dense_shape.dual)
+
+    def compress(self, dims: DimFilter):
+        c_dims = self._shape.only(dims, reorder=True)
+        u_dims = self._dense_shape.without(c_dims)
+        c_idx_packed, u_idx_packed = self._pack_indices(c_dims, u_dims)
+        # --- Use scipy.sparse.csr_matrix to reorder values ---
+        idx = np.arange(1, c_idx_packed.shape[-1] + 1)  # start indexing at 1 since 0 might get removed
+        scipy_csr = scipy.sparse.csr_matrix((idx, (c_idx_packed[0], u_idx_packed[0])), shape=(c_dims.volume, u_dims.volume))
+        assert c_idx_packed.shape[1] == len(scipy_csr.data), "Failed to create CSR matrix because the CSR matrix contains fewer non-zero values than COO. This can happen when the `x` tensor is too small for the stencil."
+        # --- Construct CompressedSparseMatrix ---
+        entries_dim = instance(self._values).name
+        perm = {entries_dim: wrap(scipy_csr.data - 1, instance(entries_dim))}
+        values = self._values[perm]  # Change order accordingly
+        indices = wrap(scipy_csr.indices, instance(entries_dim))
+        pointers = wrap(scipy_csr.indptr, instance('pointers'))
+        return CompressedSparseMatrix(indices, pointers, values, u_dims, c_dims, self._default, uncompressed_indices=self._indices, uncompressed_indices_perm=perm)
+
+    def __pack_dims__(self, dims: Tuple[str, ...], packed_dim: Shape, pos: Union[int, None], **kwargs) -> 'Tensor':
+        dims = self._shape.only(dims)
+        assert dims in self._dense_shape, f"Can only pack sparse dimensions on SparseCoordinateTensor but got {dims} of which {dims.without(self._dense_shape)} are not sparse"
+        assert self._indices.default_backend is NUMPY, "Can only pack NumPy indices as of yet"
+        from ._ops import reshaped_native
+        idx = self._indices.vector[dims.names]
+        idx_packed = np.ravel_multi_index(reshaped_native(idx, [channel, instance]), dims.sizes)
+        idx_packed = expand(wrap(idx_packed, instance(self._indices)), channel(vector=packed_dim.name))
+        indices = concat([self._indices.vector[self._dense_shape.without(dims).names], idx_packed], 'vector')
+        dense_shape = concat_shapes(self._dense_shape.without(dims), packed_dim.with_size(dims.volume))
+        idx_sorted = self._indices_sorted and False  # ToDo still sorted if dims are ordered correctly and no other dim in between and inserted at right point
+        return SparseCoordinateTensor(indices, self._values, dense_shape, self._can_contain_double_entries, idx_sorted, self._default)
+
+    def _with_shape_replaced(self, new_shape: Shape):
+        assert self._shape.rank == new_shape.rank
+        dense_shape = new_shape[self._shape.indices(self._dense_shape)]
+        new_item_names = new_shape[self._shape.indices(self._indices.shape.get_item_names('vector'))].names
+        values = self._values._with_shape_replaced(self._values.shape.replace(self._shape, new_shape))
+        non_vec = self._shape.without('vector')
+        new_non_vec = new_shape[self._shape.indices(non_vec)]
+        indices = self._indices._with_shape_replaced(self._indices.shape.replace(non_vec, new_non_vec).with_dim_size('vector', new_item_names))
+        return SparseCoordinateTensor(indices, values, dense_shape, self._can_contain_double_entries, self._indices_sorted, self._default)
+
+    def _op1(self, native_function):
+        return self._with_values(self._values._op1(native_function))
+
+    def _op2(self, other, operator: Callable, native_function: Callable, op_name: str = 'unknown', op_symbol: str = '?') -> 'Tensor':
+        other_shape = shape(other)
+        affects_only_values = self._dense_shape.isdisjoint(other_shape)
+        if affects_only_values:
+            return self._with_values(operator(self._values, other))
+        if isinstance(other, CompressedSparseMatrix):
+            other = other.decompress()
+        if isinstance(other, SparseCoordinateTensor):
+            if same_sparsity_pattern(self, other):
+                return self._with_values(operator(self._values, other._values))
+            elif op_symbol == '+':
+                raise NotImplementedError("Compressed addition not yet implemented")
+            else:
+                # convert to COO, then perform operation
+                raise NotImplementedError
+        else:  # other is dense
+            if self._dense_shape in other.shape:  # all dims dense -> convert to dense
+                return dense(self)._op2(other, operator, native_function, op_name, op_symbol)
+            else:  # only some dims dense -> stay sparse
+                dense_dims = self._dense_shape.only(other.shape)
+                other_values = other[self._indices.vector[dense_dims.names]]
+                return self._with_values(self._values._op2(other_values, operator, native_function, op_name, op_symbol))
+
+    def _getitem(self, selection: dict) -> 'Tensor':
+        batch_selection = {dim: selection[dim] for dim in self._shape.only(tuple(selection)).names}
+        indices = self._indices[{dim: sel for dim, sel in batch_selection.items() if dim != 'vector'}]
+        values = self._values[batch_selection]
+        if self._dense_shape.only(tuple(selection)):
+            keep = expand(True, instance(self._indices))
+            for dim, sel in selection.items():
+                dim_indices = self._indices[dim]
+                if isinstance(sel, int):
+                    item_names = list(channel(indices).item_names[0])
+                    item_names.remove(dim)
+                    indices = indices[item_names]
+                    sel = slice(sel, sel + 1)
+                elif isinstance(sel, str):
+                    raise NotImplementedError
+                assert isinstance(sel, slice)
+                assert sel.step in (None, 1), f"Only step size 1 supported for sparse indexing but got {sel.step}"
+                start = sel.start or 0
+                stop = self._dense_shape[dim].size if sel.stop is None else sel.stop
+                keep &= (start <= dim_indices) & (dim_indices < stop)
+                from phi.math import vec
+                indices -= vec(**{d: start if d == dim else 0 for d in indices.vector.item_names})
+            from ._ops import boolean_mask
+            indices = boolean_mask(indices, instance(indices), keep)
+            values = boolean_mask(values, instance(indices), keep)
+            dense_shape = self._dense_shape.after_gather(selection)
+            return SparseCoordinateTensor(indices, values, dense_shape, self._can_contain_double_entries, self._indices_sorted, self._default)
+        else:
+            return SparseCoordinateTensor(indices, values, self._dense_shape, self._can_contain_double_entries, self._indices_sorted, self._default)
+
+    def __concat__(self, tensors: tuple, dim: str, **kwargs) -> 'SparseCoordinateTensor':
+        if not all(isinstance(t, SparseCoordinateTensor) for t in tensors):
+            return NotImplemented
+        if dim in self._dense_shape:
+            # assert all default equal
+            from phi.math import vec
+            indices = []
+            values = []
+            offset = 0
+            for t in tensors:
+                t_indices = stored_indices(t, list_dim=instance(self._indices), index_dim=channel(self._indices))
+                t_values = stored_values(t, list_dim=instance(self._values))
+                t_indices += vec(**{d: offset if d == dim else 0 for d in t_indices.vector.item_names})
+                offset += t.shape.get_size(dim)
+                indices.append(t_indices)
+                values.append(t_values)
+            indices = concat(indices, instance(self._indices))
+            values = concat(values, instance(self._values))
+            dense_shape = self._dense_shape.with_dim_size(dim, sum([t.shape.get_size(dim) for t in tensors]))
+            can_contain_double_entries = any([t._can_contain_double_entries for t in tensors])
+            indices_sorted = all([t._indices_sorted for t in tensors])
+            return SparseCoordinateTensor(indices, values, dense_shape, can_contain_double_entries, indices_sorted, self._default)
+        else:
+            raise NotImplementedError("concatenating compressed sparse tensors along non-sparse dims not yet supported")
+
+
+class CompressedSparseMatrix(Tensor):
+
+    def __init__(self,
+                 indices: Tensor,
+                 pointers: Tensor,
+                 values: Tensor,
+                 uncompressed_dims: Shape,
+                 compressed_dims: Shape,
+                 default: Number,
+                 uncompressed_offset: int = None,
+                 uncompressed_indices: Tensor = None,
+                 uncompressed_indices_perm: Tensor = None):
+        """
+
+        Args:
+            indices: indices must be sorted in ascending order by compressed_dim and other sparse dims.
+                Must have one or multiple instance dimensions and can have any number of batch dimensions.
+                No spatial and channel dimensions allowed.
+            pointers:
+            values:
+            compressed_dims: Sparse dimensions with compressed pointer representation.
+                Only one pointer array is used per matrix, i.e. the dimensions are packed internally.
+                These dimensions are indexed by `pointers`.
+            uncompressed_dims: Sparse dimensions with full index storage.
+                These dimensions are indexed by `indices`.
+            uncompressed_offset: For sliced sparse tensors.
+                If `None`, indicates that all entries lie within bounds.
+                If an `int`, indicate that this is a slice of a larger compressed sparse matrix.
+                Indices actually refer to `indices - uncompressed_offset` within this matrix, i.e. they may reference phantom values to the left or right of the matrix.
+                The `values` corresponding to phantom entries must all be 0.
+                The size of the slice is given by `compressed_dims.volume`.
+        """
+        assert instance(indices), "indices must have an instance dimension"
+        assert instance(pointers), "pointers must have an instance dimension"
+        assert instance(values) == instance(indices), "Instance dimensions of values and indices must match exactly"
+        assert not channel(indices) and not spatial(indices), f"channel and spatial dimensions not allowed on indices but got {shape(indices)}"
+        assert not channel(pointers) and not spatial(pointers), f"channel and spatial dimensions not allowed on pointers but got {shape(pointers)}"
+        assert uncompressed_dims.isdisjoint(compressed_dims), f"Dimensions cannot be compressed and uncompressed at the same time but got compressed={compressed_dims}, uncompressed={uncompressed_dims}"
+        assert instance(pointers).size == compressed_dims.volume + 1
+        self._shape = merge_shapes(compressed_dims, uncompressed_dims, batch(indices), batch(pointers), non_instance(values))
+        self._indices = indices
+        self._pointers = rename_dims(pointers, instance, 'pointers')
+        self._values = values
+        self._uncompressed_dims = uncompressed_dims
+        self._compressed_dims = compressed_dims
+        self._default = default
+        self._uncompressed_offset = uncompressed_offset
+        self._uncompressed_indices = uncompressed_indices
+        self._uncompressed_indices_perm = uncompressed_indices_perm
+
+    @property
+    def shape(self) -> Shape:
+        return self._shape
+
+    @property
+    def sparse_dims(self):
+        return self._compressed_dims & self._uncompressed_dims
+
+    @property
+    def sparsity_batch(self):
+        return batch(self._indices) & batch(self._pointers)
+
+    @property
+    def dtype(self) -> DType:
+        return self._values.dtype
+
+    @property
+    def _is_tracer(self) -> bool:
+        return self._values._is_tracer or self._indices._is_tracer or self._pointers._is_tracer
+
+    def _natives(self) -> tuple:
+        indices_const = self._indices.default_backend is not self._values.default_backend
+        pointers_const = self._pointers.default_backend is not self._values.default_backend
+        result = self._values._natives()
+        if not indices_const:
+            result += self._indices._natives()
+        if not pointers_const:
+            result += self._pointers._natives()
+        return result
+
+    def _spec_dict(self) -> dict:
+        indices_const = self._indices.default_backend is not self._values.default_backend
+        pointers_const = self._pointers.default_backend is not self._values.default_backend
+        return {'type': CompressedSparseMatrix,
+                'shape': self._shape,
+                'values': self._values._spec_dict(),
+                'indices': self._indices if indices_const else self._indices._spec_dict(),
+                'pointers': self._pointers if pointers_const else self._pointers._spec_dict(),
+                'uncompressed_dims': self._uncompressed_dims,
+                'compressed_dims': self._compressed_dims,
+                'uncompressed_offset': self._uncompressed_offset,
+                'uncompressed_indices': self._uncompressed_indices,
+                'uncompressed_indices_perm': self._uncompressed_indices_perm,
+                'default': self._default,
+                }
+
+    @classmethod
+    def _from_spec_and_natives(cls, spec: dict, natives: list):
+        values = spec['values']['type']._from_spec_and_natives(spec['values'], natives)
+        indices_or_spec = spec['indices']
+        if isinstance(indices_or_spec, Tensor):
+            indices = indices_or_spec
+        else:
+            indices = spec['indices']['type']._from_spec_and_natives(spec['indices'], natives)
+        pointers_or_spec = spec['pointers']
+        if isinstance(pointers_or_spec, Tensor):
+            pointers = pointers_or_spec
+        else:
+            pointers = spec['pointers']['type']._from_spec_and_natives(spec['pointers'], natives)
+        return CompressedSparseMatrix(indices, pointers, values, spec['uncompressed_dims'], spec['compressed_dims'], spec['default'], spec['uncompressed_offset'], spec['uncompressed_indices'], spec['uncompressed_indices_perm'])
+
+    def _getitem(self, selection: dict) -> 'Tensor':
+        batch_selection = {dim: selection[dim] for dim in self._shape.only(tuple(selection)).names}
+        indices = self._indices[batch_selection]
+        pointers = self._pointers[batch_selection]
+        values = self._values[batch_selection]
+        uncompressed = self._uncompressed_dims
+        compressed = self._compressed_dims
+        uncompressed_offset = self._uncompressed_offset
+        if compressed.only(tuple(selection)):
+            if compressed.rank > 1:
+                raise NotImplementedError
+            ptr_sel = selection[compressed.name]
+            if isinstance(ptr_sel, int):
+                raise NotImplementedError(f"Slicing with int not yet supported for sparse tensors. Use a range instead, e.g. [{ptr_sel}:{ptr_sel+1}] instead of [{ptr_sel}]")
+            elif isinstance(ptr_sel, slice):
+                assert ptr_sel.step in (None, 1), f"Only step size 1 supported for sparse indexing but got {ptr_sel.step}"
+                if batch(indices):
+                    raise NotImplementedError("Slicing not yet supported for batched sparse tensors")
+                start = ptr_sel.start or 0
+                stop = compressed.volume if ptr_sel.stop is None else ptr_sel.stop
+                pointers = pointers[start:stop+1]
+                indices = indices[{instance(indices).name: slice(int(pointers[0]), int(pointers[-1]))}]
+                values = values[{instance(values).name: slice(int(pointers[0]), int(pointers[-1]))}]
+                pointers -= pointers[0]
+                compressed = compressed.after_gather({compressed.name: ptr_sel})
+            else:
+                raise NotImplementedError
+        if uncompressed.only(tuple(selection)):
+            if self._uncompressed_dims.rank > 1:
+                raise NotImplementedError
+            ind_sel = selection[uncompressed.name]
+            if isinstance(ind_sel, int):
+                raise NotImplementedError(f"Slicing with int not yet supported for sparse tensors. Use a range instead, e.g. [{ind_sel}:{ind_sel+1}] instead of [{ind_sel}]")
+            elif isinstance(ind_sel, slice):
+                assert ind_sel.step in (None, 1), f"Only step size 1 supported for sparse indexing but got {ind_sel.step}"
+                start = ind_sel.start or 0
+                stop = uncompressed.volume if ind_sel.stop is None else ind_sel.stop
+                keep = (start <= self._indices) & (self._indices < stop)
+                from ._ops import where
+                values = where(keep, values, 0)
+                uncompressed_offset = start
+                uncompressed = uncompressed.after_gather({uncompressed.name: ind_sel})
+            else:
+                raise NotImplementedError
+        return CompressedSparseMatrix(indices, pointers, values, uncompressed, compressed, self._default, uncompressed_offset)
+
+    def __concat__(self, tensors: tuple, dim: str, **kwargs) -> 'CompressedSparseMatrix':
+        if not all(isinstance(t, CompressedSparseMatrix) for t in tensors):
+            return NotImplemented
+        if dim == self._compressed_dims[0].name:
+            indices = concat([t._indices for t in tensors], instance(self._indices), **kwargs)
+            values = concat([t._values for t in tensors], instance(self._values), **kwargs)
+            pointers = []
+            pointer_offset = 0
+            for i, t in enumerate(tensors):
+                pointers.append((t._pointers[1:] if i else t._pointers) + pointer_offset)
+                pointer_offset += t._pointers[-1]
+            assert pointer_offset == instance(indices).volume
+            pointers = concat(pointers, instance(self._pointers))
+            compressed = self._compressed_dims.with_dim_size(dim, sum(t.shape.get_size(dim) for t in tensors))
+            return CompressedSparseMatrix(indices, pointers, values, self._uncompressed_dims, compressed, self._default, self._uncompressed_offset)
+        elif dim == self._uncompressed_dims[0].name:
+            if all([same_sparsity_pattern(self, t) for t in tensors]):
+                # ToDo test if offsets match and ordered correctly
+                from ._ops import sum_
+                values = sum_([t._values for t in tensors], '0')
+                uncompressed = self._uncompressed_dims.with_dim_size(dim, sum(t.shape.get_size(dim) for t in tensors))
+                return CompressedSparseMatrix(self._indices, self._pointers, values, uncompressed, self._compressed_dims, self._default, uncompressed_offset=None)
+            else:
+                raise NotImplementedError("concatenating arbitrary compressed sparse tensors along uncompressed dim is not yet supported")
+        else:
+            raise NotImplementedError("concatenating compressed sparse tensors along non-sparse dims not yet supported")
+
+    def _op1(self, native_function):
+        return self._with_values(self._values._op1(native_function))
+
+    def _op2(self, other, operator: Callable, native_function: Callable, op_name: str = 'unknown', op_symbol: str = '?') -> 'Tensor':
+        other_shape = shape(other)
+        affects_only_values = self.sparse_dims.isdisjoint(other_shape) and non_instance(self._indices).isdisjoint(other_shape)
+        if affects_only_values:
+            return self._with_values(operator(self._values, other))
+        elif isinstance(other, CompressedSparseMatrix):
+            if same_sparsity_pattern(self, other):
+                result = operator(self._values, other._values)
+                if self._uncompressed_offset is not None:
+                    from ._ops import where
+                    result = where(self._valid_mask(), result, 0)
+                return self._with_values(result)
+            elif op_symbol == '+':
+                raise NotImplementedError("Compressed addition not yet implemented")
+            else:
+                # convert to COO, then perform operation
+                raise NotImplementedError
+        elif self._uncompressed_dims in other_shape and self._compressed_dims.isdisjoint(other_shape):
+            from ._ops import gather, boolean_mask, clip, where
+            if self._uncompressed_offset is None:
+                other_values = gather(other, self._indices, self._uncompressed_dims)
+                return self._with_values(operator(self._values, other_values))
+            # if bake_slice:
+            #     baked = self._bake_slice()
+            #     other_values = gather(other, baked._indices, self._uncompressed_dims)
+            #     return baked._with_values(operator(baked._values, other_values))
+            indices = clip(self._indices - self._uncompressed_offset, 0, self._uncompressed_dims.volume - 1)
+            other_values = gather(other, indices, self._uncompressed_dims)
+            return self._with_values(where(self._valid_mask(), operator(self._values, other_values), 0))
+        elif self._compressed_dims in other_shape and self._uncompressed_dims.isdisjoint(other_shape):
+            from ._ops import gather, boolean_mask, clip, where
+            row_indices, _ = self._coo_indices('clamp')
+            other_values = gather(other, row_indices, self._compressed_dims)
+            result_values = operator(self._values, other_values)
+            if self._uncompressed_offset is not None:
+                result_values = where(self._valid_mask(), result_values, 0)
+            return self._with_values(result_values)
+        else:
+            raise NotImplementedError
+
+    def _with_values(self, new_values: Tensor):
+        return CompressedSparseMatrix(self._indices, self._pointers, new_values, self._uncompressed_dims, self._compressed_dims, self._default, self._uncompressed_offset, self._uncompressed_indices, self._uncompressed_indices_perm)
+
+    def _with_shape_replaced(self, new_shape: Shape):
+        assert self._shape.rank == new_shape.rank
+        raise NotImplementedError
+
+    def _native_csr_components(self, invalid='clamp'):
+        assert invalid in ['clamp', 'discard', 'keep']
+        from ._ops import reshaped_native
+        ind_batch = batch(self._indices) & batch(self._pointers)
+        channels = non_instance(self._values).without(ind_batch)
+        native_indices = reshaped_native(self._indices, [ind_batch, instance])
+        native_pointers = reshaped_native(self._pointers, [ind_batch, instance])
+        native_values = reshaped_native(self._values, [ind_batch, instance, channels])
+        native_shape = self._compressed_dims.volume, self._uncompressed_dims.volume
+        if self._uncompressed_offset is not None:
+            native_indices -= self._uncompressed_offset
+            if invalid == 'clamp':
+                native_indices = choose_backend(native_indices).clip(native_indices, 0, self._uncompressed_dims.volume - 1)
+            elif invalid == 'discard':
+                assert ind_batch.volume == 1, f"Variable number of indices not supported, batch shape = {ind_batch}"
+                b = choose_backend(native_indices, native_pointers)
+                in_range = (0 <= native_indices) & (native_indices < self._uncompressed_dims.volume)
+                native_indices = b.boolean_mask(native_indices, in_range[0], 1)
+                native_values = choose_backend(native_values).boolean_mask(native_values, in_range[0], 1)
+                removed = b.cumsum(~in_range, 1)
+                removed = b.batched_gather_1d(removed, native_pointers[:, 1:]-1)
+                removed = b.concat([b.zeros((b.staticshape(removed)[0], 1), b.dtype(removed)), removed], 1)
+                native_pointers -= removed
+        return ind_batch, channels, native_indices, native_pointers, native_values, native_shape
+
+    def _bake_slice(self) -> 'CompressedSparseMatrix':
+        from ._ops import boolean_mask, cumulative_sum, pad
+        valid = (self._uncompressed_offset <= self._indices) & (self._indices < self._uncompressed_offset + self._uncompressed_dims.volume)
+        indices = boolean_mask(self._indices, instance(self._indices), valid)
+        values = boolean_mask(self._values, instance(self._values), valid)
+        removed = cumulative_sum(~valid, instance(valid))
+        removed = removed[self._pointers.pointers[1:] - 1]
+        removed = pad(removed, {'pointers': (1, 0)}, 1)
+        pointers = self._pointers - removed
+        return CompressedSparseMatrix(indices, pointers, values, self._uncompressed_dims, self._compressed_dims, self._default)
+
+    def _valid_mask(self):
+        return (self._uncompressed_offset <= self._indices) & (self._indices < self._uncompressed_offset + self._uncompressed_dims.volume)
+
+    def _coo_indices(self, invalid='clamp', stack_dim: Shape = None):
+        ind_batch, channels, native_indices, native_pointers, native_values, native_shape = self._native_csr_components(invalid)
+        native_indices = choose_backend(native_indices, native_pointers).csr_to_coo(native_indices, native_pointers)
+        from ._ops import reshaped_tensor
+        if stack_dim is not None:
+            item_names = self._compressed_dims.name, self._uncompressed_dims.name
+            indices = reshaped_tensor(native_indices, [ind_batch, instance(self._indices), stack_dim.with_size(item_names)], convert=False)
+            return indices
+        else:
+            rows = reshaped_tensor(native_indices[..., 0], [ind_batch, instance(self._indices)], convert=False)
+            cols = reshaped_tensor(native_indices[..., 1], [ind_batch, instance(self._indices)], convert=False)
+            return rows, cols
+
+    def decompress(self):
+        if self._uncompressed_indices is None:
+            ind_batch, channels, native_indices, native_pointers, native_values, native_shape = self._native_csr_components(invalid='discard')
+            native_indices = choose_backend(native_indices, native_pointers).csr_to_coo(native_indices, native_pointers)
+            from ._ops import reshaped_tensor
+            if self._compressed_dims.rank == self._uncompressed_dims.rank == 1:
+                indices = reshaped_tensor(native_indices, [ind_batch, instance(self._indices), channel(vector=(self._compressed_dims.name, self._uncompressed_dims.name))], convert=False)
+                values = reshaped_tensor(native_values, [ind_batch, instance(self._values), channel(self._values)])
+            else:
+                raise NotImplementedError()
+            return SparseCoordinateTensor(indices, values, concat_shapes(self._compressed_dims, self._uncompressed_dims), False, True, self._default)
+        if self._uncompressed_indices_perm is not None:
+            self._uncompressed_indices = self._uncompressed_indices[self._uncompressed_indices_perm]
+            self._uncompressed_indices_perm = None
+        return SparseCoordinateTensor(self._uncompressed_indices, self._values, self._compressed_dims & self._uncompressed_dims, False, False, self._default)
+
+    def native(self, order: Union[str, tuple, list, Shape] = None, singleton_for_const=False):
+        assert order is None, f"sparse matrices are always ordered (primal, dual). For custom ordering, use math.dense(tensor).native() instead."
+        return native_matrix(self, self.default_backend)
+
+    def __pack_dims__(self, dims: Tuple[str, ...], packed_dim: Shape, pos: Union[int, None], **kwargs) -> 'Tensor':
+        assert all(d in self._shape for d in dims)
+        dims = self._shape.only(dims, reorder=True)
+        if dims.only(self._compressed_dims).is_empty:  # pack cols
+            assert self._uncompressed_dims.are_adjacent(dims), f"Can only compress adjacent dims but got {dims} for matrix {self._shape}"
+            uncompressed_dims = self._uncompressed_dims.replace(dims, packed_dim.with_size(dims.volume))
+            return CompressedSparseMatrix(self._indices, self._pointers, self._values, uncompressed_dims, self._compressed_dims, self._default, self._uncompressed_offset)
+        elif dims.only(self._uncompressed_dims).is_empty:   # pack rows
+            assert self._compressed_dims.are_adjacent(dims), f"Can only compress adjacent dims but got {dims} for matrix {self._shape}"
+            compressed_dims = self._compressed_dims.replace(dims, packed_dim.with_size(dims.volume))
+            return CompressedSparseMatrix(self._indices, self._pointers, self._values, self._uncompressed_dims, compressed_dims, self._default, self._uncompressed_offset)
+        else:
+            raise NotImplementedError(f"Cannot pack dimensions from both columns and rows with compressed sparse matrices but got {dims}")
+
+
+def get_format(x: Tensor) -> str:
+    """
+    Returns the sparse storage format of a tensor.
+
+    Args:
+        x: `Tensor`
+
+    Returns:
+        One of `'coo'`, `'csr'`, `'csc'`, `'dense'`.
+    """
+    if isinstance(x, SparseCoordinateTensor):
+        return 'coo'
+    elif isinstance(x, CompressedSparseMatrix):
+        if dual(x._uncompressed_dims):
+            return 'csr'
+        else:
+            assert not dual(x._uncompressed_dims), f"Compressed matrix {x.shape} does not match 'csr' or 'csc' because dual dimensions are present in rows and columns."
+            return 'csc'
+    elif isinstance(x, TensorStack):
+        formats = [get_format(t) for t in x._tensors]
+        if all(f == formats[0] for f in formats):
+            return formats[0]
+        return 'mixed'
+    else:
+        return 'dense'
+
+
+def is_sparse(x: Tensor):
+    f = get_format(x)
+    if f == 'dense':
+        return False
+    if f in ['csr', 'csc', 'coo']:
+        return True
+    raise AssertionError(f"Tensor {x} is neither sparse nor dense")
+
+
+def to_format(x: Tensor, format: str):
+    assert format in ('coo', 'csr', 'csc', 'dense'), f"Invalid format: '{format}'. Must be one of 'coo', 'csr', 'csc', 'dense'"
+    if get_format(x) == format:
+        return x
+    if format == 'dense':
+        return dense(x)
+    if isinstance(x, SparseCoordinateTensor):
+        if format == 'csr':
+            return x.compress_rows()
+        elif format == 'csc':
+            return x.compress_cols()
+    elif isinstance(x, CompressedSparseMatrix):
+        if format == 'coo':
+            return x.decompress()
+        else:
+            return to_format(x.decompress(), format)
+    else:  # dense to sparse
+        raise NotImplementedError('dense to sparse not yet supported')
+        # from ._ops import nonzero
+        # indices = nonzero(x)
+        # values = x[indices]
+        # coo = SparseCoordinateTensor(indices, values, dense_shape, can_contain_double_entries=False, indices_sorted=False)
+        # return to_format(coo, format)
+
+
+def sparse_dims(x: Tensor) -> Shape:
+    """
+    Returns the dimensions of a `Tensor` that are explicitly stored in a sparse format.
+
+    Args:
+        x: Any `Tensor`
+
+    Returns:
+        `Shape`
+    """
+    if isinstance(x, SparseCoordinateTensor):
+        return x._dense_shape
+    elif isinstance(x, CompressedSparseMatrix):
+        return concat_shapes(x._compressed_dims, x._uncompressed_dims)
+    else:
+        return EMPTY_SHAPE
+
+
+def get_sparsity(x: Tensor):
+    """
+    Fraction of values currently stored on disk for the given `Tensor` `x`.
+    For sparse tensors, this is `nnz / shape`.
+
+    This is a lower limit on the number of values that will need to be processed for operations involving `x`.
+    The actual number is often higher since many operations require data be laid out in a certain format.
+    In these cases, missing values, such as zeros, are filled in before the operation.
+
+    The following operations may return tensors whose values are only partially stored:
+
+    * `phi.math.expand()`
+    * `phi.math.pairwise_distance()` with `max_distance` set.
+    * Tracers used in `phi.math.jit_compile_linear()`
+    * Stacking any of the above.
+
+    Args:
+        x: `Tensor`
+
+    Returns:
+        The number of values that are actually stored on disk.
+        This does not include additional information, such as position information / indices.
+        For sparse matrices, this is equal to the number of nonzero values.
+    """
+    return stored_values(x, invalid='keep').shape.volume / x.shape.volume
+
+
+def stored_values(x: Tensor, list_dim=instance('entries'), invalid='discard') -> Tensor:
+    """
+    Returns the stored values for a given `Tensor``.
+    For sparse tensors, this will return only the stored entries.
+    For collapsed tensors, only the stored dimensions will be returned.
+    Dense tensors are returned as-is.
+
+    Args:
+        x: `Tensor`
+        list_dim: Dimension along which stored values should be laid out.
+        invalid: One of `'discard'`, `'clamp'`, `'keep'` Filter result by valid indices.
+            Internally, invalid indices may be stored for performance reasons.
+
+    Returns:
+        `Tensor` representing all values stored to represent `x`.
+    """
+    assert invalid in ['discard', 'clamp', 'keep'], f"invalid handling must be one of 'discard', 'clamp', 'keep' but got {invalid}"
+    if isinstance(x, NativeTensor):
+        return expand(NativeTensor(x._native, x._native_shape, x._native_shape), list_dim.with_size(1))
+    if isinstance(x, TensorStack):
+        if x.is_cached:
+            return stored_values(cached(x))
+        return stack([stored_values(t, list_dim) for t in x._tensors], x._stack_dim)
+    elif isinstance(x, CompressedSparseMatrix):
+        if invalid in ['keep', 'clamp']:
+            return rename_dims(x._values, instance, list_dim)
+        else:
+            x = x.decompress()  # or apply slices, then return values
+    if isinstance(x, SparseCoordinateTensor):
+        if x._can_contain_double_entries:
+            warnings.warn(f"stored_values of sparse tensor {x.shape} may contain multiple values for the same position.")
+        return rename_dims(x._values, instance, list_dim)
+    raise ValueError(x)
+
+
+def stored_indices(x: Tensor, list_dim=instance('entries'), index_dim=channel('index'), invalid='discard') -> Tensor:
+    """
+    Returns the indices of the stored values for a given `Tensor``.
+    For sparse tensors, this will return the stored indices tensor.
+    For collapsed tensors, only the stored dimensions will be returned.
+
+    Args:
+        x: `Tensor`
+        list_dim: Dimension along which stored indices should be laid out.
+        invalid: One of `'discard'`, `'clamp'`, `'keep'` Filter result by valid indices.
+            Internally, invalid indices may be stored for performance reasons.
+
+    Returns:
+        `Tensor` representing all indices of stored values.
+    """
+    assert invalid in ['discard', 'clamp', 'keep'], f"invalid handling must be one of 'discard', 'clamp', 'keep' but got {invalid}"
+    if isinstance(x, NativeTensor):
+        from ._ops import meshgrid
+        if batch(x):
+            raise NotImplementedError
+        indices = meshgrid(x._native_shape.non_batch.non_channel)
+        return pack_dims(indices, non_channel, list_dim)
+    if isinstance(x, TensorStack):
+        if x.is_cached or not x.requires_broadcast:
+            return stored_indices(cached(x))
+        raise NotImplementedError
+        return stack([stored_indices(t, list_dim) for t in x._tensors], x._stack_dim)  # ToDo add index for stack dim
+    elif isinstance(x, CompressedSparseMatrix):
+        return rename_dims(x._coo_indices(invalid, stack_dim=index_dim), instance, list_dim)
+    if isinstance(x, SparseCoordinateTensor):
+        if x._can_contain_double_entries:
+            warnings.warn(f"stored_values of sparse tensor {x.shape} may contain multiple values for the same position.")
+        new_index_dim = index_dim.with_size(channel(x._indices).item_names[0])
+        return rename_dims(x._indices, [instance(x._indices).name, channel(x._indices).name], [list_dim, new_index_dim])
+    raise ValueError(x)
+
+
+def same_sparsity_pattern(t1: Tensor, t2: Tensor, allow_const=False):
+    if isinstance(t1, TensorStack):
+        raise NotImplementedError
+    if isinstance(t2, TensorStack):
+        raise NotImplementedError
+    from ._ops import close
+    if allow_const:
+        if is_sparse(t1) and not is_sparse(t2) and sparse_dims(t1) not in t2.shape:
+            return True
+        if is_sparse(t2) and not is_sparse(t1) and sparse_dims(t2) not in t2.shape:
+            return True
+    if type(t1) != type(t2):
+        return False
+    if isinstance(t1, CompressedSparseMatrix):
+        if t2._indices is t1._indices and t2._pointers is t1._pointers:
+            return True
+        return close(t1._indices, t2._indices) and close(t1._pointers, t2._pointers)
+    if isinstance(t1, SparseCoordinateTensor):
+        if t1._indices is t2._indices:
+            return True
+        return close(t1._indices, t2._indices)
+    raise NotImplementedError
+
+
+def dense(x: Tensor) -> Tensor:
+    """
+    Convert a sparse tensor representation to an equivalent dense one in which all values are explicitly stored contiguously in memory.
+
+    Args:
+        x: Any `Tensor`.
+            Python primitives like `float`, `int` or `bool` will be converted to `Tensors` in the process.
+
+    Returns:
+        Dense tensor.
+    """
+    from phi.math import reshaped_tensor
+    if isinstance(x, SparseCoordinateTensor):
+        from ._ops import scatter
+        return scatter(x.shape, x._indices, x._values, mode='add', outside_handling='undefined')
+    elif isinstance(x, CompressedSparseMatrix):
+        ind_batch, channels, native_indices, native_pointers, native_values, native_shape = x._native_csr_components()
+        native_dense = x.default_backend.csr_to_dense(native_indices, native_pointers, native_values, native_shape)
+        return reshaped_tensor(native_dense, [ind_batch, x._compressed_dims, x._uncompressed_dims, channels])
+    elif isinstance(x, NativeTensor):
+        return x
+    elif isinstance(x, Tensor):
+        return cached(x)
+    elif isinstance(x, (Number, bool)):
+        return wrap(x)
+
+
+def dot_compressed_dense(compressed: CompressedSparseMatrix, cdims: Shape, dense: Tensor, ddims: Shape):
+    from phi.math import reshaped_native, reshaped_tensor
+    backend = choose_backend(*compressed._natives() + dense._natives())
+    if compressed._uncompressed_dims in cdims:  # proper matrix-vector multiplication
+        ind_batch, channels, native_indices, native_pointers, native_values, native_shape = compressed._native_csr_components()
+        rhs_channels = shape(dense).without(ddims).without(channels)
+        dense_native = reshaped_native(dense, [ind_batch, ddims, channels, rhs_channels])
+        result_native = backend.mul_csr_dense(native_indices, native_pointers, native_values, native_shape, dense_native)
+        result = reshaped_tensor(result_native, [ind_batch, channels, compressed._compressed_dims, rhs_channels])
+        return result
+    else:  # transposed matrix vector multiplication. This is inefficient
+        raise NotImplementedError("Transposed sparse matrix multiplication not yet implemented")
+
+
+def dot_coordinate_dense(sparse: SparseCoordinateTensor, sdims: Shape, dense: Tensor, ddims: Shape):
+    from phi.math import reshaped_native, reshaped_tensor
+    backend = choose_backend(*sparse._natives() + dense._natives())
+    ind_batch, channels, native_indices, native_values, native_shape = sparse._native_coo_components(sdims, matrix=True)
+    rhs_channels = shape(dense).without(ddims).without(channels)
+    dense_native = reshaped_native(dense, [ind_batch, ddims, channels, rhs_channels])
+    result_native = backend.mul_coo_dense(native_indices, native_values, native_shape, dense_native)
+    result = reshaped_tensor(result_native, [ind_batch, channels, sparse._dense_shape.without(sdims), rhs_channels])
+    return result
+
+
+def native_matrix(value: Tensor, target_backend: Backend):
+    target_backend = target_backend or value.default_backend
+    cols = dual(value)
+    rows = non_batch(value).non_dual
+    if isinstance(value, SparseCoordinateTensor):
+        ind_batch, channels, indices, values, shape = value._native_coo_components(dual, matrix=True)
+        if ind_batch.volume > 1 or channels.volume > 1:
+            return target_backend.sparse_coo_tensor_batched(indices, values, shape)
+        else:
+            return target_backend.sparse_coo_tensor(indices[0], values[0, :, 0], shape)
+    elif isinstance(value, CompressedSparseMatrix):
+        assert not non_instance(value._values), f"native_matrix does not support vector-valued matrices. Vector dims: {non_batch(value).without(sparse_dims(value))}"
+        ind_batch, channels, indices, pointers, values, shape = value._native_csr_components()
+        if dual(value._uncompressed_dims):  # CSR
+            assert not dual(value._compressed_dims), "Dual dimensions on both compressed and uncompressed dimensions"
+            if ind_batch.volume > 1 or channels.volume > 1:
+                return target_backend.csr_matrix_batched(indices, pointers, values, shape)
+            else:
+                return target_backend.csr_matrix(indices[0], pointers[0], values[0, :, 0], shape)
+        else:  # CSC
+            assert not dual(value._uncompressed_dims)
+            if ind_batch.volume > 1 or channels.volume > 1:
+                return target_backend.csc_matrix_batched(pointers, indices, values, shape)
+            else:
+                return target_backend.csc_matrix(pointers[0], indices[0], values[0, :, 0], shape)
+    else:
+        if batch(value):
+            raise NotImplementedError
+        v = pack_dims(value, rows, channel('_row'))
+        v = pack_dims(v, cols, channel('_col'))
+        from ._ops import reshaped_native
+        return reshaped_native(v, ['_row', '_col'])
```

### Comparing `phiflow-2.3.4/phi/math/_tensors.py` & `phiflow-2.4.0/phi/math/_tensors.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,2607 +1,2538 @@
-import copy
-import numbers
-import warnings
-from collections import namedtuple
-from contextlib import contextmanager
-from typing import Tuple, Callable, List, TypeVar, Union
-
-from dataclasses import dataclass
-import numpy
-import numpy as np
-
-from .magic import Shapable
-from ._magic_ops import PhiTreeNodeType, variable_attributes, copy_with, stack, pack_dims, expand
-from ._shape import (Shape,
-                     CHANNEL_DIM, BATCH_DIM, SPATIAL_DIM, EMPTY_SHAPE,
-                     parse_dim_order, shape_stack, merge_shapes, channel, concat_shapes,
-                     TYPE_ABBR, IncompatibleShapes, INSTANCE_DIM, batch, spatial, dual, instance, shape, DimFilter, non_batch)
-from .backend import NoBackendFound, choose_backend, BACKENDS, get_precision, default_backend, convert as convert_, \
-    Backend, ComputeDevice
-from .backend._dtype import DType, combine_types
-from .magic import BoundDim, PhiTreeNode, slicing_dict
-
-
-class Tensor:
-    """
-    Abstract base class to represent structured data of one data type.
-    This class replaces the native tensor classes `numpy.ndarray`, `torch.Tensor`, `tensorflow.Tensor` or `jax.numpy.ndarray` as the main data container in Φ<sub>Flow</sub>.
-
-    `Tensor` instances are different from native tensors in two important ways:
-
-    * The dimensions of Tensors have *names* and *types*.
-    * Tensors can have non-uniform shapes, meaning that the size of dimensions can vary along other dimensions.
-
-    To check whether a value is a tensor, use `isinstance(value, Tensor)`.
-
-    To construct a Tensor, use `phi.math.tensor()`, `phi.math.wrap()` or one of the basic tensor creation functions,
-    see https://tum-pbs.github.io/PhiFlow/Math.html#tensor-creation .
-
-    Tensors are not editable.
-    When backed by an editable native tensor, e.g. a `numpy.ndarray`, do not edit the underlying data structure.
-    """
-
-    def native(self, order: Union[str, tuple, list, Shape] = None):
-        """
-        Returns a native tensor object with the dimensions ordered according to `order`.
-        
-        Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
-        If a dimension of the tensor is not listed in `order`, a `ValueError` is raised.
-
-        Args:
-            order: (Optional) Order of dimension names as comma-separated string, list or `Shape`.
-
-        Returns:
-            Native tensor representation, such as PyTorch tensor or NumPy array.
-
-        Raises:
-            ValueError if the tensor cannot be transposed to match target_shape
-        """
-        raise NotImplementedError(self.__class__)
-
-    def numpy(self, order: Union[str, tuple, list, Shape] = None) -> np.ndarray:
-        """
-        Converts this tensor to a `numpy.ndarray` with dimensions ordered according to `order`.
-        
-        *Note*: Using this function breaks the autograd chain. The returned tensor is not differentiable.
-        To get a differentiable tensor, use `Tensor.native()` instead.
-        
-        Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
-        If a dimension of the tensor is not listed in `order`, a `ValueError` is raised.
-
-        If this `Tensor` is backed by a NumPy array, a reference to this array may be returned.
-
-        See Also:
-            `phi.math.numpy()`
-
-        Args:
-            order: (Optional) Order of dimension names as comma-separated string, list or `Shape`.
-
-        Returns:
-            NumPy representation
-
-        Raises:
-            ValueError if the tensor cannot be transposed to match target_shape
-        """
-        native = self.native(order=order)
-        return choose_backend(native).numpy(native)
-
-    def __array__(self, dtype=None):  # NumPy conversion
-        if self.rank > 1:
-            warnings.warn("Automatic conversion of Φ-Flow tensors to NumPy can cause problems because the dimension order is not guaranteed.", SyntaxWarning, stacklevel=3)
-        return self.numpy(self._shape)
-
-    def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):  # NumPy interface
-        if len(inputs) != 2:
-            return NotImplemented
-        if ufunc.__name__ == 'multiply':
-            if inputs[0] is self:
-                return self._op2(inputs[1], lambda x, y: x * y, lambda x, y: choose_backend(x, y).mul(x, y), 'mul', '*')
-            else:
-                return self._op2(inputs[0], lambda x, y: y * x, lambda x, y: choose_backend(x, y).mul(y, x), 'rmul', '*')
-        if ufunc.__name__ == 'add':
-            if inputs[0] is self:
-                return self._op2(inputs[1], lambda x, y: x + y, lambda x, y: choose_backend(x, y).add(x, y), 'add', '+')
-            else:
-                return self._op2(inputs[0], lambda x, y: y + x, lambda x, y: choose_backend(x, y).add(y, x), 'radd', '+')
-        if ufunc.__name__ == 'subtract':
-            if inputs[0] is self:
-                return self._op2(inputs[1], lambda x, y: x - y, lambda x, y: choose_backend(x, y).sub(x, y), 'add', '-')
-            else:
-                return self._op2(inputs[0], lambda x, y: y - x, lambda x, y: choose_backend(x, y).sub(y, x), 'rsub', '-')
-        if ufunc.__name__ in ['divide', 'true_divide']:
-            if inputs[0] is self:
-                return self._op2(inputs[1], lambda x, y: x / y, lambda x, y: choose_backend(x, y).div(x, y), 'true_divide', '/')
-            else:
-                return self._op2(inputs[0], lambda x, y: y / x, lambda x, y: choose_backend(x, y).div(y, x), 'r_true_divide', '/')
-        if ufunc.__name__ == 'floor_divide':
-            if inputs[0] is self:
-                return self._op2(inputs[1], lambda x, y: x // y, lambda x, y: choose_backend(x, y).floordiv(x, y), 'floor_divide', '//')
-            else:
-                return self._op2(inputs[0], lambda x, y: y // x, lambda x, y: choose_backend(x, y).floordiv(y, x), 'r_floor_divide', '//')
-        if ufunc.__name__ == 'remainder':
-            if inputs[0] is self:
-                return self._op2(inputs[1], lambda x, y: x % y, lambda x, y: choose_backend(x, y).mod(x, y), 'remainder', '%')
-            else:
-                return self._op2(inputs[0], lambda x, y: y % x, lambda x, y: choose_backend(x, y).mod(y, x), 'r_remainder', '%')
-        if ufunc.__name__ == 'power':
-            if inputs[0] is self:
-                return self._op2(inputs[1], lambda x, y: x ** y, lambda x, y: choose_backend(x, y).pow(x, y), 'power', '**')
-            else:
-                return self._op2(inputs[0], lambda x, y: y ** x, lambda x, y: choose_backend(x, y).pow(y, x), 'r_power', '**')
-        if ufunc.__name__ == 'equal':
-            if _EQUALITY_BY_REF:
-                return wrap(inputs[0] is inputs[1])
-            if inputs[0] is self:
-                return self._op2(inputs[1], lambda x, y: x == y, lambda x, y: choose_backend(x, y).equal(x, y), 'equal', '==')
-            else:
-                return self._op2(inputs[0], lambda x, y: y == x, lambda x, y: choose_backend(x, y).equal(y, x), 'r_equal', '==')
-        if ufunc.__name__ == 'not_equal':
-            if _EQUALITY_BY_REF:
-                return wrap(inputs[0] is not inputs[1])
-            if inputs[0] is self:
-                return self._op2(inputs[1], lambda x, y: x != y, lambda x, y: choose_backend(x, y).not_equal(x, y), 'equal', '!=')
-            else:
-                return self._op2(inputs[0], lambda x, y: y != x, lambda x, y: choose_backend(x, y).not_equal(y, x), 'r_equal', '!=')
-        if ufunc.__name__ == 'greater':
-            if inputs[0] is self:
-                return self._op2(inputs[1], lambda x, y: x > y, lambda x, y: choose_backend(x, y).greater_than(x, y), 'greater', '>')
-            else:
-                return self._op2(inputs[0], lambda x, y: y > x, lambda x, y: choose_backend(x, y).greater_than(y, x), 'r_greater', '>')
-        if ufunc.__name__ == 'greater_equal':
-            if inputs[0] is self:
-                return self._op2(inputs[1], lambda x, y: x >= y, lambda x, y: choose_backend(x, y).greater_or_equal(x, y), 'greater_equal', '>=')
-            else:
-                return self._op2(inputs[0], lambda x, y: y >= x, lambda x, y: choose_backend(x, y).greater_or_equal(y, x), 'r_greater_equal', '>=')
-        if ufunc.__name__ == 'less':
-            if inputs[0] is self:
-                return self._op2(inputs[1], lambda x, y: x < y, lambda x, y: choose_backend(x, y).greater_than(y, x), 'less', '<')
-            else:
-                return self._op2(inputs[0], lambda x, y: y < x, lambda x, y: choose_backend(x, y).greater_than(x, y), 'r_less', '<')
-        if ufunc.__name__ == 'less_equal':
-            if inputs[0] is self:
-                return self._op2(inputs[1], lambda x, y: x <= y, lambda x, y: choose_backend(x, y).greater_or_equal(y, x), 'less_equal', '<=')
-            else:
-                return self._op2(inputs[0], lambda x, y: y <= x, lambda x, y: choose_backend(x, y).greater_or_equal(x, y), 'r_less_equal', '<=')
-        if ufunc.__name__ == 'left_shift':
-            if inputs[0] is self:
-                return self._op2(inputs[1], lambda x, y: x << y, lambda x, y: choose_backend(x, y).shift_bits_left(x, y), 'left_shift', '<<')
-            else:
-                return self._op2(inputs[0], lambda x, y: y << x, lambda x, y: choose_backend(x, y).shift_bits_left(y, x), 'r_left_shift', '<<')
-        if ufunc.__name__ == 'right_shift':
-            if inputs[0] is self:
-                return self._op2(inputs[1], lambda x, y: x >> y, lambda x, y: choose_backend(x, y).shift_bits_right(x, y), 'right_shift', '>>')
-            else:
-                return self._op2(inputs[0], lambda x, y: y >> x, lambda x, y: choose_backend(x, y).shift_bits_right(y, x), 'r_right_shift', '>>')
-        raise NotImplementedError(f"NumPy function '{ufunc.__name__}' is not compatible with Φ-Flow tensors.")
-
-    @property
-    def dtype(self) -> DType:
-        """ Data type of the elements of this `Tensor`. """
-        raise NotImplementedError()
-
-    @property
-    def shape(self) -> Shape:
-        """ The `Shape` lists the dimensions with their sizes, names and types. """
-        raise NotImplementedError()
-
-    @property
-    def default_backend(self) -> Backend:
-        from ._ops import choose_backend_t
-        return choose_backend_t(self)
-
-    def _with_shape_replaced(self, new_shape: Shape):
-        raise NotImplementedError()
-
-    def _with_natives_replaced(self, natives: list):
-        """ Replaces all n _natives() of this Tensor with the first n elements of the list and removes them from the list. """
-        raise NotImplementedError()
-
-    @property
-    def rank(self) -> int:
-        """
-        Number of explicit dimensions of this `Tensor`. Equal to `tensor.shape.rank`.
-        This replaces [`numpy.ndarray.ndim`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.ndim.html) /
-        [`torch.Tensor.dim`](https://pytorch.org/docs/master/generated/torch.Tensor.dim.html) /
-        [`tf.rank()`](https://www.tensorflow.org/api_docs/python/tf/rank) /
-        [`jax.numpy.ndim()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndim.html).
-        """
-        return self.shape.rank
-
-    @property
-    def _is_tracer(self) -> bool:
-        """
-        Tracers store additional internal information.
-        They should not be converted to `native()` in intermediate operations.
-        
-        TensorStack prevents performing the actual stack operation if one of its component tensors is special.
-        """
-        raise NotImplementedError(self.__class__)
-
-    def _to_dict(self):
-        return cached(self)._to_dict()
-
-    def __len__(self):
-        return self.shape.volume if self.rank == 1 else NotImplemented
-
-    def __bool__(self):
-        assert self.rank == 0, f"Cannot convert tensor with non-empty shape {self.shape} to bool. Use tensor.any or tensor.all instead."
-        from ._ops import all_
-        if not self.default_backend.supports(Backend.jit_compile):  # NumPy
-            return bool(self.native()) if self.rank == 0 else bool(all_(self).native())
-        else:
-            # __bool__ does not work with TensorFlow tracing.
-            # TensorFlow needs to see a tf.Tensor in loop conditions but won't allow bool() invocations.
-            # However, this function must always return a Python bool.
-            raise AssertionError("To evaluate the boolean value of a Tensor, use 'Tensor.all'.")
-
-    @property
-    def all(self):
-        """ Whether all values of this `Tensor` are `True` as a native bool. """
-        from ._ops import all_, cast
-        if self.rank == 0:
-            return cast(self, DType(bool)).native()
-        else:
-            return all_(self, dim=self.shape).native()
-
-    @property
-    def any(self):
-        """ Whether this `Tensor` contains a `True` value as a native bool. """
-        from ._ops import any_, cast
-        if self.rank == 0:
-            return cast(self, DType(bool)).native()
-        else:
-            return any_(self, dim=self.shape).native()
-
-    @property
-    def mean(self):
-        """ Mean value of this `Tensor` as a native scalar. """
-        from ._ops import mean
-        return mean(self, dim=self.shape).native()
-
-    @property
-    def finite_mean(self):
-        """ Mean value of all finite values in this `Tensor` as a native scalar. """
-        from ._ops import finite_mean
-        return finite_mean(self, dim=self.shape).native()
-
-    @property
-    def std(self):
-        """ Standard deviation of this `Tensor` as a native scalar. """
-        from ._ops import std
-        return std(self, dim=self.shape).native()
-
-    @property
-    def sum(self):
-        """ Sum of all values of this `Tensor` as a native scalar. """
-        from ._ops import sum_
-        return sum_(self, dim=self.shape).native()
-
-    @property
-    def finite_sum(self):
-        """ Sum of all finite values of this `Tensor` as a native scalar. """
-        from ._ops import finite_sum
-        return finite_sum(self, dim=self.shape).native()
-
-    @property
-    def min(self):
-        """ Minimum value of this `Tensor` as a native scalar. """
-        from ._ops import min_
-        return min_(self, dim=self.shape).native()
-
-    @property
-    def finite_min(self):
-        """ Minimum finite value of this `Tensor` as a native scalar. """
-        from ._ops import finite_min
-        return finite_min(self, dim=self.shape).native()
-
-    @property
-    def max(self):
-        """ Maximum value of this `Tensor` as a native scalar. """
-        from ._ops import max_
-        return max_(self, dim=self.shape).native()
-
-    @property
-    def finite_max(self):
-        """ Maximum finite value of this `Tensor` as a native scalar. """
-        from ._ops import finite_max
-        return finite_max(self, dim=self.shape).native()
-
-    @property
-    def real(self) -> 'Tensor':
-        """
-        Returns the real part of this tensor.
-
-        See Also:
-            `phi.math.real()`
-        """
-        from ._ops import real
-        return real(self)
-
-    @property
-    def imag(self) -> 'Tensor':
-        """
-        Returns the imaginary part of this tensor.
-        If this tensor does not store complex numbers, returns a zero tensor with the same shape and dtype as this tensor.
-
-        See Also:
-            `phi.math.imag()`
-        """
-        from ._ops import imag
-        return imag(self)
-
-    @property
-    def available(self) -> bool:
-        """
-        A tensor is available if it stores concrete values and these can currently be read.
-
-        Tracers used inside jit compilation are typically not available.
-
-        See Also:
-            `phi.math.jit_compile()`.
-        """
-        if self._is_tracer:
-            return False
-        natives = self._natives()
-        natives_available = [choose_backend(native).is_available(native) for native in natives]
-        return all(natives_available)
-
-    @property
-    def device(self) -> Union[ComputeDevice, None]:
-        """
-        Returns the `ComputeDevice` that this tensor is allocated on.
-        The device belongs to this tensor's `default_backend`.
-
-        See Also:
-            `Tensor.default_backend`.
-        """
-        natives = self._natives()
-        if not natives:
-            return None
-        return self.default_backend.get_device(natives[0])
-
-    def __int__(self):
-        return int(self.native()) if self.shape.volume == 1 else NotImplemented
-
-    def __float__(self):
-        return float(self.native()) if self.shape.volume == 1 else NotImplemented
-
-    def __complex__(self):
-        return complex(self.native()) if self.shape.volume == 1 else NotImplemented
-
-    def __index__(self):
-        assert self.shape.volume == 1, f"Only scalar tensors can be converted to index but has shape {self.shape}"
-        assert self.dtype.kind == int, f"Only int tensors can be converted to index but dtype is {self.dtype}"
-        return int(self.native())
-
-    def __repr__(self):
-        return format_tensor(self, PrintOptions())
-
-    def _repr_pretty_(self, printer, cycle):
-        printer.text(format_tensor(self, PrintOptions(colors=DEFAULT_COLORS)))
-
-    def __format__(self, format_spec: str):
-        specs = format_spec.split(':')
-        layout_ = 'auto'
-        for possible_layout in ['summary', 'full', 'row', 'numpy']:
-            if possible_layout in specs:
-                assert layout_ == 'auto', f"Two layout identifiers encountered in '{format_spec}'"
-                layout_ = possible_layout
-        include_shape = 'shape' in specs or (False if 'no-shape' in specs else None)
-        include_dtype = 'dtype' in specs or (False if 'no-dtype' in specs else None)
-        color = 'color' in specs or (False if 'no-color' in specs else None)
-        threshold = 8
-        float_format = None
-        for spec in specs:
-            if spec.startswith('threshold='):
-                threshold = int(spec[len('threshold='):])
-            elif '.' in spec:
-                float_format = spec
-        return format_tensor(self, PrintOptions(layout_, float_format, threshold, color, include_shape, include_dtype))
-
-    def __getitem__(self, item) -> 'Tensor':
-        if isinstance(item, Tensor):
-            from ._ops import gather
-            return gather(self, item)
-        item = slicing_dict(self, item)
-        selections = {}
-        sliced = self
-        for dim, selection in item.items():
-            if dim not in self.shape:
-                continue
-            selection = self.shape.prepare_gather(dim, selection)
-            # Either handle slicing directly or add it to the dict
-            if isinstance(selection, (tuple, list)):
-                from ._magic_ops import stack
-                result = [sliced[{dim: i}] for i in selection]
-                stack_dim = sliced.shape[dim].after_gather({dim: selection})
-                sliced = stack(result, stack_dim)
-            elif isinstance(selection, Tensor) and selection.dtype.kind == bool:
-                from ._ops import boolean_mask
-                sliced = boolean_mask(sliced, dim, selection)
-            elif isinstance(selection, Tensor) and selection.dtype.kind == int:
-                from ._ops import gather
-                sliced = gather(sliced, selection, dims=dim)
-            else:
-                selections[dim] = selection
-        return sliced._getitem(selections) if selections else sliced
-
-    def _getitem(self, selection: dict) -> 'Tensor':
-        """
-        Slice the tensor along specified dimensions.
-
-        Args:
-          selection: dim_name: str -> Union[int, slice]
-          selection: dict: 
-
-        Returns:
-
-        """
-        raise NotImplementedError()
-
-    def __setitem__(self, key, value):
-        raise NotImplementedError("Tensors are not editable to preserve the autodiff chain. This feature might be added in the future. To update part of a tensor, use math.where() or math.scatter()")
-
-    def flip(self, *dims: str) -> 'Tensor':
-        """
-        Reverses the order of elements along one or multiple dimensions.
-
-        Args:
-            *dims: dimensions to flip
-
-        Returns:
-            `Tensor` of the same `Shape`
-        """
-        raise NotImplementedError()
-
-    def __unstack__(self, dims: Tuple[str, ...]) -> Tuple['Tensor', ...]:  # from phi.math.magic.Sliceable
-        if len(dims) == 1:
-            return self.unstack(dims[0])
-        else:
-            return NotImplemented
-
-    def unstack(self, dimension: str):
-        """
-        Splits this tensor along the specified dimension.
-        The returned tensors have the same dimensions as this tensor save the unstacked dimension.
-
-        Raises an error if the dimension is not part of the `Shape` of this `Tensor`.
-
-        See Also:
-            `TensorDim.unstack()`
-
-        Args:
-          dimension(str or int or TensorDim): name of dimension or Dimension or None for component dimension
-
-        Returns:
-          tuple of tensors
-
-        """
-        raise NotImplementedError()
-
-    @staticmethod
-    def __stack__(values: tuple, dim: Shape, **_kwargs) -> 'Tensor':
-        from ._ops import stack_tensors
-        return stack_tensors(values, dim)
-
-    def __expand__(self, dims: Shape, **kwargs) -> 'Tensor':
-        from ._ops import expand_tensor
-        return expand_tensor(self, dims)
-
-    @staticmethod
-    def __concat__(values: tuple, dim: str, **kwargs) -> 'Tensor':
-        from ._ops import concat_tensor
-        return concat_tensor(values, dim)
-
-    def __replace_dims__(self, dims: Tuple[str, ...], new_dims: Shape, **kwargs) -> 'Tensor':
-        from ._magic_ops import rename_dims
-        return self._with_shape_replaced(rename_dims(self.shape, dims, new_dims))
-
-    def __unpack_dim__(self, dim: str, unpacked_dims: Shape, **kwargs) -> 'Tensor':
-        if self.shape.is_uniform:
-            native = self.native(self.shape.names)
-            new_shape = self.shape.without(dim)
-            i = self.shape.index(dim)
-            for d in unpacked_dims:
-                new_shape = new_shape._expand(d, pos=i)
-                i += 1
-            native_reshaped = choose_backend(native).reshape(native, new_shape.sizes)
-            return NativeTensor(native_reshaped, new_shape)
-        else:
-            tensors = self._tensors
-            if dim == self._stack_dim.name:
-                for udim in unpacked_dims:
-                    tensors = [TensorStack(tensors[o::len(tensors)//udim.size], udim) for o in range(len(tensors)//udim.size)]
-                assert len(tensors) == 1
-                return tensors[0]
-            raise NotImplementedError
-
-    def __pack_dims__(self, dims: Tuple[str, ...], packed_dim: Shape, pos: Union[int, None], **kwargs) -> 'Tensor':
-        order = self.shape._order_group(dims)
-        if self.shape.is_uniform:
-            native = self.native(order)
-            if pos is None:
-                pos = min(self.shape.indices(dims))
-            new_shape = self.shape.without(dims)._expand(packed_dim.with_sizes([self.shape.only(dims).volume]), pos)
-            native = choose_backend(native).reshape(native, new_shape.sizes)
-            return NativeTensor(native, new_shape)
-        else:
-            from ._ops import concat_tensor
-            from ._magic_ops import pack_dims
-            value = cached(self)
-            assert isinstance(value, TensorStack)
-            assert value._stack_dim.name in dims
-            inner_packed = [pack_dims(t, dims, packed_dim) for t in value._tensors]
-            return concat_tensor(inner_packed, packed_dim.name)
-
-    def __cast__(self, dtype: DType):
-        return self._op1(lambda native: choose_backend(native).cast(native, dtype=dtype))
-
-    def dimension(self, name: Union[str, Shape]) -> 'TensorDim':
-        """
-        Returns a reference to a specific dimension of this tensor.
-        This is equivalent to the syntax `tensor.<name>`.
-
-        The dimension need not be part of the `Tensor.shape` in which case its size is 1.
-
-        Args:
-            name: dimension name
-
-        Returns:
-            `TensorDim` corresponding to a dimension of this tensor
-        """
-        if isinstance(name, str):
-            return TensorDim(self, name)
-        elif isinstance(name, Shape):
-            return TensorDim(self, name.name)
-        else:
-            raise ValueError(name)
-
-    def pack(self, dims, packed_dim):
-        """ See `pack_dims()` """
-        from ._ops import pack_dims
-        return pack_dims(self, dims, packed_dim)
-
-    def unpack(self, dim, unpacked_dims):
-        """ See `unpack_dim()` """
-        from ._ops import unpack_dim
-        return unpack_dim(self, dim, unpacked_dims)
-
-    def __getattr__(self, name):
-        if name.startswith('__'):  # called by hasattr in magic ops
-            raise AttributeError
-        if name.startswith('_'):
-            raise AttributeError(f"'{type(self)}' object has no attribute '{name}'")
-        if name == 'is_tensor_like':  # TensorFlow replaces abs() while tracing and checks for this attribute
-            raise AttributeError(f"'{type(self)}' object has no attribute '{name}'")
-        assert name not in ('shape', '_shape', 'tensor'), name
-        return TensorDim(self, name)
-
-    def __add__(self, other):
-        return self._op2(other, lambda x, y: x + y, lambda x, y: choose_backend(x, y).add(x, y), 'add', '+')
-
-    def __radd__(self, other):
-        return self._op2(other, lambda x, y: y + x, lambda x, y: choose_backend(x, y).add(y, x), 'radd', '+')
-
-    def __sub__(self, other):
-        return self._op2(other, lambda x, y: x - y, lambda x, y: choose_backend(x, y).sub(x, y), 'sub', '-')
-
-    def __rsub__(self, other):
-        return self._op2(other, lambda x, y: y - x, lambda x, y: choose_backend(x, y).sub(y, x), 'rsub', '-')
-
-    def __and__(self, other):
-        return self._op2(other, lambda x, y: x & y, lambda x, y: choose_backend(x, y).and_(x, y), 'and', '&')
-
-    def __rand__(self, other):
-        return self._op2(other, lambda x, y: y & x, lambda x, y: choose_backend(x, y).and_(y, x), 'rand', '&')
-
-    def __or__(self, other):
-        return self._op2(other, lambda x, y: x | y, lambda x, y: choose_backend(x, y).or_(x, y), 'or', '|')
-
-    def __ror__(self, other):
-        return self._op2(other, lambda x, y: y | x, lambda x, y: choose_backend(x, y).or_(y, x), 'ror', '|')
-
-    def __xor__(self, other):
-        return self._op2(other, lambda x, y: x ^ y, lambda x, y: choose_backend(x, y).xor(x, y), 'xor', '^')
-
-    def __rxor__(self, other):
-        return self._op2(other, lambda x, y: y ^ x, lambda x, y: choose_backend(x, y).xor(y, x), 'rxor', '^')
-
-    def __mul__(self, other):
-        return self._op2(other, lambda x, y: x * y, lambda x, y: choose_backend(x, y).mul(x, y), 'mul', '*')
-
-    def __rmul__(self, other):
-        return self._op2(other, lambda x, y: y * x, lambda x, y: choose_backend(x, y).mul(y, x), 'rmul', '*')
-
-    def __truediv__(self, other):
-        return self._op2(other, lambda x, y: x / y, lambda x, y: choose_backend(x, y).div(x, y), 'truediv', '/')
-
-    def __rtruediv__(self, other):
-        return self._op2(other, lambda x, y: y / x, lambda x, y: choose_backend(x, y).div(y, x), 'rtruediv', '/')
-
-    def __divmod__(self, other):
-        return self._op2(other, lambda x, y: divmod(x, y), lambda x, y: divmod(x, y), 'divmod', 'divmod')
-
-    def __rdivmod__(self, other):
-        return self._op2(other, lambda x, y: divmod(y, x), lambda x, y: divmod(y, x), 'rdivmod', 'divmod')
-
-    def __floordiv__(self, other):
-        return self._op2(other, lambda x, y: x // y, lambda x, y: choose_backend(x, y).floordiv(x, y), 'floordiv', '//')
-
-    def __rfloordiv__(self, other):
-        return self._op2(other, lambda x, y: y // x, lambda x, y: choose_backend(x, y).floordiv(y, x), 'rfloordiv', '//')
-
-    def __pow__(self, power, modulo=None):
-        assert modulo is None
-        return self._op2(power, lambda x, y: x ** y, lambda x, y: choose_backend(x, y).pow(x, y), 'pow', '**')
-
-    def __rpow__(self, other):
-        return self._op2(other, lambda x, y: y ** x, lambda x, y: choose_backend(x, y).pow(y, x), 'rpow', '**')
-
-    def __mod__(self, other):
-        return self._op2(other, lambda x, y: x % y, lambda x, y: choose_backend(x, y).mod(x, y), 'mod', '%')
-
-    def __rmod__(self, other):
-        return self._op2(other, lambda x, y: y % x, lambda x, y: choose_backend(x, y).mod(y, x), 'rmod', '%')
-
-    def __eq__(self, other):
-        if _EQUALITY_BY_REF:
-            return wrap(self is other)
-        if other is None:
-            other = float('nan')
-        return self._op2(other, lambda x, y: x == y, lambda x, y: choose_backend(x, y).equal(x, y), 'eq', '==')
-
-    def __ne__(self, other):
-        if _EQUALITY_BY_REF:
-            return wrap(self is not other)
-        if other is None:
-            other = float('nan')
-        return self._op2(other, lambda x, y: x != y, lambda x, y: choose_backend(x, y).not_equal(x, y), 'ne', '!=')
-
-    def __lt__(self, other):
-        return self._op2(other, lambda x, y: x < y, lambda x, y: choose_backend(x, y).greater_than(y, x), 'lt', '<')
-
-    def __le__(self, other):
-        return self._op2(other, lambda x, y: x <= y, lambda x, y: choose_backend(x, y).greater_or_equal(y, x), 'le', '<=')
-
-    def __gt__(self, other):
-        return self._op2(other, lambda x, y: x > y, lambda x, y: choose_backend(x, y).greater_than(x, y), 'gt', '>')
-
-    def __ge__(self, other):
-        return self._op2(other, lambda x, y: x >= y, lambda x, y: choose_backend(x, y).greater_or_equal(x, y), 'ge', '>=')
-
-    def __lshift__(self, other):
-        return self._op2(other, lambda x, y: x << y, lambda x, y: choose_backend(x, y).shift_bits_left(x, y), 'lshift', '<<')
-
-    def __rlshift__(self, other):
-        return self._op2(other, lambda y, x: x << y, lambda y, x: choose_backend(x, y).shift_bits_left(x, y), 'lshift', '<<')
-
-    def __rshift__(self, other):
-        return self._op2(other, lambda x, y: x >> y, lambda x, y: choose_backend(x, y).shift_bits_right(x, y), 'rshift', '>>')
-
-    def __rrshift__(self, other):
-        return self._op2(other, lambda y, x: x >> y, lambda y, x: choose_backend(x, y).shift_bits_right(x, y), 'rshift', '>>')
-
-    def __abs__(self):
-        return self._op1(lambda t: choose_backend(t).abs(t))
-
-    def __round__(self, n=None):
-        return self._op1(lambda t: choose_backend(t).round(t))
-
-    def __copy__(self):
-        return self._op1(lambda t: choose_backend(t).copy(t, only_mutable=True))
-
-    def __deepcopy__(self, memodict={}):
-        return self._op1(lambda t: choose_backend(t).copy(t, only_mutable=False))
-
-    def __neg__(self):
-        return self._op1(lambda t: -t)
-
-    def __invert__(self):
-        return self._op1(lambda t: ~t)
-
-    def __reversed__(self):
-        assert self.shape.channel.rank == 1
-        return self[::-1]
-
-    def __iter__(self):
-        if self.rank == 1:
-            return iter(self.native())
-        elif self.rank == 0:
-            return iter([self.native()])
-        else:
-            from ._ops import reshaped_native
-            native = reshaped_native(self, [self.shape])
-            return iter(native)
-
-    def __matmul__(self, other):
-        assert isinstance(other, Tensor), f"Matmul '@' requires two Tensor arguments but got {type(other)}"
-        dims = batch(**self.shape.dual.untyped_dict).names
-        match = other.shape.only(dims, reorder=True)
-        if not match:
-            assert non_batch(other).non_dual.rank == 1, f"Cannot multiply {self.shape} @ {other.shape} because arg2 does not have appropriate non-dual dimensions"
-            match = non_batch(other).non_dual
-        assert len(dims) == match.rank, f"Dual dimensions {dual} do not match shape of second argument {other.shape}"
-        left_arg = pack_dims(self, dual, dual('_reduce')) if len(dims) > 1 else self
-        right_arg = pack_dims(other, match, channel('_reduce'))
-        from ._ops import dot
-        return dot(left_arg, dual, right_arg, '_reduce')
-
-    # def __rmatmul__(self, other):
-
-
-    def _tensor(self, other):
-        if isinstance(other, Tensor):
-            return other
-        elif isinstance(other, (tuple, list)) and any(isinstance(v, Tensor) for v in other):
-            if 'vector' in self.shape:
-                outer_dim = self.shape['vector']
-            elif self.shape.channel_rank == 1:
-                outer_dim = self.shape.channel
-            else:
-                raise ValueError(f"Cannot combine tensor of shape {self.shape} with tuple {tuple([type(v).__name__ for v in other])}")
-            remaining_shape = self.shape.without(outer_dim)
-            other_items = [v if isinstance(v, Tensor) else compatible_tensor(v, compat_shape=remaining_shape, compat_natives=self._natives(), convert=False) for v in other]
-            other_stacked = stack(other_items, outer_dim, expand_values=True)
-            return other_stacked
-        else:
-            return compatible_tensor(other, compat_shape=self.shape, compat_natives=self._natives(), convert=False)
-
-    def _op1(self, native_function):
-        """
-        Transform the values of this tensor given a function that can be applied to any native tensor.
-
-        Args:
-          native_function:
-
-        Returns:
-
-        """
-        raise NotImplementedError(self.__class__)
-
-    def _op2(self, other, operator: Callable, native_function: Callable, op_name: str = 'unknown', op_symbol: str = '?') -> 'Tensor':
-        """
-        Apply a broadcast operation on two tensors.
-
-        Args:
-            other: second argument
-            operator: function (Tensor, Tensor) -> Tensor, used to propagate the operation to children tensors to have Python choose the callee
-            native_function: function (native tensor, native tensor) -> native tensor
-            op_name: Name of the python function without leading and trailing `__`.
-                Examples: 'add', 'radd', 'sub', 'mul', 'and', 'eq', 'ge'.
-            op_symbol: Operation symbol, such as '+', '-', '&', '%', '>='
-
-        Returns:
-            `Tensor`
-        """
-        raise NotImplementedError()
-
-    def _natives(self) -> tuple:
-        raise NotImplementedError(self.__class__)
-
-    def _spec_dict(self) -> dict:
-        raise NotImplementedError(self.__class__)
-
-    @classmethod
-    def _from_spec_and_natives(cls, spec: dict, natives: list):
-        raise NotImplementedError(cls)
-
-    def _expand(self):
-        """ Expands all compressed tensors to their defined size as if they were being used in `Tensor.native()`. """
-        warnings.warn("Tensor._expand() is deprecated, use cached(Tensor) instead.", DeprecationWarning)
-        raise NotImplementedError(self.__class__)
-
-    def _simplify(self):
-        """ Does not cache this value but if it is already cached, returns the cached version. """
-        return self
-
-
-class TensorDim(BoundDim):
-    """
-    Reference to a specific dimension of a `Tensor`.
-
-    To obtain a `TensorDim`, use `Tensor.dimension()` or the syntax `tensor.<dim>`.
-
-    Indexing a `TensorDim` as `tdim[start:stop:step]` returns a sliced `Tensor`.
-
-    See the documentation at https://tum-pbs.github.io/PhiFlow/Math.html#indexing-slicing-unstacking .
-    """
-
-    def __init__(self, tensor: Tensor, name: str):
-        super().__init__(tensor, name)
-        self.tensor = tensor
-
-    def __len__(self):
-        warnings.warn("Use Tensor.dim.size instead of len(Tensor.dim). len() only supports with integer sizes.", DeprecationWarning)
-        return self.size
-
-    def as_batch(self, name: str = None):
-        """ Returns a shallow copy of the `Tensor` where the type of this dimension is *batch*. """
-        return self._as(BATCH_DIM, name)
-
-    def as_spatial(self, name: str = None):
-        """ Returns a shallow copy of the `Tensor` where the type of this dimension is *spatial*. """
-        return self._as(SPATIAL_DIM, name)
-
-    def as_channel(self, name: str = None):
-        """ Returns a shallow copy of the `Tensor` where the type of this dimension is *channel*. """
-        return self._as(CHANNEL_DIM, name)
-
-    def as_instance(self, name: str = None):
-        """ Returns a shallow copy of the `Tensor` where the type of this dimension is *instance*. """
-        return self._as(INSTANCE_DIM, name)
-
-    def as_type(self, dim_type: Union[Callable, str]):
-        return self._as(dim_type('d').type if callable(dim_type) else dim_type, None)
-
-    def _as(self, dim_type: str, name: Union[str, None]):
-        if not self.exists:
-            return self.tensor
-        shape = self.tensor.shape
-        new_types = list(shape.types)
-        new_types[shape.index(self.name)] = dim_type
-        new_names = shape.names
-        if name is not None:
-            new_names = list(new_names)
-            new_names[shape.index(self.name)] = name
-        new_shape = Shape(shape.sizes, tuple(new_names), tuple(new_types), shape.item_names)
-        return self.tensor._with_shape_replaced(new_shape)
-
-    @property
-    def index(self):
-        return self.tensor.shape.index(self.name)
-
-    def flip(self):
-        """ Flips the element order along this dimension and returns the result as a `Tensor`. """
-        warnings.warn("dim.flip() is deprecated. Use dim[::-1] instead", DeprecationWarning, stacklevel=2)
-        return self.tensor.flip(self.name)
-
-    def split(self, split_dimensions: Shape):
-        """ See `phi.math.unpack_dim()` """
-        warnings.warn("dim.split() is deprecated. Use math.split_dims() instead.", stacklevel=2)
-        from ._magic_ops import unpack_dim
-        return unpack_dim(self.tensor, self.name, split_dimensions)
-
-    def __mul__(self, other):
-        from ._ops import dot
-        if isinstance(other, TensorDim):
-            return dot(self.tensor, (self.name,), other.tensor, (other.name,))
-        if isinstance(other, (tuple, list)):
-            other = wrap(other, self.obj.shape[self.name])
-        if isinstance(other, Tensor):
-            assert self.name in other.shape, f"Canno reduce '{self.name}' of tensor with shape {self.obj.shape} against tensor with shape {other.shape}. Dimension must be present on both tensors."
-            return dot(self.tensor, (self.name,), other, (self.name,))
-        else:
-            return NotImplemented
-
-    def sum(self):
-        from ._ops import sum_
-        return sum_(self.tensor, self.name)
-
-    def prod(self):
-        from ._ops import prod
-        return prod(self.tensor, self.name)
-
-
-_EQUALITY_BY_REF = []
-
-
-@contextmanager
-def equality_by_ref():
-    """
-    Enables Tensor.__bool__
-    """
-    _EQUALITY_BY_REF.append(True)
-    try:
-        yield None
-    finally:
-        _EQUALITY_BY_REF.pop(-1)
-
-
-class Layout(Tensor):
-    """
-    Tensor representation of a PyTree consisting of only lists, tuples and leaves.
-    Leaves can be any Python object or primitive, including tuples and lists.
-    The PyTree may be deeper but only the outer `shape.rank` levels are represented as a tensor.
-    """
-
-    def __init__(self, obj, shape: Shape):
-        self._obj = obj
-        self._shape = shape
-
-    @property
-    def shape(self) -> Shape:
-        return self._shape
-
-    @property
-    def dtype(self) -> DType:
-        if isinstance(self._obj, bool):
-            return DType(bool)
-        if isinstance(self._obj, int):
-            return DType(int, 64)
-        elif isinstance(self._obj, (float, complex)):
-            return DType(type(self._obj), precision=64)
-        else:
-            return DType(object)
-
-    @property
-    def default_backend(self):
-        return None
-
-    def native(self, order: Union[str, tuple, list, Shape] = None):
-        order = parse_dim_order(order)
-        assert order is None or order == self._shape.names, "Layout.native() does not allow for changing the dimension order"
-        return self._obj
-
-    def numpy(self, order: Union[str, tuple, list, Shape] = None) -> np.ndarray:
-        native = self.native(order=order)
-        return numpy.asarray(native)
-
-    def _getitem(self, selection: dict) -> 'Tensor':
-        selection_list = [selection.get(dim, None) for dim in self._shape.names]
-        native = self._getitem_recursive(self._obj, tuple(selection_list))
-        new_shape = self._shape.after_gather(selection)
-        return Layout(native, new_shape)
-
-    def __repr__(self):
-        return repr(self._obj)
-
-    def __format__(self, format_spec):
-        return repr(self._obj)
-
-    def unstack(self, dimension: str):
-        if dimension == self._shape.names[0]:
-            native = tuple(self._obj.values()) if isinstance(self._obj, dict) else self._obj
-            inner_shape = self._shape[1:]
-            return tuple([Layout(n, inner_shape) for n in native])
-        else:
-            raise NotImplementedError()
-
-    @staticmethod
-    def _getitem_recursive(native, selection: tuple):
-        if not selection:
-            return native
-        native = tuple(native.values()) if isinstance(native, dict) else native
-        if len(selection) == 1:
-            return native if selection[0] is None else native[selection[0]]
-        else:
-            if selection[0] is None:
-                return type(native)([Layout._getitem_recursive(n, selection[1:]) for n in native])
-            if isinstance(selection[0], int):
-                return Layout._getitem_recursive(native[selection[0]], selection[1:])
-            elif isinstance(selection[0], slice):
-                subset = native[selection[0]]
-                return type(subset)([Layout._getitem_recursive(n, selection[1:]) for n in subset])
-            else:
-                raise ValueError(f"Illegal selection: {selection}")
-
-    def _as_list(self):
-        return self._as_list_recursive(self._obj, self._shape.rank, [])
-
-    @staticmethod
-    def _as_list_recursive(native, dims: int, result: list):
-        if dims == 0:
-            result.append(native)
-        else:
-            native = tuple(native.values()) if isinstance(native, dict) else native
-            for n in native:
-                Layout._as_list_recursive(n, dims - 1, result)
-        return result
-
-    @property
-    def _is_tracer(self) -> bool:
-        return False
-
-    def __bool__(self):
-        assert self.rank == 0, f"Cannot convert tensor with non-empty shape {self.shape} to bool. Use tensor.any or tensor.all instead."
-        return bool(self._obj)
-
-    def __stack__(self, values: tuple, dim: Shape, **kwargs) -> 'Layout':
-        obj = [v.native(self._shape) for v in values]
-        new_shape = concat_shapes(dim, self._shape)
-        return Layout(obj, new_shape)
-
-    @staticmethod
-    def __concat__(values: tuple, dim: str, **kwargs) -> 'Shapable':
-        return NotImplemented
-
-    def __flatten__(self, flat_dim: Shape, flatten_batch: bool):
-        if not flatten_batch and self._shape.batch:
-            raise NotImplementedError
-        return layout(self._as_list(), flat_dim)
-
-    def __expand__(self, dims: Shape, **kwargs) -> 'Tensor':
-        new_dims = dims.without(self._shape)
-        if not new_dims:
-            return self
-        obj = self._obj
-        for dim in reversed(new_dims):
-            assert isinstance(dim.size, int), "Can only expand layouts by integer-sized dimensions"
-            obj = [obj] * dim.size
-        return Layout(obj, concat_shapes(new_dims, self._shape))
-
-    def __replace_dims__(self, dims: Tuple[str, ...], new_dims: Shape, **kwargs) -> 'Tensor':
-        new_shape = self._shape.replace(dims, new_dims)
-        return Layout(self._obj, new_shape)
-
-    def __pack_dims__(self, dims: Tuple[str, ...], packed_dim: Shape, pos: Union[int, None], **kwargs) -> 'Layout':
-        if dims == self.shape.names:
-            native = self._as_list()
-            return Layout(native, packed_dim.with_size(len(native)))
-        else:
-            obj = []
-            for i in self._shape.only(dims, reorder=True).meshgrid():
-                obj.append(self[i].native())
-            return Layout(obj, concat_shapes(packed_dim, self._shape.without(dims)))
-
-    def __unpack_dim__(self, dim: str, unpacked_dims: Shape, **kwargs) -> 'Layout':
-        return NotImplemented
-
-    def __cast__(self, dtype: DType):
-        obj = self._recursive_cast(self._obj, self._shape, dtype)
-        return Layout(obj, self._shape)
-
-    def __copy__(self):
-        return Layout(self._obj, self._shape)
-
-    def __iter__(self):
-        if self.rank == 1:
-            return iter(self._obj)
-        elif self.rank == 0:
-            return iter([self._obj])
-        else:
-            return iter(self._as_list())
-
-    def __eq__(self, other):
-        if _EQUALITY_BY_REF:
-            return wrap(self is other)
-        return self._op2(other, lambda x, y: x == y, lambda x, y: x == y, 'eq', '==')
-
-    def __ne__(self, other):
-        if _EQUALITY_BY_REF:
-            return wrap(self is not other)
-        return self._op2(other, lambda x, y: x != y, lambda x, y: x != y, 'ne', '!=')
-    
-    def _assert_close(self, other: Tensor, rel_tolerance: float, abs_tolerance: float, msg: str, verbose: bool):
-        from ._ops import assert_close
-        inner_test = lambda x, y: assert_close(x, y, rel_tolerance=rel_tolerance, abs_tolerance=abs_tolerance, msg=msg, verbose=verbose)
-        return self._op2(other, inner_test, inner_test, 'assert_close', '≈')
-
-    def _op2(self, other, operator: Callable, native_function: Callable, op_name: str = 'unknown', op_symbol: str = '?') -> Tensor:
-        obj = self._recursive_op2(self._obj, self._shape, other, operator, native_function, op_name)
-        new_shape = concat_shapes(self._shape, other.shape.without(self._shape)) if isinstance(other, Tensor) else self._shape
-        return Layout(obj, new_shape)
-
-    @staticmethod
-    def _recursive_op2(obj, shape: Shape, other, operator, native_function, op_name):
-        if shape:
-            dim = shape.names[0]
-            if isinstance(other, Tensor) and dim in other.shape:
-                assert other.shape.get_size(dim) == len(obj), f"Shape mismatch during {op_name}: '{dim}' has size {len(obj)} on layout but {other.shape.get_size(dim)} on other tensor."
-                others = [other[{dim: i}] for i in range(len(obj))]
-            else:
-                others = [other] * len(obj)
-            if isinstance(obj, (tuple, list)):
-                return type(obj)([Layout._recursive_op2(i, shape[1:], o, operator, native_function, op_name) for i, o in zip(obj, others)])
-            elif isinstance(obj, dict):
-                return {k: Layout._recursive_op2(v, shape[1:], o, operator, native_function, op_name) for (k, v), o in zip(obj.items(), others)}
-        else:  # leaf
-            if isinstance(other, Layout) and not other.shape:
-                return native_function(obj, other.native())
-            if isinstance(other, Tensor):
-                return operator(obj, other)
-            else:
-                return native_function(obj, other)
-
-    def _op1(self, native_function):
-        return Layout(self._recursive_op1(self._obj, self._shape, native_function), self._shape)
-
-    @staticmethod
-    def _recursive_op1(obj, shape: Shape, native_function):
-        raise NotImplementedError
-        # if shape:
-        #     if isinstance(obj, (tuple, list)):
-        #         return type(obj)([Layout._recursive_op1(i, shape[1:], native_function) for i in obj])
-        #     else:
-        # else:
-        #     return native_function(obj)
-
-    @staticmethod
-    def _recursive_cast(obj, shape: Shape, dtype: DType):
-        if shape:
-            if isinstance(obj, (tuple, list)):
-                return type(obj)([Layout._recursive_cast(i, shape[1:], dtype) for i in obj])
-            elif isinstance(obj, dict):
-                return {k: Layout._recursive_cast(v, shape[1:], dtype) for k, v in obj.items()}
-            elif isinstance(obj, Tensor):
-                assert obj.shape == shape
-                from ._ops import cast
-                return cast(obj, dtype)
-            else:
-                raise ValueError(obj)
-        else:
-            return dtype.kind(obj)
-
-
-class NativeTensor(Tensor):
-
-    def __init__(self, native_tensor, shape: Shape):
-        shape._check_is_valid_tensor_shape()
-        assert isinstance(shape, Shape), f"Expected Shape but got '{type(shape)}'"
-        backend = choose_backend(native_tensor)
-        # if backend.is_available(native_tensor):
-        assert backend.staticshape(native_tensor) == shape.sizes, f"Shape {shape} does not match native tensor with shape {backend.staticshape(native_tensor)}"
-        self._native = native_tensor
-        self._shape = shape
-
-    def native(self, order: Union[str, tuple, list, Shape] = None):
-        order = parse_dim_order(order, check_rank=self.rank)
-        if order is None or tuple(order) == self.shape.names:
-            if self.dtype.precision in [None, get_precision()]:
-                return self._native
-            else:
-                return self.default_backend.cast(self._native, DType(self.dtype.kind, precision=get_precision()))
-        # --- Insert missing dims ---
-        native = self._native
-        shape = self.shape
-        for name in order:
-            if name not in self.shape:
-                native = self.default_backend.expand_dims(native, axis=-1)
-                shape = concat_shapes(shape, batch(**{name: 1}))
-        # --- Transpose ---
-        perm = shape._perm(order)
-        native = self.default_backend.transpose(native, perm)  # this will cast automatically
-        return native
-
-    @property
-    def dtype(self):
-        return choose_backend(self._native).dtype(self._native)
-
-    @property
-    def shape(self):
-        return self._shape
-
-    @property
-    def default_backend(self) -> Backend:
-        return choose_backend(self._native)
-
-    def _with_shape_replaced(self, new_shape):
-        if new_shape.rank != self._shape.rank:
-            raise IncompatibleShapes(f"Tensor {self} is not compatible with shape {new_shape}", self._shape, new_shape)
-        new_shape = Shape(self._shape.sizes, new_shape.names, new_shape.types, new_shape.item_names)
-        return NativeTensor(self._native, new_shape)
-
-    def _with_natives_replaced(self, natives: list):
-        native = natives.pop(0)
-        new_shape = self._shape.with_sizes(choose_backend(native).shape(native))
-        return NativeTensor(native, new_shape)
-
-    @property
-    def _is_tracer(self) -> bool:
-        return False
-
-    def _to_dict(self):
-        result = self.shape._to_dict(include_sizes=False)
-        if self.rank == 0:
-            result['data'] = self.numpy().item()
-        else:
-            result['data'] = self.numpy(self._shape).tolist()  # works for all 1+ dimensional arrays
-        return result
-
-    def _getitem(self, selection: dict):
-        if len(selection) == 0:
-            return self
-        selections = [slice(None)] * self.rank
-        for name, sel in selection.items():
-            if name in self.shape:
-                selections[self.shape.index(name)] = sel
-            else:
-                assert isinstance(sel, int), f"Attempting slice missing dimension {name} with {selection}"
-        if len(selections) == 0:
-            return self
-        gathered = self.default_backend.multi_slice(self._native, tuple(selections))
-        new_shape = self._shape.after_gather(selection)
-        return NativeTensor(gathered, new_shape)
-
-    def flip(self, *dims: str) -> 'Tensor':
-        dims = [dim for dim in dims if dim in self._shape]
-        native = choose_backend(self._native).flip(self._native, self._shape.indices(dims))
-        return NativeTensor(native, self._shape.flipped(dims))
-
-    def unstack(self, dimension):
-        dim_index = self.shape.index(dimension)
-        new_shape = self.shape.without(dimension)
-        tensors = choose_backend(self._native).unstack(self._native, axis=dim_index)
-        return tuple([NativeTensor(t, new_shape) for t in tensors])
-
-    def _op1(self, native_function):
-        native = native_function(self._native)
-        return NativeTensor(native, self.shape) if native is not None else self
-
-    def _op2(self, other, operator, native_function, op_name: str = 'unknown', op_symbol: str = '?'):
-        try:
-            other_tensor = self._tensor(other)
-        except NoBackendFound:
-            return NotImplemented
-        if isinstance(other_tensor, NativeTensor) or (isinstance(other_tensor, Tensor) and not isinstance(other, Tensor)):
-            return op2_native(self, other_tensor, native_function)
-        else:
-            return NotImplemented
-
-    def _natives(self) -> tuple:
-        return self._native,
-
-    def _spec_dict(self) -> dict:
-        return {'type': NativeTensor, 'shape': self._shape}
-
-    @classmethod
-    def _from_spec_and_natives(cls, spec: dict, natives: list):
-        return NativeTensor(natives.pop(0), spec['shape'])
-
-    def _expand(self):
-        pass
-
-
-class CollapsedTensor(Tensor):  # package-private
-    """
-    Tensor that is constant along some dimensions.
-    Non-constant dimensions are represented by `_inner` while `_shape` lists all dimensions.
-
-    When cached via `_cache()`, `_inner` is replaced by `_cached` which is a NativeTensor.
-    From this point on, all operations must use `_cached`, otherwise gradients will be incorrect.
-    The method `Tensor._expand()` causes a full Tensor structure to cache collapsed dimensions and must be called before gradients are recorded.
-    """
-
-    def __init__(self, inner: Tensor, shape: Shape):
-        shape._check_is_valid_tensor_shape()
-        assert inner.shape != shape
-        for name in inner.shape.names:
-            assert name in shape
-        for size, name, dim_type, *_ in inner.shape._dimensions:
-            assert wrap(shape.get_size(name) == size).all, f"Shape mismatch while trying to set {name}={shape.get_size(name)} but has size {size}"
-            assert shape.get_type(name) == dim_type, f"Dimension type mismatch for dimension '{name}': {shape.get_type(name)}, {dim_type}"
-        if isinstance(inner, CollapsedTensor):
-            if inner.is_cached:
-                self._inner = inner._cached
-            else:
-                self._inner = inner._inner
-            assert self._inner is not None
-        else:
-            self._inner = inner  # this will be set to None once cached. Otherwise gradients will be incorrect.
-        self._shape = shape
-        self._cached = None  # NativeTensor. Once cached, use only _cached
-
-    @property
-    def collapsed_dims(self):
-        return self._shape.without(self._inner.shape)
-
-    def _cache(self):
-        if self._cached is None:
-            if self._inner._is_tracer:
-                return None
-            if self.shape.is_uniform:
-                native = self._inner.native(order=self.shape.names)
-                multiples = [1 if name in self._inner.shape else size for size, name, *_ in self.shape._dimensions]
-                tiled = choose_backend(native).tile(native, multiples)
-                self._cached = NativeTensor(tiled, self.shape)
-                self._inner = None
-            else:
-                raise NotImplementedError()
-        return self._cached
-
-    @property
-    def is_cached(self):
-        return self._cached is not None
-
-    def _simplify(self):
-        if self.is_cached:
-            return self._cached
-        else:
-            return self
-
-    def native(self, order: Union[str, tuple, list, Shape] = None):
-        if self.is_cached:
-            return self._cached.native(order)
-        order = parse_dim_order(order, check_rank=self.rank)
-        if order is None or tuple(order) == self.shape.names:
-            return self._cache().native(order)
-        else:
-            native = self._inner.native(order=order)
-            multiples = [1 if name in self._inner.shape else (self.shape.get_size(name) if name in self.shape else 1) for name in order]
-            assert all(isinstance(m, int) for m in multiples), f"Cannot get native representation of Tensor {self.shape} because Shape is non-uniform"
-            tiled = choose_backend(native).tile(native, multiples)
-            return tiled
-
-    @property
-    def dtype(self):
-        if self.is_cached:
-            return self._cached.dtype
-        else:
-            return self._inner.dtype
-
-    @property
-    def shape(self):
-        return self._shape
-
-    def unstack(self, dimension):
-        if self.is_cached:
-            return self._cached.unstack(dimension)
-        if dimension in self._inner.shape:
-            unstacked = self._inner.unstack(dimension)
-            unstacked_shapes = [self.shape.after_gather({dimension: i}).without(dimension) for i in range(len(unstacked))]
-            return tuple([CollapsedTensor(t, s) for t, s in zip(unstacked, unstacked_shapes)])
-        else:
-            unstacked_shapes = [self.shape.after_gather({dimension: i}) for i in range(self.shape.get_size(dimension))]  # can vary if non-uniform
-            return tuple([expand(self._inner, s) for s in unstacked_shapes])
-
-    def _with_shape_replaced(self, new_shape: Shape):
-        if self.is_cached:
-            return self._cached._with_shape_replaced(new_shape)
-        else:
-            inner_indices = [self.shape.index(d) for d in self._inner.shape.names]
-            new_inner_shape = new_shape[inner_indices]
-            result = CollapsedTensor(self._inner._with_shape_replaced(new_inner_shape), new_shape)
-            return result
-
-    @property
-    def _is_tracer(self) -> bool:
-        if self.is_cached:
-            return self._cached._is_tracer
-        else:
-            return self._inner._is_tracer
-
-    def _getitem(self, selection: dict):
-        if self.is_cached:
-            return self._cached._getitem(selection)
-        else:
-            inner_dict = {name: selection for name, selection in selection.items() if name in self._inner.shape}
-            inner = self._inner._getitem(inner_dict)
-            new_shape = self.shape.after_gather(selection)
-            merge_shapes(inner.shape, new_shape)  # check that sizes match
-            return expand(inner, new_shape)
-
-    def flip(self, *dims: str) -> 'Tensor':
-        if self.is_cached:
-            return self._cached.flip(*dims)
-        else:
-            return CollapsedTensor(self._inner.flip(*dims), self._shape.flipped(dims))
-
-    def _op1(self, native_function):
-        if self.is_cached:
-            return self._cached._op1(native_function)
-        else:
-            return CollapsedTensor(self._inner._op1(native_function), self._shape)
-
-    def _op2(self, other, operator, native_function, op_name: str = 'unknown', op_symbol: str = '?'):
-        try:
-            other_t = self._tensor(other)
-        except NoBackendFound:
-            return NotImplemented
-        if isinstance(other_t, CollapsedTensor) and other_t.is_cached:
-            other_t = other_t._cached
-        if isinstance(other_t, NativeTensor):
-            if self._shape in other_t.shape:
-                return op2_native(self, other_t, native_function)
-        if isinstance(other_t, (NativeTensor, CollapsedTensor)):
-            if isinstance(other_t, CollapsedTensor):
-                other_inner = other_t._inner  # case that other is cached handled above
-            else:
-                other_inner = other_t
-            self_inner = self._cached if self.is_cached else self._inner
-            inner = operator(self_inner, other_inner)
-            if all(dim in inner.shape for dim in self.shape.names + other_t.shape.names):  # shape already complete
-                result = inner._with_shape_replaced(inner.shape._with_types(self._shape & other_t._shape))
-                return result
-            else:
-                combined_shape = (self._shape & other_t._shape).with_sizes(inner.shape)
-                return CollapsedTensor(inner, combined_shape)
-        elif not isinstance(other, Tensor):  # was converted to Tensor, probably TensorStack
-            return operator(self, other_t)
-        else:
-            return NotImplemented
-
-    def _natives(self) -> tuple:
-        if self.is_cached:
-            return self._cached._natives()
-        else:
-            return self._inner._natives()
-
-    def _spec_dict(self) -> dict:
-        if self.is_cached:
-            return self._cached._spec_dict()
-        else:
-            return {'type': CollapsedTensor, 'shape': self._shape, 'inner': self._inner._spec_dict()}
-
-    @classmethod
-    def _from_spec_and_natives(cls, spec: dict, natives: list):
-        shape0 = choose_backend(natives[0]).staticshape(natives[0])
-        if len(shape0) > spec['inner']['shape'].rank:  # new native is expanded
-            assert len(shape0) == spec['shape'].rank
-            return NativeTensor(natives[0], spec['shape'])
-        inner = spec['inner']['type']._from_spec_and_natives(spec['inner'], natives)
-        return CollapsedTensor(inner, spec['shape'])
-
-    def _with_natives_replaced(self, natives: list):
-        assert self.is_cached, "Cannot replace natives in uncached state. Expand tensor beforehand."
-        return self._cached._with_natives_replaced(natives)
-
-    def _expand(self):
-        self._cache()
-
-
-class TensorStack(Tensor):
-    """
-    Implicit stack of multiple tensors.
-    List of tensors, does not store stacked tensor in memory.
-
-    Args:
-
-    Returns:
-
-    """
-
-    def __init__(self, components: Union[tuple, list], stack_dim: Shape):
-        assert isinstance(stack_dim, Shape) and stack_dim.rank == 1, f"stack_dim must be a single-dimension Shape object but got {type(stack_dim)}"
-        # assert len(components) > 1, "Use a CollapsedTensor instead"
-        for t in components:
-            assert isinstance(t, Tensor)
-            assert stack_dim.name not in t.shape, f"Cannot stack along '{stack_dim.name}' because the dimension already exists."
-        self._tensors = tuple(components)
-        self._stack_dim = stack_dim.with_sizes([len(components)], keep_item_names=True)
-        try:
-            merge_shapes(*self._tensors)
-            self._varying_shapes = False
-        except IncompatibleShapes:
-            self._varying_shapes = True
-        self._shape = shape_stack(self._stack_dim, *[t.shape for t in self._tensors])
-        self._cached = None
-
-    @property
-    def _is_tracer(self) -> bool:
-        return any([t._is_tracer for t in self._tensors])
-
-    @property
-    def requires_broadcast(self):
-        return self._varying_shapes or not self._shape.well_defined or self._is_tracer
-    
-    @property
-    def stack_dim(self):
-        warnings.warn("TensorStack.stack_dim is deprecated", DeprecationWarning, stacklevel=2)
-        return self._stack_dim
-
-    def _cache(self):
-        if self._cached is None:
-            if self.requires_broadcast:
-                return None
-            elif all([t.shape.is_uniform for t in self._tensors]):
-                natives = [t.native(order=self._shape.names) for t in self._tensors]
-                native = choose_backend(*natives).concat(natives, axis=self.shape.index(self._stack_dim.name))
-                self._cached = NativeTensor(native, self._shape)
-            else:  # cache stack_dim on inner tensors
-                non_uniform_dim = self._tensors[0].shape.shape.without('dims')
-                unstacked = [t.unstack(non_uniform_dim.name) for t in self._tensors]
-                stacked = []
-                for to_stack in zip(*unstacked):
-                    tensor = TensorStack(to_stack, self._stack_dim)._cache()
-                    stacked.append(tensor)
-                self._cached = TensorStack(stacked, non_uniform_dim)
-        return self._cached
-
-    @property
-    def dtype(self):
-        return combine_types(*[t.dtype for t in self._tensors])
-
-    @property
-    def shape(self):
-        return self._shape
-
-    def native(self, order: Union[str, tuple, list, Shape] = None):
-        if self._cached is not None:
-            return self._cached.native(order=order)
-        else:
-            order = parse_dim_order(order, check_rank=self.rank)
-            # Is only the stack dimension shifted?
-            if order is not None and self._shape.without(self._stack_dim).names == tuple(filter(lambda name: name != self._stack_dim.name, order)):
-                inner_order = [dim for dim in order if dim != self._stack_dim.name]
-                natives = [t.native(inner_order) for t in self._tensors]
-                assert self._stack_dim.name in order, f"Dimension {self._stack_dim} missing from 'order'. Got {order} but tensor has shape {self.shape}."
-                native = choose_backend(*natives).stack(natives, axis=order.index(self._stack_dim.name))
-                return native
-            assert not self.shape.is_non_uniform, f"Cannot convert non-uniform tensor with shape {self.shape} to native tensor."
-            return self._cache().native(order=order)
-
-    def _with_shape_replaced(self, new_shape: Shape):
-        if self._cached is not None:
-            return self._cached._with_shape_replaced(new_shape)
-        else:
-            new_stack_dim = new_shape[self._shape.index(self._stack_dim.name)]
-            new_tensors = []
-            for t in self._tensors:
-                inner_indices = [self.shape.index(d) for d in t.shape.names]
-                new_inner_shape = new_shape[inner_indices]
-                new_tensors.append(t._with_shape_replaced(new_inner_shape))
-            return TensorStack(new_tensors, new_stack_dim)
-
-    def _getitem(self, selection: dict):
-        if self._cached is not None:
-            return self._cached._getitem(selection)
-        if (self._stack_dim.name not in selection or len(selection) != 1) and not self.requires_broadcast:
-            return self._cache()._getitem(selection)
-        # --- Inner dims ---
-        inner_dict = {dim: sel for dim, sel in selection.items() if dim != self._stack_dim.name}
-        tensors = self._tensors
-        if len(inner_dict) > 0:
-            tensors = [t[inner_dict] for t in tensors]
-        # --- stack dimension ---
-        if self._stack_dim.name in selection:
-            selection = selection[self._stack_dim.name]
-            if isinstance(selection, int):
-                return self._tensors[selection]
-            elif isinstance(selection, slice):
-                return TensorStack(tensors[selection], self._stack_dim)
-            else:
-                raise NotImplementedError(f"{type(selection)} not supported. Only (int, slice) allwoed")
-        else:
-            return TensorStack(tensors, self._stack_dim)
-
-    def flip(self, *dims: str) -> 'Tensor':
-        if self._cached is not None:
-            return self._cached.flip(*dims)
-        else:
-            tensors = [t.flip(*dims) for t in self._tensors]
-            if self._stack_dim.name in dims:
-                tensors = tensors[::-1]
-            return TensorStack(tensors, self._stack_dim)
-
-    def unstack(self, dimension):
-        if self._cached is not None:
-            return self._cached.unstack(dimension)
-        if dimension == self._stack_dim.name:
-            return self._tensors
-        else:
-            if self.requires_broadcast:
-                unstacked = [t.unstack(dimension) for t in self._tensors]
-                return tuple([TensorStack(items, self._stack_dim) for items in zip(*unstacked)])
-            else:
-                return self._cache().unstack(dimension=dimension)
-
-    def _op1(self, native_function):
-        if self.requires_broadcast:
-            tensors = [t._op1(native_function) for t in self._tensors]
-            return TensorStack(tensors, self._stack_dim)
-        else:
-            return self._cache()._op1(native_function)
-
-    def _op2(self, other, operator, native_function, op_name: str = 'unknown', op_symbol: str = '?'):
-        other = self._tensor(other)
-        if self.requires_broadcast:
-            if self._stack_dim.name in other.shape:
-                other = other.unstack(self._stack_dim.name)
-                tensors = [operator(t1, t2) for t1, t2 in zip(self._tensors, other)]
-            else:
-                tensors = [operator(t, other) for t in self._tensors]
-            return TensorStack(tensors, self._stack_dim)
-        elif isinstance(other, (CollapsedTensor, NativeTensor)):
-            return op2_native(self, other, native_function)
-        elif isinstance(other, TensorStack) and not other.requires_broadcast:
-            return op2_native(self, other, native_function)
-        else:
-            return NotImplemented
-
-    def _natives(self) -> tuple:
-        if self._cached is not None:
-            return self._cached._natives()
-        else:
-            return sum([t._natives() for t in self._tensors], ())
-
-    def _spec_dict(self) -> dict:
-        if self._cached is not None:
-            return self._cached._spec_dict()
-        else:
-            return {'type': TensorStack, 'stack_dim': self._stack_dim, 'tensors': [t._spec_dict() for t in self._tensors]}
-
-    @classmethod
-    def _from_spec_and_natives(cls, spec: dict, natives: list):
-        tensors = [t['type']._from_spec_and_natives(t, natives) for t in spec['tensors']]
-        return TensorStack(tensors, spec['stack_dim'])
-
-    def _with_natives_replaced(self, natives: list):
-        if self._cached is not None:
-            return self._cached._with_natives_replaced(natives)
-        else:
-            tensors = [t._with_natives_replaced(natives) for t in self._tensors]
-            return TensorStack(tensors, self._stack_dim)
-
-    def _expand(self):
-        if self.requires_broadcast:
-            for t in self._tensors:
-                t._expand()
-        self._cache()
-
-    @property
-    def is_cached(self):
-        return self._cached is not None
-
-    def _simplify(self):
-        if self.is_cached:
-            return self._cached
-        else:
-            return self
-
-
-def tensor(data: Union[Tensor, Shape, tuple, list, numbers.Number],
-           *shape: Shape,
-           convert: bool = True,
-           default_list_dim=channel('vector')) -> Tensor:  # TODO assume convert_unsupported, add convert_external=False for constants
-    """
-    Create a Tensor from the specified `data`.
-    If `convert=True`, converts `data` to the preferred format of the default backend.
-
-    `data` must be one of the following:
-    
-    * Number: returns a dimensionless Tensor.
-    * Native tensor such as NumPy array, TensorFlow tensor or PyTorch tensor.
-    * `tuple` or `list` of numbers: backs the Tensor with native tensor.
-    * `tuple` or `list` of non-numbers: creates tensors for the items and stacks them.
-    * Tensor: renames dimensions and dimension types if `names` is specified. Converts all internal native values of the tensor if `convert=True`.
-    * Shape: creates a 1D tensor listing the dimension sizes.
-    
-    While specifying `names` is optional in some cases, it is recommended to always specify them.
-    
-    Dimension types are always inferred from the dimension names if specified.
-
-    Implementations:
-
-    * NumPy: [`numpy.array`](https://numpy.org/doc/stable/reference/generated/numpy.array.html)
-    * PyTorch: [`torch.tensor`](https://pytorch.org/docs/stable/generated/torch.tensor.html), [`torch.from_numpy`](https://pytorch.org/docs/stable/generated/torch.from_numpy.html)
-    * TensorFlow: [`tf.convert_to_tensor`](https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor)
-    * Jax: [`jax.numpy.array`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.array.html)
-
-    See Also:
-        `phi.math.wrap()` which uses `convert=False`, `layout()`.
-
-    Args:
-        data: native tensor, scalar, sequence, Shape or Tensor
-        shape: Ordered dimensions and types. If sizes are defined, they will be checked against `data`.`
-        convert: If True, converts the data to the native format of the current default backend.
-            If False, wraps the data in a `Tensor` but keeps the given data reference if possible.
-
-    Raises:
-        AssertionError: if dimension names are not provided and cannot automatically be inferred
-        ValueError: if `data` is not tensor-like
-
-    Returns:
-        Tensor containing same values as data
-
-    Examples:
-        >>> tensor([1, 2, 3], channel(vector='x,y,z'))
-        (x=1, y=2, z=3)
-
-        >>> tensor([1., 2, 3], channel(vector='x,y,z'))
-        (x=1.000, y=2.000, z=3.000) float64
-
-        >>> tensor(numpy.zeros([10, 8, 6, 2]), batch('batch'), spatial('x,y'), channel(vector='x,y'))
-        (batchᵇ=10, xˢ=8, yˢ=6, vectorᶜ=x,y) float64 const 0.0
-
-        >>> tensor([(0, 1), (0, 2), (1, 3)], instance('particles'), channel(vector='x,y'))
-        (x=0, y=1); (x=0, y=2); (x=1, y=3) (particlesⁱ=3, vectorᶜ=x,y)
-
-        >>> tensor(numpy.random.randn(10))
-        (vectorᶜ=10) float64 -0.128 ± 1.197 (-2e+00...2e+00)
-    """
-    assert all(isinstance(s, Shape) for s in shape), f"Cannot create tensor because shape needs to be one or multiple Shape instances but got {shape}"
-    shape = None if len(shape) == 0 else concat_shapes(*shape)
-    if isinstance(data, Tensor):
-        if convert:
-            backend = data.default_backend
-            if backend != default_backend():
-                data = data._op1(lambda n: convert_(n, use_dlpack=False))
-        if shape is None:
-            return data
-        else:
-            if None in shape.sizes:
-                shape = shape.with_sizes(data.shape.sizes)
-            return data._with_shape_replaced(shape)
-    elif isinstance(data, Shape):
-        if shape is None:
-            shape = channel('dims')
-        else:
-            assert shape.rank == 1, "Can only convert 1D shapes to Tensors"
-        shape = shape.with_size(data.names)
-        data = data.sizes
-    elif isinstance(data, str) or data is None:
-        return layout(data)
-    elif isinstance(data, (numbers.Number, bool)):
-        assert not shape, f"Trying to create a zero-dimensional Tensor from value '{data}' but shape={shape}"
-        if convert:
-            data = default_backend().as_tensor(data, convert_external=True)
-        return NativeTensor(data, EMPTY_SHAPE)
-    if isinstance(data, (tuple, list)):
-        if all(isinstance(d, (bool, int, float, complex)) for d in data):
-            array = np.array(data)
-            assert array.dtype != object
-            data = array
-        elif all(isinstance(d, str) for d in data):
-            return layout(data, shape or default_list_dim)
-        else:
-            try:
-                inner_shape = [] if shape is None else [shape[1:]]
-                tensors = [d if isinstance(d, Tensor) else tensor(d, *inner_shape, convert=convert) for d in data]
-                return stack(tensors, default_list_dim if shape is None else shape[0].with_sizes([len(tensors)]), expand_values=True)
-            except IncompatibleShapes:
-                assert not convert, f"Cannot convert {data} to tensor given shape {shape}"
-                return layout(data, shape or default_list_dim)
-            except ValueError:
-                assert not convert, f"Cannot convert {data} to tensor"
-                return layout(data, shape or default_list_dim)
-    try:
-        backend = choose_backend(data)
-        if shape is None:
-            assert backend.ndims(data) <= 1, "Specify dimension names for tensors with more than 1 dimension"
-            shape = default_list_dim if backend.ndims(data) == 1 else EMPTY_SHAPE
-            shape = shape.with_sizes(backend.staticshape(data))
-        else:
-            # fill in sizes or check them
-            sizes = backend.staticshape(data)
-            if len(sizes) != len(shape):
-                raise IncompatibleShapes(f"Rank of given shape {shape} does not match data with sizes {sizes}")
-            for size, s in zip(sizes, shape.sizes):
-                if s is not None:
-                    assert s == size, f"Given shape {shape} does not match data with sizes {sizes}. Consider leaving the sizes undefined."
-            shape = shape.with_sizes(sizes, keep_item_names=True)
-        if convert:
-            data = convert_(data, use_dlpack=False)
-        return NativeTensor(data, shape)
-    except NoBackendFound:
-        raise ValueError(f"{type(data)} is not supported. Only (Tensor, tuple, list, np.ndarray, native tensors) are allowed.\nCurrent backends: {BACKENDS}")
-
-
-def wrap(data: Union[Tensor, Shape, tuple, list, numbers.Number],
-         *shape: Shape) -> Tensor:
-    """ Short for `phi.math.tensor()` with `convert=False`. """
-    return tensor(data, *shape, convert=False)  # TODO inline, simplify
-
-
-def layout(objects, *shape: Shape) -> Tensor:
-    """
-    Wraps a Python tree in a `Tensor`, allowing elements to be accessed via dimensions.
-    A python tree is a structure of nested `tuple`, `list`, `dict` and *leaf* objects where leaves can be any Python object.
-
-    All keys of `dict` containers must be of type `str`.
-    The keys are automatically assigned as item names along that dimension unless conflicting with other elements.
-
-    Strings may also be used as containers.
-
-    Example:
-    >>> t = layout({'a': 'text', 'b': [0, 1]}, channel('dict,inner'))
-    >>> t.inner[1].dict['a'].native()
-    'e'
-
-    See Also:
-        `tensor()`, `wrap()`.
-
-    Args:
-        objects: PyTree of `list` or `tuple`.
-        *shape: Tensor dimensions
-
-    Returns:
-        `Tensor`.
-        Calling `Tensor.native()` on the returned tensor will return `objects`.
-    """
-    assert all(isinstance(s, Shape) for s in shape), f"shape needs to be one or multiple Shape instances but got {shape}"
-    shape = EMPTY_SHAPE if len(shape) == 0 else concat_shapes(*shape)
-    if isinstance(objects, Layout):
-        assert objects.shape == shape
-        return objects
-
-    if not shape.well_defined:
-
-        def recursive_determine_shape(native, shape: Shape):
-            if not shape:
-                return shape
-            if isinstance(native, dict):
-                assert all([isinstance(k, str) for k in native.keys()]), f"All dict keys in PyTrees must be str but got {tuple(native.keys())}"
-                shape = shape.replace(shape[0], shape[0].with_size(tuple(native.keys())))
-            if shape.rank == 1:
-                return shape.with_sizes((len(native),))
-            inner_shape = shape[1:]
-            if isinstance(native, (tuple, list)):
-                inner_shapes = [recursive_determine_shape(n, inner_shape) for n in native]
-            elif isinstance(native, dict):
-                inner_shapes = [recursive_determine_shape(n, inner_shape) for n in native.values()]
-            else:
-                raise ValueError(native)
-            return shape_stack(shape[0], *inner_shapes)
-
-        shape = recursive_determine_shape(objects, shape)
-
-    return Layout(objects, shape)
-    # if shape.volume == 1:
-    #     objects = np.asarray(objects, dtype=object)
-    #
-    # if isinstance(objects, (tuple, list)):
-    #     objects = np.asarray(objects, dtype=object)
-    # if isinstance(objects, np.ndarray) and objects.dtype == object:
-    #     return Layout(objects, shape)
-    # else:
-    #     assert shape.volume == 1, f"Cannot layout object of type {objects} along {shape}, a tuple, list or object array is required."
-
-
-def compatible_tensor(data, compat_shape: Shape = None, compat_natives=(), convert=False):
-    if isinstance(data, Tensor):
-        return data
-    elif isinstance(data, Shape):
-        if data.spatial.rank == 1:
-            return wrap(data.spatial.size)
-        assert compat_shape.channel.rank == 1, "Only single-channel tensors support implicit casting from Shape to tensor"
-        assert data.rank == compat_shape.channel.volume
-        return wrap(data.spatial.sizes, *compat_shape.channel.with_size(data.names))
-    else:
-        data_type = type(data)
-        backend = choose_backend(*compat_natives, data)
-        try:
-            data = backend.as_tensor(data, convert_external=convert)
-            shape = backend.staticshape(data)
-        except ValueError as e:
-            raise ValueError(e)
-        if len(shape) == 0:
-            return NativeTensor(data, EMPTY_SHAPE)
-        elif isinstance(data, (tuple, list)):  # always channel, add vector if not available
-            data = backend.as_tensor(data)
-        if len(shape) == compat_shape.channel_rank:
-            other_tensor = wrap(data, compat_shape.channel)
-            return other_tensor
-        if compat_shape.channel_rank > 1 and len(shape) == 1 and 'vector' in compat_shape.channel:
-            return wrap(data, compat_shape['vector'].without_sizes())
-        elif len(shape) == compat_shape.rank:
-            warnings.warn(f"Combining a phi.math.Tensor with a {data_type} of same shape is not invariant under shape permutations. Please convert the {data_type} to a phi.math.Tensor first. Shapes: {shape} and {compat_shape}", SyntaxWarning, stacklevel=5)
-            return NativeTensor(data, compat_shape.with_sizes(shape))
-        else:
-            raise ValueError(f"Cannot combine tensor of shape {shape} with tensor of shape {compat_shape}")
-
-
-def broadcastable_native_tensors(*tensors):
-    """
-    Expands and transposes the dimensions of the given tensors so that they all have the same dimension order.
-
-    Args:
-      *tensors: sequence of Tensors
-
-    Returns:
-      shape, native tensors)
-
-    """
-    from ._sparse import SparseCoordinateTensor, CompressedSparseMatrix, dense
-    if any(isinstance(t, (SparseCoordinateTensor, CompressedSparseMatrix)) for t in tensors) and not all(isinstance(t, (SparseCoordinateTensor, CompressedSparseMatrix)) for t in tensors):
-        tensors = [dense(t) for t in tensors]
-    broadcast_shape = merge_shapes(*[t.shape for t in tensors])
-    natives = [t.native(order=broadcast_shape.names) if t.rank > 0 else t.native() for t in tensors]
-    return broadcast_shape, natives
-
-
-def op2_native(x: Tensor, y: Tensor, native_function: Callable):
-    new_shape, (native1, native2) = broadcastable_native_tensors(x, y)
-    result_tensor = native_function(native1, native2)
-    return NativeTensor(result_tensor, new_shape)
-
-
-def custom_op2(x: Union[Tensor, float], y: Union[Tensor, float], l_operator, l_native_function, r_operator=None, r_native_function=None, op_name: str = 'unknown', op_symbol: str = None) -> Tensor:
-    """
-    Perform a custom operator on two tensors.
-    This method first tries calling _op2() on the first tensor and if that fails, tries it on the second tensor.
-
-    Args:
-      x: Left argument
-      y: Right argument
-      l_operator: Operator function acting on Tensors
-      l_native_function: Operator function acting on natives
-      r_operator:  Argument-reversed operator function acting on Tensors
-      r_native_function:  Argument-reversed operator function acting on natives
-      op_name: Name of the operator function for debugging purposes. Leading 'r' will be added for the operand-reversed version.
-      op_symbol: Short name for the operator, independent of argument order.
-
-    Returns:
-        `Tensor`
-    """
-    if op_symbol is None:
-        op_symbol = op_name
-    x = wrap(x)
-    y = wrap(y)
-    result = x._op2(y, l_operator, l_native_function, op_name, op_symbol)
-    if result is NotImplemented:
-        if r_operator is None:
-            r_operator = lambda a, b: l_operator(b, a)
-        if r_native_function is None:
-            r_native_function = lambda a, b: l_native_function(b, a)
-        result = y._op2(x, r_operator, r_native_function, f'r{op_name}', op_symbol)
-        if result is NotImplemented:
-            raise NotImplementedError(f"Operation not supported between {type(x)} and {type(y)}")
-    return result
-
-
-def disassemble_tensors(tensors: Union[Tuple[Tensor, ...], List[Tensor]], expand: bool) -> Tuple[tuple, Tuple[Shape], tuple]:
-    """
-    Args:
-        tensors: Tuple or list of Tensors.
-        expand: Whether to add collapsed dimensions to the native tensors.
-
-    Returns:
-        natives: tuple of native tensors
-        specs: Identification primitives from which the tensor can be reconstructed given the natives.
-            One per tensor.
-    """
-    for t in tensors:
-        if isinstance(t, TensorStack) or expand:
-            t._expand()
-    natives = sum([t._natives() for t in tensors], ())
-    shapes = tuple([t.shape for t in tensors])
-    specs = tuple([t._spec_dict() for t in tensors])
-    return natives, shapes, specs
-
-
-def assemble_tensors(natives: Union[tuple, list], specs: Union[Tuple[dict, ...], List[dict]]):
-    natives = list(natives)
-    result = []
-    for spec in specs:
-        t = spec['type']._from_spec_and_natives(spec, natives)
-        result.append(t)
-    return result
-
-
-MISSING_TENSOR = 'missing'
-NATIVE_TENSOR = 'native'
-
-
-def disassemble_tree(obj: PhiTreeNodeType) -> Tuple[PhiTreeNodeType, List[Tensor]]:
-    """
-    Splits a nested structure of Tensors into the structure without the tensors and an ordered list of tensors.
-    Native tensors will be wrapped in phi.math.Tensors with default dimension names and dimension types `None`.
-
-    See Also:
-        `assemble_tree()`
-
-    Args:
-        obj: Nested structure of `Tensor` objects.
-            Nested structures include: `tuple`, `list`, `dict`, `phi.math.magic.PhiTreeNode`.
-
-    Returns:
-        empty structure: Same structure as `obj` but with the tensors replaced by `None`.
-        tensors: Ordered `list` of all contained `Tensor` objects.
-    """
-    if obj is None:
-        return MISSING_TENSOR, []
-    elif isinstance(obj, Tensor):
-        return None, [obj]
-    elif isinstance(obj, (tuple, list)):
-        keys = []
-        values = []
-        for item in obj:
-            key, value = disassemble_tree(item)
-            keys.append(key)
-            values.extend(value)
-        return (tuple(keys) if isinstance(obj, tuple) else keys), values
-    elif isinstance(obj, dict):
-        keys = {}
-        values = []
-        for name, item in obj.items():
-            key, value = disassemble_tree(item)
-            keys[name] = key
-            values.extend(value)
-        return keys, values
-    elif isinstance(obj, PhiTreeNode):
-        attributes = variable_attributes(obj)
-        keys = {}
-        values = []
-        for attr in attributes:
-            key, value = disassemble_tree(getattr(obj, attr))
-            keys[attr] = key
-            values.extend(value)
-        return copy_with(obj, **keys), values
-    else:
-        try:
-            backend = choose_backend(obj)
-            sizes = backend.staticshape(obj)
-            shape = Shape(sizes, tuple([f"dim{i}" for i in range(len(sizes))]), (None,) * len(sizes), (None,) * len(sizes))
-            return NATIVE_TENSOR, [NativeTensor(obj, shape)]
-        except NoBackendFound:
-            return obj, []
-
-
-def assemble_tree(obj: PhiTreeNodeType, values: List[Tensor]) -> PhiTreeNodeType:
-    """ Reverses `disassemble_tree()` given an empty nested structure and a list of tensors. """
-    if obj is MISSING_TENSOR:
-        return None
-    elif obj is NATIVE_TENSOR:
-        value = values.pop(0)
-        assert isinstance(value, NativeTensor), f"Failed to assemble tree structure. Encountered {value}"
-        return value._native
-    elif obj is None:
-        value = values.pop(0)
-        assert isinstance(value, Tensor)
-        return value
-    elif isinstance(obj, list):
-        return [assemble_tree(item, values) for item in obj]
-    elif isinstance(obj, tuple):
-        return tuple([assemble_tree(item, values) for item in obj])
-    elif isinstance(obj, dict):
-        return {name: assemble_tree(val, values) for name, val in obj.items()}
-    elif isinstance(obj, PhiTreeNode):
-        attributes = variable_attributes(obj)
-        values = {a: assemble_tree(getattr(obj, a), values) for a in attributes}
-        return copy_with(obj, **values)
-    else:
-        return obj
-
-
-def cached(t: Union[Tensor, 'PhiTreeNode']) -> Union[Tensor, 'PhiTreeNode']:
-    assert isinstance(t, (Tensor, PhiTreeNode)), f"All arguments must be Tensors but got {type(t)}"
-    if isinstance(t, NativeTensor):
-        return t
-    elif isinstance(t, CollapsedTensor):
-        if t.is_cached:
-            return t._cached
-        if t._inner._is_tracer:
-            return t
-        if t.shape.is_uniform:
-            native = t._inner.native(order=t.shape.names)
-            multiples = [1 if name in t._inner.shape else size for size, name, *_ in t.shape._dimensions]
-            backend = choose_backend(native)
-            tiled = backend.tile(native, multiples)
-            return NativeTensor(tiled, t.shape)
-        else:
-            raise NotImplementedError()
-    elif isinstance(t, TensorStack):
-        if t._cached is not None:
-            return t._cached
-        inners = cached(t._tensors)
-        if t.requires_broadcast:
-            return TensorStack(inners, t._stack_dim)
-        else:
-            natives = [t.native(order=t.shape.names) for t in inners]
-            native = choose_backend(*natives).stack(natives, axis=t.shape.index(t._stack_dim.name))
-            return NativeTensor(native, t.shape)
-    elif isinstance(t, Layout):
-        return t
-    elif isinstance(t, PhiTreeNode):
-        tree, tensors = disassemble_tree(t)
-        tensors_ = [cached(t_) for t_ in tensors]
-        return assemble_tree(tree, tensors_)
-    else:
-        raise AssertionError(f"Cannot cache {type(t)} {t}")
-
-
-class Dict(dict):
-    """
-    Dictionary of `Tensor` or `phi.math.magic.PhiTreeNode` values.
-    Dicts are not themselves tensors and do not have a shape.
-    Use `layout()` to treat `dict` instances like tensors.
-
-    In addition to dictionary functions, supports mathematical operators with other `Dict`s and lookup via `.key` syntax.
-    `Dict` implements `phi.math.magic.PhiTreeNode` so instances can be passed to math operations like `sin`.
-    """
-
-    def __value_attrs__(self):
-        return tuple(self.keys())
-    
-    # --- Dict[key] ---
-
-    def __getattr__(self, key):
-        try:
-            return self[key]
-        except KeyError as k:
-            raise AttributeError(k)
-
-    def __setattr__(self, key, value):
-        self[key] = value
-
-    def __delattr__(self, key):
-        try:
-            del self[key]
-        except KeyError as k:
-            raise AttributeError(k)
-        
-    # --- operators ---
-    
-    def __neg__(self):
-        return Dict({k: -v for k, v in self.items()})
-    
-    def __invert__(self):
-        return Dict({k: ~v for k, v in self.items()})
-    
-    def __abs__(self):
-        return Dict({k: abs(v) for k, v in self.items()})
-    
-    def __round__(self, n=None):
-        return Dict({k: round(v) for k, v in self.items()})
-
-    def __add__(self, other):
-        if isinstance(other, Dict):
-            return Dict({key: val + other[key] for key, val in self.items()})
-        else:
-            return Dict({key: val + other for key, val in self.items()})
-
-    def __radd__(self, other):
-        if isinstance(other, Dict):
-            return Dict({key: other[key] + val for key, val in self.items()})
-        else:
-            return Dict({key: other + val for key, val in self.items()})
-
-    def __sub__(self, other):
-        if isinstance(other, Dict):
-            return Dict({key: val - other[key] for key, val in self.items()})
-        else:
-            return Dict({key: val - other for key, val in self.items()})
-
-    def __rsub__(self, other):
-        if isinstance(other, Dict):
-            return Dict({key: other[key] - val for key, val in self.items()})
-        else:
-            return Dict({key: other - val for key, val in self.items()})
-
-    def __mul__(self, other):
-        if isinstance(other, Dict):
-            return Dict({key: val * other[key] for key, val in self.items()})
-        else:
-            return Dict({key: val * other for key, val in self.items()})
-
-    def __rmul__(self, other):
-        if isinstance(other, Dict):
-            return Dict({key: other[key] * val for key, val in self.items()})
-        else:
-            return Dict({key: other * val for key, val in self.items()})
-
-    def __truediv__(self, other):
-        if isinstance(other, Dict):
-            return Dict({key: val / other[key] for key, val in self.items()})
-        else:
-            return Dict({key: val / other for key, val in self.items()})
-
-    def __rtruediv__(self, other):
-        if isinstance(other, Dict):
-            return Dict({key: other[key] / val for key, val in self.items()})
-        else:
-            return Dict({key: other / val for key, val in self.items()})
-
-    def __floordiv__(self, other):
-        if isinstance(other, Dict):
-            return Dict({key: val // other[key] for key, val in self.items()})
-        else:
-            return Dict({key: val // other for key, val in self.items()})
-
-    def __rfloordiv__(self, other):
-        if isinstance(other, Dict):
-            return Dict({key: other[key] // val for key, val in self.items()})
-        else:
-            return Dict({key: other // val for key, val in self.items()})
-
-    def __pow__(self, power, modulo=None):
-        assert modulo is None
-        if isinstance(power, Dict):
-            return Dict({key: val ** power[key] for key, val in self.items()})
-        else:
-            return Dict({key: val ** power for key, val in self.items()})
-
-    def __rpow__(self, other):
-        if isinstance(other, Dict):
-            return Dict({key: other[key] ** val for key, val in self.items()})
-        else:
-            return Dict({key: other ** val for key, val in self.items()})
-
-    def __mod__(self, other):
-        if isinstance(other, Dict):
-            return Dict({key: val % other[key] for key, val in self.items()})
-        else:
-            return Dict({key: val % other for key, val in self.items()})
-
-    def __rmod__(self, other):
-        if isinstance(other, Dict):
-            return Dict({key: other[key] % val for key, val in self.items()})
-        else:
-            return Dict({key: other % val for key, val in self.items()})
-
-    def __eq__(self, other):
-        if isinstance(other, Dict):
-            return Dict({key: val == other[key] for key, val in self.items()})
-        else:
-            return Dict({key: val == other for key, val in self.items()})
-
-    def __ne__(self, other):
-        if isinstance(other, Dict):
-            return Dict({key: val != other[key] for key, val in self.items()})
-        else:
-            return Dict({key: val != other for key, val in self.items()})
-
-    def __lt__(self, other):
-        if isinstance(other, Dict):
-            return Dict({key: val < other[key] for key, val in self.items()})
-        else:
-            return Dict({key: val < other for key, val in self.items()})
-
-    def __le__(self, other):
-        if isinstance(other, Dict):
-            return Dict({key: val <= other[key] for key, val in self.items()})
-        else:
-            return Dict({key: val <= other for key, val in self.items()})
-
-    def __gt__(self, other):
-        if isinstance(other, Dict):
-            return Dict({key: val > other[key] for key, val in self.items()})
-        else:
-            return Dict({key: val > other for key, val in self.items()})
-
-    def __ge__(self, other):
-        if isinstance(other, Dict):
-            return Dict({key: val >= other[key] for key, val in self.items()})
-        else:
-            return Dict({key: val >= other for key, val in self.items()})
-
-    # --- overridden methods ---
-
-    def copy(self):
-        return Dict(self)
-
-
-def to_dict(value: Union[Tensor, Shape]):
-    """
-    Returns a serializable form of a `Tensor` or `Shape`.
-    The result can be written to a JSON file, for example.
-
-    See Also:
-        `from_dict()`.
-
-    Args:
-        value: `Tensor` or `Shape`
-
-    Returns:
-        Serializable Python tree of primitives
-    """
-    if isinstance(value, Shape):
-        return value._to_dict(include_sizes=True)
-    elif isinstance(value, Tensor):
-        return value._to_dict()
-    raise ValueError(f"Cannot convert {value} to a dict")
-
-
-def from_dict(dict_: dict, convert=False):
-    """
-    Loads a `Tensor` or `Shape` from a serialized form.
-
-    See Also:
-        `to_dict()`.
-
-    Args:
-        dict_: Serialized tensor properties.
-        convert: Whether to convert the data to the current backend format or keep it as a Numpy array.
-
-    Returns:
-        `Tensor` or `Shape`.
-    """
-    shape = Shape._from_dict(dict_)
-    if 'data' in dict_:
-        return tensor(dict_['data'], shape, convert=convert)
-    else:
-        return shape
-
-
-@dataclass
-class Color:
-    name: str
-    console_foreground_begin: str
-
-    def __call__(self, obj, **kwargs):
-        text = str(obj).replace(CONSOLE_END, self.console_foreground_begin)
-        return f"{self.console_foreground_begin}{text}{CONSOLE_END if self.console_foreground_begin else ''}"
-
-
-DEFAULT = Color("Default", '')
-BLUE = Color("Blue", '\033[94m')
-GREEN = Color("Green", '\033[92m')
-YELLOW = Color("Yellow", '\033[93m')
-GREY = Color("Grey", '\033[37m')
-CONSOLE_END = '\033[0m'
-
-
-@dataclass
-class ColorScheme:
-    value: Color
-    shape: Color
-    dtype: Color
-    fine: Color
-
-
-DEFAULT_COLORS = ColorScheme(BLUE, GREEN, YELLOW, GREY)
-NO_COLORS = ColorScheme(DEFAULT, DEFAULT, DEFAULT, DEFAULT)
-
-
-@dataclass
-class PrintOptions:
-    layout: str = 'auto'
-    float_format: str = None
-    threshold: int = 8
-    colors: ColorScheme = None
-    include_shape: bool = None
-    include_dtype: bool = None
-
-    def get_colors(self):
-        if self.colors is True:
-            return DEFAULT_COLORS
-        elif self.colors is False:
-            return NO_COLORS
-        elif self.colors is not None:
-            return self.colors
-        else:  # None
-            return DEFAULT_COLORS if check_is_printing() else NO_COLORS
-
-
-def check_is_printing():
-    import traceback, sys
-    stack = traceback.extract_stack()
-    for frame in stack:
-        if "_pydevd_bundle\\pydevd_xml.py" in frame.filename:
-            return False
-    for frame in stack:
-        if frame.line.strip().startswith('print('):
-            return True
-    if 'ipykernel' in sys.modules:
-        return True
-    return False
-
-
-def format_summary(self: Tensor, options: PrintOptions) -> str:
-    """
-    Returns shape + dtype + content summary
-
-    * `bool`: n / N True
-    * `float`: mean ± std (min...max)
-    """
-    if not self.available:
-        return format_tracer(self, options)
-    from ._sparse import SparseCoordinateTensor, CompressedSparseMatrix
-    if isinstance(self, (SparseCoordinateTensor, CompressedSparseMatrix)):
-        return sparse_summary(self, options)
-    colors = options.get_colors()
-    tokens = []
-    if self.shape if options.include_shape is None else options.include_shape:
-        tokens.append(f"{colors.shape(self.shape)}")
-    if is_unexpected_dtype(self.dtype) if options.include_dtype is None else options.include_dtype:
-        tokens.append(f"{colors.dtype(self.dtype)}")
-    try:
-        if self.rank == 0:
-            tokens.append(colors.value(self.numpy()))
-        elif self.dtype.kind == bool:
-            tokens.append(colors.value(f"{self.sum} / {self.shape.volume} True"))
-        elif self.dtype.kind in (float, int):
-            min_val, max_val, mean, std = [float(f) for f in [self.finite_min, self.finite_max, self.finite_mean, self.std]]
-            if std == 0:
-                tokens.append(colors.value(f"const {mean:{options.float_format or ''}}"))
-            else:
-                if any([abs(val) < 0.001 or abs(val) > 1000 for val in [mean, std]]):
-                    tokens.append(colors.value(f"{mean:{options.float_format or '.2e'}} ± {std:{options.float_format or '.1e'}}"))
-                else:
-                    tokens.append(colors.value(f"{mean:{options.float_format or '.3f'}} ± {std:{options.float_format or '.3f'}}"))
-                tokens.append(colors.fine(f"({min_val:{options.float_format or '.0e'}}...{max_val:{options.float_format or '.0e'}})"))
-        elif self.dtype.kind == complex:
-            tokens.append(colors.value(f"|...| < {abs(self).max}"))
-    except BaseException as err:
-        tokens.append(f"failed to fetch values: {err}")
-    return " ".join(tokens)
-
-
-def sparse_summary(value: Tensor, options: PrintOptions) -> str:
-    colors = options.get_colors()
-    from ._sparse import SparseCoordinateTensor, CompressedSparseMatrix
-    tokens = []
-    if is_unexpected_dtype(value.dtype) if options.include_dtype is None else options.include_dtype:
-        tokens.append(f"{colors.dtype(value.dtype)}")
-    tokens.append("sparse" if isinstance(value, SparseCoordinateTensor) else "compressed sparse")
-    if options.include_shape is not False:
-        tokens.append(f"{colors.shape(value.shape)}")
-    tokens.append(f"with {instance(value._values).volume} entries")
-    return " ".join(tokens)
-
-
-def is_unexpected_dtype(dtype: DType):
-    if dtype in [DType(bool), DType(int, 32)]:
-        return False
-    if dtype.kind == float and dtype.precision == get_precision():
-        return False
-    return True
-
-
-def format_tracer(self: Tensor, options: PrintOptions) -> str:
-    colors = options.get_colors()
-    if self._is_tracer:
-        return f"{colors.shape(self.shape)} {colors.dtype(self.dtype)} {colors.value(f'linear tracer for {self.default_backend}')}"
-    else:
-        return f"{colors.shape(self.shape)} {colors.dtype(self.dtype)} {colors.value(f'{self.default_backend} tracer')}"
-
-
-def format_full(value: Tensor, options: PrintOptions) -> str:  # multi-line content
-    if not value.available:
-        return format_tracer(value, options)
-    from ._sparse import dense
-    value = dense(value)
-    import re
-    colors = options.get_colors()
-    dim_order = tuple(sorted(value.shape.spatial.names, reverse=True))
-    lines = []
-    formatter = {}
-    if options.float_format:
-        formatter['float_kind'] = ('{:' + options.float_format + '}').format
-    with numpy.printoptions(threshold=np.inf, formatter=formatter):
-        if value.shape.dual_rank > 0:  # matrix
-            if options.include_shape is not None:
-                lines.append(colors.shape(value.shape))
-            if value.shape.dual_rank > 1:
-                raise NotImplementedError("Multiple dual dimensions cannot currently be printed")
-            dual_dim = dual(value).name
-            primal = spatial(**dual(value).untyped_dict).name
-            if primal not in value.shape:
-                primal = non_batch(value).non_dual.name
-            for b in batch(value).meshgrid(names=True):
-                text = " " + np.array2string(value[b].numpy([primal, dual_dim]), separator=', ', max_line_width=np.inf) + " "
-                text = re.sub('[\\[\\]]', '', text).replace(',', ' ')
-                prefixes = prefix_indices(non_batch(value).non_dual, colors)
-                if options.include_shape is not False:
-                    for line, prefix in zip(text.split("\n"), prefixes):
-                        lines.append(f"{prefix}  {colors.value(line)} along {colors.shape(dual_dim)}")
-                else:
-                    lines.append(colors.value(text))
-        elif value.shape.spatial_rank == 0:  # no spatial or dual dimensions
-            if options.include_shape is not None:
-                lines.append(colors.shape(value.shape))
-            if value.shape.rank <= 1:
-                text = np.array2string(value.numpy(), separator=', ', max_line_width=np.inf)
-                lines.append(' ' + re.sub('[\\[\\]]', '', text))
-            else:
-                text = np.array2string(value.numpy(value.shape), separator=', ', max_line_width=np.inf)
-                lines.append(text)
-        elif value.shape.spatial_rank in (1, 2):
-            if value.shape.non_spatial.volume > 1:
-                indices = [f"{colors.shape(', '.join(f'{name}={idx}' for name, idx in index_dict.items()))}" for index_dict in value.shape.non_spatial.meshgrid(names=True)]
-                max_index_length = max(len(index) for index in indices)
-            for i, index_dict in enumerate(value.shape.non_spatial.meshgrid(names=True)):
-                row = ""
-                if value.shape.non_spatial.volume > 1:
-                    row += indices[i] + " " * (max_index_length - len(indices[i]) + 2)
-                    if value.shape.spatial_rank == 2:
-                        row += "\n"
-                if value.shape.spatial_rank == 1:
-                    text = np.array2string(value[index_dict].numpy(dim_order), separator=', ', max_line_width=np.inf)
-                else:
-                    text = " " + np.array2string(value[index_dict].numpy(dim_order)[::-1], separator=', ', max_line_width=np.inf)
-                lines.append(row + colors.value(re.sub('[\\[\\]]', '', text)) + (f"  along {colors.shape(spatial(value))}" if options.include_shape is not False else ""))
-        else:
-            raise NotImplementedError('Can only print tensors with up to 2 spatial dimensions.')
-    return "\n".join(lines)
-
-
-def prefix_indices(index_shape, colors: ColorScheme):
-    prefixes = [f"{colors.shape(', '.join(f'{name}={idx}' for name, idx in index_dict.items()))}" for index_dict in index_shape.meshgrid(names=True)]
-    max_len = max(len(p) for p in prefixes)
-    prefixes = [p + " " * (max_len - len(p) + 2) for p in prefixes]
-    return prefixes
-
-
-def format_row(self: Tensor, options: PrintOptions) -> str:  # all values in a single line
-    """
-    Including shape:  (x=5, y=4) along vector
-    Without shape: (5, 4)
-    Auto: don't show if 'vector' but show item names
-
-    Args:
-        self:
-        options:
-
-    Returns:
-
-    """
-    if not self.available:
-        return format_tracer(self, options)
-    from ._sparse import dense
-    self = dense(self)
-    colors = options.get_colors()
-    if self.shape.rank == 1:
-        content = _format_vector(self, options)
-        if self.shape.name != 'vector' or self.shape.non_channel if options.include_shape is None else options.include_shape:
-            content += f" along {colors.shape(f'{self.shape.name}{TYPE_ABBR[self.shape.type]}')}"
-    else:
-        if channel(self):
-            rows = [_format_vector(self[b], options) for b in self.shape.non_channel.meshgrid()]
-        else:
-            rows = [_format_number(self[b].numpy(), options, self.dtype) for b in self.shape.non_channel.meshgrid()]
-        content = "; ".join(rows)
-        if options.include_shape is not False:
-            content += " " + colors.shape(self.shape)
-    if is_unexpected_dtype(self.dtype) if options.include_dtype is None else options.include_dtype:
-        content += f" {colors.dtype(self.dtype)}"
-    return content
-
-
-def format_numpy(self: Tensor, options: PrintOptions) -> str:
-    from ._sparse import dense
-    self = dense(self)
-    header = []
-    colors = options.get_colors()
-    if options.include_shape:
-        header.append(colors.shape(self.shape))
-    if options.include_dtype:
-        header.append(colors.dtype(self.dtype))
-    numpy_array = self.numpy(self.shape)
-    formatter = {}
-    if options.float_format:
-        formatter['float_kind'] = ('{:' + options.float_format + '}').format
-    with numpy.printoptions(threshold=options.threshold, formatter=formatter):
-        content = colors.value(numpy_array)
-    return " ".join(header) + "\n" + content if header else content
-
-
-def _format_vector(self: Tensor, options: PrintOptions) -> str:
-    colors = options.get_colors()
-    if self.shape.rank > 1:
-        from ._ops import flatten
-        self = flatten(self, channel('flat'))
-    if self.shape.get_item_names(0) is not None and options.include_shape is not False:
-        content = ", ".join([f"{item}={_format_number(number, options, self.dtype)}" for number, item in zip(self, self.shape.get_item_names(0))])
-    else:
-        content = ", ".join([_format_number(num, options, self.dtype) for num in self])
-    return colors.value(f"({content})")
-
-
-def _format_number(num, options: PrintOptions, dtype: DType):
-    if options.float_format is not None:
-        return format(num, options.float_format)
-    if dtype.kind == int:
-        return format(num, 'd')
-    if dtype.kind == bool:
-        return str(bool(num))
-    if dtype.kind == float:
-        return format(num, options.float_format or '.3f')
-    return str(num)
-
-
-def format_tensor(self: Tensor, options: PrintOptions) -> str:
-    if not self.available:
-        return format_tracer(self, options)
-    if options.layout == 'auto':
-        if not self.shape:
-            return format_summary(self, options)
-        if self.shape.volume is not None and self.shape.volume < options.threshold:
-            return format_row(self, options)
-        else:
-            return format_summary(self, options)
-    elif options.layout == 'summary':
-        return format_summary(self, options)
-    elif options.layout == 'full':
-        return format_full(self, options)
-    elif options.layout == 'row':
-        return format_row(self, options)
-    elif options.layout == 'numpy':
-        return format_numpy(self, options)
-    else:
-        raise NotImplementedError(f"Layout '{options.layout}' is not supported.")
-
-
-def is_scalar(value) -> bool:
-    """
-    Checks whether `value` has no dimensions.
-
-    Args:
-        value: `Tensor` or Python primitive or native tensor.
-
-    Returns:
-        `bool`
-    """
-    if isinstance(value, Tensor):
-        return value.shape.rank == 0
-    elif isinstance(value, numbers.Number):
-        return True
-    else:
-        return len(choose_backend(value).staticshape(value)) == 0
-
-
-def may_vary_along(value, dims: DimFilter):
-    if isinstance(value, CollapsedTensor) and value._inner is not None:
-        return may_vary_along(value._inner, dims)
-    s = shape(value)
-    dims = s.only(dims)
-    return dims.volume > 1
+import numbers
+import warnings
+from contextlib import contextmanager
+from typing import Union
+
+from dataclasses import dataclass
+from typing import Tuple, Callable, List
+
+import numpy
+import numpy as np
+
+from ._magic_ops import PhiTreeNodeType, variable_attributes, copy_with, stack, pack_dims, expand
+from ._shape import (Shape,
+                     CHANNEL_DIM, BATCH_DIM, SPATIAL_DIM, EMPTY_SHAPE,
+                     parse_dim_order, shape_stack, merge_shapes, channel, concat_shapes, primal,
+                     TYPE_ABBR, IncompatibleShapes, INSTANCE_DIM, batch, spatial, dual, instance, shape, DimFilter, non_batch, DEBUG_CHECKS)
+from .backend import NoBackendFound, choose_backend, BACKENDS, get_precision, default_backend, convert as convert_, \
+    Backend, ComputeDevice, OBJECTS
+from .backend._dtype import DType, combine_types
+from .magic import BoundDim, PhiTreeNode, slicing_dict
+from .magic import Shapable
+
+
+class Tensor:
+    """
+    Abstract base class to represent structured data of one data type.
+    This class replaces the native tensor classes `numpy.ndarray`, `torch.Tensor`, `tensorflow.Tensor` or `jax.numpy.ndarray` as the main data container in Φ<sub>Flow</sub>.
+
+    `Tensor` instances are different from native tensors in two important ways:
+
+    * The dimensions of Tensors have *names* and *types*.
+    * Tensors can have non-uniform shapes, meaning that the size of dimensions can vary along other dimensions.
+
+    To check whether a value is a tensor, use `isinstance(value, Tensor)`.
+
+    To construct a Tensor, use `phi.math.tensor()`, `phi.math.wrap()` or one of the basic tensor creation functions,
+    see https://tum-pbs.github.io/PhiFlow/Math.html#tensor-creation .
+
+    Tensors are not editable.
+    When backed by an editable native tensor, e.g. a `numpy.ndarray`, do not edit the underlying data structure.
+    """
+
+    def native(self, order: Union[str, tuple, list, Shape] = None, singleton_for_const=False):
+        """
+        Returns a native tensor object with the dimensions ordered according to `order`.
+        
+        Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
+        If a dimension of the tensor is not listed in `order`, a `ValueError` is raised.
+
+        Args:
+            order: (Optional) Order of dimension names as comma-separated string, list or `Shape`.
+            singleton_for_const: If `True`, dimensions along which values are guaranteed to be constant will not be expanded to their true size but returned as singleton dimensions.
+
+        Returns:
+            Native tensor representation, such as PyTorch tensor or NumPy array.
+
+        Raises:
+            ValueError if the tensor cannot be transposed to match target_shape
+        """
+        raise NotImplementedError(self.__class__)
+
+    def numpy(self, order: Union[str, tuple, list, Shape] = None) -> np.ndarray:
+        """
+        Converts this tensor to a `numpy.ndarray` with dimensions ordered according to `order`.
+        
+        *Note*: Using this function breaks the autograd chain. The returned tensor is not differentiable.
+        To get a differentiable tensor, use `Tensor.native()` instead.
+        
+        Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
+        If a dimension of the tensor is not listed in `order`, a `ValueError` is raised.
+
+        If this `Tensor` is backed by a NumPy array, a reference to this array may be returned.
+
+        See Also:
+            `phi.math.numpy()`
+
+        Args:
+            order: (Optional) Order of dimension names as comma-separated string, list or `Shape`.
+
+        Returns:
+            NumPy representation
+
+        Raises:
+            ValueError if the tensor cannot be transposed to match target_shape
+        """
+        native = self.native(order=order)
+        return choose_backend(native).numpy(native)
+
+    def __array__(self, dtype=None):  # NumPy conversion
+        if self.rank > 1:
+            warnings.warn("Automatic conversion of Φ-Flow tensors to NumPy can cause problems because the dimension order is not guaranteed.", SyntaxWarning, stacklevel=3)
+        return self.numpy(self._shape)
+
+    def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):  # NumPy interface
+        if len(inputs) != 2:
+            return NotImplemented
+        if ufunc.__name__ == 'multiply':
+            if inputs[0] is self:
+                return self._op2(inputs[1], lambda x, y: x * y, lambda x, y: choose_backend(x, y).mul(x, y), 'mul', '*')
+            else:
+                return self._op2(inputs[0], lambda x, y: y * x, lambda x, y: choose_backend(x, y).mul(y, x), 'rmul', '*')
+        if ufunc.__name__ == 'add':
+            if inputs[0] is self:
+                return self._op2(inputs[1], lambda x, y: x + y, lambda x, y: choose_backend(x, y).add(x, y), 'add', '+')
+            else:
+                return self._op2(inputs[0], lambda x, y: y + x, lambda x, y: choose_backend(x, y).add(y, x), 'radd', '+')
+        if ufunc.__name__ == 'subtract':
+            if inputs[0] is self:
+                return self._op2(inputs[1], lambda x, y: x - y, lambda x, y: choose_backend(x, y).sub(x, y), 'add', '-')
+            else:
+                return self._op2(inputs[0], lambda x, y: y - x, lambda x, y: choose_backend(x, y).sub(y, x), 'rsub', '-')
+        if ufunc.__name__ in ['divide', 'true_divide']:
+            if inputs[0] is self:
+                return self._op2(inputs[1], lambda x, y: x / y, lambda x, y: choose_backend(x, y).div(x, y), 'true_divide', '/')
+            else:
+                return self._op2(inputs[0], lambda x, y: y / x, lambda x, y: choose_backend(x, y).div(y, x), 'r_true_divide', '/')
+        if ufunc.__name__ == 'floor_divide':
+            if inputs[0] is self:
+                return self._op2(inputs[1], lambda x, y: x // y, lambda x, y: choose_backend(x, y).floordiv(x, y), 'floor_divide', '//')
+            else:
+                return self._op2(inputs[0], lambda x, y: y // x, lambda x, y: choose_backend(x, y).floordiv(y, x), 'r_floor_divide', '//')
+        if ufunc.__name__ == 'remainder':
+            if inputs[0] is self:
+                return self._op2(inputs[1], lambda x, y: x % y, lambda x, y: choose_backend(x, y).mod(x, y), 'remainder', '%')
+            else:
+                return self._op2(inputs[0], lambda x, y: y % x, lambda x, y: choose_backend(x, y).mod(y, x), 'r_remainder', '%')
+        if ufunc.__name__ == 'power':
+            if inputs[0] is self:
+                return self._op2(inputs[1], lambda x, y: x ** y, lambda x, y: choose_backend(x, y).pow(x, y), 'power', '**')
+            else:
+                return self._op2(inputs[0], lambda x, y: y ** x, lambda x, y: choose_backend(x, y).pow(y, x), 'r_power', '**')
+        if ufunc.__name__ == 'equal':
+            if _EQUALITY_BY_REF:
+                return wrap(inputs[0] is inputs[1])
+            if inputs[0] is self:
+                return self._op2(inputs[1], lambda x, y: x == y, lambda x, y: choose_backend(x, y).equal(x, y), 'equal', '==')
+            else:
+                return self._op2(inputs[0], lambda x, y: y == x, lambda x, y: choose_backend(x, y).equal(y, x), 'r_equal', '==')
+        if ufunc.__name__ == 'not_equal':
+            if _EQUALITY_BY_REF:
+                return wrap(inputs[0] is not inputs[1])
+            if inputs[0] is self:
+                return self._op2(inputs[1], lambda x, y: x != y, lambda x, y: choose_backend(x, y).not_equal(x, y), 'equal', '!=')
+            else:
+                return self._op2(inputs[0], lambda x, y: y != x, lambda x, y: choose_backend(x, y).not_equal(y, x), 'r_equal', '!=')
+        if ufunc.__name__ == 'greater':
+            if inputs[0] is self:
+                return self._op2(inputs[1], lambda x, y: x > y, lambda x, y: choose_backend(x, y).greater_than(x, y), 'greater', '>')
+            else:
+                return self._op2(inputs[0], lambda x, y: y > x, lambda x, y: choose_backend(x, y).greater_than(y, x), 'r_greater', '>')
+        if ufunc.__name__ == 'greater_equal':
+            if inputs[0] is self:
+                return self._op2(inputs[1], lambda x, y: x >= y, lambda x, y: choose_backend(x, y).greater_or_equal(x, y), 'greater_equal', '>=')
+            else:
+                return self._op2(inputs[0], lambda x, y: y >= x, lambda x, y: choose_backend(x, y).greater_or_equal(y, x), 'r_greater_equal', '>=')
+        if ufunc.__name__ == 'less':
+            if inputs[0] is self:
+                return self._op2(inputs[1], lambda x, y: x < y, lambda x, y: choose_backend(x, y).greater_than(y, x), 'less', '<')
+            else:
+                return self._op2(inputs[0], lambda x, y: y < x, lambda x, y: choose_backend(x, y).greater_than(x, y), 'r_less', '<')
+        if ufunc.__name__ == 'less_equal':
+            if inputs[0] is self:
+                return self._op2(inputs[1], lambda x, y: x <= y, lambda x, y: choose_backend(x, y).greater_or_equal(y, x), 'less_equal', '<=')
+            else:
+                return self._op2(inputs[0], lambda x, y: y <= x, lambda x, y: choose_backend(x, y).greater_or_equal(x, y), 'r_less_equal', '<=')
+        if ufunc.__name__ == 'left_shift':
+            if inputs[0] is self:
+                return self._op2(inputs[1], lambda x, y: x << y, lambda x, y: choose_backend(x, y).shift_bits_left(x, y), 'left_shift', '<<')
+            else:
+                return self._op2(inputs[0], lambda x, y: y << x, lambda x, y: choose_backend(x, y).shift_bits_left(y, x), 'r_left_shift', '<<')
+        if ufunc.__name__ == 'right_shift':
+            if inputs[0] is self:
+                return self._op2(inputs[1], lambda x, y: x >> y, lambda x, y: choose_backend(x, y).shift_bits_right(x, y), 'right_shift', '>>')
+            else:
+                return self._op2(inputs[0], lambda x, y: y >> x, lambda x, y: choose_backend(x, y).shift_bits_right(y, x), 'r_right_shift', '>>')
+        raise NotImplementedError(f"NumPy function '{ufunc.__name__}' is not compatible with Φ-Flow tensors.")
+
+    @property
+    def dtype(self) -> DType:
+        """ Data type of the elements of this `Tensor`. """
+        raise NotImplementedError()
+
+    @property
+    def shape(self) -> Shape:
+        """ The `Shape` lists the dimensions with their sizes, names and types. """
+        raise NotImplementedError()
+
+    @property
+    def default_backend(self) -> Backend:
+        from ._ops import choose_backend_t
+        return choose_backend_t(self)
+
+    def _with_shape_replaced(self, new_shape: Shape):
+        raise NotImplementedError()
+
+    def _with_natives_replaced(self, natives: list):
+        """ Replaces all n _natives() of this Tensor with the first n elements of the list and removes them from the list. """
+        raise NotImplementedError()
+
+    @property
+    def rank(self) -> int:
+        """
+        Number of explicit dimensions of this `Tensor`. Equal to `tensor.shape.rank`.
+        This replaces [`numpy.ndarray.ndim`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.ndim.html) /
+        [`torch.Tensor.dim`](https://pytorch.org/docs/master/generated/torch.Tensor.dim.html) /
+        [`tf.rank()`](https://www.tensorflow.org/api_docs/python/tf/rank) /
+        [`jax.numpy.ndim()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndim.html).
+        """
+        return self.shape.rank
+
+    @property
+    def _is_tracer(self) -> bool:
+        """
+        Tracers store additional internal information.
+        They should not be converted to `native()` in intermediate operations.
+        
+        TensorStack prevents performing the actual stack operation if one of its component tensors is special.
+        """
+        raise NotImplementedError(self.__class__)
+
+    def _to_dict(self):
+        return cached(self)._to_dict()
+
+    def __len__(self):
+        return self.shape.volume if self.rank == 1 else NotImplemented
+
+    def __bool__(self):
+        assert self.rank == 0, f"Cannot convert tensor with non-empty shape {self.shape} to bool. Use tensor.any or tensor.all instead."
+        from ._ops import all_
+        if not self.default_backend.supports(Backend.jit_compile):  # NumPy
+            return bool(self.native()) if self.rank == 0 else bool(all_(self).native())
+        else:
+            # __bool__ does not work with TensorFlow tracing.
+            # TensorFlow needs to see a tf.Tensor in loop conditions but won't allow bool() invocations.
+            # However, this function must always return a Python bool.
+            raise AssertionError("To evaluate the boolean value of a Tensor, use 'Tensor.all'.")
+
+    @property
+    def all(self):
+        """ Whether all values of this `Tensor` are `True` as a native bool. """
+        from ._ops import all_, cast
+        if self.rank == 0:
+            return cast(self, DType(bool)).native()
+        else:
+            return all_(self, dim=self.shape).native()
+
+    @property
+    def any(self):
+        """ Whether this `Tensor` contains a `True` value as a native bool. """
+        from ._ops import any_, cast
+        if self.rank == 0:
+            return cast(self, DType(bool)).native()
+        else:
+            return any_(self, dim=self.shape).native()
+
+    @property
+    def mean(self):
+        """ Mean value of this `Tensor` as a native scalar. """
+        from ._ops import mean
+        return mean(self, dim=self.shape).native()
+
+    @property
+    def finite_mean(self):
+        """ Mean value of all finite values in this `Tensor` as a native scalar. """
+        from ._ops import finite_mean
+        return finite_mean(self, dim=self.shape).native()
+
+    @property
+    def std(self):
+        """ Standard deviation of this `Tensor` as a native scalar. """
+        from ._ops import std
+        return std(self, dim=self.shape).native()
+
+    @property
+    def sum(self):
+        """ Sum of all values of this `Tensor` as a native scalar. """
+        from ._ops import sum_
+        return sum_(self, dim=self.shape).native()
+
+    @property
+    def finite_sum(self):
+        """ Sum of all finite values of this `Tensor` as a native scalar. """
+        from ._ops import finite_sum
+        return finite_sum(self, dim=self.shape).native()
+
+    @property
+    def min(self):
+        """ Minimum value of this `Tensor` as a native scalar. """
+        from ._ops import min_
+        return min_(self, dim=self.shape).native()
+
+    @property
+    def finite_min(self):
+        """ Minimum finite value of this `Tensor` as a native scalar. """
+        from ._ops import finite_min
+        return finite_min(self, dim=self.shape).native()
+
+    @property
+    def max(self):
+        """ Maximum value of this `Tensor` as a native scalar. """
+        from ._ops import max_
+        return max_(self, dim=self.shape).native()
+
+    @property
+    def finite_max(self):
+        """ Maximum finite value of this `Tensor` as a native scalar. """
+        from ._ops import finite_max
+        return finite_max(self, dim=self.shape).native()
+
+    @property
+    def real(self) -> 'Tensor':
+        """
+        Returns the real part of this tensor.
+
+        See Also:
+            `phi.math.real()`
+        """
+        from ._ops import real
+        return real(self)
+
+    @property
+    def imag(self) -> 'Tensor':
+        """
+        Returns the imaginary part of this tensor.
+        If this tensor does not store complex numbers, returns a zero tensor with the same shape and dtype as this tensor.
+
+        See Also:
+            `phi.math.imag()`
+        """
+        from ._ops import imag
+        return imag(self)
+
+    @property
+    def available(self) -> bool:
+        """
+        A tensor is available if it stores concrete values and these can currently be read.
+
+        Tracers used inside jit compilation are typically not available.
+
+        See Also:
+            `phi.math.jit_compile()`.
+        """
+        if self._is_tracer:
+            return False
+        natives = self._natives()
+        natives_available = [choose_backend(native).is_available(native) for native in natives]
+        return all(natives_available)
+
+    @property
+    def device(self) -> Union[ComputeDevice, None]:
+        """
+        Returns the `ComputeDevice` that this tensor is allocated on.
+        The device belongs to this tensor's `default_backend`.
+
+        See Also:
+            `Tensor.default_backend`.
+        """
+        natives = self._natives()
+        if not natives:
+            return None
+        return self.default_backend.get_device(natives[0])
+
+    def __int__(self):
+        return int(self.native()) if self.shape.volume == 1 else NotImplemented
+
+    def __float__(self):
+        return float(self.native()) if self.shape.volume == 1 else NotImplemented
+
+    def __complex__(self):
+        return complex(self.native()) if self.shape.volume == 1 else NotImplemented
+
+    def __index__(self):
+        assert self.shape.volume == 1, f"Only scalar tensors can be converted to index but has shape {self.shape}"
+        assert self.dtype.kind == int, f"Only int tensors can be converted to index but dtype is {self.dtype}"
+        return int(self.native())
+
+    def __repr__(self):
+        return format_tensor(self, PrintOptions())
+
+    def _repr_pretty_(self, printer, cycle):
+        printer.text(format_tensor(self, PrintOptions(colors=DEFAULT_COLORS)))
+
+    def __format__(self, format_spec: str):
+        if BROADCAST_FORMATTER.values is not None:
+            return BROADCAST_FORMATTER.register_formatted(self, format_spec)
+        specs = format_spec.split(':')
+        layout_ = 'auto'
+        for possible_layout in ['summary', 'full', 'row', 'numpy']:
+            if possible_layout in specs:
+                assert layout_ == 'auto', f"Two layout identifiers encountered in '{format_spec}'"
+                layout_ = possible_layout
+        include_shape = 'shape' in specs or (False if 'no-shape' in specs else None)
+        include_dtype = 'dtype' in specs or (False if 'no-dtype' in specs else None)
+        color = 'color' in specs or (False if 'no-color' in specs else None)
+        threshold = 8
+        float_format = None
+        for spec in specs:
+            if spec.startswith('threshold='):
+                threshold = int(spec[len('threshold='):])
+            elif '.' in spec:
+                float_format = spec
+        result = format_tensor(self, PrintOptions(layout_, float_format, threshold, color, include_shape, include_dtype))
+        return result
+
+    def __getitem__(self, item) -> 'Tensor':
+        if isinstance(item, Tensor):
+            from ._ops import gather
+            return gather(self, item)
+        item = slicing_dict(self, item)
+        selections = {}
+        sliced = self
+        for dim, selection in item.items():
+            if dim not in self.shape:
+                continue
+            selection = self.shape.prepare_gather(dim, selection)
+            # Either handle slicing directly or add it to the dict
+            if isinstance(selection, (tuple, list)):
+                from ._magic_ops import stack
+                result = [sliced[{dim: i}] for i in selection]
+                stack_dim = sliced.shape[dim].after_gather({dim: selection})
+                sliced = stack(result, stack_dim)
+            elif isinstance(selection, Tensor) and selection.dtype.kind == bool:
+                from ._ops import boolean_mask
+                sliced = boolean_mask(sliced, dim, selection)
+            elif isinstance(selection, Tensor) and selection.dtype.kind == int:
+                from ._ops import gather
+                sliced = gather(sliced, selection, dims=dim)
+            else:
+                selections[dim] = selection
+        return sliced._getitem(selections) if selections else sliced
+
+    def _getitem(self, selection: dict) -> 'Tensor':
+        """
+        Slice the tensor along specified dimensions.
+
+        Args:
+          selection: dim_name: str -> Union[int, slice]
+          selection: dict: 
+
+        Returns:
+
+        """
+        raise NotImplementedError()
+
+    def __setitem__(self, key, value):
+        raise SyntaxError("Tensors are not editable to preserve the autodiff chain. This feature might be added in the future. To update part of a tensor, use math.where() or math.scatter()")
+
+    def flip(self, *dims: str) -> 'Tensor':
+        """
+        Reverses the order of elements along one or multiple dimensions.
+
+        Args:
+            *dims: dimensions to flip
+
+        Returns:
+            `Tensor` of the same `Shape`
+        """
+        raise NotImplementedError()
+
+    def __unstack__(self, dims: Tuple[str, ...]) -> Tuple['Tensor', ...]:  # from phi.math.magic.Sliceable
+        if len(dims) == 1:
+            return self.unstack(dims[0])
+        else:
+            return NotImplemented
+
+    def unstack(self, dim: str):
+        """
+        Splits this tensor along the specified dimension.
+        The returned tensors have the same dimensions as this tensor save the unstacked dimension.
+
+        Raises an error if the dimension is not part of the `Shape` of this `Tensor`.
+
+        See Also:
+            `TensorDim.unstack()`
+
+        Args:
+            dim: name of dimension to unstack
+
+        Returns:
+            tuple of tensors
+
+        """
+        raise NotImplementedError()
+
+    @staticmethod
+    def __stack__(values: tuple, dim: Shape, **_kwargs) -> 'Tensor':
+        from ._ops import stack_tensors
+        return stack_tensors(values, dim)
+
+    def __expand__(self, dims: Shape, **kwargs) -> 'Tensor':
+        return expand_tensor(self, dims)
+
+    @staticmethod
+    def __concat__(values: tuple, dim: str, **kwargs) -> 'Tensor':
+        from ._ops import concat_tensor
+        return concat_tensor(values, dim)
+
+    def __replace_dims__(self, dims: Tuple[str, ...], new_dims: Shape, **kwargs) -> 'Tensor':
+        from ._magic_ops import rename_dims
+        return self._with_shape_replaced(rename_dims(self.shape, dims, new_dims))
+
+    def __unpack_dim__(self, dim: str, unpacked_dims: Shape, **kwargs) -> 'Tensor':
+        if self.shape.is_uniform:
+            native = self.native(self.shape.names)
+            new_shape = self.shape.without(dim)
+            i = self.shape.index(dim)
+            for d in unpacked_dims:
+                new_shape = new_shape._expand(d, pos=i)
+                i += 1
+            native_reshaped = choose_backend(native).reshape(native, new_shape.sizes)
+            return NativeTensor(native_reshaped, new_shape)
+        else:
+            tensors = self._tensors
+            if dim == self._stack_dim.name:
+                for udim in unpacked_dims:
+                    tensors = [TensorStack(tensors[o::len(tensors)//udim.size], udim) for o in range(len(tensors)//udim.size)]
+                assert len(tensors) == 1
+                return tensors[0]
+            raise NotImplementedError
+
+    def __pack_dims__(self, dims: Tuple[str, ...], packed_dim: Shape, pos: Union[int, None], **kwargs) -> 'Tensor':
+        order = self.shape._order_group(dims)
+        if self.shape.is_uniform:
+            native = self.native(order)
+            if pos is None:
+                pos = min(self.shape.indices(dims))
+            new_shape = self.shape.without(dims)._expand(packed_dim.with_sizes([self.shape.only(dims).volume]), pos)
+            native = choose_backend(native).reshape(native, new_shape.sizes)
+            return NativeTensor(native, new_shape)
+        else:
+            from ._ops import concat_tensor
+            from ._magic_ops import pack_dims
+            value = cached(self)
+            assert isinstance(value, TensorStack)
+            assert value._stack_dim.name in dims
+            inner_packed = [pack_dims(t, dims, packed_dim) for t in value._tensors]
+            return concat_tensor(inner_packed, packed_dim.name)
+
+    def __cast__(self, dtype: DType):
+        return self._op1(lambda native: choose_backend(native).cast(native, dtype=dtype))
+
+    def dimension(self, name: Union[str, Shape]) -> 'TensorDim':
+        """
+        Returns a reference to a specific dimension of this tensor.
+        This is equivalent to the syntax `tensor.<name>`.
+
+        The dimension need not be part of the `Tensor.shape` in which case its size is 1.
+
+        Args:
+            name: dimension name
+
+        Returns:
+            `TensorDim` corresponding to a dimension of this tensor
+        """
+        if isinstance(name, str):
+            return TensorDim(self, name)
+        elif isinstance(name, Shape):
+            return TensorDim(self, name.name)
+        else:
+            raise ValueError(name)
+
+    def pack(self, dims, packed_dim):
+        """ See `pack_dims()` """
+        from ._ops import pack_dims
+        return pack_dims(self, dims, packed_dim)
+
+    def unpack(self, dim, unpacked_dims):
+        """ See `unpack_dim()` """
+        from ._ops import unpack_dim
+        return unpack_dim(self, dim, unpacked_dims)
+
+    def __getattr__(self, name):
+        if name.startswith('__'):  # called by hasattr in magic ops
+            raise AttributeError
+        if name.startswith('_'):
+            raise AttributeError(f"'{type(self)}' object has no attribute '{name}'")
+        if name == 'is_tensor_like':  # TensorFlow replaces abs() while tracing and checks for this attribute
+            raise AttributeError(f"'{type(self)}' object has no attribute '{name}'")
+        assert name not in ('shape', '_shape', 'tensor'), name
+        return TensorDim(self, name)
+
+    def __add__(self, other):
+        return self._op2(other, lambda x, y: x + y, lambda x, y: choose_backend(x, y).add(x, y), 'add', '+')
+
+    def __radd__(self, other):
+        return self._op2(other, lambda x, y: y + x, lambda x, y: choose_backend(x, y).add(y, x), 'radd', '+')
+
+    def __sub__(self, other):
+        return self._op2(other, lambda x, y: x - y, lambda x, y: choose_backend(x, y).sub(x, y), 'sub', '-')
+
+    def __rsub__(self, other):
+        return self._op2(other, lambda x, y: y - x, lambda x, y: choose_backend(x, y).sub(y, x), 'rsub', '-')
+
+    def __and__(self, other):
+        return self._op2(other, lambda x, y: x & y, lambda x, y: choose_backend(x, y).and_(x, y), 'and', '&')
+
+    def __rand__(self, other):
+        return self._op2(other, lambda x, y: y & x, lambda x, y: choose_backend(x, y).and_(y, x), 'rand', '&')
+
+    def __or__(self, other):
+        return self._op2(other, lambda x, y: x | y, lambda x, y: choose_backend(x, y).or_(x, y), 'or', '|')
+
+    def __ror__(self, other):
+        return self._op2(other, lambda x, y: y | x, lambda x, y: choose_backend(x, y).or_(y, x), 'ror', '|')
+
+    def __xor__(self, other):
+        return self._op2(other, lambda x, y: x ^ y, lambda x, y: choose_backend(x, y).xor(x, y), 'xor', '^')
+
+    def __rxor__(self, other):
+        return self._op2(other, lambda x, y: y ^ x, lambda x, y: choose_backend(x, y).xor(y, x), 'rxor', '^')
+
+    def __mul__(self, other):
+        return self._op2(other, lambda x, y: x * y, lambda x, y: choose_backend(x, y).mul(x, y), 'mul', '*')
+
+    def __rmul__(self, other):
+        return self._op2(other, lambda x, y: y * x, lambda x, y: choose_backend(x, y).mul(y, x), 'rmul', '*')
+
+    def __truediv__(self, other):
+        return self._op2(other, lambda x, y: x / y, lambda x, y: choose_backend(x, y).div(x, y), 'truediv', '/')
+
+    def __rtruediv__(self, other):
+        return self._op2(other, lambda x, y: y / x, lambda x, y: choose_backend(x, y).div(y, x), 'rtruediv', '/')
+
+    def __divmod__(self, other):
+        return self._op2(other, lambda x, y: divmod(x, y), lambda x, y: divmod(x, y), 'divmod', 'divmod')
+
+    def __rdivmod__(self, other):
+        return self._op2(other, lambda x, y: divmod(y, x), lambda x, y: divmod(y, x), 'rdivmod', 'divmod')
+
+    def __floordiv__(self, other):
+        return self._op2(other, lambda x, y: x // y, lambda x, y: choose_backend(x, y).floordiv(x, y), 'floordiv', '//')
+
+    def __rfloordiv__(self, other):
+        return self._op2(other, lambda x, y: y // x, lambda x, y: choose_backend(x, y).floordiv(y, x), 'rfloordiv', '//')
+
+    def __pow__(self, power, modulo=None):
+        assert modulo is None
+        return self._op2(power, lambda x, y: x ** y, lambda x, y: choose_backend(x, y).pow(x, y), 'pow', '**')
+
+    def __rpow__(self, other):
+        return self._op2(other, lambda x, y: y ** x, lambda x, y: choose_backend(x, y).pow(y, x), 'rpow', '**')
+
+    def __mod__(self, other):
+        return self._op2(other, lambda x, y: x % y, lambda x, y: choose_backend(x, y).mod(x, y), 'mod', '%')
+
+    def __rmod__(self, other):
+        return self._op2(other, lambda x, y: y % x, lambda x, y: choose_backend(x, y).mod(y, x), 'rmod', '%')
+
+    def __eq__(self, other):
+        if _EQUALITY_BY_REF:
+            return wrap(self is other)
+        if other is None:
+            other = float('nan')
+        return self._op2(other, lambda x, y: x == y, lambda x, y: choose_backend(x, y).equal(x, y), 'eq', '==')
+
+    def __ne__(self, other):
+        if _EQUALITY_BY_REF:
+            return wrap(self is not other)
+        if other is None:
+            other = float('nan')
+        return self._op2(other, lambda x, y: x != y, lambda x, y: choose_backend(x, y).not_equal(x, y), 'ne', '!=')
+
+    def __lt__(self, other):
+        return self._op2(other, lambda x, y: x < y, lambda x, y: choose_backend(x, y).greater_than(y, x), 'lt', '<')
+
+    def __le__(self, other):
+        return self._op2(other, lambda x, y: x <= y, lambda x, y: choose_backend(x, y).greater_or_equal(y, x), 'le', '<=')
+
+    def __gt__(self, other):
+        return self._op2(other, lambda x, y: x > y, lambda x, y: choose_backend(x, y).greater_than(x, y), 'gt', '>')
+
+    def __ge__(self, other):
+        return self._op2(other, lambda x, y: x >= y, lambda x, y: choose_backend(x, y).greater_or_equal(x, y), 'ge', '>=')
+
+    def __lshift__(self, other):
+        return self._op2(other, lambda x, y: x << y, lambda x, y: choose_backend(x, y).shift_bits_left(x, y), 'lshift', '<<')
+
+    def __rlshift__(self, other):
+        return self._op2(other, lambda y, x: x << y, lambda y, x: choose_backend(x, y).shift_bits_left(x, y), 'lshift', '<<')
+
+    def __rshift__(self, other):
+        return self._op2(other, lambda x, y: x >> y, lambda x, y: choose_backend(x, y).shift_bits_right(x, y), 'rshift', '>>')
+
+    def __rrshift__(self, other):
+        return self._op2(other, lambda y, x: x >> y, lambda y, x: choose_backend(x, y).shift_bits_right(x, y), 'rshift', '>>')
+
+    def __abs__(self):
+        return self._op1(lambda t: choose_backend(t).abs(t))
+
+    def __round__(self, n=None):
+        return self._op1(lambda t: choose_backend(t).round(t))
+
+    def __copy__(self):
+        return self._op1(lambda t: choose_backend(t).copy(t, only_mutable=True))
+
+    def __deepcopy__(self, memodict={}):
+        return self._op1(lambda t: choose_backend(t).copy(t, only_mutable=False))
+
+    def __neg__(self):
+        return self._op1(lambda t: -t)
+
+    def __invert__(self):
+        return self._op1(lambda t: ~t)
+
+    def __reversed__(self):
+        assert self.shape.channel.rank == 1
+        return self[::-1]
+
+    def __iter__(self):
+        if self.rank == 1:
+            return iter(self.native())
+        elif self.rank == 0:
+            return iter([self.native()])
+        else:
+            from ._ops import reshaped_native
+            native = reshaped_native(self, [self.shape])
+            return iter(native)
+
+    def __matmul__(self, other):
+        assert isinstance(other, Tensor), f"Matmul '@' requires two Tensor arguments but got {type(other)}"
+        dims = batch(**self.shape.dual.untyped_dict).names
+        if not dims:  # this is not a matrix
+            assert self.shape.primal.only(other.shape).is_empty, f"Cannot compute matmul {self.shape} @ {other.shape}. First argument is not a matrix; it has no dual dimensions."
+            return self * other
+        match = other.shape.only(dims, reorder=True)
+        if not match:
+            assert non_batch(other).non_dual.rank == 1, f"Cannot multiply {self.shape} @ {other.shape} because arg2 does not have appropriate non-dual dimensions"
+            match = non_batch(other).non_dual
+        assert len(dims) == match.rank, f"Dual dimensions {dual} do not match shape of second argument {other.shape}"
+        left_arg = pack_dims(self, dual, dual('_reduce')) if len(dims) > 1 else self
+        right_arg = pack_dims(other, match, channel('_reduce'))
+        from ._ops import dot
+        return dot(left_arg, dual, right_arg, '_reduce')
+
+    # def __rmatmul__(self, other):
+
+    def _tensor(self, other) -> 'Tensor':
+        if isinstance(other, Tensor):
+            return other
+        elif isinstance(other, (tuple, list)) and any(isinstance(v, Tensor) for v in other):
+            if 'vector' in self.shape:
+                outer_dim = self.shape['vector']
+            elif self.shape.channel_rank == 1:
+                outer_dim = self.shape.channel
+            else:
+                raise ValueError(f"Cannot combine tensor of shape {self.shape} with tuple {tuple([type(v).__name__ for v in other])}")
+            remaining_shape = self.shape.without(outer_dim)
+            other_items = [v if isinstance(v, Tensor) else compatible_tensor(v, compat_shape=remaining_shape, compat_natives=self._natives(), convert=False) for v in other]
+            other_stacked = stack(other_items, outer_dim, expand_values=True)
+            return other_stacked
+        else:
+            return compatible_tensor(other, compat_shape=self.shape, compat_natives=self._natives(), convert=False)
+
+    def _op1(self, native_function):
+        """
+        Transform the values of this tensor given a function that can be applied to any native tensor.
+
+        Args:
+          native_function:
+
+        Returns:
+
+        """
+        raise NotImplementedError(self.__class__)
+
+    def _op2(self, other, operator: Callable, native_function: Callable, op_name: str = 'unknown', op_symbol: str = '?') -> 'Tensor':
+        """
+        Apply a broadcast operation on two tensors.
+
+        Args:
+            other: second argument
+            operator: function (Tensor, Tensor) -> Tensor, used to propagate the operation to children tensors to have Python choose the callee
+            native_function: function (native tensor, native tensor) -> native tensor
+            op_name: Name of the python function without leading and trailing `__`.
+                Examples: 'add', 'radd', 'sub', 'mul', 'and', 'eq', 'ge'.
+            op_symbol: Operation symbol, such as '+', '-', '&', '%', '>='
+
+        Returns:
+            `Tensor`
+        """
+        raise NotImplementedError(self.__class__)
+
+    def _natives(self) -> tuple:
+        raise NotImplementedError(self.__class__)
+
+    def _spec_dict(self) -> dict:
+        raise NotImplementedError(self.__class__)
+
+    @classmethod
+    def _from_spec_and_natives(cls, spec: dict, natives: list):
+        raise NotImplementedError(cls)
+
+    def _expand(self):
+        """ Expands all compressed tensors to their defined size as if they were being used in `Tensor.native()`. """
+        warnings.warn("Tensor._expand() is deprecated, use cached(Tensor) instead.", DeprecationWarning)
+        raise NotImplementedError(self.__class__)
+
+    def _simplify(self):
+        """ Does not cache this value but if it is already cached, returns the cached version. """
+        return self
+
+
+class TensorDim(BoundDim):
+    """
+    Reference to a specific dimension of a `Tensor`.
+
+    To obtain a `TensorDim`, use `Tensor.dimension()` or the syntax `tensor.<dim>`.
+
+    Indexing a `TensorDim` as `tdim[start:stop:step]` returns a sliced `Tensor`.
+
+    See the documentation at https://tum-pbs.github.io/PhiFlow/Math.html#indexing-slicing-unstacking .
+    """
+
+    def __init__(self, tensor: Tensor, name: str):
+        super().__init__(tensor, name)
+        self.tensor = tensor
+
+    def __len__(self):
+        warnings.warn("Use Tensor.dim.size instead of len(Tensor.dim). len() only supports with integer sizes.", DeprecationWarning)
+        return self.size
+
+    def as_batch(self, name: str = None):
+        """ Returns a shallow copy of the `Tensor` where the type of this dimension is *batch*. """
+        return self._as(BATCH_DIM, name)
+
+    def as_spatial(self, name: str = None):
+        """ Returns a shallow copy of the `Tensor` where the type of this dimension is *spatial*. """
+        return self._as(SPATIAL_DIM, name)
+
+    def as_channel(self, name: str = None):
+        """ Returns a shallow copy of the `Tensor` where the type of this dimension is *channel*. """
+        return self._as(CHANNEL_DIM, name)
+
+    def as_instance(self, name: str = None):
+        """ Returns a shallow copy of the `Tensor` where the type of this dimension is *instance*. """
+        return self._as(INSTANCE_DIM, name)
+
+    def as_type(self, dim_type: Union[Callable, str]):
+        return self._as(dim_type('d').type if callable(dim_type) else dim_type, None)
+
+    def _as(self, dim_type: str, name: Union[str, None]):
+        if not self.exists:
+            return self.tensor
+        shape = self.tensor.shape
+        new_types = list(shape.types)
+        new_types[shape.index(self.name)] = dim_type
+        new_names = shape.names
+        if name is not None:
+            new_names = list(new_names)
+            new_names[shape.index(self.name)] = name
+        new_shape = Shape(shape.sizes, tuple(new_names), tuple(new_types), shape.item_names)
+        return self.tensor._with_shape_replaced(new_shape)
+
+    @property
+    def index(self):
+        return self.tensor.shape.index(self.name)
+
+    def flip(self):
+        """ Flips the element order along this dimension and returns the result as a `Tensor`. """
+        warnings.warn("dim.flip() is deprecated. Use dim[::-1] instead", DeprecationWarning, stacklevel=2)
+        return self.tensor.flip(self.name)
+
+    def split(self, split_dimensions: Shape):
+        """ See `phi.math.unpack_dim()` """
+        warnings.warn("dim.split() is deprecated. Use math.split_dims() instead.", stacklevel=2)
+        from ._magic_ops import unpack_dim
+        return unpack_dim(self.tensor, self.name, split_dimensions)
+
+    def __mul__(self, other):
+        from ._ops import dot
+        if isinstance(other, BoundDim):
+            return dot(self.obj, (self.name,), other.obj, (other.name,))
+        if isinstance(other, (tuple, list)):
+            other = wrap(other, self.obj.shape[self.name])
+        if isinstance(other, Tensor):
+            assert self.name in other.shape, f"Canno reduce '{self.name}' of tensor with shape {self.obj.shape} against tensor with shape {other.shape}. Dimension must be present on both tensors."
+            return dot(self.tensor, (self.name,), other, (self.name,))
+        else:
+            return NotImplemented
+
+    __rmul__ = __matmul__ = __rmatmul__ = __mul__
+
+    def sum(self):
+        from ._ops import sum_
+        return sum_(self.tensor, self.name)
+
+    def prod(self):
+        from ._ops import prod
+        return prod(self.tensor, self.name)
+
+
+_EQUALITY_BY_REF = []
+
+
+@contextmanager
+def equality_by_ref():
+    """
+    Enables Tensor.__bool__
+    """
+    _EQUALITY_BY_REF.append(True)
+    try:
+        yield None
+    finally:
+        _EQUALITY_BY_REF.pop(-1)
+
+
+class Layout(Tensor):
+    """
+    Tensor representation of a PyTree consisting of only lists, tuples and leaves.
+    Leaves can be any Python object or primitive, including tuples and lists.
+    The PyTree may be deeper but only the outer `shape.rank` levels are represented as a tensor.
+    """
+
+    def __init__(self, obj, shape: Shape):
+        self._obj = obj
+        self._shape = shape
+
+    @property
+    def shape(self) -> Shape:
+        return self._shape
+
+    @property
+    def dtype(self) -> DType:
+        if isinstance(self._obj, bool):
+            return DType(bool)
+        if isinstance(self._obj, int):
+            return DType(int, 64)
+        elif isinstance(self._obj, (float, complex)):
+            return DType(type(self._obj), precision=64)
+        else:
+            return DType(object)
+
+    @property
+    def default_backend(self):
+        return None
+
+    def native(self, order: Union[str, tuple, list, Shape] = None, singleton_for_const=False):
+        order = parse_dim_order(order)
+        assert order is None or order == self._shape.names, "Layout.native() does not allow for changing the dimension order"
+        return self._obj
+
+    def numpy(self, order: Union[str, tuple, list, Shape] = None) -> np.ndarray:
+        native = self.native(order=order)
+        return numpy.asarray(native)
+
+    def _getitem(self, selection: dict) -> 'Tensor':
+        selection_list = [selection.get(dim, None) for dim in self._shape.names]
+        native = self._getitem_recursive(self._obj, tuple(selection_list))
+        new_shape = self._shape.after_gather(selection)
+        return Layout(native, new_shape)
+
+    def __repr__(self):
+        return repr(self._obj)
+
+    def __format__(self, format_spec):
+        if BROADCAST_FORMATTER.values is not None:
+            return BROADCAST_FORMATTER.register_formatted(self, format_spec)
+        return repr(self._obj)
+
+    def unstack(self, dimension: str):
+        if dimension == self._shape.names[0]:
+            native = tuple(self._obj.values()) if isinstance(self._obj, dict) else self._obj
+            inner_shape = self._shape[1:]
+            return tuple([Layout(n, inner_shape) for n in native])
+        else:
+            raise NotImplementedError()
+
+    @staticmethod
+    def _getitem_recursive(native, selection: tuple):
+        if not selection:
+            return native
+        native = tuple(native.values()) if isinstance(native, dict) else native
+        if len(selection) == 1:
+            return native if selection[0] is None else native[selection[0]]
+        else:
+            if selection[0] is None:
+                return type(native)([Layout._getitem_recursive(n, selection[1:]) for n in native])
+            if isinstance(selection[0], int):
+                return Layout._getitem_recursive(native[selection[0]], selection[1:])
+            elif isinstance(selection[0], slice):
+                subset = native[selection[0]]
+                return type(subset)([Layout._getitem_recursive(n, selection[1:]) for n in subset])
+            else:
+                raise ValueError(f"Illegal selection: {selection}")
+
+    def _as_list(self):
+        return self._as_list_recursive(self._obj, self._shape.rank, [])
+
+    @staticmethod
+    def _as_list_recursive(native, dims: int, result: list):
+        if dims == 0:
+            result.append(native)
+        else:
+            native = tuple(native.values()) if isinstance(native, dict) else native
+            for n in native:
+                Layout._as_list_recursive(n, dims - 1, result)
+        return result
+
+    @property
+    def _is_tracer(self) -> bool:
+        return False
+
+    def __bool__(self):
+        assert self.rank == 0, f"Cannot convert tensor with non-empty shape {self.shape} to bool. Use tensor.any or tensor.all instead."
+        return bool(self._obj)
+
+    def __stack__(self, values: tuple, dim: Shape, **kwargs) -> 'Layout':
+        obj = [v.native(self._shape) for v in values]
+        new_shape = concat_shapes(dim, self._shape)
+        return Layout(obj, new_shape)
+
+    @staticmethod
+    def __concat__(values: tuple, dim: str, **kwargs) -> 'Shapable':
+        return NotImplemented
+
+    def __flatten__(self, flat_dim: Shape, flatten_batch: bool):
+        if not flatten_batch and self._shape.batch:
+            raise NotImplementedError
+        return layout(self._as_list(), flat_dim)
+
+    def __expand__(self, dims: Shape, **kwargs) -> 'Tensor':
+        new_dims = dims.without(self._shape)
+        if not new_dims:
+            return self
+        obj = self._obj
+        for dim in reversed(new_dims):
+            assert isinstance(dim.size, int), "Can only expand layouts by integer-sized dimensions"
+            obj = [obj] * dim.size
+        return Layout(obj, concat_shapes(new_dims, self._shape))
+
+    def __replace_dims__(self, dims: Tuple[str, ...], new_dims: Shape, **kwargs) -> 'Tensor':
+        new_shape = self._shape.replace(dims, new_dims)
+        return Layout(self._obj, new_shape)
+
+    def __pack_dims__(self, dims: Tuple[str, ...], packed_dim: Shape, pos: Union[int, None], **kwargs) -> 'Layout':
+        if dims == self.shape.names:
+            native = self._as_list()
+            return Layout(native, packed_dim.with_size(len(native)))
+        else:
+            obj = []
+            for i in self._shape.only(dims, reorder=True).meshgrid():
+                obj.append(self[i].native())
+            return Layout(obj, concat_shapes(packed_dim.with_size(self.shape.only(dims).volume), self._shape.without(dims)))
+
+    def __unpack_dim__(self, dim: str, unpacked_dims: Shape, **kwargs) -> 'Layout':
+        return NotImplemented
+
+    def __cast__(self, dtype: DType):
+        obj = self._recursive_cast(self._obj, self._shape, dtype)
+        return Layout(obj, self._shape)
+
+    def __copy__(self):
+        return Layout(self._obj, self._shape)
+
+    def __iter__(self):
+        if self.rank == 1:
+            return iter(self._obj)
+        elif self.rank == 0:
+            return iter([self._obj])
+        else:
+            return iter(self._as_list())
+
+    def __eq__(self, other):
+        if _EQUALITY_BY_REF:
+            return wrap(self is other)
+        return self._op2(other, lambda x, y: x == y, lambda x, y: x == y, 'eq', '==')
+
+    def __ne__(self, other):
+        if _EQUALITY_BY_REF:
+            return wrap(self is not other)
+        return self._op2(other, lambda x, y: x != y, lambda x, y: x != y, 'ne', '!=')
+    
+    def _assert_close(self, other: Tensor, rel_tolerance: float, abs_tolerance: float, msg: str, verbose: bool):
+        from ._ops import assert_close
+        inner_test = lambda x, y: assert_close(x, y, rel_tolerance=rel_tolerance, abs_tolerance=abs_tolerance, msg=msg, verbose=verbose)
+        return self._op2(other, inner_test, inner_test, 'assert_close', '≈')
+
+    def _op2(self, other, operator: Callable, native_function: Callable, op_name: str = 'unknown', op_symbol: str = '?') -> Tensor:
+        obj = self._recursive_op2(self._obj, self._shape, other, operator, native_function, op_name)
+        new_shape = concat_shapes(self._shape, other.shape.without(self._shape)) if isinstance(other, Tensor) else self._shape
+        return Layout(obj, new_shape)
+
+    @staticmethod
+    def _recursive_op2(obj, shape: Shape, other, operator, native_function, op_name):
+        if shape:
+            dim = shape.names[0]
+            if isinstance(other, Tensor) and dim in other.shape:
+                assert other.shape.get_size(dim) == len(obj), f"Shape mismatch during {op_name}: '{dim}' has size {len(obj)} on layout but {other.shape.get_size(dim)} on other tensor."
+                others = [other[{dim: i}] for i in range(len(obj))]
+            else:
+                others = [other] * len(obj)
+            if isinstance(obj, (tuple, list)):
+                return type(obj)([Layout._recursive_op2(i, shape[1:], o, operator, native_function, op_name) for i, o in zip(obj, others)])
+            elif isinstance(obj, dict):
+                return {k: Layout._recursive_op2(v, shape[1:], o, operator, native_function, op_name) for (k, v), o in zip(obj.items(), others)}
+        else:  # leaf
+            if isinstance(other, Layout) and not other.shape:
+                return native_function(obj, other.native())
+            if isinstance(other, Tensor):
+                return operator(obj, other)
+            else:
+                return native_function(obj, other)
+
+    def _op1(self, native_function):
+        return Layout(self._recursive_op1(self._obj, self._shape, native_function), self._shape)
+
+    @staticmethod
+    def _recursive_op1(obj, shape: Shape, native_function):
+        raise NotImplementedError
+        # if shape:
+        #     if isinstance(obj, (tuple, list)):
+        #         return type(obj)([Layout._recursive_op1(i, shape[1:], native_function) for i in obj])
+        #     else:
+        # else:
+        #     return native_function(obj)
+
+    @staticmethod
+    def _recursive_cast(obj, shape: Shape, dtype: DType):
+        if shape:
+            if isinstance(obj, (tuple, list)):
+                return type(obj)([Layout._recursive_cast(i, shape[1:], dtype) for i in obj])
+            elif isinstance(obj, dict):
+                return {k: Layout._recursive_cast(v, shape[1:], dtype) for k, v in obj.items()}
+            elif isinstance(obj, Tensor):
+                assert obj.shape == shape
+                from ._ops import cast
+                return cast(obj, dtype)
+            else:
+                raise ValueError(obj)
+        else:
+            return dtype.kind(obj)
+
+
+class NativeTensor(Tensor):
+    """
+    Tensor backed by a (possibly lower-rank) backend-specific tensor.
+    The dimension names and types corresponding to the native tensor are stored in _native_shape.
+    The property _shape can contain additional dimensions along which the tensor is constant.
+    """
+
+    def __init__(self, native_tensor, native_shape: Shape, expanded_shape: Shape = None):
+        expanded_shape = native_shape if expanded_shape is None else expanded_shape
+        if DEBUG_CHECKS:
+            expanded_shape._check_is_valid_tensor_shape()
+            backend = choose_backend(native_tensor)
+            assert native_shape.is_uniform
+            assert expanded_shape.is_uniform
+            assert backend.staticshape(native_tensor) == native_shape.sizes, f"Shape {native_shape} does not match native tensor with shape {backend.staticshape(native_tensor)}"
+            assert native_shape in expanded_shape
+        self._native = native_tensor
+        self._shape = expanded_shape
+        self._native_shape = native_shape
+
+    def native(self, order: Union[str, tuple, list, Shape] = None, singleton_for_const=False):
+        order = parse_dim_order(order, check_rank=self.rank)
+        order = self._shape.names if order is None else order
+        assert isinstance(order, tuple)  # should not be necessary
+        assert all([n in order for n in self._native_shape.names]), f"order must list all essential dimensions but got {order} for tensor {self.shape}"
+        backend = self.default_backend
+        if order == self._native_shape.names:
+            if self.dtype.precision in [None, get_precision()]:
+                return self._native
+            else:
+                return backend.cast(self._native, DType(self.dtype.kind, precision=get_precision()))
+        # --- Transpose ---
+        perm = self._native_shape.only(order, reorder=False)._perm(self._native_shape.only(order, reorder=True).names)
+        if perm != list(range(len(perm))):
+            transposed = backend.transpose(self._native, perm)  # this will cast automatically
+        else:
+            transposed = backend.as_tensor(self._native)
+        if len(order) == len(perm):
+            return transposed  # nothing to expand
+        # --- Expand ---
+        slices = [slice(None) if dim in self._native_shape else None for dim in order]
+        expanded = transposed[tuple(slices)]
+        if not singleton_for_const:
+            multiples = [self._shape.get_size(dim) if dim in self._shape and dim not in self._native_shape else 1 for dim in order]
+            expanded = backend.tile(expanded, multiples)
+        return expanded
+
+    def _cache(self):
+        if self._shape == self._native_shape:
+            return
+        self._native = self.native(order=self._shape)
+        self._native_shape = self._shape
+
+    def _cached(self, dims: Shape = None) -> 'NativeTensor':
+        if dims is None or self._shape in (dims & self._native_shape):
+            return NativeTensor(self.native(order=self._shape), self._shape, self._shape)
+        else:
+            new_native_shape = dims & self._native_shape
+            tmp_tensor = NativeTensor(self._native, self._native_shape, new_native_shape)
+            return NativeTensor(tmp_tensor.native(new_native_shape), new_native_shape, self._shape)
+
+    @property
+    def collapsed_dims(self):
+        return self._shape.without(self._native_shape)
+
+    @property
+    def dtype(self):
+        return choose_backend(self._native).dtype(self._native)
+
+    @property
+    def shape(self):
+        return self._shape
+
+    @property
+    def default_backend(self) -> Backend:
+        return choose_backend(self._native)
+
+    def _with_shape_replaced(self, new_shape):
+        if new_shape.rank != self._shape.rank:
+            raise IncompatibleShapes(f"Tensor {self} is not compatible with shape {new_shape}", self._shape, new_shape)
+        new_shape = Shape(self._shape.sizes, new_shape.names, new_shape.types, new_shape.item_names)
+        native_indices = self._shape.indices(self._native_shape)
+        new_native_shape = new_shape[native_indices]
+        return NativeTensor(self._native, new_native_shape, new_shape)
+
+    def _with_natives_replaced(self, natives: list):
+        native = natives.pop(0)
+        new_native_shape = self._native_shape.with_sizes(choose_backend(native).shape(native))
+        new_shape = self._shape.with_sizes(new_native_shape)
+        return NativeTensor(native, new_native_shape, new_shape)
+
+    @property
+    def _is_tracer(self) -> bool:
+        return False
+
+    def _to_dict(self):
+        result = self.shape._to_dict(include_sizes=False)
+        if self.rank == 0:
+            result['data'] = self.numpy().item()
+        else:
+            result['data'] = self.numpy(self._shape).tolist()  # works for all 1+ dimensional arrays
+        return result
+
+    def _getitem(self, selection: dict):
+        if not selection:
+            return self
+        selections = [slice(None)] * self._native_shape.rank
+        for name, sel in selection.items():
+            if name in self._native_shape:
+                selections[self._native_shape.index(name)] = sel
+            elif name not in self._shape:
+                assert isinstance(sel, int), f"Attempting slice missing dimension {name} with {selection}"
+        gathered = self.default_backend.multi_slice(self._native, tuple(selections)) if selections else self._native
+        new_native_shape = self._native_shape.after_gather(selection)
+        new_shape = self._shape.after_gather(selection)
+        return NativeTensor(gathered, new_native_shape, new_shape)
+
+    def flip(self, *dims: str) -> 'Tensor':
+        native_dims = [dim for dim in dims if dim in self._native_shape]
+        native = self.default_backend.flip(self._native, self._native_shape.indices(native_dims))
+        return NativeTensor(native, self._native_shape.flipped(native_dims), self._shape.flipped(dims))
+
+    def unstack(self, dim):
+        new_shape = self._shape.without(dim)
+        new_native_shape = self._native_shape.without(dim)
+        if dim in self._native_shape:
+            tensors = self.default_backend.unstack(self._native, axis=self._native_shape.index(dim))
+            return tuple([NativeTensor(t, new_native_shape, new_shape) for t in tensors])
+        else:
+            assert dim in self._shape, f"Cannot unstack tensor {self._shape} along non-existant dimension '{dim}'"
+            return (NativeTensor(self._native, new_native_shape, new_shape),) * self._shape.get_size(dim)
+
+    def _op1(self, native_function):
+        native = native_function(self._native)
+        return NativeTensor(native, self._native_shape, self._shape) if native is not None else self
+
+    def _op2(self, other, operator, native_function, op_name: str = 'unknown', op_symbol: str = '?', switch_args=False):
+        try:
+            other_tensor = self._tensor(other)
+            was_converted = not isinstance(other, Tensor)
+        except NoBackendFound:
+            return NotImplemented
+        if not isinstance(other_tensor, NativeTensor) and not was_converted:
+            return NotImplemented
+        if not isinstance(other_tensor, NativeTensor):
+            other_tensor = NativeTensor(other_tensor.native(other_tensor.shape), other_tensor.shape, other_tensor.shape)
+        broadcast_shape = self._native_shape & other_tensor._native_shape
+        natives = [t.native(order=broadcast_shape, singleton_for_const=True) if t.rank > 0 else t.native() for t in [self, other_tensor]]
+        if switch_args:
+            natives = natives[::-1]
+        result_tensor = native_function(*natives)
+        return NativeTensor(result_tensor, broadcast_shape, self._shape & other_tensor._shape)
+
+    def _natives(self) -> tuple:
+        return self._native,
+
+    def _spec_dict(self) -> dict:
+        return {'type': NativeTensor, 'native_shape': self._native_shape, 'shape': self._shape}
+
+    @classmethod
+    def _from_spec_and_natives(cls, spec: dict, natives: list):
+        return NativeTensor(natives.pop(0), spec['native_shape'], spec['shape'])
+
+    def _expand(self):
+        self._cache()
+
+
+class TensorStack(Tensor):
+    """
+    Implicit stack of multiple tensors.
+    List of tensors, does not store stacked tensor in memory.
+
+    Args:
+
+    Returns:
+
+    """
+
+    def __init__(self, components: Union[tuple, list], stack_dim: Shape):
+        assert isinstance(stack_dim, Shape) and stack_dim.rank == 1, f"stack_dim must be a single-dimension Shape object but got {type(stack_dim)}"
+        # assert len(components) > 1, "Use a CollapsedTensor instead"
+        for t in components:
+            assert isinstance(t, Tensor)
+            assert stack_dim.name not in t.shape, f"Cannot stack along '{stack_dim.name}' because the dimension already exists."
+        self._tensors = tuple(components)
+        self._stack_dim = stack_dim.with_sizes([len(components)], keep_item_names=True)
+        try:
+            merge_shapes(*self._tensors)
+            self._varying_shapes = False
+        except IncompatibleShapes:
+            self._varying_shapes = True
+        self._shape = shape_stack(self._stack_dim, *[t.shape for t in self._tensors])
+        self._cached = None
+
+    @property
+    def _is_tracer(self) -> bool:
+        return any([t._is_tracer for t in self._tensors])
+
+    @property
+    def requires_broadcast(self):
+        return self._varying_shapes or not self._shape.well_defined or self._is_tracer or self._tensors[0].shape.is_non_uniform
+    
+    @property
+    def stack_dim(self):
+        warnings.warn("TensorStack.stack_dim is deprecated", DeprecationWarning, stacklevel=2)
+        return self._stack_dim
+
+    def _cache(self):
+        if self._cached is None:
+            if self.requires_broadcast:
+                return None
+            elif all([t.shape.is_uniform for t in self._tensors]):
+                natives = [t.native(order=self._shape.names) for t in self._tensors]
+                native = choose_backend(*natives).concat(natives, axis=self.shape.index(self._stack_dim.name))
+                self._cached = NativeTensor(native, self._shape)
+            else:  # cache stack_dim on inner tensors
+                non_uniform_dim = self._tensors[0].shape.shape.without('dims')
+                if len(non_uniform_dim) > 1:
+                    raise NotImplementedError
+                unstacked = [t.unstack(non_uniform_dim.name) for t in self._tensors]
+                stacked = []
+                for to_stack in zip(*unstacked):
+                    tensor = TensorStack(to_stack, self._stack_dim)._cache()
+                    stacked.append(tensor)
+                self._cached = TensorStack(stacked, non_uniform_dim)
+        return self._cached
+
+    @property
+    def dtype(self):
+        return combine_types(*[t.dtype for t in self._tensors])
+
+    @property
+    def shape(self):
+        return self._shape
+
+    def native(self, order: Union[str, tuple, list, Shape] = None, singleton_for_const=False):
+        if self._cached is not None:
+            return self._cached.native(order=order)
+        else:
+            order = parse_dim_order(order, check_rank=self.rank)
+            # Is only the stack dimension shifted?
+            if order is not None and self._shape.without(self._stack_dim).names == tuple(filter(lambda name: name != self._stack_dim.name, order)):
+                inner_order = [dim for dim in order if dim != self._stack_dim.name]
+                natives = [t.native(inner_order) for t in self._tensors]
+                assert self._stack_dim.name in order, f"Dimension {self._stack_dim} missing from 'order'. Got {order} but tensor has shape {self.shape}."
+                native = choose_backend(*natives).stack(natives, axis=order.index(self._stack_dim.name))
+                return native
+            assert not self.shape.is_non_uniform, f"Cannot convert non-uniform tensor with shape {self.shape} to native tensor."
+            return self._cache().native(order=order)
+
+    def _with_shape_replaced(self, new_shape: Shape):
+        if self._cached is not None:
+            return self._cached._with_shape_replaced(new_shape)
+        else:
+            new_stack_dim = new_shape[self._shape.index(self._stack_dim.name)]
+            new_tensors = []
+            for t in self._tensors:
+                inner_indices = [self.shape.index(d) for d in t.shape.names]
+                new_inner_shape = new_shape[inner_indices]
+                new_tensors.append(t._with_shape_replaced(new_inner_shape))
+            return TensorStack(new_tensors, new_stack_dim)
+
+    def _getitem(self, selection: dict):
+        if self._cached is not None:
+            return self._cached._getitem(selection)
+        if (self._stack_dim.name not in selection or len(selection) != 1) and not self.requires_broadcast:
+            return self._cache()._getitem(selection)
+        # --- Inner dims ---
+        inner_dict = {dim: sel for dim, sel in selection.items() if dim != self._stack_dim.name}
+        tensors = self._tensors
+        if len(inner_dict) > 0:
+            tensors = [t[inner_dict] for t in tensors]
+        # --- stack dimension ---
+        if self._stack_dim.name in selection:
+            selection = selection[self._stack_dim.name]
+            if isinstance(selection, int):
+                return tensors[selection]
+            elif isinstance(selection, slice):
+                return TensorStack(tensors[selection], self._stack_dim)
+            else:
+                raise NotImplementedError(f"{type(selection)} not supported. Only (int, slice) allwoed")
+        else:
+            return TensorStack(tensors, self._stack_dim)
+
+    def flip(self, *dims: str) -> 'Tensor':
+        if self._cached is not None:
+            return self._cached.flip(*dims)
+        else:
+            tensors = [t.flip(*dims) for t in self._tensors]
+            if self._stack_dim.name in dims:
+                tensors = tensors[::-1]
+            return TensorStack(tensors, self._stack_dim)
+
+    def unstack(self, dimension):
+        if self._cached is not None:
+            return self._cached.unstack(dimension)
+        if dimension == self._stack_dim.name:
+            return self._tensors
+        else:
+            if self.requires_broadcast:
+                unstacked = [t.unstack(dimension) for t in self._tensors]
+                return tuple([TensorStack(items, self._stack_dim) for items in zip(*unstacked)])
+            else:
+                return self._cache().unstack(dimension)
+
+    def _op1(self, native_function):
+        if self.requires_broadcast:
+            tensors = [t._op1(native_function) for t in self._tensors]
+            return TensorStack(tensors, self._stack_dim)
+        else:
+            return self._cache()._op1(native_function)
+
+    def _op2(self, other, operator, native_function, op_name: str = 'unknown', op_symbol: str = '?'):
+        other = self._tensor(other)
+        if self.requires_broadcast:
+            if self._stack_dim.name in other.shape:
+                other = other.unstack(self._stack_dim.name)
+                tensors = [operator(t1, t2) for t1, t2 in zip(self._tensors, other)]
+            else:
+                tensors = [operator(t, other) for t in self._tensors]
+            return TensorStack(tensors, self._stack_dim)
+        elif isinstance(other, NativeTensor) or (isinstance(other, TensorStack) and not other.requires_broadcast):
+            new_shape, (native1, native2) = broadcastable_native_tensors(self, other)  # ToDo we don't have to expand all
+            result_tensor = native_function(native1, native2)
+            return NativeTensor(result_tensor, new_shape, new_shape)
+        else:
+            return NotImplemented
+
+    def _natives(self) -> tuple:
+        if self._cached is not None:
+            return self._cached._natives()
+        else:
+            return sum([t._natives() for t in self._tensors], ())
+
+    def _spec_dict(self) -> dict:
+        if self._cached is not None:
+            return self._cached._spec_dict()
+        else:
+            return {'type': TensorStack, 'stack_dim': self._stack_dim, 'tensors': [t._spec_dict() for t in self._tensors]}
+
+    @classmethod
+    def _from_spec_and_natives(cls, spec: dict, natives: list):
+        tensors = [t['type']._from_spec_and_natives(t, natives) for t in spec['tensors']]
+        return TensorStack(tensors, spec['stack_dim'])
+
+    def _with_natives_replaced(self, natives: list):
+        if self._cached is not None:
+            return self._cached._with_natives_replaced(natives)
+        else:
+            tensors = [t._with_natives_replaced(natives) for t in self._tensors]
+            return TensorStack(tensors, self._stack_dim)
+
+    def _expand(self):
+        if self.requires_broadcast:
+            for t in self._tensors:
+                t._expand()
+        self._cache()
+
+    @property
+    def is_cached(self):
+        return self._cached is not None
+
+    def _simplify(self):
+        if self.is_cached:
+            return self._cached
+        else:
+            return self
+
+
+def tensor(data,
+           *shape: Shape,
+           convert: bool = True,
+           default_list_dim=channel('vector')) -> Tensor:  # TODO assume convert_unsupported, add convert_external=False for constants
+    """
+    Create a Tensor from the specified `data`.
+    If `convert=True`, converts `data` to the preferred format of the default backend.
+
+    `data` must be one of the following:
+    
+    * Number: returns a dimensionless Tensor.
+    * Native tensor such as NumPy array, TensorFlow tensor or PyTorch tensor.
+    * `tuple` or `list` of numbers: backs the Tensor with native tensor.
+    * `tuple` or `list` of non-numbers: creates tensors for the items and stacks them.
+    * Tensor: renames dimensions and dimension types if `names` is specified. Converts all internal native values of the tensor if `convert=True`.
+    * Shape: creates a 1D tensor listing the dimension sizes.
+    
+    While specifying `names` is optional in some cases, it is recommended to always specify them.
+    
+    Dimension types are always inferred from the dimension names if specified.
+
+    Implementations:
+
+    * NumPy: [`numpy.array`](https://numpy.org/doc/stable/reference/generated/numpy.array.html)
+    * PyTorch: [`torch.tensor`](https://pytorch.org/docs/stable/generated/torch.tensor.html), [`torch.from_numpy`](https://pytorch.org/docs/stable/generated/torch.from_numpy.html)
+    * TensorFlow: [`tf.convert_to_tensor`](https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor)
+    * Jax: [`jax.numpy.array`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.array.html)
+
+    See Also:
+        `phi.math.wrap()` which uses `convert=False`, `layout()`.
+
+    Args:
+        data: native tensor, scalar, sequence, Shape or Tensor
+        shape: Ordered dimensions and types. If sizes are defined, they will be checked against `data`.`
+        convert: If True, converts the data to the native format of the current default backend.
+            If False, wraps the data in a `Tensor` but keeps the given data reference if possible.
+
+    Raises:
+        AssertionError: if dimension names are not provided and cannot automatically be inferred
+        ValueError: if `data` is not tensor-like
+
+    Returns:
+        Tensor containing same values as data
+
+    Examples:
+        >>> tensor([1, 2, 3], channel(vector='x,y,z'))
+        (x=1, y=2, z=3)
+
+        >>> tensor([1., 2, 3], channel(vector='x,y,z'))
+        (x=1.000, y=2.000, z=3.000) float64
+
+        >>> tensor(numpy.zeros([10, 8, 6, 2]), batch('batch'), spatial('x,y'), channel(vector='x,y'))
+        (batchᵇ=10, xˢ=8, yˢ=6, vectorᶜ=x,y) float64 const 0.0
+
+        >>> tensor([(0, 1), (0, 2), (1, 3)], instance('particles'), channel(vector='x,y'))
+        (x=0, y=1); (x=0, y=2); (x=1, y=3) (particlesⁱ=3, vectorᶜ=x,y)
+
+        >>> tensor(numpy.random.randn(10))
+        (vectorᶜ=10) float64 -0.128 ± 1.197 (-2e+00...2e+00)
+    """
+    assert all(isinstance(s, Shape) for s in shape), f"Cannot create tensor because shape needs to be one or multiple Shape instances but got {shape}"
+    shape = None if len(shape) == 0 else concat_shapes(*shape)
+    if isinstance(data, Tensor):
+        if convert:
+            backend = data.default_backend
+            if backend != default_backend():
+                data = data._op1(lambda n: convert_(n, use_dlpack=False))
+        if shape is None:
+            return data
+        else:
+            if None in shape.sizes:
+                shape = shape.with_sizes(data.shape.sizes)
+            return data._with_shape_replaced(shape)
+    elif isinstance(data, Shape):
+        if shape is None:
+            shape = channel('dims')
+        else:
+            assert shape.rank == 1, "Can only convert 1D shapes to Tensors"
+        shape = shape.with_size(data.names)
+        data = data.sizes
+    elif isinstance(data, str) or data is None:
+        return layout(data)
+    elif isinstance(data, (numbers.Number, bool)):
+        assert not shape, f"Trying to create a zero-dimensional Tensor from value '{data}' but shape={shape}"
+        if convert:
+            data = default_backend().as_tensor(data, convert_external=True)
+        return NativeTensor(data, EMPTY_SHAPE)
+    if isinstance(data, (tuple, list)):
+        if all(isinstance(d, (bool, int, float, complex)) for d in data):
+            array = np.array(data)
+            assert array.dtype != object
+            data = array
+        elif all(isinstance(d, str) for d in data):
+            return layout(data, shape or default_list_dim)
+        else:
+            try:
+                inner_shape = [] if shape is None else [shape[1:]]
+                tensors = [d if isinstance(d, Tensor) else tensor(d, *inner_shape, convert=convert) for d in data]
+                return stack(tensors, default_list_dim if shape is None else shape[0].with_sizes([len(tensors)]), expand_values=True)
+            except IncompatibleShapes:
+                assert not convert, f"Cannot convert {data} to tensor given shape {shape}"
+                return layout(data, shape or default_list_dim)
+            except ValueError:
+                assert not convert, f"Cannot convert {data} to tensor"
+                return layout(data, shape or default_list_dim)
+    try:
+        backend = choose_backend(data)
+        if shape is None:
+            assert backend.ndims(data) <= 1, "Specify dimension names for tensors with more than 1 dimension"
+            shape = default_list_dim if backend.ndims(data) == 1 else EMPTY_SHAPE
+            shape = shape.with_sizes(backend.staticshape(data))
+        else:
+            # fill in sizes or check them
+            sizes = backend.staticshape(data)
+            if len(sizes) != len(shape):
+                raise IncompatibleShapes(f"Rank of given shape {shape} does not match data with sizes {sizes}")
+            for size, s in zip(sizes, shape.sizes):
+                if s is not None:
+                    assert s == size, f"Given shape {shape} does not match data with sizes {sizes}. Consider leaving the sizes undefined."
+            shape = shape.with_sizes(sizes, keep_item_names=True)
+        if convert:
+            data = convert_(data, use_dlpack=False)
+        return NativeTensor(data, shape)
+    except NoBackendFound:
+        raise ValueError(f"{type(data)} is not supported. Only (Tensor, tuple, list, np.ndarray, native tensors) are allowed.\nCurrent backends: {BACKENDS}")
+
+
+def wrap(data,
+         *shape: Shape) -> Tensor:
+    """ Short for `phi.math.tensor()` with `convert=False`. """
+    return tensor(data, *shape, convert=False)  # TODO inline, simplify
+
+
+def layout(objects, *shape: Shape) -> Tensor:
+    """
+    Wraps a Python tree in a `Tensor`, allowing elements to be accessed via dimensions.
+    A python tree is a structure of nested `tuple`, `list`, `dict` and *leaf* objects where leaves can be any Python object.
+
+    All keys of `dict` containers must be of type `str`.
+    The keys are automatically assigned as item names along that dimension unless conflicting with other elements.
+
+    Strings may also be used as containers.
+
+    Example:
+    >>> t = layout({'a': 'text', 'b': [0, 1]}, channel('dict,inner'))
+    >>> t.inner[1].dict['a'].native()
+    'e'
+
+    See Also:
+        `tensor()`, `wrap()`.
+
+    Args:
+        objects: PyTree of `list` or `tuple`.
+        *shape: Tensor dimensions
+
+    Returns:
+        `Tensor`.
+        Calling `Tensor.native()` on the returned tensor will return `objects`.
+    """
+    assert all(isinstance(s, Shape) for s in shape), f"shape needs to be one or multiple Shape instances but got {shape}"
+    shape = EMPTY_SHAPE if len(shape) == 0 else concat_shapes(*shape)
+    if isinstance(objects, Layout):
+        assert objects.shape == shape
+        return objects
+
+    if not shape.well_defined:
+
+        def recursive_determine_shape(native, shape: Shape):
+            if not shape:
+                return shape
+            if isinstance(native, dict):
+                assert all([isinstance(k, str) for k in native.keys()]), f"All dict keys in PyTrees must be str but got {tuple(native.keys())}"
+                shape = shape.replace(shape[0], shape[0].with_size(tuple(native.keys())))
+            if shape.rank == 1:
+                return shape.with_sizes((len(native),))
+            inner_shape = shape[1:]
+            if isinstance(native, (tuple, list)):
+                inner_shapes = [recursive_determine_shape(n, inner_shape) for n in native]
+            elif isinstance(native, dict):
+                inner_shapes = [recursive_determine_shape(n, inner_shape) for n in native.values()]
+            else:
+                raise ValueError(native)
+            return shape_stack(shape[0], *inner_shapes)
+
+        shape = recursive_determine_shape(objects, shape)
+
+    return Layout(objects, shape)
+    # if shape.volume == 1:
+    #     objects = np.asarray(objects, dtype=object)
+    #
+    # if isinstance(objects, (tuple, list)):
+    #     objects = np.asarray(objects, dtype=object)
+    # if isinstance(objects, np.ndarray) and objects.dtype == object:
+    #     return Layout(objects, shape)
+    # else:
+    #     assert shape.volume == 1, f"Cannot layout object of type {objects} along {shape}, a tuple, list or object array is required."
+
+
+def compatible_tensor(data, compat_shape: Shape = None, compat_natives=(), convert=False):
+    if isinstance(data, Tensor):
+        return data
+    elif isinstance(data, Shape):
+        if data.spatial.rank == 1:
+            return wrap(data.spatial.size)
+        assert compat_shape.channel.rank == 1, "Only single-channel tensors support implicit casting from Shape to tensor"
+        assert data.rank == compat_shape.channel.volume
+        return wrap(data.spatial.sizes, *compat_shape.channel.with_size(data.names))
+    else:
+        data_type = type(data)
+        backend = choose_backend(*compat_natives, data)
+        try:
+            data = backend.as_tensor(data, convert_external=convert)
+            shape = backend.staticshape(data)
+        except ValueError as e:
+            raise ValueError(e)
+        if len(shape) == 0:
+            return NativeTensor(data, EMPTY_SHAPE)
+        elif isinstance(data, (tuple, list)):  # always channel, add vector if not available
+            data = backend.as_tensor(data)
+        if len(shape) == compat_shape.channel_rank:
+            other_tensor = wrap(data, compat_shape.channel)
+            return other_tensor
+        if compat_shape.channel_rank > 1 and len(shape) == 1 and 'vector' in compat_shape.channel:
+            return wrap(data, compat_shape['vector'].without_sizes())
+        elif len(shape) == compat_shape.rank:
+            warnings.warn(f"Combining a phi.math.Tensor with a {data_type} of same shape is not invariant under shape permutations. Please convert the {data_type} to a phi.math.Tensor first. Shapes: {shape} and {compat_shape}", SyntaxWarning, stacklevel=5)
+            return NativeTensor(data, compat_shape.with_sizes(shape))
+        else:
+            raise ValueError(f"Cannot combine tensor of shape {shape} with tensor of shape {compat_shape}")
+
+
+def broadcastable_native_tensors(*tensors):
+    """
+    Expands and transposes the dimensions of the given tensors so that they all have the same dimension order.
+
+    Args:
+      *tensors: sequence of Tensors
+
+    Returns:
+      shape, native tensors)
+
+    """
+    from ._sparse import SparseCoordinateTensor, CompressedSparseMatrix, dense
+    if any(isinstance(t, (SparseCoordinateTensor, CompressedSparseMatrix)) for t in tensors) and not all(isinstance(t, (SparseCoordinateTensor, CompressedSparseMatrix)) for t in tensors):
+        tensors = [dense(t) for t in tensors]
+    broadcast_shape = merge_shapes(*[t.shape for t in tensors])
+    natives = [t.native(order=broadcast_shape.names) if t.rank > 0 else t.native() for t in tensors]
+    return broadcast_shape, natives
+
+
+def custom_op2(x: Union[Tensor, float], y: Union[Tensor, float], l_operator, l_native_function, r_operator=None, r_native_function=None, op_name: str = 'unknown', op_symbol: str = None) -> Tensor:
+    """
+    Perform a custom operator on two tensors.
+    This method first tries calling _op2() on the first tensor and if that fails, tries it on the second tensor.
+
+    Args:
+      x: Left argument
+      y: Right argument
+      l_operator: Operator function acting on Tensors
+      l_native_function: Operator function acting on natives
+      r_operator:  Argument-reversed operator function acting on Tensors
+      r_native_function:  Argument-reversed operator function acting on natives
+      op_name: Name of the operator function for debugging purposes. Leading 'r' will be added for the operand-reversed version.
+      op_symbol: Short name for the operator, independent of argument order.
+
+    Returns:
+        `Tensor`
+    """
+    if op_symbol is None:
+        op_symbol = op_name
+    x = wrap(x)
+    y = wrap(y)
+    result = x._op2(y, l_operator, l_native_function, op_name, op_symbol)
+    if result is NotImplemented:
+        if r_operator is None:
+            r_operator = lambda a, b: l_operator(b, a)
+        if r_native_function is None:
+            r_native_function = lambda a, b: l_native_function(b, a)
+        result = y._op2(x, r_operator, r_native_function, f'r{op_name}', op_symbol)
+        if result is NotImplemented:
+            raise NotImplementedError(f"Operation not supported between {type(x)} and {type(y)}")
+    return result
+
+
+def disassemble_tensors(tensors: Union[Tuple[Tensor, ...], List[Tensor]], expand: bool) -> Tuple[tuple, Tuple[Shape], tuple]:
+    """
+    Args:
+        tensors: Tuple or list of Tensors.
+        expand: Whether to add collapsed dimensions to the native tensors.
+
+    Returns:
+        natives: tuple of native tensors
+        specs: Identification primitives from which the tensor can be reconstructed given the natives.
+            One per tensor.
+    """
+    for t in tensors:
+        if isinstance(t, TensorStack) or expand:
+            t._expand()
+    natives = sum([t._natives() for t in tensors], ())
+    shapes = tuple([t.shape for t in tensors])
+    specs = tuple([t._spec_dict() for t in tensors])
+    return natives, shapes, specs
+
+
+def assemble_tensors(natives: Union[tuple, list], specs: Union[Tuple[dict, ...], List[dict]]):
+    natives = list(natives)
+    result = []
+    for spec in specs:
+        t = spec['type']._from_spec_and_natives(spec, natives)
+        result.append(t)
+    return result
+
+
+MISSING_TENSOR = 'missing'
+NATIVE_TENSOR = 'native'
+
+
+def disassemble_tree(obj: PhiTreeNodeType) -> Tuple[PhiTreeNodeType, List[Tensor]]:
+    """
+    Splits a nested structure of Tensors into the structure without the tensors and an ordered list of tensors.
+    Native tensors will be wrapped in phi.math.Tensors with default dimension names and dimension types `None`.
+
+    See Also:
+        `assemble_tree()`
+
+    Args:
+        obj: Nested structure of `Tensor` objects.
+            Nested structures include: `tuple`, `list`, `dict`, `phi.math.magic.PhiTreeNode`.
+
+    Returns:
+        empty structure: Same structure as `obj` but with the tensors replaced by `None`.
+        tensors: Ordered `list` of all contained `Tensor` objects.
+    """
+    if obj is None:
+        return MISSING_TENSOR, []
+    elif isinstance(obj, Tensor):
+        return None, [obj]
+    elif isinstance(obj, (tuple, list)):
+        keys = []
+        values = []
+        for item in obj:
+            key, value = disassemble_tree(item)
+            keys.append(key)
+            values.extend(value)
+        return (tuple(keys) if isinstance(obj, tuple) else keys), values
+    elif isinstance(obj, dict):
+        keys = {}
+        values = []
+        for name, item in obj.items():
+            key, value = disassemble_tree(item)
+            keys[name] = key
+            values.extend(value)
+        return keys, values
+    elif isinstance(obj, PhiTreeNode):
+        attributes = variable_attributes(obj)
+        keys = {}
+        values = []
+        for attr in attributes:
+            key, value = disassemble_tree(getattr(obj, attr))
+            keys[attr] = key
+            values.extend(value)
+        return copy_with(obj, **keys), values
+    else:
+        try:
+            backend = choose_backend(obj)
+            if backend == OBJECTS:
+                return obj, []
+            sizes = backend.staticshape(obj)
+            shape = Shape(sizes, tuple([f"dim{i}" for i in range(len(sizes))]), (None,) * len(sizes), (None,) * len(sizes))
+            return NATIVE_TENSOR, [NativeTensor(obj, shape)]
+        except NoBackendFound:
+            return obj, []
+
+
+def assemble_tree(obj: PhiTreeNodeType, values: List[Tensor]) -> PhiTreeNodeType:
+    """ Reverses `disassemble_tree()` given an empty nested structure and a list of tensors. """
+    if obj is MISSING_TENSOR:
+        return None
+    elif obj is NATIVE_TENSOR:
+        value = values.pop(0)
+        assert isinstance(value, NativeTensor), f"Failed to assemble tree structure. Encountered {value}"
+        return value._native
+    elif obj is None:
+        value = values.pop(0)
+        assert isinstance(value, Tensor)
+        return value
+    elif isinstance(obj, list):
+        return [assemble_tree(item, values) for item in obj]
+    elif isinstance(obj, tuple):
+        return tuple([assemble_tree(item, values) for item in obj])
+    elif isinstance(obj, dict):
+        return {name: assemble_tree(val, values) for name, val in obj.items()}
+    elif isinstance(obj, PhiTreeNode):
+        attributes = variable_attributes(obj)
+        values = {a: assemble_tree(getattr(obj, a), values) for a in attributes}
+        return copy_with(obj, **values)
+    else:
+        return obj
+
+
+def cached(t: Union[Tensor, 'PhiTreeNode']) -> Union[Tensor, 'PhiTreeNode']:
+    assert isinstance(t, (Tensor, PhiTreeNode)), f"All arguments must be Tensors but got {type(t)}"
+    if isinstance(t, NativeTensor):
+        return t._cached()
+    elif isinstance(t, TensorStack):
+        if t._cached is not None:
+            return t._cached
+        inners = cached(t._tensors)
+        if t.requires_broadcast:
+            return TensorStack(inners, t._stack_dim)
+        else:
+            natives = [t.native(order=t.shape.names) for t in inners]
+            native = choose_backend(*natives).stack(natives, axis=t.shape.index(t._stack_dim.name))
+            return NativeTensor(native, t.shape)
+    elif isinstance(t, Layout):
+        return t
+    elif isinstance(t, PhiTreeNode):
+        tree, tensors = disassemble_tree(t)
+        tensors_ = [cached(t_) for t_ in tensors]
+        return assemble_tree(tree, tensors_)
+    else:
+        raise AssertionError(f"Cannot cache {type(t)} {t}")
+
+
+def expand_tensor(value: Tensor, dims: Shape):
+    if not dims:
+        return value
+    if isinstance(value, NativeTensor):
+        if dims.is_uniform:
+            return NativeTensor(value._native, value._native_shape, dims & value._shape)
+        else:
+            stack_dim = dims.shape.without('dims')
+            if stack_dim.rank > 1:
+                raise NotImplementedError("Higher-order non-uniform expand() not yet supported")
+            unstacked_dims = [dims.after_gather(i) for i in stack_dim.meshgrid()]
+            components = [NativeTensor(value._native, value._native_shape, inner_shape) for inner_shape in unstacked_dims]
+            return TensorStack(components, stack_dim)
+    if isinstance(value, TensorStack):
+        expanded = [expand_tensor(v, dims.without(value.stack_dim)) for v in value._tensors]
+        return TensorStack(expanded, value.stack_dim)
+    raise NotImplementedError
+
+
+class Dict(dict):
+    """
+    Dictionary of `Tensor` or `phi.math.magic.PhiTreeNode` values.
+    Dicts are not themselves tensors and do not have a shape.
+    Use `layout()` to treat `dict` instances like tensors.
+
+    In addition to dictionary functions, supports mathematical operators with other `Dict`s and lookup via `.key` syntax.
+    `Dict` implements `phi.math.magic.PhiTreeNode` so instances can be passed to math operations like `sin`.
+    """
+
+    def __value_attrs__(self):
+        return tuple(self.keys())
+    
+    # --- Dict[key] ---
+
+    def __getattr__(self, key):
+        try:
+            return self[key]
+        except KeyError as k:
+            raise AttributeError(k)
+
+    def __setattr__(self, key, value):
+        self[key] = value
+
+    def __delattr__(self, key):
+        try:
+            del self[key]
+        except KeyError as k:
+            raise AttributeError(k)
+        
+    # --- operators ---
+    
+    def __neg__(self):
+        return Dict({k: -v for k, v in self.items()})
+    
+    def __invert__(self):
+        return Dict({k: ~v for k, v in self.items()})
+    
+    def __abs__(self):
+        return Dict({k: abs(v) for k, v in self.items()})
+    
+    def __round__(self, n=None):
+        return Dict({k: round(v) for k, v in self.items()})
+
+    def __add__(self, other):
+        if isinstance(other, Dict):
+            return Dict({key: val + other[key] for key, val in self.items()})
+        else:
+            return Dict({key: val + other for key, val in self.items()})
+
+    def __radd__(self, other):
+        if isinstance(other, Dict):
+            return Dict({key: other[key] + val for key, val in self.items()})
+        else:
+            return Dict({key: other + val for key, val in self.items()})
+
+    def __sub__(self, other):
+        if isinstance(other, Dict):
+            return Dict({key: val - other[key] for key, val in self.items()})
+        else:
+            return Dict({key: val - other for key, val in self.items()})
+
+    def __rsub__(self, other):
+        if isinstance(other, Dict):
+            return Dict({key: other[key] - val for key, val in self.items()})
+        else:
+            return Dict({key: other - val for key, val in self.items()})
+
+    def __mul__(self, other):
+        if isinstance(other, Dict):
+            return Dict({key: val * other[key] for key, val in self.items()})
+        else:
+            return Dict({key: val * other for key, val in self.items()})
+
+    def __rmul__(self, other):
+        if isinstance(other, Dict):
+            return Dict({key: other[key] * val for key, val in self.items()})
+        else:
+            return Dict({key: other * val for key, val in self.items()})
+
+    def __truediv__(self, other):
+        if isinstance(other, Dict):
+            return Dict({key: val / other[key] for key, val in self.items()})
+        else:
+            return Dict({key: val / other for key, val in self.items()})
+
+    def __rtruediv__(self, other):
+        if isinstance(other, Dict):
+            return Dict({key: other[key] / val for key, val in self.items()})
+        else:
+            return Dict({key: other / val for key, val in self.items()})
+
+    def __floordiv__(self, other):
+        if isinstance(other, Dict):
+            return Dict({key: val // other[key] for key, val in self.items()})
+        else:
+            return Dict({key: val // other for key, val in self.items()})
+
+    def __rfloordiv__(self, other):
+        if isinstance(other, Dict):
+            return Dict({key: other[key] // val for key, val in self.items()})
+        else:
+            return Dict({key: other // val for key, val in self.items()})
+
+    def __pow__(self, power, modulo=None):
+        assert modulo is None
+        if isinstance(power, Dict):
+            return Dict({key: val ** power[key] for key, val in self.items()})
+        else:
+            return Dict({key: val ** power for key, val in self.items()})
+
+    def __rpow__(self, other):
+        if isinstance(other, Dict):
+            return Dict({key: other[key] ** val for key, val in self.items()})
+        else:
+            return Dict({key: other ** val for key, val in self.items()})
+
+    def __mod__(self, other):
+        if isinstance(other, Dict):
+            return Dict({key: val % other[key] for key, val in self.items()})
+        else:
+            return Dict({key: val % other for key, val in self.items()})
+
+    def __rmod__(self, other):
+        if isinstance(other, Dict):
+            return Dict({key: other[key] % val for key, val in self.items()})
+        else:
+            return Dict({key: other % val for key, val in self.items()})
+
+    def __eq__(self, other):
+        if isinstance(other, Dict):
+            return Dict({key: val == other[key] for key, val in self.items()})
+        else:
+            return Dict({key: val == other for key, val in self.items()})
+
+    def __ne__(self, other):
+        if isinstance(other, Dict):
+            return Dict({key: val != other[key] for key, val in self.items()})
+        else:
+            return Dict({key: val != other for key, val in self.items()})
+
+    def __lt__(self, other):
+        if isinstance(other, Dict):
+            return Dict({key: val < other[key] for key, val in self.items()})
+        else:
+            return Dict({key: val < other for key, val in self.items()})
+
+    def __le__(self, other):
+        if isinstance(other, Dict):
+            return Dict({key: val <= other[key] for key, val in self.items()})
+        else:
+            return Dict({key: val <= other for key, val in self.items()})
+
+    def __gt__(self, other):
+        if isinstance(other, Dict):
+            return Dict({key: val > other[key] for key, val in self.items()})
+        else:
+            return Dict({key: val > other for key, val in self.items()})
+
+    def __ge__(self, other):
+        if isinstance(other, Dict):
+            return Dict({key: val >= other[key] for key, val in self.items()})
+        else:
+            return Dict({key: val >= other for key, val in self.items()})
+
+    # --- overridden methods ---
+
+    def copy(self):
+        return Dict(self)
+
+
+def to_dict(value: Union[Tensor, Shape]):
+    """
+    Returns a serializable form of a `Tensor` or `Shape`.
+    The result can be written to a JSON file, for example.
+
+    See Also:
+        `from_dict()`.
+
+    Args:
+        value: `Tensor` or `Shape`
+
+    Returns:
+        Serializable Python tree of primitives
+    """
+    if isinstance(value, Shape):
+        return value._to_dict(include_sizes=True)
+    elif isinstance(value, Tensor):
+        return value._to_dict()
+    raise ValueError(f"Cannot convert {value} to a dict")
+
+
+def from_dict(dict_: dict, convert=False):
+    """
+    Loads a `Tensor` or `Shape` from a serialized form.
+
+    See Also:
+        `to_dict()`.
+
+    Args:
+        dict_: Serialized tensor properties.
+        convert: Whether to convert the data to the current backend format or keep it as a Numpy array.
+
+    Returns:
+        `Tensor` or `Shape`.
+    """
+    shape = Shape._from_dict(dict_)
+    if 'data' in dict_:
+        return tensor(dict_['data'], shape, convert=convert)
+    else:
+        return shape
+
+
+
+
+class BroadcastFormatter:
+    """
+    Usage documented in math.__init__.
+
+    How it works:
+    * -f calls __neg__ which tells tensors to call register_formatted() instead of formatting normally.
+    * Then __sub__ is called which maps the actual string formatting.
+    """
+
+    def __init__(self):
+        self.values: List[Tensor] = None
+
+    def register_formatted(self, value: Tensor, format_spec: str):
+        self.values.append(value)
+        return "{" + f"{len(self.values) - 1}:{format_spec}" + "}"
+
+    def format(self, other: str):
+        assert isinstance(other, str), "math.f must be used on a string"
+        from ._ops import map_
+        if self.values is None:
+            raise SyntaxError("Use the syntax -f-f\"{tensor}\". Leading '-' is missing.")
+        result = map_(lambda *args: other.format(*args), *self.values)
+        self.values = None
+        return result
+
+    def __sub__(self, other):
+        return self.format(other)
+
+    def __neg__(self):
+        if self.values is not None:
+            raise SyntaxError("-f called twice without formatting string.")
+        self.values = []
+        return self
+
+
+BROADCAST_FORMATTER = BroadcastFormatter()
+
+
+@dataclass
+class Color:
+    name: str
+    console_foreground_begin: str
+
+    def __call__(self, obj, **kwargs):
+        text = str(obj).replace(CONSOLE_END, self.console_foreground_begin)
+        return f"{self.console_foreground_begin}{text}{CONSOLE_END if self.console_foreground_begin else ''}"
+
+
+DEFAULT = Color("Default", '')
+BLUE = Color("Blue", '\033[94m')
+GREEN = Color("Green", '\033[92m')
+YELLOW = Color("Yellow", '\033[93m')
+GREY = Color("Grey", '\033[37m')
+CONSOLE_END = '\033[0m'
+
+
+@dataclass
+class ColorScheme:
+    value: Color
+    shape: Color
+    dtype: Color
+    fine: Color
+
+
+DEFAULT_COLORS = ColorScheme(BLUE, GREEN, YELLOW, GREY)
+NO_COLORS = ColorScheme(DEFAULT, DEFAULT, DEFAULT, DEFAULT)
+
+
+@dataclass
+class PrintOptions:
+    layout: str = 'auto'
+    float_format: str = None
+    threshold: int = 8
+    colors: ColorScheme = None
+    include_shape: bool = None
+    include_dtype: bool = None
+
+    def get_colors(self):
+        if self.colors is True:
+            return DEFAULT_COLORS
+        elif self.colors is False:
+            return NO_COLORS
+        elif self.colors is not None:
+            return self.colors
+        else:  # None
+            return DEFAULT_COLORS if check_is_printing() else NO_COLORS
+
+
+def check_is_printing():
+    import traceback, sys
+    stack = traceback.extract_stack()
+    for frame in stack:
+        if "_pydevd_bundle\\pydevd_xml.py" in frame.filename:
+            return False
+    for frame in stack:
+        if frame.line.strip().startswith('print('):
+            return True
+    if 'ipykernel' in sys.modules:
+        return True
+    return False
+
+
+def format_summary(self: Tensor, options: PrintOptions) -> str:
+    """
+    Returns shape + dtype + content summary
+
+    * `bool`: n / N True
+    * `float`: mean ± std (min...max)
+    """
+    if not self.available:
+        return format_tracer(self, options)
+    from ._sparse import SparseCoordinateTensor, CompressedSparseMatrix
+    if isinstance(self, (SparseCoordinateTensor, CompressedSparseMatrix)):
+        return sparse_summary(self, options)
+    colors = options.get_colors()
+    tokens = []
+    if self.shape if options.include_shape is None else options.include_shape:
+        tokens.append(f"{colors.shape(self.shape)}")
+    if is_unexpected_dtype(self.dtype) if options.include_dtype is None else options.include_dtype:
+        tokens.append(f"{colors.dtype(self.dtype)}")
+    try:
+        if self.rank == 0:
+            tokens.append(colors.value(self.numpy()))
+        elif self.dtype.kind == bool:
+            tokens.append(colors.value(f"{self.sum} / {self.shape.volume} True"))
+        elif self.dtype.kind in (float, int):
+            min_val, max_val, mean, std = [float(f) for f in [self.finite_min, self.finite_max, self.finite_mean, self.std]]
+            if std == 0:
+                tokens.append(colors.value(f"const {mean:{options.float_format or ''}}"))
+            else:
+                if any([abs(val) < 0.001 or abs(val) > 1000 for val in [mean, std]]):
+                    tokens.append(colors.value(f"{mean:{options.float_format or '.2e'}} ± {std:{options.float_format or '.1e'}}"))
+                else:
+                    tokens.append(colors.value(f"{mean:{options.float_format or '.3f'}} ± {std:{options.float_format or '.3f'}}"))
+                tokens.append(colors.fine(f"({min_val:{options.float_format or '.0e'}}...{max_val:{options.float_format or '.0e'}})"))
+        elif self.dtype.kind == complex:
+            tokens.append(colors.value(f"|...| < {abs(self).max}"))
+    except BaseException as err:
+        tokens.append(f"failed to fetch values: {err}")
+    return " ".join(tokens)
+
+
+def sparse_summary(value: Tensor, options: PrintOptions) -> str:
+    colors = options.get_colors()
+    from ._sparse import get_format
+    tokens = []
+    if is_unexpected_dtype(value.dtype) if options.include_dtype is None else options.include_dtype:
+        tokens.append(f"{colors.dtype(value.dtype)}")
+    tokens.append("sparse " + get_format(value))
+    if options.include_shape is not False:
+        tokens.append(f"{colors.shape(value.shape)}")
+    tokens.append(f"with {instance(value._values).volume} entries:")
+    tokens.append(format_summary(value._values, options))
+    return " ".join(tokens)
+
+
+def is_unexpected_dtype(dtype: DType):
+    if dtype in [DType(bool), DType(int, 32)]:
+        return False
+    if dtype.kind == float and dtype.precision == get_precision():
+        return False
+    return True
+
+
+def format_tracer(self: Tensor, options: PrintOptions) -> str:
+    colors = options.get_colors()
+    if self._is_tracer:
+        return f"{colors.shape(self.shape)} {colors.dtype(self.dtype)} {colors.value(f'linear tracer for {self.default_backend}')}"
+    else:
+        return f"{colors.shape(self.shape)} {colors.dtype(self.dtype)} {colors.value(f'{self.default_backend} tracer')}"
+
+
+def format_full(value: Tensor, options: PrintOptions) -> str:  # multi-line content
+    if not value.available:
+        return format_tracer(value, options)
+    from ._sparse import dense
+    value = dense(value)
+    import re
+    colors = options.get_colors()
+    dim_order = tuple(sorted(value.shape.spatial.names, reverse=True))
+    lines = []
+    formatter = {}
+    if options.float_format:
+        formatter['float_kind'] = ('{:' + options.float_format + '}').format
+    with numpy.printoptions(threshold=np.inf, formatter=formatter):
+        if value.shape.dual_rank > 0:  # matrix
+            if options.include_shape is not None:
+                lines.append(colors.shape(value.shape))
+            if value.shape.dual_rank > 1:
+                raise NotImplementedError("Multiple dual dimensions cannot currently be printed")
+            dual_dim = dual(value).name
+            primal = spatial(**dual(value).untyped_dict).name
+            if primal not in value.shape:
+                primal = non_batch(value).non_dual.name
+            for b in batch(value).meshgrid(names=True):
+                text = " " + np.array2string(value[b].numpy([primal, dual_dim]), separator=', ', max_line_width=np.inf) + " "
+                text = re.sub('[\\[\\]]', '', text).replace(',', ' ')
+                prefixes = prefix_indices(non_batch(value).non_dual, colors)
+                if options.include_shape is not False:
+                    for line, prefix in zip(text.split("\n"), prefixes):
+                        lines.append(f"{prefix}  {colors.value(line)} along {colors.shape(dual_dim)}")
+                else:
+                    lines.append(colors.value(text))
+        elif value.shape.spatial_rank == 0:  # no spatial or dual dimensions
+            if options.include_shape is not None:
+                lines.append(colors.shape(value.shape))
+            if value.shape.rank <= 1:
+                text = np.array2string(value.numpy(), separator=', ', max_line_width=np.inf)
+                lines.append(' ' + re.sub('[\\[\\]]', '', text))
+            else:
+                text = np.array2string(value.numpy(value.shape), separator=', ', max_line_width=np.inf)
+                lines.append(text)
+        elif value.shape.spatial_rank in (1, 2):
+            if value.shape.non_spatial.volume > 1:
+                indices = [f"{colors.shape(', '.join(f'{name}={idx}' for name, idx in index_dict.items()))}" for index_dict in value.shape.non_spatial.meshgrid(names=True)]
+                max_index_length = max(len(index) for index in indices)
+            for i, index_dict in enumerate(value.shape.non_spatial.meshgrid(names=True)):
+                row = ""
+                if value.shape.non_spatial.volume > 1:
+                    row += indices[i] + " " * (max_index_length - len(indices[i]) + 2)
+                    if value.shape.spatial_rank == 2:
+                        row += "\n"
+                if value.shape.spatial_rank == 1:
+                    text = np.array2string(value[index_dict].numpy(dim_order), separator=', ', max_line_width=np.inf)
+                else:
+                    text = " " + np.array2string(value[index_dict].numpy(dim_order)[::-1], separator=', ', max_line_width=np.inf)
+                lines.append(row + colors.value(re.sub('[\\[\\]]', '', text)) + (f"  along {colors.shape(spatial(value))}" if options.include_shape is not False else ""))
+        else:
+            raise NotImplementedError('Can only print tensors with up to 2 spatial dimensions.')
+    return "\n".join(lines)
+
+
+def prefix_indices(index_shape, colors: ColorScheme):
+    prefixes = [f"{colors.shape(', '.join(f'{name}={idx}' for name, idx in index_dict.items()))}" for index_dict in index_shape.meshgrid(names=True)]
+    max_len = max(len(p) for p in prefixes)
+    prefixes = [p + " " * (max_len - len(p) + 2) for p in prefixes]
+    return prefixes
+
+
+def format_row(self: Tensor, options: PrintOptions) -> str:  # all values in a single line
+    """
+    Including shape:  (x=5, y=4) along vector
+    Without shape: (5, 4)
+    Auto: don't show if 'vector' but show item names
+
+    Args:
+        self:
+        options:
+
+    Returns:
+
+    """
+    if not self.available:
+        return format_tracer(self, options)
+    from ._sparse import dense
+    self = dense(self)
+    colors = options.get_colors()
+    if self.shape.rank == 1:
+        content = _format_vector(self, options)
+        is_vector = self.shape.name == 'vector' and self.shape.channel_rank == 1
+        is_dual_vector = self.shape.name == '~vector'
+        if (not is_vector and not is_dual_vector) if options.include_shape is None else options.include_shape:
+            content += f" along {colors.shape(f'{self.shape.name}{TYPE_ABBR[self.shape.type]}')}"
+        elif is_dual_vector:
+            content = "~" + content
+    else:
+        if channel(self):
+            rows = [_format_vector(self[b], options) for b in self.shape.non_channel.meshgrid()]
+        else:
+            rows = [_format_number(self[b].numpy(), options, self.dtype) for b in self.shape.non_channel.meshgrid()]
+        content = "; ".join(rows)
+        if options.include_shape is not False:
+            content += " " + colors.shape(self.shape)
+    if is_unexpected_dtype(self.dtype) if options.include_dtype is None else options.include_dtype:
+        content += f" {colors.dtype(self.dtype)}"
+    return content
+
+
+def format_numpy(self: Tensor, options: PrintOptions) -> str:
+    from ._sparse import dense
+    self = dense(self)
+    header = []
+    colors = options.get_colors()
+    if options.include_shape:
+        header.append(colors.shape(self.shape))
+    if options.include_dtype:
+        header.append(colors.dtype(self.dtype))
+    numpy_array = self.numpy(self.shape)
+    formatter = {}
+    if options.float_format:
+        formatter['float_kind'] = ('{:' + options.float_format + '}').format
+    with numpy.printoptions(threshold=options.threshold, formatter=formatter):
+        content = colors.value(numpy_array)
+    return " ".join(header) + "\n" + content if header else content
+
+
+def _format_vector(self: Tensor, options: PrintOptions) -> str:
+    colors = options.get_colors()
+    if self.shape.rank > 1:
+        from ._magic_ops import flatten
+        self = flatten(self, channel('flat'))
+    if self.shape.get_item_names(0) is not None and options.include_shape is not False:
+        content = ", ".join([f"{item}={_format_number(number, options, self.dtype)}" for number, item in zip(self, self.shape.get_item_names(0))])
+    else:
+        content = ", ".join([_format_number(num, options, self.dtype) for num in self])
+    return colors.value(f"({content})")
+
+
+def _format_number(num, options: PrintOptions, dtype: DType):
+    if options.float_format is not None:
+        return format(num, options.float_format)
+    if dtype.kind == int:
+        return format(num, 'd')
+    if dtype.kind == bool:
+        return str(bool(num))
+    if dtype.kind == float:
+        return format(num, options.float_format or '.3f')
+    return str(num)
+
+
+def format_tensor(self: Tensor, options: PrintOptions) -> str:
+    if not self.available:
+        return format_tracer(self, options)
+    if self.shape.is_non_uniform:
+        return f"{options.get_colors().shape(self.shape)} non-uniform"
+    if options.layout == 'auto':
+        if not self.shape:
+            return format_summary(self, options)
+        if self.shape.volume is not None and self.shape.volume < options.threshold:
+            return format_row(self, options)
+        else:
+            return format_summary(self, options)
+    elif options.layout == 'summary':
+        return format_summary(self, options)
+    elif options.layout == 'full':
+        return format_full(self, options)
+    elif options.layout == 'row':
+        return format_row(self, options)
+    elif options.layout == 'numpy':
+        return format_numpy(self, options)
+    else:
+        raise NotImplementedError(f"Layout '{options.layout}' is not supported.")
+
+
+def is_scalar(value) -> bool:
+    """
+    Checks whether `value` has no dimensions.
+
+    Args:
+        value: `Tensor` or Python primitive or native tensor.
+
+    Returns:
+        `bool`
+    """
+    if isinstance(value, Tensor):
+        return value.shape.rank == 0
+    elif isinstance(value, numbers.Number):
+        return True
+    else:
+        return len(choose_backend(value).staticshape(value)) == 0
+
+
+def may_vary_along(value, dims: DimFilter):
+    s = value._native_shape if isinstance(value, NativeTensor) else shape(value)
+    return s.only(dims).volume > 1
+
+
+def specs_equal(spec1, spec2):
+    if isinstance(spec1, Tensor) or isinstance(spec2, Tensor):
+        if isinstance(spec1, Tensor) and isinstance(spec2, Tensor):
+            from ._ops import close
+            return close(spec1, spec2, rel_tolerance=0, abs_tolerance=0)
+        return False
+    if isinstance(spec1, dict):
+        return set(spec1) == set(spec2) and all([key in spec2 and specs_equal(spec1[key], spec2[key]) for key in spec1.keys()])
+    if isinstance(spec1, (tuple, list)):
+        return len(spec1) == len(spec2) and all([specs_equal(s1, s2) for s1, s2 in zip(spec1, spec2)])
+    return spec1 == spec2
```

### Comparing `phiflow-2.3.4/phi/math/_trace.py` & `phiflow-2.4.0/phi/math/_trace.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,356 +1,357 @@
-from collections import namedtuple
-from typing import Callable, Dict, Set, Tuple, Union
-
-import numpy
-import numpy as np
-
-from .backend import choose_backend, NUMPY, Backend
-from ._shape import Shape, parse_dim_order, merge_shapes, spatial, instance, batch, concat_shapes, EMPTY_SHAPE, dual, channel, non_batch
-from ._magic_ops import stack, expand
-from ._tensors import Tensor, wrap, disassemble_tree, disassemble_tensors, assemble_tree, CollapsedTensor, TensorStack
-from ._sparse import SparseCoordinateTensor
-from . import _ops as math
-
-
-TracerSource = namedtuple('TracerSource', ['shape', 'dtype', 'name', 'index'])
-
-
-class ShiftLinTracer(Tensor):
-    """
-    Tracer object for linear and affine functions.
-    The sparsity pattern is assumed equal for all grid cells and is reflected in `val` (e.g. for a 5-point stencil, `val` has 5 items).
-    The Tensors stored in `val` include position-dependent dimensions, allowing for different stencils at different positions.
-    Dimensions not contained in any `val` Tensor are treated as independent (batch dimensions).
-    """
-
-    def __init__(self, source: TracerSource, values_by_shift: dict, shape: Shape, bias: Tensor):
-        """
-        Args:
-            source: placeholder tensor
-            values_by_shift: `dict` mapping relative shifts (`Shape`) to value Tensors.
-                Shape keys only contain non-zero shift dims. Missing dims are interpreted as independent.
-            shape: shape of this tensor
-            bias: Constant Tensor to be added to the multiplication output, A*x + b.
-                A bias naturally arises at boundary cells with non-trivial boundary conditions if no ghost cells are added to the matrix.
-                When non-zero, this tracer technically represents an affine function, not a linear one.
-                However, the bias can be subtracted from the solution vector when solving a linear system, allowing this function to be solved with regular linear system solvers.
-        """
-        assert isinstance(source, TracerSource)
-        self.source = source
-        self.val: Dict[Shape, Tensor] = simplify_add(values_by_shift)
-        for shift_ in self.val.keys():
-            assert shift_.only(sorted(shift_.names), reorder=True) == shift_
-        self.bias = bias
-        self._shape = shape
-
-    def __repr__(self):
-        return f"Linear tracer {self._shape}"
-
-    def native(self, order: Union[str, tuple, list, Shape] = None):
-        """
-        Evaluates the value of the linear operation applied to the original source tensor.
-
-        This is done by building a sparse matrix for all dimensions that are affected by the linear operation.
-        These dimensions are detected automatically during the creation of the linear operation.
-        All other dimensions (independent dimensions) are combined into a single batch dimensions for the sparse matrix multiplication.
-
-        Args:
-          order: str or tuple or list:  (Default value = None)
-
-        Returns:
-
-        """
-        order = parse_dim_order(order, check_rank=self.rank)
-        result = self.apply(self.source)
-        result_order = order if order is not None else self._shape.names
-        return result.native(result_order)
-
-    @property
-    def dependent_dims(self) -> Set[str]:
-        """
-        Dimensions relevant to the linear operation.
-        This includes `pattern_dims` as well as dimensions along which only the values vary.
-        These dimensions cannot be parallelized trivially with a non-batched matrix.
-        """
-        return self.pattern_dim_names | set(sum([t.shape.names for t in self.val.values()], ())) | set(self.bias.shape.names)
-
-    @property
-    def pattern_dim_names(self) -> Set[str]:
-        """
-        Dimensions along which the sparse matrix contains off-diagonal elements.
-        These dimensions must be part of the sparse matrix and cannot be parallelized.
-        """
-        return set(sum([offset.names for offset in self.val], ()))
-
-    @property
-    def pattern_dims(self) -> Shape:
-        return self.source.shape.only(self.pattern_dim_names)
-
-    @property
-    def dtype(self):
-        return self.source.dtype
-
-    @property
-    def shape(self):
-        return self._shape
-
-    def _with_shape_replaced(self, new_shape):
-        raise NotImplementedError()
-
-    @property
-    def _is_tracer(self) -> bool:
-        return True
-
-    def _getitem(self, selection: dict):
-        starts = {dim: (item.start or 0) if isinstance(item, slice) else item for dim, item in selection.items()}
-        new_shape = math.zeros(self._shape)[selection].shape
-        return self.shift(starts, new_shape, lambda v: v[selection], lambda b: b[selection])
-
-    def shift(self, shifts: dict,
-              new_shape: Shape,
-              val_fun: Callable,
-              bias_fun: Callable = None):
-        """
-        Shifts all values of this tensor by `shifts`.
-        Values shifted outside will be mapped with periodic boundary conditions when the matrix is built.
-
-        Args:
-            shifts: Offsets by dimension
-            new_shape: Shape of the shifted tensor, must match the shape returned by `val_fun`.
-            val_fun: Function to apply to the matrix values, may change the tensor shapes
-            bias_fun: Function to apply to the bias vector, may change the tensor shape
-
-        Returns:
-            Shifted tensor, possibly with altered values.
-        """
-        val = {}
-        for shift, values in self.val.items():
-            assert isinstance(shift, Shape)
-            for dim, delta in reversed(tuple(shifts.items())):
-                if dim not in values.shape:
-                    values = math.expand(values, self._shape.only(dim))  # dim order may be scrambled
-                if delta:
-                    shift = shift._replace_single_size(dim, shift.get_size(dim) + delta) if dim in shift else shift._expand(spatial(**{dim: delta}))
-            val[shift.only(sorted(shift.names), reorder=True)] = val_fun(values)
-        bias = bias_fun(self.bias)
-        return ShiftLinTracer(self.source, val, new_shape, bias)
-
-    def unstack(self, dimension):
-        raise NotImplementedError()
-
-    def __neg__(self):
-        return ShiftLinTracer(self.source, {shift: -values for shift, values in self.val.items()}, self._shape, -self.bias)
-
-    def _op1(self, native_function):
-        # __neg__ is the only proper linear op1 and is implemented above.
-        if native_function.__name__ == 'isfinite':
-            test_output = self.apply(math.ones(self.source.shape, dtype=self.source.dtype))
-            return math.is_finite(test_output)
-        else:
-            raise NotImplementedError('Only linear operations are supported')
-
-    def _op2(self, other: Tensor,
-             operator: Callable,
-             native_function: Callable,
-             op_name: str = 'unknown',
-             op_symbol: str = '?') -> 'ShiftLinTracer':
-        """
-        Tensor-tensor operation.
-
-        Args:
-            other:
-            operator:
-            native_function:
-        """
-        assert op_symbol in '+-*/', f"Unsupported operation encountered while tracing linear function: {native_function}"
-        zeros_for_missing_self = op_name not in ['add', 'radd', 'rsub']  # perform `operator` where `self == 0`
-        zeros_for_missing_other = op_name not in ['add', 'radd', 'sub']  # perform `operator` where `other == 0`
-
-        if isinstance(other, ShiftLinTracer):
-            assert self.source is other.source, "Multiple linear tracers are not yet supported."
-            assert set(self._shape) == set(other._shape), f"Tracers have different shapes: {self._shape} and {other._shape}"
-            values = {}
-            for dim_shift in self.val.keys():
-                if dim_shift in other.val:
-                    values[dim_shift] = operator(self.val[dim_shift], other.val[dim_shift])
-                else:
-                    if zeros_for_missing_other:
-                        values[dim_shift] = operator(self.val[dim_shift], math.zeros_like(self.val[dim_shift]))
-                    else:
-                        values[dim_shift] = self.val[dim_shift]
-            for dim_shift, other_values in other.val.items():
-                if dim_shift not in self.val:
-                    if zeros_for_missing_self:
-                        values[dim_shift] = operator(math.zeros_like(other_values), other_values)
-                    else:
-                        values[dim_shift] = other_values
-            bias = operator(self.bias, other.bias)
-            return ShiftLinTracer(self.source, values, self._shape, bias)
-        else:
-            other = self._tensor(other)
-            if op_symbol in '*/':
-                values = {}
-                for dim_shift, val in self.val.items():
-                    values[dim_shift] = operator(val, other)
-                bias = operator(self.bias, other)
-                return ShiftLinTracer(self.source, values, self._shape & other.shape, bias)
-            elif op_symbol in '+-':
-                bias = operator(self.bias, other)
-                return ShiftLinTracer(self.source, self.val, self._shape & other.shape, bias)
-            else:
-                raise ValueError(f"Unsupported operation encountered while tracing linear function: {native_function}")
-
-    def _natives(self) -> tuple:
-        """
-        This function should only be used to determine the compatible backends, this tensor should be regarded as not available.
-        """
-        return sum([v._natives() for v in self.val.values()], ()) + self.bias._natives()
-
-    def _spec_dict(self) -> dict:
-        raise LinearTraceInProgress(self)
-
-
-class LinearTraceInProgress(Exception):
-
-    def __init__(self, tracer: ShiftLinTracer):
-        self.tracer = tracer
-
-
-def simplify_add(val: dict) -> Dict[Shape, Tensor]:
-    result = {}
-    for shift, values in val.items():
-        shift = shift[[i for i, size in enumerate(shift.sizes) if size != 0]]  # discard zeros
-        if shift in result:
-            result[shift] += values
-        else:
-            result[shift] = values
-    return result
-
-
-def matrix_from_function(f: Callable,
-                         *args,
-                         auxiliary_args=None,
-                         auto_compress=False,
-                         sparsify_batch=None,
-                         separate_independent=False,  # not fully implemented, requires auto_compress=False
-                         **kwargs) -> Tuple[Tensor, Tensor]:
-    """
-    Trace a linear function and construct a matrix.
-    Depending on the functional form of `f`, the returned matrix may be dense or sparse.
-
-    Args:
-        f: Function to trace.
-        *args: Arguments for `f`.
-        auxiliary_args: Arguments in which the function is not linear.
-            These parameters are not traced but passed on as given in `args` and `kwargs`.
-        auto_compress: If `True`, returns a compressed matrix if supported by the backend.
-        sparsify_batch: If `False`, the matrix will be batched.
-            If `True`, will create dual dimensions for the involved batch dimensions.
-            This will result in one large matrix instead of a batch of matrices.
-        **kwargs: Keyword arguments for `f`.
-
-    Returns:
-        matrix: Matrix representing the linear dependency of the output `f` on the input of `f`.
-            Input dimensions will be `dual` dimensions of the matrix while output dimensions will be regular.
-        bias: Bias for affine functions or zero-vector if the function is purely linear.
-    """
-    assert isinstance(auxiliary_args, str) or auxiliary_args is None, f"auxiliary_args must be a comma-separated str but got {auxiliary_args}"
-    from ._functional import function_parameters, f_name
-    f_params = function_parameters(f)
-    aux = set(s.strip() for s in auxiliary_args.split(',') if s.strip()) if isinstance(auxiliary_args, str) else f_params[1:]
-    all_args = {**kwargs, **{f_params[i]: v for i, v in enumerate(args)}}
-    aux_args = {k: v for k, v in all_args.items() if k in aux}
-    trace_args = {k: v for k, v in all_args.items() if k not in aux}
-    tree, tensors = disassemble_tree(trace_args)
-    # tracing = not math.all_available(*tensors)
-    natives, shapes, native_dims = disassemble_tensors(tensors, expand=False)
-    # --- Trace function ---
-    with NUMPY:
-        src = TracerSource(tensors[0].shape, tensors[0].dtype, tuple(trace_args.keys())[0], 0)
-        tracer = ShiftLinTracer(src, {EMPTY_SHAPE: math.ones()}, tensors[0].shape, math.zeros(tensors[0].shape, dtype=tensors[0].dtype))
-        x_kwargs = assemble_tree(tree, [tracer])
-        result = f(**x_kwargs, **aux_args)
-    _, result_tensors = disassemble_tree(result)
-    assert len(result_tensors) == 1, f"Linear function output must be or contain a single Tensor but got {result}"
-    tracer = result_tensors[0]._simplify()
-    assert tracer._is_tracer, f"Tracing linear function '{f_name(f)}' failed. Make sure only linear operations are used. Output: {tracer.shape}"
-    # --- Convert to COO ---
-    if sparsify_batch is None:
-        if auto_compress:
-            sparsify_batch = not tracer.default_backend.supports(Backend.csr_matrix_batched)
-        else:
-            sparsify_batch = not tracer.default_backend.supports(Backend.sparse_coo_tensor_batched)
-    matrix, bias = tracer_to_coo(tracer, sparsify_batch, separate_independent)
-    # --- Compress ---
-    if not auto_compress:
-        return matrix, bias
-    if matrix.default_backend.supports(Backend.mul_csr_dense):
-        return matrix.compress_rows(), bias
-    # elif backend.supports(Backend.mul_csc_dense):
-    #     return matrix.compress_cols(), tracer.bias
-    else:
-        return matrix, bias
-    
-
-def tracer_to_coo(tracer: Tensor, sparsify_batch: bool, separate_independent: bool):
-    if isinstance(tracer, CollapsedTensor):
-        tracer = tracer._cached if tracer.is_cached else tracer._inner  # ignore collapsed dimensions. Alternatively, we could expand the result
-        return tracer_to_coo(tracer, sparsify_batch, separate_independent)
-    elif isinstance(tracer, TensorStack):  # This indicates separable solves
-        matrices, biases = zip(*[tracer_to_coo(t, sparsify_batch, separate_independent) for t in tracer._tensors])
-        bias = stack(biases, tracer._stack_dim)
-        if not separate_independent:
-            indices = [math.concat_tensor([m._indices, expand(i, instance(m._indices), channel(vector=tracer._stack_dim.name))], 'vector') for i, m in enumerate(matrices)]
-            indices = math.concat_tensor(indices, 'entries')
-            values = math.concat_tensor([m._values for m in matrices], 'entries')
-            # matrix = stack(matrices, tracer._stack_dim)
-            dense_shape = concat_shapes(matrices[0]._dense_shape, tracer._stack_dim)
-            matrix = SparseCoordinateTensor(indices, values, dense_shape, can_contain_double_entries=False, indices_sorted=False)
-        else:
-            matrix = stack(matrices, tracer._stack_dim)
-        return matrix, bias
-    elif not tracer._is_tracer:  # This part of the output is independent of the input
-        return expand(0, tracer.shape), tracer
-    assert isinstance(tracer, ShiftLinTracer), f"Tracing linear function returned an unsupported construct: {type(tracer)}"
-    assert batch(tracer.pattern_dims).is_empty, f"Batch dimensions may not be sliced in linear operations but got pattern for {batch(tracer.pattern_dims)}"
-    missing_dims = tracer.source.shape.without(tracer.shape)  # these were sliced off
-    ignored_dims = tracer.source.shape.without(tracer.shape.only(tracer.dependent_dims) if sparsify_batch else tracer.pattern_dim_names).without(missing_dims)  # these will be parallelized and not added to the matrix
-    out_shape = tracer.shape.without(ignored_dims)
-    typed_src_shape = tracer.source.shape.without(ignored_dims)
-    src_shape = dual(**typed_src_shape.untyped_dict)
-    sliced_src_shape = src_shape.without(dual(**missing_dims.untyped_dict))
-    batch_val = merge_shapes(*tracer.val.values()).without(out_shape)
-    if non_batch(out_shape).is_empty:
-        assert len(tracer.val) == 1 and non_batch(tracer.val[EMPTY_SHAPE]) == EMPTY_SHAPE
-        return tracer.val[EMPTY_SHAPE], tracer.bias
-    out_indices = []
-    src_indices = []
-    values = []
-    for shift_, shift_val in tracer.val.items():
-        if shift_val.default_backend is NUMPY:  # sparsify stencil further
-            native_shift_values = math.reshaped_native(shift_val, [batch_val, *out_shape], force_expand=True)
-            mask = np.sum(abs(native_shift_values), 0)  # only 0 where no batch entry has a non-zero value
-            out_idx = numpy.nonzero(mask)
-            src_idx = [(component + shift_.get_size(dim) if dim in shift_ else component) % typed_src_shape.get_size(dim) for component, dim in zip(out_idx, out_shape)]
-            values.append(native_shift_values[(slice(None), *out_idx)])
-        else:  # add full stencil tensor
-            out_idx = np.unravel_index(np.arange(out_shape.volume), out_shape.sizes) if out_shape else 0
-            src_idx = [(component + shift_.get_size(dim) if dim in shift_ else component) % typed_src_shape.get_size(dim) for component, dim in zip(out_idx, out_shape)]
-            values.append(math.reshaped_native(shift_val, [batch_val, out_shape], force_expand=True))
-        out_indices.append(out_idx)
-        src_idx_all = []
-        for dim in typed_src_shape:
-            if dim in missing_dims:
-                if not separate_independent:
-                    offset = shift_.get_size(dim, default=0)
-                    src_idx_all.append(np.zeros_like(src_idx[0]) + offset)
-            else:
-                src_idx_all.append(src_idx[out_shape.index(dim)])
-        src_indices.append(src_idx_all)
-    indices_np = np.concatenate([np.concatenate(src_indices, axis=1), np.concatenate(out_indices, axis=1)]).T
-    indices = wrap(indices_np, instance('entries'), channel(vector=(sliced_src_shape if separate_independent else src_shape).names + out_shape.names))
-    backend = choose_backend(*values)
-    values = math.reshaped_tensor(backend.concat(values, axis=-1), [batch_val, instance('entries')])
-    dense_shape = concat_shapes((sliced_src_shape if separate_independent else src_shape) & out_shape)
-    matrix = SparseCoordinateTensor(indices, values, dense_shape, can_contain_double_entries=False, indices_sorted=False)
-    return matrix, tracer.bias
+from collections import namedtuple
+from typing import Callable, Dict, Set, Tuple, Union
+
+import numpy
+import numpy as np
+
+from ._ops import choose_backend_t
+from .backend import choose_backend, NUMPY, Backend
+from ._shape import Shape, parse_dim_order, merge_shapes, spatial, instance, batch, concat_shapes, EMPTY_SHAPE, dual, channel, non_batch
+from ._magic_ops import stack, expand
+from ._tensors import Tensor, wrap, disassemble_tree, disassemble_tensors, assemble_tree, TensorStack, may_vary_along
+from ._sparse import SparseCoordinateTensor
+from . import _ops as math
+
+
+TracerSource = namedtuple('TracerSource', ['shape', 'dtype', 'name', 'index'])
+
+
+class ShiftLinTracer(Tensor):
+    """
+    Tracer object for linear and affine functions.
+    The sparsity pattern is assumed equal for all grid cells and is reflected in `val` (e.g. for a 5-point stencil, `val` has 5 items).
+    The Tensors stored in `val` include position-dependent dimensions, allowing for different stencils at different positions.
+    Dimensions not contained in any `val` Tensor are treated as independent (batch dimensions).
+    """
+
+    def __init__(self, source: TracerSource, values_by_shift: dict, shape: Shape, bias: Tensor):
+        """
+        Args:
+            source: placeholder tensor
+            values_by_shift: `dict` mapping relative shifts (`Shape`) to value Tensors.
+                Shape keys only contain non-zero shift dims. Missing dims are interpreted as independent.
+            shape: shape of this tensor
+            bias: Constant Tensor to be added to the multiplication output, A*x + b.
+                A bias naturally arises at boundary cells with non-trivial boundary conditions if no ghost cells are added to the matrix.
+                When non-zero, this tracer technically represents an affine function, not a linear one.
+                However, the bias can be subtracted from the solution vector when solving a linear system, allowing this function to be solved with regular linear system solvers.
+        """
+        assert isinstance(source, TracerSource)
+        self.source = source
+        self.val: Dict[Shape, Tensor] = simplify_add(values_by_shift)
+        for shift_ in self.val.keys():
+            assert shift_.only(sorted(shift_.names), reorder=True) == shift_
+        self.bias = bias
+        self._shape = shape
+
+    def __repr__(self):
+        return f"Linear tracer {self._shape}"
+
+    def native(self, order: Union[str, tuple, list, Shape] = None):
+        """
+        Evaluates the value of the linear operation applied to the original source tensor.
+
+        This is done by building a sparse matrix for all dimensions that are affected by the linear operation.
+        These dimensions are detected automatically during the creation of the linear operation.
+        All other dimensions (independent dimensions) are combined into a single batch dimensions for the sparse matrix multiplication.
+
+        Args:
+          order: str or tuple or list:  (Default value = None)
+
+        Returns:
+
+        """
+        order = parse_dim_order(order, check_rank=self.rank)
+        result = self.apply(self.source)
+        result_order = order if order is not None else self._shape.names
+        return result.native(result_order)
+
+    @property
+    def dependent_dims(self) -> Set[str]:
+        """
+        Dimensions relevant to the linear operation.
+        This includes `pattern_dims` as well as dimensions along which only the values vary.
+        These dimensions cannot be parallelized trivially with a non-batched matrix.
+        """
+        bias_dims = [dim for dim in self.bias.shape.names if may_vary_along(self.bias, dim)]
+        return self.pattern_dim_names | set(sum([t.shape.names for t in self.val.values()], ())) | set(bias_dims)
+
+    @property
+    def pattern_dim_names(self) -> Set[str]:
+        """
+        Dimensions along which the sparse matrix contains off-diagonal elements.
+        These dimensions must be part of the sparse matrix and cannot be parallelized.
+        """
+        return set(sum([offset.names for offset in self.val], ()))
+
+    @property
+    def pattern_dims(self) -> Shape:
+        return self.source.shape.only(self.pattern_dim_names)
+
+    @property
+    def dtype(self):
+        return self.source.dtype
+
+    @property
+    def shape(self):
+        return self._shape
+
+    def _with_shape_replaced(self, new_shape):
+        raise NotImplementedError()
+
+    @property
+    def _is_tracer(self) -> bool:
+        return True
+
+    def _getitem(self, selection: dict):
+        starts = {dim: (item.start or 0) if isinstance(item, slice) else item for dim, item in selection.items()}
+        new_shape = math.zeros(self._shape)[selection].shape
+        return self.shift(starts, new_shape, lambda v: v[selection], lambda b: b[selection])
+
+    def shift(self, shifts: dict,
+              new_shape: Shape,
+              val_fun: Callable,
+              bias_fun: Callable = None):
+        """
+        Shifts all values of this tensor by `shifts`.
+        Values shifted outside will be mapped with periodic boundary conditions when the matrix is built.
+
+        Args:
+            shifts: Offsets by dimension
+            new_shape: Shape of the shifted tensor, must match the shape returned by `val_fun`.
+            val_fun: Function to apply to the matrix values, may change the tensor shapes
+            bias_fun: Function to apply to the bias vector, may change the tensor shape
+
+        Returns:
+            Shifted tensor, possibly with altered values.
+        """
+        val = {}
+        for shift, values in self.val.items():
+            assert isinstance(shift, Shape)
+            for dim, delta in reversed(tuple(shifts.items())):
+                if dim not in values.shape:
+                    values = math.expand(values, self._shape.only(dim))  # dim order may be scrambled
+                if delta:
+                    shift = shift._replace_single_size(dim, shift.get_size(dim) + delta) if dim in shift else shift._expand(spatial(**{dim: delta}))
+            val[shift.only(sorted(shift.names), reorder=True)] = val_fun(values)
+        bias = bias_fun(self.bias)
+        return ShiftLinTracer(self.source, val, new_shape, bias)
+
+    def unstack(self, dimension):
+        raise NotImplementedError()
+
+    def __neg__(self):
+        return ShiftLinTracer(self.source, {shift: -values for shift, values in self.val.items()}, self._shape, -self.bias)
+
+    def _op1(self, native_function):
+        # __neg__ is the only proper linear op1 and is implemented above.
+        if native_function.__name__ == 'isfinite':
+            test_output = self.apply(math.ones(self.source.shape, dtype=self.source.dtype))
+            return math.is_finite(test_output)
+        else:
+            raise NotImplementedError('Only linear operations are supported')
+
+    def _op2(self, other: Tensor,
+             operator: Callable,
+             native_function: Callable,
+             op_name: str = 'unknown',
+             op_symbol: str = '?') -> 'ShiftLinTracer':
+        """
+        Tensor-tensor operation.
+
+        Args:
+            other:
+            operator:
+            native_function:
+        """
+        assert op_symbol in '+-*/', f"Unsupported operation encountered while tracing linear function: {native_function}"
+        zeros_for_missing_self = op_name not in ['add', 'radd', 'rsub']  # perform `operator` where `self == 0`
+        zeros_for_missing_other = op_name not in ['add', 'radd', 'sub']  # perform `operator` where `other == 0`
+
+        if isinstance(other, ShiftLinTracer):
+            assert self.source is other.source, "Multiple linear tracers are not yet supported."
+            assert set(self._shape) == set(other._shape), f"Tracers have different shapes: {self._shape} and {other._shape}"
+            values = {}
+            for dim_shift in self.val.keys():
+                if dim_shift in other.val:
+                    values[dim_shift] = operator(self.val[dim_shift], other.val[dim_shift])
+                else:
+                    if zeros_for_missing_other:
+                        values[dim_shift] = operator(self.val[dim_shift], math.zeros_like(self.val[dim_shift]))
+                    else:
+                        values[dim_shift] = self.val[dim_shift]
+            for dim_shift, other_values in other.val.items():
+                if dim_shift not in self.val:
+                    if zeros_for_missing_self:
+                        values[dim_shift] = operator(math.zeros_like(other_values), other_values)
+                    else:
+                        values[dim_shift] = other_values
+            bias = operator(self.bias, other.bias)
+            return ShiftLinTracer(self.source, values, self._shape, bias)
+        else:
+            other = self._tensor(other)
+            if op_symbol in '*/':
+                values = {}
+                for dim_shift, val in self.val.items():
+                    values[dim_shift] = operator(val, other)
+                bias = operator(self.bias, other)
+                return ShiftLinTracer(self.source, values, self._shape & other.shape, bias)
+            elif op_symbol in '+-':
+                bias = operator(self.bias, other)
+                return ShiftLinTracer(self.source, self.val, self._shape & other.shape, bias)
+            else:
+                raise ValueError(f"Unsupported operation encountered while tracing linear function: {native_function}")
+
+    def _natives(self) -> tuple:
+        """
+        This function should only be used to determine the compatible backends, this tensor should be regarded as not available.
+        """
+        return sum([v._natives() for v in self.val.values()], ()) + self.bias._natives()
+
+    def _spec_dict(self) -> dict:
+        raise LinearTraceInProgress(self)
+
+
+class LinearTraceInProgress(Exception):
+
+    def __init__(self, tracer: ShiftLinTracer):
+        self.tracer = tracer
+
+
+def simplify_add(val: dict) -> Dict[Shape, Tensor]:
+    result = {}
+    for shift, values in val.items():
+        shift = shift[[i for i, size in enumerate(shift.sizes) if size != 0]]  # discard zeros
+        if shift in result:
+            result[shift] += values
+        else:
+            result[shift] = values
+    return result
+
+
+def matrix_from_function(f: Callable,
+                         *args,
+                         auxiliary_args=None,
+                         auto_compress=False,
+                         sparsify_batch=None,
+                         separate_independent=False,  # not fully implemented, requires auto_compress=False
+                         **kwargs) -> Tuple[Tensor, Tensor]:
+    """
+    Trace a linear function and construct a matrix.
+    Depending on the functional form of `f`, the returned matrix may be dense or sparse.
+
+    Args:
+        f: Function to trace.
+        *args: Arguments for `f`.
+        auxiliary_args: Arguments in which the function is not linear.
+            These parameters are not traced but passed on as given in `args` and `kwargs`.
+        auto_compress: If `True`, returns a compressed matrix if supported by the backend.
+        sparsify_batch: If `False`, the matrix will be batched.
+            If `True`, will create dual dimensions for the involved batch dimensions.
+            This will result in one large matrix instead of a batch of matrices.
+        **kwargs: Keyword arguments for `f`.
+
+    Returns:
+        matrix: Matrix representing the linear dependency of the output `f` on the input of `f`.
+            Input dimensions will be `dual` dimensions of the matrix while output dimensions will be regular.
+        bias: Bias for affine functions or zero-vector if the function is purely linear.
+    """
+    assert isinstance(auxiliary_args, str) or auxiliary_args is None, f"auxiliary_args must be a comma-separated str but got {auxiliary_args}"
+    from ._functional import function_parameters, f_name
+    f_params = function_parameters(f)
+    aux = set(s.strip() for s in auxiliary_args.split(',') if s.strip()) if isinstance(auxiliary_args, str) else f_params[1:]
+    all_args = {**kwargs, **{f_params[i]: v for i, v in enumerate(args)}}
+    aux_args = {k: v for k, v in all_args.items() if k in aux}
+    trace_args = {k: v for k, v in all_args.items() if k not in aux}
+    tree, tensors = disassemble_tree(trace_args)
+    target_backend = choose_backend_t(*tensors)
+    # --- Trace function ---
+    with NUMPY:
+        src = TracerSource(tensors[0].shape, tensors[0].dtype, tuple(trace_args.keys())[0], 0)
+        tracer = ShiftLinTracer(src, {EMPTY_SHAPE: math.ones()}, tensors[0].shape, math.zeros(tensors[0].shape, dtype=tensors[0].dtype))
+        x_kwargs = assemble_tree(tree, [tracer])
+        result = f(**x_kwargs, **aux_args)
+    _, result_tensors = disassemble_tree(result)
+    assert len(result_tensors) == 1, f"Linear function output must be or contain a single Tensor but got {result}"
+    tracer = result_tensors[0]._simplify()
+    assert tracer._is_tracer, f"Tracing linear function '{f_name(f)}' failed. Make sure only linear operations are used. Output: {tracer.shape}"
+    # --- Convert to COO ---
+    if sparsify_batch is None:
+        if auto_compress:
+            sparsify_batch = not target_backend.supports(Backend.csr_matrix_batched)
+        else:
+            sparsify_batch = not target_backend.supports(Backend.sparse_coo_tensor_batched)
+    matrix, bias = tracer_to_coo(tracer, sparsify_batch, separate_independent)
+    # --- Compress ---
+    if not auto_compress:
+        return matrix, bias
+    if matrix.default_backend.supports(Backend.mul_csr_dense) and target_backend.supports(Backend.mul_csr_dense):
+        return matrix.compress_rows(), bias
+    # elif backend.supports(Backend.mul_csc_dense):
+    #     return matrix.compress_cols(), tracer.bias
+    else:
+        return matrix, bias
+    
+
+def tracer_to_coo(tracer: Tensor, sparsify_batch: bool, separate_independent: bool):
+    # if isinstance(tracer, CollapsedTensor):
+    #     tracer = tracer._cached if tracer.is_cached else tracer._inner  # ignore collapsed dimensions. Alternatively, we could expand the result
+    #     return tracer_to_coo(tracer, sparsify_batch, separate_independent)
+    if isinstance(tracer, TensorStack):  # This indicates separable solves
+        matrices, biases = zip(*[tracer_to_coo(t, sparsify_batch, separate_independent) for t in tracer._tensors])
+        bias = stack(biases, tracer._stack_dim)
+        if not separate_independent:
+            indices = [math.concat_tensor([m._indices, expand(i, instance(m._indices), channel(vector=tracer._stack_dim.name))], 'vector') for i, m in enumerate(matrices)]
+            indices = math.concat_tensor(indices, 'entries')
+            values = math.concat_tensor([m._values for m in matrices], 'entries')
+            # matrix = stack(matrices, tracer._stack_dim)
+            dense_shape = concat_shapes(matrices[0]._dense_shape, tracer._stack_dim)
+            matrix = SparseCoordinateTensor(indices, values, dense_shape, can_contain_double_entries=False, indices_sorted=False, default=0)
+        else:
+            matrix = stack(matrices, tracer._stack_dim)
+        return matrix, bias
+    elif not tracer._is_tracer:  # This part of the output is independent of the input
+        return expand(0, tracer.shape), tracer
+    assert isinstance(tracer, ShiftLinTracer), f"Tracing linear function returned an unsupported construct: {type(tracer)}"
+    assert batch(tracer.pattern_dims).is_empty, f"Batch dimensions may not be sliced in linear operations but got pattern for {batch(tracer.pattern_dims)}"
+    missing_dims = tracer.source.shape.without(tracer.shape)  # these were sliced off
+    ignored_dims = tracer.source.shape.without(tracer.shape.only(tracer.dependent_dims) if sparsify_batch else tracer.pattern_dim_names).without(missing_dims)  # these will be parallelized and not added to the matrix
+    out_shape = tracer.shape.without(ignored_dims)
+    typed_src_shape = tracer.source.shape.without(ignored_dims)
+    src_shape = dual(**typed_src_shape.untyped_dict)
+    sliced_src_shape = src_shape.without(dual(**missing_dims.untyped_dict))
+    batch_val = merge_shapes(*tracer.val.values()).without(out_shape)
+    if non_batch(out_shape).is_empty:
+        assert len(tracer.val) == 1 and non_batch(tracer.val[EMPTY_SHAPE]) == EMPTY_SHAPE
+        return tracer.val[EMPTY_SHAPE], tracer.bias
+    out_indices = []
+    src_indices = []
+    values = []
+    for shift_, shift_val in tracer.val.items():
+        if shift_val.default_backend is NUMPY:  # sparsify stencil further
+            native_shift_values = math.reshaped_native(shift_val, [batch_val, *out_shape])
+            mask = np.sum(abs(native_shift_values), 0)  # only 0 where no batch entry has a non-zero value
+            out_idx = numpy.nonzero(mask)
+            src_idx = [(component + shift_.get_size(dim) if dim in shift_ else component) % typed_src_shape.get_size(dim) for component, dim in zip(out_idx, out_shape)]
+            values.append(native_shift_values[(slice(None), *out_idx)])
+        else:  # add full stencil tensor
+            out_idx = np.unravel_index(np.arange(out_shape.volume), out_shape.sizes) if out_shape else 0
+            src_idx = [(component + shift_.get_size(dim) if dim in shift_ else component) % typed_src_shape.get_size(dim) for component, dim in zip(out_idx, out_shape)]
+            values.append(math.reshaped_native(shift_val, [batch_val, out_shape]))
+        out_indices.append(out_idx)
+        src_idx_all = []
+        for dim in typed_src_shape:
+            if dim in missing_dims:
+                if not separate_independent:
+                    offset = shift_.get_size(dim, default=0)
+                    src_idx_all.append(np.zeros_like(src_idx[0]) + offset)
+            else:
+                src_idx_all.append(src_idx[out_shape.index(dim)])
+        src_indices.append(src_idx_all)
+    indices_np = np.concatenate([np.concatenate(src_indices, axis=1), np.concatenate(out_indices, axis=1)]).T
+    indices = wrap(indices_np, instance('entries'), channel(vector=(sliced_src_shape if separate_independent else src_shape).names + out_shape.names))
+    backend = choose_backend(*values)
+    values = math.reshaped_tensor(backend.concat(values, axis=-1), [batch_val, instance('entries')], convert=False)
+    dense_shape = concat_shapes((sliced_src_shape if separate_independent else src_shape) & out_shape)
+    matrix = SparseCoordinateTensor(indices, values, dense_shape, can_contain_double_entries=False, indices_sorted=False, default=0)
+    return matrix, tracer.bias
```

### Comparing `phiflow-2.3.4/phi/math/backend/_backend.py` & `phiflow-2.4.0/phi/math/backend/_backend.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,1590 +1,1744 @@
-import sys
-import warnings
-from collections import namedtuple
-from contextlib import contextmanager
-from typing import List, Callable, TypeVar, Tuple, Any, Union
-
-import logging
-import numpy
-
-from ._dtype import DType, combine_types, to_numpy_dtype
-
-
-SolveResult = namedtuple('SolveResult', [
-    'method', 'x', 'residual', 'iterations', 'function_evaluations', 'converged', 'diverged', 'message',
-])
-
-TensorType = TypeVar('TensorType')
-
-
-class ComputeDevice:
-    """
-    A physical device that can be selected to perform backend computations.
-    """
-
-    def __init__(self, backend: 'Backend', name: str, device_type: str, memory: int, processor_count: int, description: str, ref):
-        assert device_type in ('CPU', 'GPU', 'TPU')
-        self.name: str = name
-        """ Name of the compute device. CPUs are typically called `'CPU'`. """
-        self.device_type: str = device_type
-        """ Type of device such as `'CPU'`, `'GPU'` or `'TPU'`. """
-        self.memory: int = memory
-        """ Maximum memory of the device that can be allocated (in bytes). -1 for n/a. """
-        self.processor_count: int = processor_count
-        """ Number of CPU cores or GPU multiprocessors. -1 for n/a. """
-        self.description: str = description
-        """ Further information about the device such as driver version. """
-        self.ref = ref
-        """ Reference to the internal device representation. Two devices are equal if their refs are equal. """
-        self.backend: 'Backend' = backend
-        """ Backend that this device belongs to. Different backends represent the same device with different objects. """
-
-    def __repr__(self):
-        mem = f"{(self.memory / 1024 ** 2):.0f} MB" if self.memory > 0 else "memory: n/a"
-        pro = f"{self.processor_count} processors" if self.processor_count > 0 else "processors: n/a"
-        ref = f" '{self.ref}'" if isinstance(self.ref, str) else ""
-        descr = self.description.replace('\n', '  ')
-        if len(descr) > 30:
-            descr = descr[:28] + "..."
-        return f"{self.backend} device '{self.name}' ({self.device_type}{ref}) | {mem} | {pro} | {descr}"
-
-    def __eq__(self, other):
-        return isinstance(other, ComputeDevice) and other.ref == self.ref
-
-    def __hash__(self):
-        return hash(self.ref)
-
-
-class Backend:
-
-    def __init__(self, name: str, devices: List[ComputeDevice], default_device: ComputeDevice):
-        """
-        Backends delegate low-level operations to a compute library or emulate them.
-
-        The methods of `Backend` form a comprehensive list of available operations.
-
-        To support a compute library, subclass `Backend` and register it by adding it to `BACKENDS`.
-
-        Args:
-            name: Human-readable string
-            default_device: `ComputeDevice` being used by default
-        """
-        self._name = name
-        self._devices = tuple(devices)
-        self._default_device = default_device
-
-    def __enter__(self):
-        _DEFAULT.append(self)
-
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        _DEFAULT.pop(-1)
-
-    @property
-    def name(self) -> str:
-        return self._name
-
-    def supports(self, feature: Union[str, Callable]) -> bool:
-        """
-        Tests if this backend supports the given feature.
-        Features correspond to a method of this backend that must be implemented if the feature is supported.
-
-        Possible features:
-
-        * `sparse_coo_tensor`
-        * `gradients
-
-        Args:
-            feature: `str` or unbound Backend method, e.g. `Backend.sparse_coo_tensor`
-
-        Returns:
-            Whether the feature is supported.
-        """
-        feature = feature if isinstance(feature, str) else feature.__name__
-        if not hasattr(Backend, feature):
-            raise ValueError(f"Not a valid feature: '{feature}'")
-        backend_fun = getattr(Backend, feature)
-        impl_fun = getattr(self.__class__, feature)
-        return impl_fun is not backend_fun
-
-    def prefers_channels_last(self) -> bool:
-        raise NotImplementedError()
-
-    @property
-    def precision(self) -> int:
-        """ Short for math.backend.get_precision() """
-        return get_precision()
-
-    @property
-    def float_type(self) -> DType:
-        return DType(float, self.precision)
-
-    @property
-    def as_registered(self) -> 'Backend':
-        from phi.math.backend import BACKENDS
-        for backend in BACKENDS:
-            if self.name in backend.name:
-                return backend
-        raise RuntimeError(f"Backend '{self}' is not visible.")
-
-    @property
-    def complex_type(self) -> DType:
-        return DType(complex, max(64, self.precision))
-
-    def combine_types(self, *dtypes: DType) -> DType:
-        return combine_types(*dtypes, fp_precision=self.precision)
-
-    def auto_cast(self, *tensors, bool_to_int=False, int_to_float=False) -> list:
-        """
-        Determins the appropriate values type resulting from operations involving the tensors as input.
-        
-        This method is called by the default implementations of basic operators.
-        Backends can override this method to prevent unnecessary casting.
-
-        Args:
-            *tensors: tensors to cast and to consider when determining the common data type
-            bool_to_int: Whether to convert boolean values to integers if all values are boolean.
-
-        Returns:
-            tensors cast to a common data type
-        """
-        dtypes = [self.dtype(t) for t in tensors]
-        result_type = self.combine_types(*dtypes)
-        if result_type.kind == bool and bool_to_int:
-            result_type = DType(int, 32)
-        if result_type.kind == int and int_to_float:
-            result_type = DType(float, self.precision)
-        if result_type.kind in (int, float, complex, bool):  # do not cast everything to string!
-            tensors = [self.cast(t, result_type) for t in tensors]
-        return tensors
-
-    def __str__(self):
-        return self.name
-
-    def __repr__(self):
-        return self.name
-
-    def list_devices(self, device_type: Union[str, None] = None) -> List[ComputeDevice]:
-        """
-        Fetches information about all available compute devices this backend can use.
-
-        Implementations:
-
-        * NumPy: [`os.cpu_count`](https://docs.python.org/3/library/os.html#os.cpu_count)
-        * PyTorch: [`torch.cuda.get_device_properties`](https://pytorch.org/docs/stable/cuda.html#torch.cuda.get_device_properties)
-        * TensorFlow: `tensorflow.python.client.device_lib.list_local_devices`
-        * Jax: [`jax.devices`](https://jax.readthedocs.io/en/latest/jax.html#jax.devices)
-
-        See Also:
-            `Backend.set_default_device()`.
-
-        Args:
-            device_type: (optional) Return only devices of this type, e.g. `'GPU'` or `'CPU'`. See `ComputeDevice.device_type`.
-
-        Returns:
-            `list` of all currently available devices.
-        """
-        if device_type is None:
-            return list(self._devices)
-        else:
-            assert device_type in ('CPU', 'GPU', 'TPU'), "Device"
-            return [d for d in self._devices if d.device_type == device_type]
-
-    def get_default_device(self) -> ComputeDevice:
-        return self._default_device
-
-    def set_default_device(self, device: Union[ComputeDevice, str]) -> bool:
-        """
-        Sets the device new tensors will be allocated on.
-        This function will do nothing if the target device type is not available.
-
-        See Also:
-            `Backend.list_devices()`, `Backend.get_default_device()`.
-
-        Args:
-            device: `ComputeDevice` or device type as `str`, such as `'CPU'` or `'GPU'`.
-
-        Returns:
-            `bool` whether the device was successfully set.
-        """
-        if isinstance(device, str):
-            devices = self.list_devices(device)
-            if not devices:
-                warnings.warn(f"{self.name}: Cannot select '{device}' because no device of this type is available.", RuntimeWarning)
-                return False
-            device = devices[0]
-        assert device.backend is self, f"Cannot set default device to {device.name} for backend {self.name} because the devices belongs to backend {device.backend.name}"
-        self._default_device = device
-        return True
-
-    def get_device(self, tensor: TensorType) -> ComputeDevice:
-        """ Returns the device `tensor` is located on. """
-        raise NotImplementedError()
-
-    def get_device_by_ref(self, ref):
-        for device in self._devices:
-            if device.ref == ref:
-                return device
-        raise KeyError(f"{self.name} has no device with ref '{ref}'. Available: {[d.ref for d in self._devices]}")
-
-    def allocate_on_device(self, tensor: TensorType, device: ComputeDevice) -> TensorType:
-        """
-        Moves `tensor` to `device`. May copy the tensor if it is already on the device.
-
-        Args:
-            tensor: Existing tensor native to this backend.
-            device: Target device, associated with this backend.
-        """
-        raise NotImplementedError()
-
-    def seed(self, seed: int):
-        raise NotImplementedError()
-
-    def is_module(self, obj) -> bool:
-        """
-        Tests if `obj` is of a type that is specific to this backend, e.g. a neural network.
-        If `True`, this backend will be chosen for operations involving `obj`.
-
-        See Also:
-            `Backend.is_tensor()`.
-
-        Args:
-            obj: Object to test.
-        """
-        raise NotImplementedError()
-
-    def is_tensor(self, x, only_native=False):
-        """
-        An object is considered a native tensor by a backend if no internal conversion is required by backend methods.
-        An object is considered a tensor (nativer or otherwise) by a backend if it is not a struct (e.g. tuple, list) and all methods of the backend accept it as a tensor argument.
-
-        If `True`, this backend will be chosen for operations involving `x`.
-
-        See Also:
-            `Backend.is_module()`.
-
-        Args:
-          x: object to check
-          only_native: If True, only accepts true native tensor representations, not Python numbers or others that are also supported as tensors (Default value = False)
-
-        Returns:
-          bool: whether `x` is considered a tensor by this backend
-
-        """
-        raise NotImplementedError()
-
-    def as_tensor(self, x, convert_external=True):
-        """
-        Converts a tensor-like object to the native tensor representation of this backend.
-        If x is a native tensor of this backend, it is returned without modification.
-        If x is a Python number (numbers.Number instance), `convert_numbers` decides whether to convert it unless the backend cannot handle Python numbers.
-        
-        *Note:* There may be objects that are considered tensors by this backend but are not native and thus, will be converted by this method.
-
-        Args:
-          x: tensor-like, e.g. list, tuple, Python number, tensor
-          convert_external: if False and `x` is a Python number that is understood by this backend, this method returns the number as-is. This can help prevent type clashes like int32 vs int64. (Default value = True)
-
-        Returns:
-          tensor representation of `x`
-
-        """
-        raise NotImplementedError()
-
-    def is_available(self, tensor) -> bool:
-        """
-        Tests if the value of the tensor is known and can be read at this point.
-        If true, `numpy(tensor)` must return a valid NumPy representation of the value.
-        
-        Tensors are typically available when the backend operates in eager mode.
-
-        Args:
-          tensor: backend-compatible tensor
-
-        Returns:
-          bool
-
-        """
-        raise NotImplementedError()
-
-    def numpy(self, tensor) -> numpy.ndarray:
-        """
-        Returns a NumPy representation of the given tensor.
-        If `tensor` is already a NumPy array, it is returned without modification.
-        
-        This method raises an error if the value of the tensor is not known at this point, e.g. because it represents a node in a graph.
-        Use `is_available(tensor)` to check if the value can be represented as a NumPy array.
-
-        Args:
-          tensor: backend-compatible tensor
-
-        Returns:
-          NumPy representation of the values stored in the tensor
-
-        """
-        raise NotImplementedError()
-
-    def to_dlpack(self, tensor):
-        raise NotImplementedError()
-
-    def from_dlpack(self, capsule):
-        raise NotImplementedError()
-
-    def copy(self, tensor, only_mutable=False):
-        raise NotImplementedError()
-
-    def copy_leaves(self, tree, only_mutable=False):
-        if isinstance(tree, tuple):
-            return tuple([self.copy_leaves(e, only_mutable) for e in tree])
-        elif isinstance(tree, list):
-            return [self.copy_leaves(e, only_mutable) for e in tree]
-        elif isinstance(tree, dict):
-            return {k: self.copy_leaves(e, only_mutable) for k, e in tree.items()}
-        else:
-            return self.copy(tree, only_mutable=only_mutable)
-
-    def call(self, f: Callable, *args, name=None):
-        """
-        Calls `f(*args)` and returns the result.
-        This method may be used to register internal calls with the profiler.
-
-        Usage:
-
-            choose_backend(key).call(custom_function, *args)
-        """
-        return f(*args)
-
-    def block_until_ready(self, values):
-        pass
-
-    def jit_compile(self, f: Callable) -> Callable:
-        return NotImplemented
-
-    def jacobian(self, f: Callable, wrt: Union[tuple, list], get_output: bool, is_f_scalar: bool):
-        """
-        Args:
-            f: Function to differentiate. Returns a tuple containing `(reduced_loss, output)`
-            wrt: Argument indices for which to compute the gradient.
-            get_output: Whether the derivative function should return the output of `f` in addition to the gradient.
-            is_f_scalar: Whether `f` is guaranteed to return a scalar output.
-
-        Returns:
-            A function `g` with the same arguments as `f`.
-            If `get_output=True`, `g` returns a `tuple`containing the outputs of `f` followed by the gradients.
-            The gradients retain the dimensions of `reduced_loss` in order as outer (first) dimensions.
-        """
-        raise NotImplementedError(self)
-
-    def hessian(self, f: Callable, wrt: Union[tuple, list], get_output: bool, get_gradient: bool) -> tuple:
-        """
-        First dimension of all inputs/outputs of `f` is assumed to be a batch dimension.
-        Element-wise Hessians will be computed along the batch dimension.
-        All other dimensions are parameter dimensions and will appear twice in the Hessian matrices.
-
-        Args:
-            f: Function whose first output is a scalar float or complex value.
-            wrt:
-            get_output:
-            get_gradient:
-
-        Returns:
-            Function returning `(f(x), g(x), H(x))` or less depending on `get_output` and `get_gradient`.
-            The result is always a `tuple` holding at most these three items.
-        """
-        raise NotImplementedError(self)
-
-    def custom_gradient(self, f: Callable, gradient: Callable, get_external_cache: Callable = None, on_call_skipped: Callable = None) -> Callable:
-        """
-        Creates a function based on `f` that uses a custom gradient for backprop.
-
-        Args:
-            f: Forward function.
-            gradient: Function for backprop. Will be called as `gradient(*d_out)` to compute the gradient of `f`.
-
-        Returns:
-            Function with similar signature and return values as `f`. However, the returned function does not support keyword arguments.
-        """
-        return NotImplemented
-
-    def jit_compile_grad(self, f: Callable, wrt: Union[tuple, list], get_output: bool, is_f_scalar: bool):
-        raise NotImplementedError()
-
-    def jit_compile_hessian(self, f: Callable, wrt: Union[tuple, list], get_output: bool, get_gradient: bool):
-        raise NotImplementedError()
-
-    def transpose(self, tensor, axes):
-        """ Transposes the dimensions of `tensor` given the new axes order. The tensor will be cast to the default precision in the process. """
-        raise NotImplementedError()
-
-    def random_uniform(self, shape, low, high, dtype: Union[DType, None]):
-        """ Float tensor of selected precision containing random values in the range [0, 1) """
-        raise NotImplementedError(self)
-
-    def random_normal(self, shape, dtype: DType):
-        """ Float tensor of selected precision containing random values sampled from a normal distribution with mean 0 and std 1. """
-        raise NotImplementedError(self)
-
-    def stack(self, values, axis=0):
-        raise NotImplementedError(self)
-
-    def stack_leaves(self, trees: Union[tuple, list], axis=0):
-        tree0 = trees[0]
-        if isinstance(tree0, tuple):
-            return tuple([self.stack_leaves([tree[i] for tree in trees], axis=axis) for i in range(len(tree0))])
-        elif isinstance(tree0, list):
-            return [self.stack_leaves([tree[i] for tree in trees], axis=axis) for i in range(len(tree0))]
-        elif isinstance(tree0, dict):
-            return {k: self.stack_leaves([tree[k] for tree in trees], axis=axis) for k in tree0}
-        else:
-            return self.stack(trees, axis=axis)
-
-    def concat(self, values, axis):
-        raise NotImplementedError(self)
-
-    def pad(self, value, pad_width, mode: str = 'constant', constant_values=0):
-        """
-        Pad a tensor with values as specified by `mode` and `constant_values`.
-        
-        If the mode is not supported, returns NotImplemented.
-
-        Args:
-          value: tensor
-          pad_width: 2D tensor specifying the number of values padded to the edges of each axis in the form [[axis 0 lower, axis 0 upper], ...] including batch and component axes.
-          mode: constant', 'boundary', 'periodic', 'symmetric', 'reflect'
-          constant_values: used for out-of-bounds points if mode='constant' (Default value = 0)
-          mode: str:  (Default value = 'constant')
-
-        Returns:
-          padded tensor or NotImplemented
-
-        """
-        raise NotImplementedError(self)
-
-    def reshape(self, value, shape):
-        raise NotImplementedError(self)
-
-    def flip(self, value, axes: Union[tuple, list]):
-        slices = tuple(slice(None, None, -1 if i in axes else None) for i in range(self.ndims(value)))
-        return value[slices]
-
-    def sum(self, value, axis=None, keepdims=False):
-        raise NotImplementedError(self)
-
-    def prod(self, value, axis=None):
-        raise NotImplementedError(self)
-
-    def divide_no_nan(self, x, y):
-        """ Computes x/y but returns 0 if y=0. """
-        raise NotImplementedError(self)
-
-    def where(self, condition, x=None, y=None):
-        raise NotImplementedError(self)
-
-    def nonzero(self, values):
-        """
-        Args:
-            values: Tensor with only spatial dimensions
-
-        Returns:
-            non-zero multi-indices as tensor of shape (nnz, vector)
-        """
-        raise NotImplementedError(self)
-
-    def mean(self, value, axis=None, keepdims=False):
-        raise NotImplementedError(self)
-
-    def range(self, start, limit=None, delta=1, dtype: DType = DType(int, 32)):
-        raise NotImplementedError(self)
-
-    def zeros(self, shape, dtype: DType = None):
-        raise NotImplementedError(self)
-
-    def zeros_like(self, tensor):
-        raise NotImplementedError(self)
-
-    def ones(self, shape, dtype: DType = None):
-        raise NotImplementedError(self)
-
-    def ones_like(self, tensor):
-        raise NotImplementedError(self)
-
-    def meshgrid(self, *coordinates):
-        raise NotImplementedError(self)
-
-    def linspace(self, start, stop, number):
-        raise NotImplementedError(self)
-
-    def tensordot(self, a, a_axes: Union[tuple, list], b, b_axes: Union[tuple, list]):
-        """ Multiply-sum-reduce a_axes of a with b_axes of b. """
-        raise NotImplementedError(self)
-
-    def mul_matrix_batched_vector(self, A, b):
-        raise NotImplementedError(self)
-
-    def einsum(self, equation, *tensors):
-        raise NotImplementedError(self)
-
-    def cumsum(self, x, axis: int):
-        raise NotImplementedError(self)
-
-    def while_loop(self, loop: Callable, values: tuple, max_iter: Union[int, Tuple[int, ...], List[int]]):
-        """
-        If `max_iter is None`, runs
-
-        ```python
-        while any(values[0]):
-            values = loop(*values)
-        return values
-        ```
-
-        This operation does not support backpropagation.
-
-        Args:
-            loop: Loop function, must return a `tuple` with entries equal to `values` in shape and data type.
-            values: Initial values of loop variables.
-            max_iter: Maximum number of iterations to run, single `int` or sequence of integers.
-        Returns:
-            Loop variables upon loop completion if `max_iter` is a single integer.
-            If `max_iter` is a sequence, stacks the variables after each entry in `max_iter`, adding an outer dimension of size `<= len(max_iter)`.
-            If the condition is fulfilled before the maximum max_iter is reached, the loop may be broken or not, depending on the implementation.
-            If the loop is broken, the values returned by the last loop are expected to be constant and filled.
-        """
-        values = self.stop_gradient_tree(values)
-        if isinstance(max_iter, (tuple, list)):
-            trj = [self.copy_leaves(values, only_mutable=True)] if 0 in max_iter else []
-            for i in range(1, max(max_iter) + 1):
-                values = loop(*values)
-                if i in max_iter:
-                    trj.append(self.copy_leaves(values, only_mutable=True))
-                if not self.any(values[0]):
-                    break
-            trj.extend([trj[-1]] * (len(max_iter) - len(trj)))  # fill trj with final values
-            return self.stop_gradient_tree(self.stack_leaves(trj))
-        else:
-            for i in range(1, max_iter + 1):
-                if not self.any(values[0]):
-                    break
-                values = loop(*values)
-            return self.stop_gradient_tree(values)
-
-    def abs(self, x):
-        raise NotImplementedError(self)
-
-    def sign(self, x):
-        raise NotImplementedError(self)
-
-    def round(self, x):
-        raise NotImplementedError(self)
-
-    def ceil(self, x):
-        raise NotImplementedError(self)
-
-    def floor(self, x):
-        raise NotImplementedError(self)
-
-    def max(self, x, axis=None, keepdims=False):
-        raise NotImplementedError(self)
-
-    def min(self, x, axis=None, keepdims=False):
-        raise NotImplementedError(self)
-
-    def maximum(self, a, b):
-        raise NotImplementedError(self)
-
-    def minimum(self, a, b):
-        raise NotImplementedError(self)
-
-    def clip(self, x, minimum, maximum):
-        raise NotImplementedError(self)
-
-    def sqrt(self, x):
-        raise NotImplementedError(self)
-
-    def exp(self, x):
-        raise NotImplementedError(self)
-
-    def conv(self, value, kernel, zero_padding=True):
-        """
-        Convolve value with kernel.
-        Depending on the tensor rank, the convolution is either 1D (rank=3), 2D (rank=4) or 3D (rank=5).
-        Higher dimensions may not be supported.
-
-        Args:
-            value: tensor of shape (batch_size, in_channel, spatial...)
-            kernel: tensor of shape (batch_size or 1, out_channel, in_channel, spatial...)
-            zero_padding: If True, pads the edges of `value` with zeros so that the result has the same shape as `value`.
-
-        Returns:
-            Convolution result as tensor of shape (batch_size, out_channel, spatial...)
-        """
-        raise NotImplementedError(self)
-
-    def expand_dims(self, a, axis=0, number=1):
-        raise NotImplementedError(self)
-
-    def shape(self, tensor):
-        """
-        Returns the shape of a tensor.
-        The shape is iterable and implements `len()`.
-        For non-eager tensors, undefined dimensions should return a placeholder value representing the size.
-
-        See Also:
-            `Backend.staticshape()`.
-
-        Args:
-            tensor: Native tensor compatible with this backend.
-
-        Returns:
-            Shape of `tensor`
-        """
-        raise NotImplementedError(self)
-
-    def staticshape(self, tensor) -> tuple:
-        """
-        Evaluates the static shape of a native tensor.
-        If the tensor is eager, the shape is a `tuple[int]`.
-        For placeholder tensors, unknown dimensions are represented as `None`.
-
-        See Also:
-            `Backend.shape()`.
-
-        Args:
-            tensor: Native tensor compatible with this backend.
-
-        Returns:
-            `tuple` of sizes. Each size is an `int` if the size is defined, else `None`.
-        """
-        raise NotImplementedError(self)
-
-    def cast(self, x, dtype: DType):
-        raise NotImplementedError(self)
-
-    def to_float(self, x):
-        """
-        Converts a tensor to floating point values with precision equal to the currently set default precision.
-
-        See Also:
-            `Backend.precision()`.
-
-        If `x` is mutable and of the correct floating type, returns a copy of `x`.
-
-        To convert float tensors to the backend precision but leave non-float tensors untouched, use `Backend.as_tensor()`.
-
-        Args:
-            x: tensor of bool, int or float
-
-        Returns:
-            Values of `x` as float tensor
-        """
-        return self.cast(x, self.float_type)
-
-    def to_int32(self, x):
-        return self.cast(x, DType(int, 32))
-
-    def to_int64(self, x):
-        return self.cast(x, DType(int, 64))
-
-    def to_complex(self, x):
-        return self.cast(x, DType(complex, max(64, self.precision * 2)))
-
-    def gather(self, values, indices, axis: int):
-        """
-        Gathers values from the tensor `values` at locations `indices`.
-
-        Args:
-            values: tensor
-            indices: 1D tensor
-            axis: Axis along which to gather slices
-
-        Returns:
-            tensor, with size along `axis` being the length of `indices`
-        """
-        raise NotImplementedError(self)
-
-    def batched_gather_nd(self, values, indices):
-        """
-        Gathers values from the tensor `values` at locations `indices`.
-        The first dimension of `values` and `indices` is the batch dimension which must be either equal for both or one for either.
-
-        Args:
-            values: tensor of shape (batch, spatial..., channel)
-            indices: int tensor of shape (batch, any..., multi_index) where the size of multi_index is values.rank - 2.
-
-        Returns:
-            Gathered values as tensor of shape (batch, any..., channel)
-        """
-        raise NotImplementedError(self)
-
-    def flatten(self, x):
-        return self.reshape(x, (-1,))
-
-    def std(self, x, axis=None, keepdims=False):
-        raise NotImplementedError(self)
-
-    def boolean_mask(self, x, mask, axis=0):
-        """
-        Args:
-            x: tensor with any number of dimensions
-            mask: 1D mask tensor
-            axis: Axis index >= 0
-        """
-        raise NotImplementedError(self)
-
-    def isfinite(self, x):
-        raise NotImplementedError(self)
-
-    def scatter(self, base_grid, indices, values, mode: str):
-        """
-        Depending on `mode`, performs scatter_update or scatter_add.
-
-        Args:
-            base_grid: Tensor into which scatter values are inserted at indices. Tensor of shape (batch_size, spatial..., channels)
-            indices: Tensor of shape (batch_size or 1, update_count, index_vector)
-            values: Values to scatter at indices. Tensor of shape (batch_size or 1, update_count or 1, channels or 1)
-            mode: One of ('update', 'add')
-
-        Returns:
-            Copy of base_grid with values at `indices` updated by `values`.
-        """
-        raise NotImplementedError(self)
-
-    def any(self, boolean_tensor, axis=None, keepdims=False):
-        raise NotImplementedError(self)
-
-    def all(self, boolean_tensor, axis=None, keepdims=False):
-        raise NotImplementedError(self)
-
-    def quantile(self, x, quantiles):
-        """
-        Reduces the last / inner axis of x.
-
-        Args:
-            x: Tensor
-            quantiles: List or 1D tensor of quantiles to compute.
-
-        Returns:
-            Tensor with shape (quantiles, *x.shape[:-1])
-        """
-        raise NotImplementedError(self)
-
-    def fft(self, x, axes: Union[tuple, list]):
-        """
-        Computes the n-dimensional FFT along all but the first and last dimensions.
-
-        Args:
-          x: tensor of dimension 3 or higher
-          axes: Along which axes to perform the FFT
-
-        Returns:
-            Complex tensor `k`
-        """
-        raise NotImplementedError(self)
-
-    def ifft(self, k, axes: Union[tuple, list]):
-        """
-        Computes the n-dimensional inverse FFT along all but the first and last dimensions.
-
-        Args:
-          k: tensor of dimension 3 or higher
-          axes: Along which axes to perform the inverse FFT
-
-        Returns:
-            Complex tensor `x`
-        """
-        raise NotImplementedError(self)
-
-    def imag(self, x):
-        raise NotImplementedError(self)
-
-    def real(self, x):
-        raise NotImplementedError(self)
-
-    def conj(self, x):
-        raise NotImplementedError(self)
-
-    def sin(self, x):
-        raise NotImplementedError(self)
-
-    def arcsin(self, x):
-        raise NotImplementedError(self)
-
-    def cos(self, x):
-        raise NotImplementedError(self)
-
-    def arccos(self, x):
-        raise NotImplementedError(self)
-
-    def tan(self, x):
-        raise NotImplementedError(self)
-
-    def arctan(self, x):
-        raise NotImplementedError(self)
-
-    def arctan2(self, y, x):
-        raise NotImplementedError(self)
-
-    def sinh(self, x):
-        raise NotImplementedError(self)
-
-    def arcsinh(self, x):
-        raise NotImplementedError(self)
-
-    def cosh(self, x):
-        raise NotImplementedError(self)
-
-    def arccosh(self, x):
-        raise NotImplementedError(self)
-
-    def tanh(self, x):
-        raise NotImplementedError(self)
-
-    def arctanh(self, x):
-        raise NotImplementedError(self)
-
-    def log(self, x):
-        """ Natural logarithm """
-        raise NotImplementedError(self)
-
-    def log2(self, x):
-        raise NotImplementedError(self)
-
-    def log10(self, x):
-        raise NotImplementedError(self)
-
-    def sigmoid(self, x):
-        return 1 / (1 + self.exp(-x))
-
-    def dtype(self, array) -> DType:
-        raise NotImplementedError(self)
-
-    def tile(self, value, multiples):
-        """
-        Repeats the full tensor along each axis the number of times given by multiples.
-        If `multiples` has more dimensions than `value`, these dimensions are added to `value` as outer dimensions.
-
-        Args:
-            value: tensor
-            multiples: tuple or list of integers
-
-        Returns:
-            tiled tensor
-        """
-        raise NotImplementedError(self)
-
-    def repeat(self, x, repeats, axis: int):
-        """
-        Repeats the elements along `axis` `repeats` times.
-
-        Args:
-            x: Tensor
-            repeats: How often to repeat each element. 1D tensor of length x.shape[axis]
-            axis: Which axis to repeat elements along
-
-        Returns:
-            repeated Tensor
-        """
-        raise NotImplementedError(self)
-
-    def get_diagonal(self, matrices, offset=0):
-        """
-
-        Args:
-            matrices: (batch, rows, cols, channels)
-            offset: 0=diagonal, positive=above diagonal, negative=below diagonal
-
-        Returns:
-            diagonal: (batch, max(rows,cols), channels)
-        """
-        raise NotImplementedError(self)
-
-    def indexed_segment_sum(self, x, indices, axis: int):
-        """
-        Args:
-            x: Values to sum. Segments are laid out contiguously along `axis`. (batch, ...)
-            indices: should start with 0 along `axis`. (batch, indices)
-            axis: Axis along which to sum
-
-        Returns:
-            Tensor with `len(indices)` elements along `axis`. (batch, ..., indices, ...)
-        """
-        raise NotImplementedError(self)
-
-    def sparse_coo_tensor(self, indices: Union[tuple, list], values, shape: tuple):
-        """
-        Create a sparse matrix in coordinate list (COO) format.
-
-        Optional feature.
-
-        See Also:
-            `Backend.csr_matrix()`, `Backend.csc_matrix()`.
-
-        Args:
-            indices: 2D tensor of shape `(nnz, dims)`.
-            values: 1D values tensor matching `indices`
-            shape: Shape of the sparse matrix
-
-        Returns:
-            Native representation of the sparse matrix
-        """
-        raise NotImplementedError(self)
-
-    def sparse_coo_tensor_batched(self, indices: Union[tuple, list], values, shape: tuple):
-        """
-        Args:
-            indices: shape (batch_size, dims, nnz)
-            values: Values tensor matching `indices`, shape (batch_size, nnz)
-            shape: tuple of two ints representing the dense shape, (dims...)
-        """
-        raise NotImplementedError(self)
-
-    def mul_coo_dense(self, indices, values, shape, dense):
-        """
-        Multiply a batch of sparse coordinate matrices by a batch of dense matrices.
-        Every backend should implement this feature.
-        This is the fallback if CSR multiplication is not supported.
-
-        Args:
-            indices: (batch, nnz, ndims)
-            values: (batch, nnz, channels)
-            shape: Shape of the full matrix, tuple of length ndims
-            dense: (batch, dense_rows=sparse_cols, channels, dense_cols)
-
-        Returns:
-            (batch, channels, dense_rows=sparse_cols, dense_cols)
-        """
-        values, dense = self.auto_cast(values, dense)
-        batch_size, nnz, channel_count = self.staticshape(values)
-        _, dense_rows, _, dense_cols = self.staticshape(dense)
-        dense_formatted = self.reshape(dense, (batch_size, dense_rows, dense_cols * channel_count))
-        dense_gathered = self.batched_gather_nd(dense_formatted, indices[:, :, 1:2])
-        base_grid = self.zeros((batch_size, shape[0], dense.shape[3] * dense_cols), self.dtype(dense))
-        assert dense_cols == 1
-        result = self.scatter(base_grid, indices[:, :, 0:1], values * dense_gathered, mode='add')
-        return self.reshape(result, (batch_size, channel_count, dense_rows, dense_cols))
-
-    def coo_to_dense(self, indices, values, shape, contains_duplicates: bool):
-        batch_size, nnz, channel_count = self.staticshape(values)
-        base = self.zeros((batch_size, *shape, channel_count))
-        result = self.scatter(base, indices, values, mode='add' if contains_duplicates else 'update')
-        return result
-
-    def ilu_coo(self, indices, values, shape, iterations: int, safe: bool):
-        """ See incomplete_lu_coo() in _linalg """
-        from ._linalg import incomplete_lu_coo
-        assert self.dtype(values).kind in (bool, int, float)
-        return incomplete_lu_coo(self, indices, self.to_float(values), shape, iterations, safe)
-
-    def ilu_dense(self, matrix, iterations: int, safe: bool):
-        """ See incomplete_lu_dense() in _linalg """
-        from ._linalg import incomplete_lu_dense
-        assert self.dtype(matrix).kind in (bool, int, float)
-        return incomplete_lu_dense(self, self.to_float(matrix), iterations, safe)
-
-    def csr_matrix(self, column_indices, row_pointers, values, shape: Tuple[int, int]):
-        """
-        Create a sparse matrix in compressed sparse row (CSR) format.
-
-        Optional feature.
-
-        See Also:
-            `Backend.sparse_coo_tensor()`, `Backend.csc_matrix()`.
-
-        Args:
-            column_indices: Column indices corresponding to `values`, 1D tensor
-            row_pointers: Indices in `values` where any row starts, 1D tensor of length `rows + 1`
-            values: Non-zero values, 1D tensor
-            shape: Shape of the full matrix
-
-        Returns:
-            Native representation of the sparse matrix
-        """
-        raise NotImplementedError(self)
-
-    def csr_matrix_batched(self, column_indices, row_pointers, values, shape: Tuple[int, int]):
-        """
-        Args:
-            column_indices: Column indices corresponding to `values`, shape (batch_size, nnz)
-            row_pointers: Indices in `values` where any row starts, shape (batch_size, rows+1)
-            values: Non-zero values, shape (batch_size, nnz, channels)
-            shape: tuple of two ints representing the dense shape, (cols, rows)
-        """
-        raise NotImplementedError(self)
-
-    def mul_csr_dense(self, column_indices, row_pointers, values, shape: Tuple[int, int], dense):
-        """
-        Multiply a batch of compressed sparse row matrices by a batch of dense matrices.
-
-        Optional feature.
-
-        See Also:
-            `Backend.sparse_coo_tensor()`, `Backend.csc_matrix()`.
-
-        Args:
-            column_indices: (batch, nnz)
-            row_pointers: (batch, rows + 1)
-            values: (batch, nnz, channels)
-            shape: Shape of the full matrix (cols, rows)
-            dense: (batch, dense_rows=sparse_cols, channels, dense_cols)
-
-        Returns:
-            (batch, channels, dense_rows=sparse_cols, dense_cols)
-        """
-        # if not self.supports(Backend.indexed_segment_sum):
-        native_coo_indices = self.csr_to_coo(column_indices, row_pointers)
-        return self.mul_coo_dense(native_coo_indices, values, shape, dense)
-        # values, dense = self.auto_cast(values, dense)
-        # batch_size, nnz, channel_count = self.staticshape(values)
-        # _, dense_rows, _, dense_cols = self.staticshape(dense)
-        # assert dense_cols == 1
-        # dense_formatted = self.reshape(dense, (batch_size, dense_rows, channel_count * dense_cols))
-        # dense_gathered = self.batched_gather_nd(dense_formatted, self.expand_dims(column_indices, -1))  # (batch, nnz, channels*rhs_cols)
-        # dense_gathered = self.reshape(dense_gathered, (batch_size, nnz, channel_count, dense_cols))
-        # values = self.reshape(values, (batch_size, nnz, channel_count, 1))
-        # result = self.indexed_segment_sum(values * dense_gathered, row_pointers[:, :-1], 1)
-        # return self.reshape(result, (batch_size, channel_count, rhs_rows, rhs_cols))
-
-    def csr_to_coo(self, column_indices, row_pointers):
-        """
-        Convert a batch of compressed sparse matrices to sparse coordinate matrices.
-
-        Args:
-            column_indices: (batch, nnz)
-            row_pointers: (batch, rows + 1)
-
-        Returns:
-            indices: (batch, nnz, 2)
-        """
-        batch_size = self.staticshape(column_indices)[0]
-        repeats = row_pointers[:, 1:] - row_pointers[:, :-1]
-        row_count = self.shape(repeats)[-1]
-        row_indices = [self.repeat(self.range(row_count, dtype=self.dtype(column_indices)), repeats[b], -1) for b in range(batch_size)]
-        return self.stack([self.stack(row_indices), column_indices], axis=-1)
-
-    def csr_to_dense(self, column_indices, row_pointers, values, shape: Tuple[int, int]):
-        indices = self.csr_to_coo(column_indices, row_pointers)
-        return self.coo_to_dense(indices, values, shape, contains_duplicates=False)
-
-    def csc_matrix(self, column_pointers, row_indices, values, shape: Tuple[int, int]):
-        """
-        Create a sparse matrix in compressed sparse column (CSC) format.
-
-        Optional feature.
-
-        See Also:
-            `Backend.sparse_coo_tensor()`, `Backend.csr_matrix()`.
-
-        Args:
-            column_pointers: Indices in `values` where any column starts, 1D tensor of length `cols + 1`
-            row_indices: Row indices corresponding to `values`.
-            values: Non-zero values, 1D tensor
-            shape: Shape of the full matrix
-
-        Returns:
-            Native representation of the sparse matrix
-        """
-        raise NotImplementedError(self)
-
-    def csc_matrix_batched(self, column_pointers, row_indices, values, shape: Tuple[int, int]):
-        """
-        Args:
-            column_pointers: Indices in `values` where any row starts, shape (batch_size, cols+1)
-            row_indices: Row indices corresponding to `values`, shape (batch_size, nnz)
-            values: Non-zero values, shape (batch_size, nnz, channels)
-            shape: tuple of two ints representing the dense shape, (cols, rows)
-        """
-        raise NotImplementedError(self)
-
-    def pairwise_distances(self, positions, max_radius, format: str, index_dtype=DType(int, 32)) -> list:
-        """
-
-        Args:
-            positions: Point locations of shape (batch, instances, vector)
-            max_radius: Scalar or (batch,) or (batch, instances)
-            format: 'csr',  Not yet implemented: 'sparse', 'coo', 'csc'
-            index_dtype: Either int32 or int64
-
-        Returns:
-            Sequence of batch_size sparse distance matrices
-        """
-        from sklearn import neighbors
-        batch_size, point_count, _vec_count = self.staticshape(positions)
-        positions_np_batched = self.numpy(positions)
-        result = []
-        for i in range(batch_size):
-            tree = neighbors.KDTree(positions_np_batched[i])
-            radius = float(max_radius) if len(self.staticshape(max_radius)) == 0 else max_radius[i]
-            nested_neighbors = tree.query_radius(positions_np_batched[i], r=radius)  # ndarray[ndarray]
-            if format == 'csr':
-                column_indices = numpy.concatenate(nested_neighbors).astype(to_numpy_dtype(index_dtype))  # flattened_neighbors
-                neighbor_counts = [len(nlist) for nlist in nested_neighbors]
-                row_pointers = numpy.concatenate([[0], numpy.cumsum(neighbor_counts)]).astype(to_numpy_dtype(index_dtype))
-                pos_neighbors = self.gather(positions[i], column_indices, 0)
-                pos_self = self.repeat(positions[i], neighbor_counts, axis=0)
-                values = pos_neighbors - pos_self
-                result.append((column_indices, row_pointers, values))
-                # sparse_matrix = self.csr_matrix(column_indices, row_pointers, values, (point_count, point_count))
-                # sparse_matrix.eliminate_zeros()  # setdiag(0) keeps zero entries
-            else:
-                raise NotImplementedError(format)
-        return result
-
-    def minimize(self, method: str, f, x0, atol, max_iter, trj: bool):
-        if method == 'auto':
-            method = 'L-BFGS-B'
-        if method == 'GD':
-            from ._minimize import gradient_descent
-            return gradient_descent(self, f, x0, atol, max_iter, trj)
-        else:
-            from ._minimize import scipy_minimize
-            return scipy_minimize(self, method, f, x0, atol, max_iter, trj)
-
-    def linear_solve(self, method: str, lin, y, x0, tol_sq, max_iter) -> SolveResult:
-        """
-        Solve the system of linear equations A · x = y.
-        This method need not provide a gradient for the operation.
-
-        Args:
-            method: Which algorithm to use. One of `('auto', 'CG', 'CG-adaptive')`.
-            lin: Linear operation. One of
-                * sparse/dense matrix valid for all instances
-                * tuple/list of sparse/dense matrices for varying matrices along batch, must have the same nonzero locations.
-                * linear function A(x), must be called on all instances in parallel
-            y: target result of A * x. 2nd order tensor (batch, vector) or list of vectors.
-            x0: Initial guess of size (batch, parameters)
-            tol_sq: Squared absolute tolerance of size (batch,)
-            max_iter: Maximum number of iterations of size (batch,) or a sequence of maximum iterations to obtain a trajectory.
-
-        Returns:
-            `SolveResult`
-        """
-        if method == 'auto':
-            return self.conjugate_gradient_adaptive(lin, y, x0, tol_sq, max_iter)
-        elif method == 'CG':
-            return self.conjugate_gradient(lin, y, x0, tol_sq, max_iter)
-        elif method == 'CG-adaptive':
-            return self.conjugate_gradient_adaptive(lin, y, x0, tol_sq, max_iter)
-        elif method in ['biCG', 'biCG-stab(0)']:
-            raise NotImplementedError("Unstabilized Bi-CG not yet supported")
-            # return self.bi_conjugate_gradient_original(lin, y, x0, tol_sq, max_iter)
-        elif method == 'biCG-stab':
-            return self.bi_conjugate_gradient(lin, y, x0, tol_sq, max_iter, poly_order=1)
-        elif method.startswith('biCG-stab('):
-            order = int(method[len('biCG-stab('):-1])
-            return self.bi_conjugate_gradient(lin, y, x0, tol_sq, max_iter, poly_order=order)
-        else:
-            raise NotImplementedError(f"Method '{method}' not supported for linear solve.")
-
-    def conjugate_gradient(self, lin, y, x0, tol_sq, max_iter) -> SolveResult:
-        """ Standard conjugate gradient algorithm. Signature matches to `Backend.linear_solve()`. """
-        from ._linalg import cg, stop_on_l2
-        return cg(self, lin, y, x0, stop_on_l2(self, tol_sq, max_iter), max_iter)
-
-    def conjugate_gradient_adaptive(self, lin, y, x0, tol_sq, max_iter) -> SolveResult:
-        """ Conjugate gradient algorithm with adaptive step size. Signature matches to `Backend.linear_solve()`. """
-        from ._linalg import cg_adaptive, stop_on_l2
-        return cg_adaptive(self, lin, y, x0, stop_on_l2(self, tol_sq, max_iter), max_iter)
-
-    def bi_conjugate_gradient(self, lin, y, x0, tol_sq, max_iter, poly_order=2) -> SolveResult:
-        """ Generalized stabilized biconjugate gradient algorithm. Signature matches to `Backend.linear_solve()`. """
-        from ._linalg import bicg, stop_on_l2
-        return bicg(self, lin, y, x0, stop_on_l2(self, tol_sq, max_iter), max_iter, poly_order)
-
-    def linear(self, lin, vector):
-        if callable(lin):
-            return lin(vector)
-        elif isinstance(lin, (tuple, list)):
-            for lin_i in lin:
-                lin_shape = self.staticshape(lin_i)
-                assert len(lin_shape) == 2
-            return self.stack([self.mul_matrix_batched_vector(m, v) for m, v in zip(lin, self.unstack(vector))])
-        else:
-            lin_shape = self.staticshape(lin)
-            assert len(lin_shape) == 2, f"A must be a matrix but got shape {lin_shape}"
-            return self.mul_matrix_batched_vector(lin, vector)
-
-    def matrix_solve_least_squares(self, matrix: TensorType, rhs: TensorType) -> Tuple[TensorType, TensorType, TensorType, TensorType]:
-        """
-        Args:
-            matrix: Shape (batch, vec, constraints)
-            rhs: Shape (batch, vec, batch_per_matrix)
-
-        Returns:
-            solution: Solution vector of Shape (batch, constraints, batch_per_matrix)
-            residuals: Optional, can be `None`
-            rank: Optional, can be `None`
-            singular_values: Optional, can be `None`
-        """
-        raise NotImplementedError(self)
-
-    def solve_triangular_dense(self, matrix, rhs, lower: bool, unit_diagonal: bool):
-        """
-
-        Args:
-            matrix: (batch_size, rows, cols)
-            rhs: (batch_size, cols)
-            lower:
-            unit_diagonal:
-
-        Returns:
-            (batch_size, cols)
-        """
-        raise NotImplementedError(self)
-
-    def stop_gradient(self, value):
-        raise NotImplementedError(self)
-
-    def stop_gradient_tree(self, tree):
-        if isinstance(tree, tuple):
-            return tuple([self.stop_gradient_tree(v) for v in tree])
-        if isinstance(tree, list):
-            return [self.stop_gradient_tree(v) for v in tree]
-        if isinstance(tree, dict):
-            return {k: self.stop_gradient_tree(v) for k, v in tree.items()}
-        return self.stop_gradient(tree)
-
-    def grid_sample(self, grid, coordinates, extrapolation: str):
-        """
-        Interpolates a regular grid at the specified coordinates.
-
-        Args:
-            grid: Tensor of shape (batch, spatial..., channel)
-            coordinates: Tensor of floating grid indices of shape (batch, instance..., vector).
-                The last dimension must match `spatial_dims`.
-                The first grid point of dimension i lies at position 0, the last at values.shape[i]-1.
-            extrapolation: Values to use for coordinates outside the grid.
-                One of `('undefined', 'zeros', 'boundary', 'periodic', 'symmetric', 'reflect')`.
-
-        Returns:
-            sampled values with linear interpolation
-        """
-        return NotImplemented
-
-    def variable(self, value):
-        return NotImplemented
-
-    def ndims(self, tensor):
-        return len(self.staticshape(tensor))
-
-    def size(self, array):
-        return self.prod(self.shape(array))
-
-    def multi_slice(self, tensor, slices: tuple):
-        """
-        Args:
-            tensor: value to slice
-            slices: `tuple` of `slice`, `int`, or scalar integer tensors
-        """
-        return tensor[slices]
-
-    def batch_gather(self, tensor, batches):
-        if isinstance(batches, int):
-            batches = [batches]
-        return tensor[batches, ...]
-
-    def unstack(self, tensor, axis=0, keepdims=False) -> tuple:
-        if axis < 0:
-            axis += len(tensor.shape)
-        if axis >= len(tensor.shape) or axis < 0:
-            raise ValueError("Illegal axis value")
-        result = []
-        for slice_idx in range(tensor.shape[axis]):
-            if keepdims:
-                component = tensor[tuple([slice(slice_idx, slice_idx + 1) if d == axis else slice(None) for d in range(len(tensor.shape))])]
-            else:
-                component = tensor[tuple([slice_idx if d == axis else slice(None) for d in range(len(tensor.shape))])]
-            result.append(component)
-        return tuple(result)
-
-    def equal(self, x, y):
-        """ Element-wise equality check """
-        raise NotImplementedError(self)
-
-    def not_equal(self, x, y):
-        return ~self.equal(x, y)
-
-    def greater_than(self, x, y):
-        x, y = self.auto_cast(x, y)
-        return x > y
-
-    def greater_or_equal(self, x, y):
-        x, y = self.auto_cast(x, y)
-        return x >= y
-
-    def add(self, a, b):
-        a, b = self.auto_cast(a, b, bool_to_int=True)
-        return a + b
-
-    def sub(self, a, b):
-        a, b = self.auto_cast(a, b, bool_to_int=True)
-        return a - b
-
-    def mul(self, a, b):
-        a, b = self.auto_cast(a, b)
-        return a * b
-
-    def div(self, numerator, denominator):
-        numerator, denominator = self.auto_cast(numerator, denominator)
-        return numerator / denominator
-
-    def pow(self, base, exp):
-        base, exp = self.auto_cast(base, exp)
-        return base ** exp
-
-    def mod(self, dividend, divisor):
-        dividend, divisor = self.auto_cast(dividend, divisor)
-        return dividend % divisor
-
-    def and_(self, a, b):
-        a, b = self.auto_cast(a, b)
-        return a & b
-
-    def or_(self, a, b):
-        a, b = self.auto_cast(a, b)
-        return a | b
-
-    def xor(self, a, b):
-        a, b = self.auto_cast(a, b)
-        return a ^ b
-
-    def floordiv(self, a, b):
-        a, b = self.auto_cast(a, b)
-        return a // b
-
-    def shift_bits_left(self, a, b):
-        a, b = self.auto_cast(a, b)
-        return a << b
-
-    def shift_bits_right(self, a, b):
-        a, b = self.auto_cast(a, b)
-        return a >> b
-
-
-BACKENDS = []
-""" Global list of all registered backends. Register a `Backend` by adding it to the list. """
-_DEFAULT = []  # [0] = global default, [1:] from 'with' blocks
-_PRECISION = [32]  # [0] = global precision in bits, [1:] from 'with' blocks
-
-
-def choose_backend(*values, prefer_default=False) -> Backend:
-    """
-    Selects a suitable backend to handle the given values.
-
-    This function is used by most math functions operating on `Tensor` objects to delegate the actual computations.
-
-    Backends need to be registered to be available, e.g. via the global import `phi.<backend>` or `phi.detect_backends()`.
-
-    Args:
-        *values:
-        prefer_default: Whether to always select the default backend if it can work with `values`, see `default_backend()`.
-
-    Returns:
-        The selected `Backend`
-    """
-    # --- Default Backend has priority ---
-    if _is_applicable(_DEFAULT[-1], values) and (prefer_default or _is_specific(_DEFAULT[-1], values)):
-        return _DEFAULT[-1]
-    # --- Filter out non-applicable ---
-    backends = [backend for backend in BACKENDS if _is_applicable(backend, values)]
-    if len(backends) == 0:
-        raise NoBackendFound(f"No backend found for types {[type(v).__name__ for v in values]}; registered backends are {BACKENDS}")
-    # --- Native tensors? ---
-    for backend in backends:
-        if _is_specific(backend, values):
-            return backend
-    return backends[0]
-
-
-class NoBackendFound(Exception):
-    """
-    Thrown by `choose_backend` if no backend can handle the given values.
-    """
-
-    def __init__(self, msg):
-        Exception.__init__(self, msg)
-
-
-def default_backend() -> Backend:
-    """
-    The default backend is preferred by `choose_backend()`.
-
-    The default backend can be set globally using `set_global_default_backend()` and locally using `with backend:`.
-
-    Returns:
-        current default `Backend`
-    """
-    return _DEFAULT[-1]
-
-
-def context_backend() -> Union[Backend, None]:
-    """
-    Returns the backend set by the inner-most surrounding `with backend:` block.
-    If called outside a backend context, returns `None`.
-
-    Returns:
-        `Backend` or `None`
-    """
-    return _DEFAULT[-1] if len(_DEFAULT) > 1 else None
-
-
-def set_global_default_backend(backend: Backend):
-    """
-    Sets the given backend as default.
-    This setting can be overridden using `with backend:`.
-
-    See `default_backend()`, `choose_backend()`.
-
-    Args:
-        backend: `Backend` to set as default
-    """
-    assert isinstance(backend, Backend)
-    _DEFAULT[0] = backend
-
-
-def set_global_precision(floating_point_bits: int):
-    """
-    Sets the floating point precision of DYNAMIC_BACKEND which affects all registered backends.
-
-    If `floating_point_bits` is an integer, all floating point tensors created henceforth will be of the corresponding data type, float16, float32 or float64.
-    Operations may also convert floating point values to this precision, even if the input had a different precision.
-
-    If `floating_point_bits` is None, new tensors will default to float32 unless specified otherwise.
-    The output of math operations has the same precision as its inputs.
-
-    Args:
-      floating_point_bits: one of (16, 32, 64, None)
-    """
-    _PRECISION[0] = floating_point_bits
-
-
-def get_precision() -> int:
-    """
-    Gets the current target floating point precision in bits.
-    The precision can be set globally using `set_global_precision()` or locally using `with precision(p):`.
-
-    Any Backend method may convert floating point values to this precision, even if the input had a different precision.
-
-    Returns:
-        16 for half, 32 for single, 64 for double
-    """
-    return _PRECISION[-1]
-
-
-@contextmanager
-def precision(floating_point_bits: int):
-    """
-    Sets the floating point precision for the local context.
-
-    Usage: `with precision(p):`
-
-    This overrides the global setting, see `set_global_precision()`.
-
-    Args:
-        floating_point_bits: 16 for half, 32 for single, 64 for double
-    """
-    _PRECISION.append(floating_point_bits)
-    try:
-        yield None
-    finally:
-        _PRECISION.pop(-1)
-
-
-def convert(tensor, backend: Backend = None, use_dlpack=True):
-    """
-    Convert a Tensor to the native format of `backend`.
-    If the target backend can operate natively on `tensor`, returns `tensor`.
-
-    If both backends support *DLPack* and `use_dlpack=True`, uses zero-copy conversion using the DLPack library.
-    Else, intermediately converts `tensor` to a NumPy array.
-
-    *Warning*: This operation breaks the automatic differentiation chain.
-
-    Args:
-        tensor: Native tensor belonging to any registered backend.
-        backend: Target backend. If `None`, uses the current default backend, see `default_backend()`.
-
-    Returns:
-        Tensor belonging to `backend`.
-    """
-    backend = backend or default_backend()
-    current_backend = choose_backend(tensor, prefer_default=False)
-    if backend.is_tensor(tensor, True) or backend is current_backend:
-        return tensor
-    if use_dlpack and current_backend.supports(Backend.to_dlpack) and backend.supports(Backend.from_dlpack):
-        capsule = current_backend.to_dlpack(tensor)
-        return backend.from_dlpack(capsule)
-    else:
-        nparray = current_backend.numpy(tensor)
-        return backend.as_tensor(nparray)
-
-
-# Backend choice utility functions
-
-def _is_applicable(backend, values):
-    for value in values:
-        if not (backend.is_tensor(value, only_native=False) or backend.is_module(value)):
-            return False
-    return True
-
-
-def _is_specific(backend: Backend, values):
-    for value in values:
-        if backend.is_tensor(value, only_native=True) or backend.is_module(value):
-            return True
-    return False
-
-
-# Other low-level helper functions
-
-def combined_dim(dim1, dim2, type_str: str = 'batch'):
-    if dim1 is None and dim2 is None:
-        return None
-    if dim1 is None or dim1 == 1:
-        return dim2
-    if dim2 is None or dim2 == 1:
-        return dim1
-    assert dim1 == dim2, f"Incompatible {type_str} dimensions: x0 {dim1}, y {dim2}"
-    return dim1
-
-
-_SPATIAL_DERIVATIVE_CONTEXT = [0]
-_FUNCTIONAL_DERIVATIVE_CONTEXT = [0]
-
-
-@contextmanager
-def spatial_derivative_evaluation(order=1):
-    _SPATIAL_DERIVATIVE_CONTEXT.append(order)
-    try:
-        yield None
-    finally:
-        assert _SPATIAL_DERIVATIVE_CONTEXT.pop(-1) == order
-
-
-def get_spatial_derivative_order():
-    """
-    Extrapolations may behave differently when extrapolating the derivative of a grid.
-    Returns 1 inside a CG loop, and 0 by default.
-    """
-    return _SPATIAL_DERIVATIVE_CONTEXT[-1]
-
-
-@contextmanager
-def functional_derivative_evaluation(order=1):
-    _FUNCTIONAL_DERIVATIVE_CONTEXT.append(order)
-    try:
-        yield None
-    finally:
-        assert _FUNCTIONAL_DERIVATIVE_CONTEXT.pop(-1) == order
-
-
-def get_functional_derivative_order():
-    """
-    Operations that do not define a first or higher-order derivative may use slower alternative code paths when the derivative is `>0`.
-    This is set when calling a function created by `math.jacobian()` or `math.hessian()`.
-    """
-    return _FUNCTIONAL_DERIVATIVE_CONTEXT[-1]
-
-
-PHI_LOGGER = logging.getLogger('Φ')  # used for warnings and debug messages by all internal PhiFlow functions
-_LOG_CONSOLE_HANDLER = logging.StreamHandler(sys.stdout)
-_LOG_CONSOLE_HANDLER.setFormatter(logging.Formatter("%(message)s (%(levelname)s), %(asctime)sn\n"))
-_LOG_CONSOLE_HANDLER.setLevel(logging.NOTSET)
-PHI_LOGGER.addHandler(_LOG_CONSOLE_HANDLER)
+import logging
+import sys
+import warnings
+from contextlib import contextmanager
+from dataclasses import dataclass
+from typing import List, Callable, TypeVar, Tuple, Union, Optional
+
+import numpy
+import numpy as np
+from numpy import ndarray
+
+from ._dtype import DType, combine_types
+
+
+TensorType = TypeVar('TensorType')
+TensorOrArray = Union[TensorType, np.ndarray]
+
+
+@dataclass
+class SolveResult:
+    method: str
+    x: TensorType
+    residual: TensorType
+    iterations: TensorType
+    function_evaluations: TensorType
+    converged: TensorType  # (max_iter+1, batch) or (batch,)
+    diverged: TensorType  # (max_iter+1, batch) or (batch,)
+    message: List[str]  # (batch,)
+
+
+class Preconditioner:
+    def apply(self, vec):
+        raise NotImplementedError
+
+    def apply_transposed(self, vec):
+        raise NotImplementedError
+
+    def apply_inv_l(self, vec):
+        raise NotImplementedError
+
+    def apply_inv_u(self, vec):
+        raise NotImplementedError
+
+
+class ComputeDevice:
+    """
+    A physical device that can be selected to perform backend computations.
+    """
+
+    def __init__(self, backend: 'Backend', name: str, device_type: str, memory: int, processor_count: int, description: str, ref):
+        assert device_type in ('CPU', 'GPU', 'TPU')
+        self.name: str = name
+        """ Name of the compute device. CPUs are typically called `'CPU'`. """
+        self.device_type: str = device_type
+        """ Type of device such as `'CPU'`, `'GPU'` or `'TPU'`. """
+        self.memory: int = memory
+        """ Maximum memory of the device that can be allocated (in bytes). -1 for n/a. """
+        self.processor_count: int = processor_count
+        """ Number of CPU cores or GPU multiprocessors. -1 for n/a. """
+        self.description: str = description
+        """ Further information about the device such as driver version. """
+        self.ref = ref
+        """ Reference to the internal device representation. Two devices are equal if their refs are equal. """
+        self.backend: 'Backend' = backend
+        """ Backend that this device belongs to. Different backends represent the same device with different objects. """
+
+    def __repr__(self):
+        mem = f"{(self.memory / 1024 ** 2):.0f} MB" if self.memory > 0 else "memory: n/a"
+        pro = f"{self.processor_count} processors" if self.processor_count > 0 else "processors: n/a"
+        ref = f" '{self.ref}'" if isinstance(self.ref, str) else ""
+        descr = self.description.replace('\n', '  ')
+        if len(descr) > 30:
+            descr = descr[:28] + "..."
+        return f"{self.backend} device '{self.name}' ({self.device_type}{ref}) | {mem} | {pro} | {descr}"
+
+    def __eq__(self, other):
+        return isinstance(other, ComputeDevice) and other.ref == self.ref
+
+    def __hash__(self):
+        return hash(self.ref)
+
+
+class Backend:
+
+    def __init__(self, name: str, devices: List[ComputeDevice], default_device: ComputeDevice):
+        """
+        Backends delegate low-level operations to a compute library or emulate them.
+
+        The methods of `Backend` form a comprehensive list of available operations.
+
+        To support a compute library, subclass `Backend` and register it by adding it to `BACKENDS`.
+
+        Args:
+            name: Human-readable string
+            default_device: `ComputeDevice` being used by default
+        """
+        self._name = name
+        self._devices = tuple(devices)
+        self._default_device = default_device
+
+    def __enter__(self):
+        _DEFAULT.append(self)
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        _DEFAULT.pop(-1)
+
+    @property
+    def name(self) -> str:
+        return self._name
+
+    def supports(self, feature: Union[str, Callable]) -> bool:
+        """
+        Tests if this backend supports the given feature.
+        Features correspond to a method of this backend that must be implemented if the feature is supported.
+
+        Possible features:
+
+        * `sparse_coo_tensor`
+        * `gradients
+
+        Args:
+            feature: `str` or unbound Backend method, e.g. `Backend.sparse_coo_tensor`
+
+        Returns:
+            Whether the feature is supported.
+        """
+        feature = feature if isinstance(feature, str) else feature.__name__
+        if not hasattr(Backend, feature):
+            raise ValueError(f"Not a valid feature: '{feature}'")
+        backend_fun = getattr(Backend, feature)
+        impl_fun = getattr(self.__class__, feature)
+        return impl_fun is not backend_fun
+
+    def prefers_channels_last(self) -> bool:
+        raise NotImplementedError()
+
+    def requires_fixed_shapes_when_tracing(self) -> bool:
+        return False
+
+    @property
+    def precision(self) -> int:
+        """ Short for math.backend.get_precision() """
+        return get_precision()
+
+    @property
+    def float_type(self) -> DType:
+        return DType(float, self.precision)
+
+    @property
+    def as_registered(self) -> 'Backend':
+        from phi.math.backend import BACKENDS
+        for backend in BACKENDS:
+            if self.name in backend.name:
+                return backend
+        raise RuntimeError(f"Backend '{self}' is not visible.")
+
+    @property
+    def complex_type(self) -> DType:
+        return DType(complex, max(64, self.precision))
+
+    def combine_types(self, *dtypes: DType) -> DType:
+        return combine_types(*dtypes, fp_precision=self.precision)
+
+    def auto_cast(self, *tensors, bool_to_int=False, int_to_float=False) -> list:
+        """
+        Determins the appropriate values type resulting from operations involving the tensors as input.
+        
+        This method is called by the default implementations of basic operators.
+        Backends can override this method to prevent unnecessary casting.
+
+        Args:
+            *tensors: tensors to cast and to consider when determining the common data type
+            bool_to_int: Whether to convert boolean values to integers if all values are boolean.
+
+        Returns:
+            tensors cast to a common data type
+        """
+        dtypes = [self.dtype(t) for t in tensors]
+        result_type = self.combine_types(*dtypes)
+        if result_type.kind == bool and bool_to_int:
+            result_type = DType(int, 32)
+        if result_type.kind == int and int_to_float:
+            result_type = DType(float, self.precision)
+        if result_type.kind in (int, float, complex, bool):  # do not cast everything to string!
+            tensors = [self.cast(t, result_type) for t in tensors]
+        return tensors
+
+    def __str__(self):
+        return self.name
+
+    def __repr__(self):
+        return self.name
+
+    def list_devices(self, device_type: Union[str, None] = None) -> List[ComputeDevice]:
+        """
+        Fetches information about all available compute devices this backend can use.
+
+        Implementations:
+
+        * NumPy: [`os.cpu_count`](https://docs.python.org/3/library/os.html#os.cpu_count)
+        * PyTorch: [`torch.cuda.get_device_properties`](https://pytorch.org/docs/stable/cuda.html#torch.cuda.get_device_properties)
+        * TensorFlow: `tensorflow.python.client.device_lib.list_local_devices`
+        * Jax: [`jax.devices`](https://jax.readthedocs.io/en/latest/jax.html#jax.devices)
+
+        See Also:
+            `Backend.set_default_device()`.
+
+        Args:
+            device_type: (optional) Return only devices of this type, e.g. `'GPU'` or `'CPU'`. See `ComputeDevice.device_type`.
+
+        Returns:
+            `list` of all currently available devices.
+        """
+        if device_type is None:
+            return list(self._devices)
+        else:
+            assert device_type in ('CPU', 'GPU', 'TPU'), "Device"
+            return [d for d in self._devices if d.device_type == device_type]
+
+    def get_default_device(self) -> ComputeDevice:
+        return self._default_device
+
+    def set_default_device(self, device: Union[ComputeDevice, str]) -> bool:
+        """
+        Sets the device new tensors will be allocated on.
+        This function will do nothing if the target device type is not available.
+
+        See Also:
+            `Backend.list_devices()`, `Backend.get_default_device()`.
+
+        Args:
+            device: `ComputeDevice` or device type as `str`, such as `'CPU'` or `'GPU'`.
+
+        Returns:
+            `bool` whether the device was successfully set.
+        """
+        if isinstance(device, str):
+            devices = self.list_devices(device)
+            if not devices:
+                warnings.warn(f"{self.name}: Cannot select '{device}' because no device of this type is available.", RuntimeWarning)
+                return False
+            device = devices[0]
+        assert device.backend is self, f"Cannot set default device to {device.name} for backend {self.name} because the devices belongs to backend {device.backend.name}"
+        self._default_device = device
+        return True
+
+    def get_device(self, tensor: TensorType) -> ComputeDevice:
+        """ Returns the device `tensor` is located on. """
+        raise NotImplementedError()
+
+    def get_device_by_ref(self, ref):
+        for device in self._devices:
+            if device.ref == ref:
+                return device
+        raise KeyError(f"{self.name} has no device with ref '{ref}'. Available: {[d.ref for d in self._devices]}")
+
+    def allocate_on_device(self, tensor: TensorType, device: ComputeDevice) -> TensorType:
+        """
+        Moves `tensor` to `device`. May copy the tensor if it is already on the device.
+
+        Args:
+            tensor: Existing tensor native to this backend.
+            device: Target device, associated with this backend.
+        """
+        raise NotImplementedError()
+
+    def seed(self, seed: int):
+        raise NotImplementedError()
+
+    def is_module(self, obj) -> bool:
+        """
+        Tests if `obj` is of a type that is specific to this backend, e.g. a neural network.
+        If `True`, this backend will be chosen for operations involving `obj`.
+
+        See Also:
+            `Backend.is_tensor()`.
+
+        Args:
+            obj: Object to test.
+        """
+        raise NotImplementedError()
+
+    def is_tensor(self, x, only_native=False):
+        """
+        An object is considered a native tensor by a backend if no internal conversion is required by backend methods.
+        An object is considered a tensor (nativer or otherwise) by a backend if it is not a struct (e.g. tuple, list) and all methods of the backend accept it as a tensor argument.
+
+        If `True`, this backend will be chosen for operations involving `x`.
+
+        See Also:
+            `Backend.is_module()`.
+
+        Args:
+          x: object to check
+          only_native: If True, only accepts true native tensor representations, not Python numbers or others that are also supported as tensors (Default value = False)
+
+        Returns:
+          bool: whether `x` is considered a tensor by this backend
+
+        """
+        raise NotImplementedError()
+
+    def is_sparse(self, x) -> bool:
+        """
+        Args:
+            x: Tensor native to this `Backend`.
+        """
+        raise NotImplementedError(self)
+
+    def as_tensor(self, x, convert_external=True):
+        """
+        Converts a tensor-like object to the native tensor representation of this backend.
+        If x is a native tensor of this backend, it is returned without modification.
+        If x is a Python number (numbers.Number instance), `convert_numbers` decides whether to convert it unless the backend cannot handle Python numbers.
+        
+        *Note:* There may be objects that are considered tensors by this backend but are not native and thus, will be converted by this method.
+
+        Args:
+          x: tensor-like, e.g. list, tuple, Python number, tensor
+          convert_external: if False and `x` is a Python number that is understood by this backend, this method returns the number as-is. This can help prevent type clashes like int32 vs int64. (Default value = True)
+
+        Returns:
+          tensor representation of `x`
+
+        """
+        raise NotImplementedError()
+
+    def is_available(self, tensor) -> bool:
+        """
+        Tests if the value of the tensor is known and can be read at this point.
+        If true, `numpy(tensor)` must return a valid NumPy representation of the value.
+        
+        Tensors are typically available when the backend operates in eager mode.
+
+        Args:
+          tensor: backend-compatible tensor
+
+        Returns:
+          bool
+
+        """
+        raise NotImplementedError()
+
+    def numpy(self, tensor) -> numpy.ndarray:
+        """
+        Returns a NumPy representation of the given tensor.
+        If `tensor` is already a NumPy array, it is returned without modification.
+        
+        This method raises an error if the value of the tensor is not known at this point, e.g. because it represents a node in a graph.
+        Use `is_available(tensor)` to check if the value can be represented as a NumPy array.
+
+        Args:
+            tensor: backend-compatible tensor or sparse tensor
+
+        Returns:
+          NumPy representation of the values stored in the tensor
+
+        """
+        raise NotImplementedError()
+
+    def to_dlpack(self, tensor):
+        raise NotImplementedError()
+
+    def from_dlpack(self, capsule):
+        raise NotImplementedError()
+
+    def copy(self, tensor, only_mutable=False):
+        raise NotImplementedError()
+
+    def copy_leaves(self, tree, only_mutable=False):
+        if isinstance(tree, tuple):
+            return tuple([self.copy_leaves(e, only_mutable) for e in tree])
+        elif isinstance(tree, list):
+            return [self.copy_leaves(e, only_mutable) for e in tree]
+        elif isinstance(tree, dict):
+            return {k: self.copy_leaves(e, only_mutable) for k, e in tree.items()}
+        else:
+            return self.copy(tree, only_mutable=only_mutable)
+
+    def call(self, f: Callable, *args, name=None):
+        """
+        Calls `f(*args)` and returns the result.
+        This method may be used to register internal calls with the profiler.
+
+        Usage:
+
+            choose_backend(key).call(custom_function, *args)
+        """
+        return f(*args)
+
+    def block_until_ready(self, values):
+        pass
+
+    def vectorized_call(self, f, *args, output_dtypes=None, **aux_args):
+        """
+        Args:
+            f: Function with only positional tensor argument, returning one or multiple tensors.
+            *args: Batched inputs for `f`. The first dimension of all `args` is vectorized.
+                All tensors in `args` must have the same size or `1` in their first dimension.
+            output_dtypes: Single `DType` or tuple of DTypes declaring the dtypes of the tensors returned by `f`.
+        """
+        batch_dim = self.determine_size(args, 0)
+        result = []
+        for b in range(batch_dim):
+            result.append(f(*[t[min(b, self.staticshape(t)[0] - 1)] for t in args], **aux_args))
+        return self.stack(result)
+
+    def determine_size(self, tensors, axis):
+        sizes = [self.staticshape(t)[axis] for t in tensors]
+        non_singleton_sizes = [b for b in sizes if b != 1]
+        size = non_singleton_sizes[0] if non_singleton_sizes else 1
+        assert all([b in (1, size) for b in sizes])
+        return size
+
+    def tile_to(self, x, axis, size):
+        current_size = self.staticshape(x)[axis]
+        if current_size == size:
+            return x
+        assert size > current_size
+        assert size % current_size == 0
+        multiples = [size // current_size if i == axis else 1 for i in range(self.ndims(x))]
+        return self.tile(x, multiples)
+
+    def jit_compile(self, f: Callable) -> Callable:
+        return NotImplemented
+
+    def jacobian(self, f: Callable, wrt: Union[tuple, list], get_output: bool, is_f_scalar: bool):
+        """
+        Args:
+            f: Function to differentiate. Returns a tuple containing `(reduced_loss, output)`
+            wrt: Argument indices for which to compute the gradient.
+            get_output: Whether the derivative function should return the output of `f` in addition to the gradient.
+            is_f_scalar: Whether `f` is guaranteed to return a scalar output.
+
+        Returns:
+            A function `g` with the same arguments as `f`.
+            If `get_output=True`, `g` returns a `tuple`containing the outputs of `f` followed by the gradients.
+            The gradients retain the dimensions of `reduced_loss` in order as outer (first) dimensions.
+        """
+        raise NotImplementedError(self)
+
+    def hessian(self, f: Callable, wrt: Union[tuple, list], get_output: bool, get_gradient: bool) -> tuple:
+        """
+        First dimension of all inputs/outputs of `f` is assumed to be a batch dimension.
+        Element-wise Hessians will be computed along the batch dimension.
+        All other dimensions are parameter dimensions and will appear twice in the Hessian matrices.
+
+        Args:
+            f: Function whose first output is a scalar float or complex value.
+            wrt:
+            get_output:
+            get_gradient:
+
+        Returns:
+            Function returning `(f(x), g(x), H(x))` or less depending on `get_output` and `get_gradient`.
+            The result is always a `tuple` holding at most these three items.
+        """
+        raise NotImplementedError(self)
+
+    def custom_gradient(self, f: Callable, gradient: Callable, get_external_cache: Callable = None, on_call_skipped: Callable = None) -> Callable:
+        """
+        Creates a function based on `f` that uses a custom gradient for backprop.
+
+        Args:
+            f: Forward function.
+            gradient: Function for backprop. Will be called as `gradient(*d_out)` to compute the gradient of `f`.
+
+        Returns:
+            Function with similar signature and return values as `f`. However, the returned function does not support keyword arguments.
+        """
+        return NotImplemented
+
+    def jit_compile_grad(self, f: Callable, wrt: Union[tuple, list], get_output: bool, is_f_scalar: bool):
+        raise NotImplementedError()
+
+    def jit_compile_hessian(self, f: Callable, wrt: Union[tuple, list], get_output: bool, get_gradient: bool):
+        raise NotImplementedError()
+
+    def transpose(self, tensor, axes):
+        """ Transposes the dimensions of `tensor` given the new axes order. The tensor will be cast to the default precision in the process. """
+        raise NotImplementedError()
+
+    def random_uniform(self, shape, low, high, dtype: Union[DType, None]):
+        """ Float tensor of selected precision containing random values in the range [0, 1) """
+        raise NotImplementedError(self)
+
+    def random_normal(self, shape, dtype: DType):
+        """ Float tensor of selected precision containing random values sampled from a normal distribution with mean 0 and std 1. """
+        raise NotImplementedError(self)
+
+    def stack(self, values, axis=0):
+        raise NotImplementedError(self)
+
+    def stack_leaves(self, trees: Union[tuple, list], axis=0):
+        tree0 = trees[0]
+        if isinstance(tree0, tuple):
+            return tuple([self.stack_leaves([tree[i] for tree in trees], axis=axis) for i in range(len(tree0))])
+        elif isinstance(tree0, list):
+            return [self.stack_leaves([tree[i] for tree in trees], axis=axis) for i in range(len(tree0))]
+        elif isinstance(tree0, dict):
+            return {k: self.stack_leaves([tree[k] for tree in trees], axis=axis) for k in tree0}
+        else:
+            return self.stack(trees, axis=axis)
+
+    def concat(self, values, axis):
+        raise NotImplementedError(self)
+
+    def pad(self, value, pad_width, mode: str = 'constant', constant_values=0):
+        """
+        Pad a tensor with values as specified by `mode` and `constant_values`.
+        
+        If the mode is not supported, returns NotImplemented.
+
+        Args:
+          value: tensor
+          pad_width: 2D tensor specifying the number of values padded to the edges of each axis in the form [[axis 0 lower, axis 0 upper], ...] including batch and component axes.
+          mode: constant', 'boundary', 'periodic', 'symmetric', 'reflect'
+          constant_values: used for out-of-bounds points if mode='constant' (Default value = 0)
+          mode: str:  (Default value = 'constant')
+
+        Returns:
+          padded tensor or NotImplemented
+
+        """
+        raise NotImplementedError(self)
+
+    def reshape(self, value, shape):
+        raise NotImplementedError(self)
+
+    def flip(self, value, axes: Union[tuple, list]):
+        slices = tuple(slice(None, None, -1 if i in axes else None) for i in range(self.ndims(value)))
+        return value[slices]
+
+    def sum(self, value, axis=None, keepdims=False):
+        raise NotImplementedError(self)
+
+    def prod(self, value, axis=None):
+        raise NotImplementedError(self)
+
+    def divide_no_nan(self, x, y):
+        """ Computes x/y but returns 0 if y=0. """
+        raise NotImplementedError(self)
+
+    def where(self, condition, x=None, y=None):
+        raise NotImplementedError(self)
+
+    def nonzero(self, values):
+        """
+        Args:
+            values: Tensor with only spatial dimensions
+
+        Returns:
+            non-zero multi-indices as tensor of shape (nnz, vector)
+        """
+        raise NotImplementedError(self)
+
+    def mean(self, value, axis=None, keepdims=False):
+        raise NotImplementedError(self)
+
+    def range(self, start, limit=None, delta=1, dtype: DType = DType(int, 32)):
+        raise NotImplementedError(self)
+
+    def zeros(self, shape, dtype: DType = None):
+        raise NotImplementedError(self)
+
+    def zeros_like(self, tensor):
+        raise NotImplementedError(self)
+
+    def ones(self, shape, dtype: DType = None):
+        raise NotImplementedError(self)
+
+    def ones_like(self, tensor):
+        raise NotImplementedError(self)
+
+    def meshgrid(self, *coordinates):
+        raise NotImplementedError(self)
+
+    def linspace(self, start, stop, number):
+        raise NotImplementedError(self)
+
+    def linspace_without_last(self, start, stop, number):
+        return self.linspace(start, stop, number+1)[:-1]
+
+    def tensordot(self, a, a_axes: Union[tuple, list], b, b_axes: Union[tuple, list]):
+        """ Multiply-sum-reduce a_axes of a with b_axes of b. """
+        raise NotImplementedError(self)
+
+    def mul_matrix_batched_vector(self, A, b):
+        raise NotImplementedError(self)
+
+    def einsum(self, equation, *tensors):
+        raise NotImplementedError(self)
+
+    def cumsum(self, x, axis: int):
+        raise NotImplementedError(self)
+
+    def while_loop(self, loop: Callable, values: tuple, max_iter: Union[int, Tuple[int, ...], List[int]]):
+        """
+        If `max_iter is None`, runs
+
+        ```python
+        while any(values[0]):
+            values = loop(*values)
+        return values
+        ```
+
+        This operation does not support backpropagation.
+
+        Args:
+            loop: Loop function, must return a `tuple` with entries equal to `values` in shape and data type.
+            values: Initial values of loop variables.
+            max_iter: Maximum number of iterations to run, single `int` or sequence of integers.
+        Returns:
+            Loop variables upon loop completion if `max_iter` is a single integer.
+            If `max_iter` is a sequence, stacks the variables after each entry in `max_iter`, adding an outer dimension of size `<= len(max_iter)`.
+            If the condition is fulfilled before the maximum max_iter is reached, the loop may be broken or not, depending on the implementation.
+            If the loop is broken, the values returned by the last loop are expected to be constant and filled.
+        """
+        values = self.stop_gradient_tree(values)
+        if isinstance(max_iter, (tuple, list)):
+            trj = [self.copy_leaves(values, only_mutable=True)] if 0 in max_iter else []
+            for i in range(1, max(max_iter) + 1):
+                values = loop(*values)
+                if i in max_iter:
+                    trj.append(self.copy_leaves(values, only_mutable=True))
+                if not self.any(values[0]):
+                    break
+            trj.extend([trj[-1]] * (len(max_iter) - len(trj)))  # fill trj with final values
+            return self.stop_gradient_tree(self.stack_leaves(trj))
+        else:
+            for i in range(1, max_iter + 1):
+                if not self.any(values[0]):
+                    break
+                values = loop(*values)
+            return self.stop_gradient_tree(values)
+
+    def abs(self, x):
+        raise NotImplementedError(self)
+
+    def sign(self, x):
+        raise NotImplementedError(self)
+
+    def round(self, x):
+        raise NotImplementedError(self)
+
+    def ceil(self, x):
+        raise NotImplementedError(self)
+
+    def floor(self, x):
+        raise NotImplementedError(self)
+
+    def max(self, x, axis=None, keepdims=False):
+        raise NotImplementedError(self)
+
+    def min(self, x, axis=None, keepdims=False):
+        raise NotImplementedError(self)
+
+    def maximum(self, a, b):
+        raise NotImplementedError(self)
+
+    def minimum(self, a, b):
+        raise NotImplementedError(self)
+
+    def clip(self, x, minimum, maximum):
+        raise NotImplementedError(self)
+
+    def sqrt(self, x):
+        raise NotImplementedError(self)
+
+    def exp(self, x):
+        raise NotImplementedError(self)
+
+    def softplus(self, x):
+        raise NotImplementedError(self)
+
+    def log_gamma(self, x):
+        raise NotImplementedError(self)
+
+    def factorial(self, x: TensorType) -> TensorType:
+        if self.dtype(x).kind == int:
+            max_factorial = {32: 12, 64: 19}[self.dtype(x).bits]
+            factorial_list = [numpy.math.factorial(i) for i in range(max_factorial+1)]
+            return self.gather(self.cast(self.as_tensor(factorial_list), self.dtype(x)), x, 0)
+        else:
+            return self.exp(self.log_gamma(self.to_float(x) + 1))
+
+    def conv(self, value, kernel, zero_padding=True):
+        """
+        Convolve value with kernel.
+        Depending on the tensor rank, the convolution is either 1D (rank=3), 2D (rank=4) or 3D (rank=5).
+        Higher dimensions may not be supported.
+
+        Args:
+            value: tensor of shape (batch_size, in_channel, spatial...)
+            kernel: tensor of shape (batch_size or 1, out_channel, in_channel, spatial...)
+            zero_padding: If True, pads the edges of `value` with zeros so that the result has the same shape as `value`.
+
+        Returns:
+            Convolution result as tensor of shape (batch_size, out_channel, spatial...)
+        """
+        raise NotImplementedError(self)
+
+    def expand_dims(self, a, axis=0, number=1):
+        raise NotImplementedError(self)
+
+    def shape(self, tensor):
+        """
+        Returns the shape of a tensor.
+        The shape is iterable and implements `len()`.
+        For non-eager tensors, undefined dimensions should return a placeholder value representing the size.
+
+        See Also:
+            `Backend.staticshape()`.
+
+        Args:
+            tensor: Native tensor compatible with this backend.
+
+        Returns:
+            Shape of `tensor`
+        """
+        raise NotImplementedError(self)
+
+    def staticshape(self, tensor) -> tuple:
+        """
+        Evaluates the static shape of a native tensor.
+        If the tensor is eager, the shape is a `tuple[int]`.
+        For placeholder tensors, unknown dimensions are represented as `None`.
+
+        See Also:
+            `Backend.shape()`.
+
+        Args:
+            tensor: Native tensor compatible with this backend.
+
+        Returns:
+            `tuple` of sizes. Each size is an `int` if the size is defined, else `None`.
+        """
+        raise NotImplementedError(self)
+
+    def cast(self, x, dtype: DType):
+        raise NotImplementedError(self)
+
+    def to_float(self, x):
+        """
+        Converts a tensor to floating point values with precision equal to the currently set default precision.
+
+        See Also:
+            `Backend.precision()`.
+
+        If `x` is mutable and of the correct floating type, returns a copy of `x`.
+
+        To convert float tensors to the backend precision but leave non-float tensors untouched, use `Backend.as_tensor()`.
+
+        Args:
+            x: tensor of bool, int or float
+
+        Returns:
+            Values of `x` as float tensor
+        """
+        return self.cast(x, self.float_type)
+
+    def to_int32(self, x):
+        return self.cast(x, DType(int, 32))
+
+    def to_int64(self, x):
+        return self.cast(x, DType(int, 64))
+
+    def to_complex(self, x):
+        return self.cast(x, DType(complex, max(64, self.precision * 2)))
+
+    def unravel_index(self, flat_index, shape):
+        strides = [1]
+        for size in reversed(shape[1:]):
+            strides.append(strides[-1] * size)
+        strides = strides[::-1]
+        result = []
+        for i in range(len(shape)):
+            result.append(flat_index // strides[i] % shape[i])
+        return self.stack(result, -1)
+
+    def ravel_multi_index(self, multi_index, shape, mode: Union[str, int] = 'undefined'):
+        """
+        Args:
+            multi_index: (batch..., index_dim)
+            shape: 1D tensor or tuple/list
+            mode: `'undefined'`, `'periodic'`, `'clamp'` or an `int` to use for all invalid indices.
+
+        Returns:
+            Integer tensor of shape (batch...)
+        """
+        strides = [self.ones((), self.dtype(multi_index))]
+        for size in reversed(shape[1:]):
+            strides.append(strides[-1] * size)
+        strides = self.stack(strides[::-1])
+        if mode == 'periodic':
+            multi_index %= self.as_tensor(shape)
+        elif mode == 'clamp':
+            multi_index = self.clip(multi_index, 0, self.as_tensor(shape) - 1)
+        result = self.sum(multi_index * strides, -1)
+        if isinstance(mode, int):
+            inside = self.all((0 <= multi_index) & (multi_index < self.as_tensor(shape)), -1)
+            result = self.where(inside, result, mode)
+        return result
+
+    def gather(self, values, indices, axis: int):
+        """
+        Gathers values from the tensor `values` at locations `indices`.
+
+        Args:
+            values: tensor
+            indices: 1D tensor
+            axis: Axis along which to gather slices
+
+        Returns:
+            tensor, with size along `axis` being the length of `indices`
+        """
+        raise NotImplementedError(self)
+
+    def gather_by_component_indices(self, values, *component_indices):
+        return values[component_indices]
+
+    def batched_gather_nd(self, values, indices):
+        """
+        Gathers values from the tensor `values` at locations `indices`.
+        The first dimension of `values` and `indices` is the batch dimension which must be either equal for both or one for either.
+
+        Args:
+            values: tensor of shape (batch, spatial..., channel)
+            indices: int tensor of shape (batch, any..., multi_index) where the size of multi_index is values.rank - 2.
+
+        Returns:
+            Gathered values as tensor of shape (batch, any..., channel)
+        """
+        raise NotImplementedError(self)
+
+    def batched_gather_1d(self, values, indices):
+        return self.batched_gather_nd(values[:, :, None], indices[:, :, None])[..., 0]
+
+    def gather_1d(self, values, indices):
+        return self.gather(values, indices, 0)
+
+    def flatten(self, x):
+        return self.reshape(x, (-1,))
+
+    def std(self, x, axis=None, keepdims=False):
+        raise NotImplementedError(self)
+
+    def boolean_mask(self, x, mask, axis=0, new_length=None, fill_value=0):
+        """
+        Args:
+            x: tensor with any number of dimensions
+            mask: 1D mask tensor
+            axis: Axis index >= 0
+            new_length: Maximum size of the output along `axis`. This must be set when jit-compiling with Jax.
+            fill_value: If `new_length` is larger than the filtered result, the remaining values will be set to `fill_value`.
+        """
+        raise NotImplementedError(self)
+
+    def isfinite(self, x):
+        raise NotImplementedError(self)
+
+    def isnan(self, x):
+        raise NotImplementedError(self)
+
+    def isinf(self, x):
+        raise NotImplementedError(self)
+
+    def scatter(self, base_grid, indices, values, mode: str):
+        """
+        Depending on `mode`, performs scatter_update or scatter_add.
+
+        Args:
+            base_grid: Tensor into which scatter values are inserted at indices. Tensor of shape (batch_size, spatial..., channels)
+            indices: Tensor of shape (batch_size or 1, update_count, index_vector)
+            values: Values to scatter at indices. Tensor of shape (batch_size or 1, update_count or 1, channels or 1)
+            mode: One of ('update', 'add')
+
+        Returns:
+            Copy of base_grid with values at `indices` updated by `values`.
+        """
+        raise NotImplementedError(self)
+
+    def histogram1d(self, values, weights, bin_edges):
+        """
+        Args:
+            values: (batch, values)
+            bin_edges: (batch, edges)
+            weights: (batch, values)
+
+        Returns:
+            (batch, edges) with dtype matching weights
+        """
+        raise NotImplementedError(self)
+
+    def bincount(self, x, weights: Optional[TensorType], bins: int):
+        raise NotImplementedError(self)
+
+    def batched_bincount(self, x, weights: Optional[TensorType], bins: int):
+        if weights is None:
+            return self.vectorized_call(self.bincount, x, weights=None, bins=bins)
+        else:
+            return self.vectorized_call(self.bincount, x, weights, bins=bins)
+
+    def any(self, boolean_tensor, axis=None, keepdims=False):
+        raise NotImplementedError(self)
+
+    def all(self, boolean_tensor, axis=None, keepdims=False):
+        raise NotImplementedError(self)
+
+    def quantile(self, x, quantiles):
+        """
+        Reduces the last / inner axis of x.
+
+        Args:
+            x: Tensor
+            quantiles: List or 1D tensor of quantiles to compute.
+
+        Returns:
+            Tensor with shape (quantiles, *x.shape[:-1])
+        """
+        raise NotImplementedError(self)
+
+    def argsort(self, x, axis=-1):
+        raise NotImplementedError(self)
+
+    def searchsorted(self, sorted_sequence, search_values, side: str, dtype=DType(int, 32)):
+        raise NotImplementedError(self)
+
+    def fft(self, x, axes: Union[tuple, list]):
+        """
+        Computes the n-dimensional FFT along all but the first and last dimensions.
+
+        Args:
+          x: tensor of dimension 3 or higher
+          axes: Along which axes to perform the FFT
+
+        Returns:
+            Complex tensor `k`
+        """
+        raise NotImplementedError(self)
+
+    def ifft(self, k, axes: Union[tuple, list]):
+        """
+        Computes the n-dimensional inverse FFT along all but the first and last dimensions.
+
+        Args:
+          k: tensor of dimension 3 or higher
+          axes: Along which axes to perform the inverse FFT
+
+        Returns:
+            Complex tensor `x`
+        """
+        raise NotImplementedError(self)
+
+    def imag(self, x):
+        raise NotImplementedError(self)
+
+    def real(self, x):
+        raise NotImplementedError(self)
+
+    def conj(self, x):
+        raise NotImplementedError(self)
+
+    def sin(self, x):
+        raise NotImplementedError(self)
+
+    def arcsin(self, x):
+        raise NotImplementedError(self)
+
+    def cos(self, x):
+        raise NotImplementedError(self)
+
+    def arccos(self, x):
+        raise NotImplementedError(self)
+
+    def tan(self, x):
+        raise NotImplementedError(self)
+
+    def arctan(self, x):
+        raise NotImplementedError(self)
+
+    def arctan2(self, y, x):
+        raise NotImplementedError(self)
+
+    def sinh(self, x):
+        raise NotImplementedError(self)
+
+    def arcsinh(self, x):
+        raise NotImplementedError(self)
+
+    def cosh(self, x):
+        raise NotImplementedError(self)
+
+    def arccosh(self, x):
+        raise NotImplementedError(self)
+
+    def tanh(self, x):
+        raise NotImplementedError(self)
+
+    def arctanh(self, x):
+        raise NotImplementedError(self)
+
+    def log(self, x):
+        """ Natural logarithm """
+        raise NotImplementedError(self)
+
+    def log2(self, x):
+        raise NotImplementedError(self)
+
+    def log10(self, x):
+        raise NotImplementedError(self)
+
+    def sigmoid(self, x):
+        return 1 / (1 + self.exp(-x))
+
+    def dtype(self, array) -> DType:
+        raise NotImplementedError(self)
+
+    def tile(self, value, multiples):
+        """
+        Repeats the full tensor along each axis the number of times given by multiples.
+        If `multiples` has more dimensions than `value`, these dimensions are added to `value` as outer dimensions.
+
+        Args:
+            value: tensor
+            multiples: tuple or list of integers
+
+        Returns:
+            tiled tensor
+        """
+        raise NotImplementedError(self)
+
+    def repeat(self, x, repeats, axis: int, new_length=None):
+        """
+        Repeats the elements along `axis` `repeats` times.
+
+        Args:
+            x: Tensor
+            repeats: How often to repeat each element. 1D tensor of length x.shape[axis]
+            axis: Which axis to repeat elements along
+            new_length: Set the length of `axis` after repeating. This is required for jit compilation with Jax.
+
+        Returns:
+            repeated Tensor
+        """
+        raise NotImplementedError(self)
+
+    def get_diagonal(self, matrices, offset=0):
+        """
+
+        Args:
+            matrices: (batch, rows, cols, channels)
+            offset: 0=diagonal, positive=above diagonal, negative=below diagonal
+
+        Returns:
+            diagonal: (batch, max(rows,cols), channels)
+        """
+        raise NotImplementedError(self)
+
+    def indexed_segment_sum(self, x, indices, axis: int):
+        """
+        Args:
+            x: Values to sum. Segments are laid out contiguously along `axis`. (batch, ...)
+            indices: should start with 0 along `axis`. (batch, indices)
+            axis: Axis along which to sum
+
+        Returns:
+            Tensor with `len(indices)` elements along `axis`. (batch, ..., indices, ...)
+        """
+        raise NotImplementedError(self)
+
+    def sparse_coo_tensor(self, indices: Union[tuple, list], values, shape: tuple):
+        """
+        Create a sparse matrix in coordinate list (COO) format.
+
+        Optional feature.
+
+        See Also:
+            `Backend.csr_matrix()`, `Backend.csc_matrix()`.
+
+        Args:
+            indices: 2D tensor of shape `(nnz, dims)`.
+            values: 1D values tensor matching `indices`
+            shape: Shape of the sparse matrix
+
+        Returns:
+            Native representation of the sparse matrix
+        """
+        raise NotImplementedError(self)
+
+    def sparse_coo_tensor_batched(self, indices: Union[tuple, list], values, shape: tuple):
+        """
+        Args:
+            indices: shape (batch_size, dims, nnz)
+            values: Values tensor matching `indices`, shape (batch_size, nnz)
+            shape: tuple of two ints representing the dense shape, (dims...)
+        """
+        raise NotImplementedError(self)
+
+    def mul_coo_dense(self, indices, values, shape, dense):
+        """
+        Multiply a batch of sparse coordinate matrices by a batch of dense matrices.
+        Every backend should implement this feature.
+        This is the fallback if CSR multiplication is not supported.
+
+        Args:
+            indices: (batch, nnz, ndims)
+            values: (batch, nnz, channels)
+            shape: Shape of the full matrix, tuple of length ndims
+            dense: (batch, dense_rows=sparse_cols, channels, dense_cols)
+
+        Returns:
+            (batch, channels, dense_rows=sparse_cols, dense_cols)
+        """
+        values, dense = self.auto_cast(values, dense)
+        batch_size, nnz, channel_count = self.staticshape(values)
+        _, dense_rows, _, dense_cols = self.staticshape(dense)
+        dense_formatted = self.reshape(dense, (batch_size, dense_rows, dense_cols * channel_count))
+        dense_gathered = self.batched_gather_nd(dense_formatted, indices[:, :, 1:2])
+        base_grid = self.zeros((batch_size, shape[0], dense.shape[3] * dense_cols), self.dtype(dense))
+        assert dense_cols == 1
+        result = self.scatter(base_grid, indices[:, :, 0:1], values * dense_gathered, mode='add')
+        return self.reshape(result, (batch_size, channel_count, dense_rows, dense_cols))
+
+    def coo_to_dense(self, indices, values, shape, contains_duplicates: bool):
+        batch_size, nnz, channel_count = self.staticshape(values)
+        base = self.zeros((batch_size, *shape, channel_count))
+        result = self.scatter(base, indices, values, mode='add' if contains_duplicates else 'update')
+        return result
+
+    def csr_matrix(self, column_indices: TensorOrArray, row_pointers: TensorOrArray, values: TensorOrArray, shape: Tuple[int, int]):
+        """
+        Create a sparse matrix in compressed sparse row (CSR) format.
+
+        Optional feature.
+
+        See Also:
+            `Backend.sparse_coo_tensor()`, `Backend.csc_matrix()`.
+
+        Args:
+            column_indices: Column indices corresponding to `values`, 1D tensor
+            row_pointers: Indices in `values` where any row starts, 1D tensor of length `rows + 1`
+            values: Non-zero values, 1D tensor
+            shape: Shape of the full matrix
+
+        Returns:
+            Native representation of the sparse matrix
+        """
+        raise NotImplementedError(self)
+
+    def csr_matrix_batched(self, column_indices, row_pointers, values, shape: Tuple[int, int]):
+        """
+        Args:
+            column_indices: Column indices corresponding to `values`, shape (batch_size, nnz)
+            row_pointers: Indices in `values` where any row starts, shape (batch_size, rows+1)
+            values: Non-zero values, shape (batch_size, nnz, channels)
+            shape: tuple of two ints representing the dense shape, (cols, rows)
+        """
+        raise NotImplementedError(self)
+
+    def mul_csr_dense(self, column_indices, row_pointers, values, shape: Tuple[int, int], dense):
+        """
+        Multiply a batch of compressed sparse row matrices by a batch of dense matrices.
+
+        Optional feature.
+
+        See Also:
+            `Backend.sparse_coo_tensor()`, `Backend.csc_matrix()`.
+
+        Args:
+            column_indices: (batch, nnz)
+            row_pointers: (batch, rows + 1)
+            values: (batch, nnz, channels)
+            shape: Shape of the full matrix (cols, rows)
+            dense: (batch, dense_rows=sparse_cols, channels, dense_cols)
+
+        Returns:
+            (batch, channels, dense_rows=sparse_cols, dense_cols)
+        """
+        # if not self.supports(Backend.indexed_segment_sum):
+        native_coo_indices = self.csr_to_coo(column_indices, row_pointers)
+        return self.mul_coo_dense(native_coo_indices, values, shape, dense)
+        # values, dense = self.auto_cast(values, dense)
+        # batch_size, nnz, channel_count = self.staticshape(values)
+        # _, dense_rows, _, dense_cols = self.staticshape(dense)
+        # assert dense_cols == 1
+        # dense_formatted = self.reshape(dense, (batch_size, dense_rows, channel_count * dense_cols))
+        # dense_gathered = self.batched_gather_nd(dense_formatted, self.expand_dims(column_indices, -1))  # (batch, nnz, channels*rhs_cols)
+        # dense_gathered = self.reshape(dense_gathered, (batch_size, nnz, channel_count, dense_cols))
+        # values = self.reshape(values, (batch_size, nnz, channel_count, 1))
+        # result = self.indexed_segment_sum(values * dense_gathered, row_pointers[:, :-1], 1)
+        # return self.reshape(result, (batch_size, channel_count, rhs_rows, rhs_cols))
+
+    def csr_to_coo(self, column_indices, row_pointers):
+        """
+        Convert a batch of compressed sparse matrices to sparse coordinate matrices.
+
+        Args:
+            column_indices: (batch, nnz)
+            row_pointers: (batch, rows + 1)
+
+        Returns:
+            indices: (batch, nnz, 2)
+        """
+        batch_size = self.staticshape(column_indices)[0]
+        repeats = row_pointers[:, 1:] - row_pointers[:, :-1]
+        row_count = self.shape(repeats)[-1]
+        row_indices = [self.repeat(self.range(row_count, dtype=self.dtype(column_indices)), repeats[b], -1) for b in range(batch_size)]
+        return self.stack([self.stack(row_indices), column_indices], axis=-1)
+
+    def csr_to_dense(self, column_indices, row_pointers, values, shape: Tuple[int, int]):
+        indices = self.csr_to_coo(column_indices, row_pointers)
+        return self.coo_to_dense(indices, values, shape, contains_duplicates=False)
+
+    def csc_matrix(self, column_pointers, row_indices, values, shape: Tuple[int, int]):
+        """
+        Create a sparse matrix in compressed sparse column (CSC) format.
+
+        Optional feature.
+
+        See Also:
+            `Backend.sparse_coo_tensor()`, `Backend.csr_matrix()`.
+
+        Args:
+            column_pointers: Indices in `values` where any column starts, 1D tensor of length `cols + 1`
+            row_indices: Row indices corresponding to `values`.
+            values: Non-zero values, 1D tensor
+            shape: Shape of the full matrix
+
+        Returns:
+            Native representation of the sparse matrix
+        """
+        raise NotImplementedError(self)
+
+    def csc_matrix_batched(self, column_pointers, row_indices, values, shape: Tuple[int, int]):
+        """
+        Args:
+            column_pointers: Indices in `values` where any row starts, shape (batch_size, cols+1)
+            row_indices: Row indices corresponding to `values`, shape (batch_size, nnz)
+            values: Non-zero values, shape (batch_size, nnz, channels)
+            shape: tuple of two ints representing the dense shape, (cols, rows)
+        """
+        raise NotImplementedError(self)
+
+    def minimize(self, method: str, f, x0, atol, max_iter, trj: bool):
+        if method == 'auto':
+            method = 'L-BFGS-B'
+        if method == 'GD':
+            from ._minimize import gradient_descent
+            return gradient_descent(self, f, x0, atol, max_iter, trj)
+        else:
+            from ._minimize import scipy_minimize
+            return scipy_minimize(self, method, f, x0, atol, max_iter, trj)
+
+    def linear_solve(self,
+                     method: str,
+                     lin: Union[Callable, TensorType],
+                     y: TensorType,
+                     x0: TensorType,
+                     rtol: Union[ndarray, TensorType],
+                     atol: Union[ndarray, TensorType],
+                     max_iter: ndarray,
+                     pre: Optional[Preconditioner]) -> SolveResult:
+        """
+        Solve the system of linear equations A · x = y.
+        This method need not provide a gradient for the operation.
+
+        Args:
+            method: Which algorithm to use. One of:
+                * 'auto'
+                * 'CG'
+                * 'CG-adaptive'
+                * 'biCG-stab' or 'biCG-stab(1)'
+                * 'biCG-stab(n)'
+                * 'scipy-direct'
+                * 'scipy-CG', 'scipy-GMres', 'scipy-biCG', 'scipy-biCG-stab', 'scipy-CGS', 'scipy-QMR', 'scipy-GCrotMK'
+            lin: Linear operation. One of
+                * sparse/dense matrix valid for all instances
+                * tuple/list of sparse/dense matrices for varying matrices along batch, must have the same nonzero locations.
+                * linear function A(x), must be called on all instances in parallel
+            y: target result of A * x. 2nd order tensor (batch, vector) or list of vectors.
+            x0: Initial guess of size (batch, parameters)
+            rtol: Relative tolerance of size (batch,)
+            atol: Absolute tolerance of size (batch,)
+            max_iter: Maximum number of iterations of shape (checkpoints, batch).
+            pre: Preconditioner, function taking one native tensor like `y` as input and returning a native tensor like `x0`.
+
+        Returns:
+            `SolveResult`
+        """
+        if method == 'auto':
+            return self.conjugate_gradient_adaptive(lin, y, x0, rtol, atol, max_iter, pre)
+        elif method.startswith('scipy-'):
+            from ._linalg import scipy_spsolve
+            if not callable(lin):
+                lin = self.numpy(lin)
+            y = self.numpy(y)
+            x0 = self.numpy(x0)
+            rtol = self.numpy(rtol) if self.is_tensor(rtol, only_native=True) else rtol
+            atol = self.numpy(atol) if self.is_tensor(atol, only_native=True) else atol
+            result = scipy_spsolve(self, method[len('scipy-'):], lin, y, x0, rtol, atol, max_iter, pre)
+            return SolveResult(result.method, self.as_tensor(result.x), self.as_tensor(result.residual), result.iterations, result.function_evaluations, result.converged, result.diverged, result.message)
+        elif method == 'CG':
+            return self.conjugate_gradient(lin, y, x0, rtol, atol, max_iter, pre)
+        elif method == 'CG-adaptive':
+            return self.conjugate_gradient_adaptive(lin, y, x0, rtol, atol, max_iter, pre)
+        elif method in ['biCG', 'biCG-stab(0)']:
+            return self.bi_conjugate_gradient(lin, y, x0, rtol, atol, max_iter, pre, poly_order=0)
+        elif method == 'biCG-stab':
+            return self.bi_conjugate_gradient(lin, y, x0, rtol, atol, max_iter, pre, poly_order=1)
+        elif method.startswith('biCG-stab('):
+            order = int(method[len('biCG-stab('):-1])
+            return self.bi_conjugate_gradient(lin, y, x0, rtol, atol, max_iter, pre, poly_order=order)
+        else:
+            raise NotImplementedError(f"Method '{method}' not supported for linear solve.")
+
+    def conjugate_gradient(self, lin, y, x0, rtol, atol, max_iter, pre) -> SolveResult:
+        """ Standard conjugate gradient algorithm. Signature matches to `Backend.linear_solve()`. """
+        from ._linalg import cg
+        return cg(self, lin, y, x0, rtol, atol, max_iter, pre)
+
+    def conjugate_gradient_adaptive(self, lin, y, x0, rtol, atol, max_iter, pre) -> SolveResult:
+        """ Conjugate gradient algorithm with adaptive step size. Signature matches to `Backend.linear_solve()`. """
+        from ._linalg import cg_adaptive
+        return cg_adaptive(self, lin, y, x0, rtol, atol, max_iter, pre)
+
+    def bi_conjugate_gradient(self, lin, y, x0, rtol, atol, max_iter, pre, poly_order=2) -> SolveResult:
+        """ Generalized stabilized biconjugate gradient algorithm. Signature matches to `Backend.linear_solve()`. """
+        from ._linalg import bicg
+        return bicg(self, lin, y, x0, rtol, atol, max_iter, pre, poly_order)
+
+    def linear(self, lin, vector):
+        if callable(lin):
+            return lin(vector)
+        elif isinstance(lin, (tuple, list)):
+            for lin_i in lin:
+                lin_shape = self.staticshape(lin_i)
+                assert len(lin_shape) == 2
+            return self.stack([self.mul_matrix_batched_vector(m, v) for m, v in zip(lin, self.unstack(vector))])
+        else:
+            lin_shape = self.staticshape(lin)
+            assert len(lin_shape) == 2, f"A must be a matrix but got shape {lin_shape}"
+            return self.mul_matrix_batched_vector(lin, vector)
+
+    def matrix_solve_least_squares(self, matrix: TensorType, rhs: TensorType) -> Tuple[TensorType, TensorType, TensorType, TensorType]:
+        """
+        Args:
+            matrix: Shape (batch, vec, constraints)
+            rhs: Shape (batch, vec, batch_per_matrix)
+
+        Returns:
+            solution: Solution vector of Shape (batch, constraints, batch_per_matrix)
+            residuals: Optional, can be `None`
+            rank: Optional, can be `None`
+            singular_values: Optional, can be `None`
+        """
+        raise NotImplementedError(self)
+
+    def solve_triangular(self, matrix, rhs, lower: bool, unit_diagonal: bool):
+        """Performs a sparse or dense triangular solve, depending on the format of `matrix`."""
+        if self.is_sparse(matrix):
+            return self.solve_triangular_sparse(matrix, rhs, lower, unit_diagonal)
+        else:
+            return self.solve_triangular_dense(matrix, rhs, lower, unit_diagonal)
+
+    def solve_triangular_dense(self, matrix, rhs, lower: bool, unit_diagonal: bool):
+        """
+        Args:
+            matrix: (batch_size, rows, cols)
+            rhs: (batch_size, cols)
+            lower:
+            unit_diagonal:
+
+        Returns:
+            (batch_size, cols)
+        """
+        raise NotImplementedError(self)
+
+    def solve_triangular_sparse(self, matrix, rhs, lower: bool, unit_diagonal: bool):
+        np_matrix = self.numpy(matrix)
+        np_rhs = self.numpy(rhs)
+        from scipy.sparse.linalg import spsolve_triangular
+        np_result = spsolve_triangular(np_matrix, np_rhs.T, lower=lower, unit_diagonal=unit_diagonal).T
+        return self.as_tensor(np_result)
+
+    def stop_gradient(self, value):
+        raise NotImplementedError(self)
+
+    def stop_gradient_tree(self, tree):
+        if isinstance(tree, tuple):
+            return tuple([self.stop_gradient_tree(v) for v in tree])
+        if isinstance(tree, list):
+            return [self.stop_gradient_tree(v) for v in tree]
+        if isinstance(tree, dict):
+            return {k: self.stop_gradient_tree(v) for k, v in tree.items()}
+        return self.stop_gradient(tree)
+
+    def grid_sample(self, grid, coordinates, extrapolation: str):
+        """
+        Interpolates a regular grid at the specified coordinates.
+
+        Args:
+            grid: Tensor of shape (batch, spatial..., channel)
+            coordinates: Tensor of floating grid indices of shape (batch, instance..., vector).
+                The last dimension must match `spatial_dims`.
+                The first grid point of dimension i lies at position 0, the last at values.shape[i]-1.
+            extrapolation: Values to use for coordinates outside the grid.
+                One of `('undefined', 'zeros', 'boundary', 'periodic', 'symmetric', 'reflect')`.
+
+        Returns:
+            sampled values with linear interpolation
+        """
+        return NotImplemented
+
+    def variable(self, value):
+        return NotImplemented
+
+    def ndims(self, tensor):
+        return len(self.staticshape(tensor))
+
+    def size(self, array):
+        return self.prod(self.shape(array))
+
+    def multi_slice(self, tensor, slices: tuple):
+        """
+        Args:
+            tensor: value to slice
+            slices: `tuple` of `slice`, `int`, or scalar integer tensors
+        """
+        return tensor[slices]
+
+    def batch_gather(self, tensor, batches):
+        if isinstance(batches, int):
+            batches = [batches]
+        return tensor[batches, ...]
+
+    def unstack(self, tensor, axis=0, keepdims=False) -> tuple:
+        if axis < 0:
+            axis += len(tensor.shape)
+        if axis >= len(tensor.shape) or axis < 0:
+            raise ValueError("Illegal axis value")
+        result = []
+        for slice_idx in range(tensor.shape[axis]):
+            if keepdims:
+                component = tensor[tuple([slice(slice_idx, slice_idx + 1) if d == axis else slice(None) for d in range(len(tensor.shape))])]
+            else:
+                component = tensor[tuple([slice_idx if d == axis else slice(None) for d in range(len(tensor.shape))])]
+            result.append(component)
+        return tuple(result)
+
+    def equal(self, x, y):
+        """ Element-wise equality check """
+        raise NotImplementedError(self)
+
+    def not_equal(self, x, y):
+        return ~self.equal(x, y)
+
+    def greater_than(self, x, y):
+        x, y = self.auto_cast(x, y)
+        return x > y
+
+    def greater_or_equal(self, x, y):
+        x, y = self.auto_cast(x, y)
+        return x >= y
+
+    def add(self, a, b):
+        a, b = self.auto_cast(a, b, bool_to_int=True)
+        return a + b
+
+    def sub(self, a, b):
+        a, b = self.auto_cast(a, b, bool_to_int=True)
+        return a - b
+
+    def mul(self, a, b):
+        a, b = self.auto_cast(a, b)
+        return a * b
+
+    def div(self, numerator, denominator):
+        numerator, denominator = self.auto_cast(numerator, denominator)
+        return numerator / denominator
+
+    def pow(self, base, exp):
+        base, exp = self.auto_cast(base, exp)
+        return base ** exp
+
+    def mod(self, dividend, divisor):
+        dividend, divisor = self.auto_cast(dividend, divisor)
+        return dividend % divisor
+
+    def and_(self, a, b):
+        a, b = self.auto_cast(a, b)
+        return a & b
+
+    def or_(self, a, b):
+        a, b = self.auto_cast(a, b)
+        return a | b
+
+    def xor(self, a, b):
+        a, b = self.auto_cast(a, b)
+        return a ^ b
+
+    def floordiv(self, a, b):
+        a, b = self.auto_cast(a, b)
+        return a // b
+
+    def shift_bits_left(self, a, b):
+        a, b = self.auto_cast(a, b)
+        return a << b
+
+    def shift_bits_right(self, a, b):
+        a, b = self.auto_cast(a, b)
+        return a >> b
+
+
+BACKENDS = []
+""" Global list of all registered backends. Register a `Backend` by adding it to the list. """
+_DEFAULT = []  # [0] = global default, [1:] from 'with' blocks
+_PRECISION = [32]  # [0] = global precision in bits, [1:] from 'with' blocks
+
+
+def choose_backend(*values, prefer_default=False) -> Backend:
+    """
+    Selects a suitable backend to handle the given values.
+
+    This function is used by most math functions operating on `Tensor` objects to delegate the actual computations.
+
+    Backends need to be registered to be available, e.g. via the global import `phi.<backend>` or `phi.detect_backends()`.
+
+    Args:
+        *values:
+        prefer_default: Whether to always select the default backend if it can work with `values`, see `default_backend()`.
+
+    Returns:
+        The selected `Backend`
+    """
+    # --- Default Backend has priority ---
+    if _is_applicable(_DEFAULT[-1], values) and (prefer_default or _is_specific(_DEFAULT[-1], values)):
+        return _DEFAULT[-1]
+    # --- Filter out non-applicable ---
+    backends = [backend for backend in BACKENDS if _is_applicable(backend, values)]
+    if len(backends) == 0:
+        raise NoBackendFound(f"No backend found for types {[type(v).__name__ for v in values]}; registered backends are {BACKENDS}")
+    # --- Native tensors? ---
+    for backend in backends:
+        if _is_specific(backend, values):
+            return backend
+    return backends[0]
+
+
+class NoBackendFound(Exception):
+    """
+    Thrown by `choose_backend` if no backend can handle the given values.
+    """
+
+    def __init__(self, msg):
+        Exception.__init__(self, msg)
+
+
+def default_backend() -> Backend:
+    """
+    The default backend is preferred by `choose_backend()`.
+
+    The default backend can be set globally using `set_global_default_backend()` and locally using `with backend:`.
+
+    Returns:
+        current default `Backend`
+    """
+    return _DEFAULT[-1]
+
+
+def context_backend() -> Union[Backend, None]:
+    """
+    Returns the backend set by the inner-most surrounding `with backend:` block.
+    If called outside a backend context, returns `None`.
+
+    Returns:
+        `Backend` or `None`
+    """
+    return _DEFAULT[-1] if len(_DEFAULT) > 1 else None
+
+
+def set_global_default_backend(backend: Backend):
+    """
+    Sets the given backend as default.
+    This setting can be overridden using `with backend:`.
+
+    See `default_backend()`, `choose_backend()`.
+
+    Args:
+        backend: `Backend` to set as default
+    """
+    assert isinstance(backend, Backend)
+    _DEFAULT[0] = backend
+
+
+def set_global_precision(floating_point_bits: int):
+    """
+    Sets the floating point precision of DYNAMIC_BACKEND which affects all registered backends.
+
+    If `floating_point_bits` is an integer, all floating point tensors created henceforth will be of the corresponding data type, float16, float32 or float64.
+    Operations may also convert floating point values to this precision, even if the input had a different precision.
+
+    If `floating_point_bits` is None, new tensors will default to float32 unless specified otherwise.
+    The output of math operations has the same precision as its inputs.
+
+    Args:
+      floating_point_bits: one of (16, 32, 64, None)
+    """
+    _PRECISION[0] = floating_point_bits
+
+
+def get_precision() -> int:
+    """
+    Gets the current target floating point precision in bits.
+    The precision can be set globally using `set_global_precision()` or locally using `with precision(p):`.
+
+    Any Backend method may convert floating point values to this precision, even if the input had a different precision.
+
+    Returns:
+        16 for half, 32 for single, 64 for double
+    """
+    return _PRECISION[-1]
+
+
+@contextmanager
+def precision(floating_point_bits: int):
+    """
+    Sets the floating point precision for the local context.
+
+    Usage: `with precision(p):`
+
+    This overrides the global setting, see `set_global_precision()`.
+
+    Args:
+        floating_point_bits: 16 for half, 32 for single, 64 for double
+    """
+    _PRECISION.append(floating_point_bits)
+    try:
+        yield None
+    finally:
+        _PRECISION.pop(-1)
+
+
+def convert(tensor, backend: Backend = None, use_dlpack=True):
+    """
+    Convert a Tensor to the native format of `backend`.
+    If the target backend can operate natively on `tensor`, returns `tensor`.
+
+    If both backends support *DLPack* and `use_dlpack=True`, uses zero-copy conversion using the DLPack library.
+    Else, intermediately converts `tensor` to a NumPy array.
+
+    *Warning*: This operation breaks the automatic differentiation chain.
+
+    Args:
+        tensor: Native tensor belonging to any registered backend.
+        backend: Target backend. If `None`, uses the current default backend, see `default_backend()`.
+
+    Returns:
+        Tensor belonging to `backend`.
+    """
+    backend = backend or default_backend()
+    current_backend = choose_backend(tensor, prefer_default=False)
+    if backend.is_tensor(tensor, True) or backend is current_backend:
+        return tensor
+    if use_dlpack and current_backend.supports(Backend.to_dlpack) and backend.supports(Backend.from_dlpack):
+        capsule = current_backend.to_dlpack(tensor)
+        return backend.from_dlpack(capsule)
+    else:
+        nparray = current_backend.numpy(tensor)
+        return backend.as_tensor(nparray)
+
+
+# Backend choice utility functions
+
+def _is_applicable(backend, values):
+    for value in values:
+        if not (backend.is_tensor(value, only_native=False) or backend.is_module(value)):
+            return False
+    return True
+
+
+def _is_specific(backend: Backend, values):
+    for value in values:
+        if backend.is_tensor(value, only_native=True) or backend.is_module(value):
+            return True
+    return False
+
+
+# Other low-level helper functions
+
+def combined_dim(dim1, dim2, type_str: str = 'batch'):
+    if dim1 is None and dim2 is None:
+        return None
+    if dim1 is None or dim1 == 1:
+        return dim2
+    if dim2 is None or dim2 == 1:
+        return dim1
+    assert dim1 == dim2, f"Incompatible {type_str} dimensions: x0 {dim1}, y {dim2}"
+    return dim1
+
+
+_SPATIAL_DERIVATIVE_CONTEXT = [0]
+_FUNCTIONAL_DERIVATIVE_CONTEXT = [0]
+
+
+@contextmanager
+def spatial_derivative_evaluation(order=1):
+    _SPATIAL_DERIVATIVE_CONTEXT.append(order)
+    try:
+        yield None
+    finally:
+        assert _SPATIAL_DERIVATIVE_CONTEXT.pop(-1) == order
+
+
+def get_spatial_derivative_order():
+    """
+    Extrapolations may behave differently when extrapolating the derivative of a grid.
+    Returns 1 inside a CG loop, and 0 by default.
+    """
+    return _SPATIAL_DERIVATIVE_CONTEXT[-1]
+
+
+@contextmanager
+def functional_derivative_evaluation(order=1):
+    _FUNCTIONAL_DERIVATIVE_CONTEXT.append(order)
+    try:
+        yield None
+    finally:
+        assert _FUNCTIONAL_DERIVATIVE_CONTEXT.pop(-1) == order
+
+
+def get_functional_derivative_order():
+    """
+    Operations that do not define a first or higher-order derivative may use slower alternative code paths when the derivative is `>0`.
+    This is set when calling a function created by `math.jacobian()` or `math.hessian()`.
+    """
+    return _FUNCTIONAL_DERIVATIVE_CONTEXT[-1]
+
+
+PHI_LOGGER = logging.getLogger('Φ')  # used for warnings and debug messages by all internal PhiFlow functions
+_LOG_CONSOLE_HANDLER = logging.StreamHandler(sys.stdout)
+_LOG_CONSOLE_HANDLER.setFormatter(logging.Formatter("%(message)s (%(levelname)s), %(asctime)sn\n"))
+_LOG_CONSOLE_HANDLER.setLevel(logging.NOTSET)
+PHI_LOGGER.addHandler(_LOG_CONSOLE_HANDLER)
```

### Comparing `phiflow-2.3.4/phi/math/backend/_dtype.py` & `phiflow-2.4.0/phi/math/backend/_dtype.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,170 +1,170 @@
-from typing import Union
-
-import numpy as np
-import sys
-
-
-class DType:
-    """
-    Instances of `DType` represent the kind and size of data elements.
-    The data type of a `Tensor` can be obtained via `phi.math.Tensor.dtype`.
-
-    The following kinds of data types are supported:
-
-    * `float` with 32 / 64 bits
-    * `complex` with 64 / 128 bits
-    * `int` with 8 / 16 / 32 / 64 bits
-    * `bool` with 8 bits
-    * `str` with 8*n* bits
-
-    Unlike with many computing libraries, there are no global variables corresponding to the available types.
-    Instead, data types can simply be instantiated as needed.
-    """
-
-    def __init__(self, kind: type, bits: int = None, precision: int = None):
-        """
-        Args:
-            kind: Python type, one of `(bool, int, float, complex, str)`
-            bits: number of bits per element, a multiple of 8.
-        """
-        assert kind in (bool, int, float, complex, str, object)
-        if kind is bool:
-            assert bits is None, "Bits may not be set for bool or object"
-            assert precision is None, f"Precision may only be specified for float or complex but got {kind}, precision={precision}"
-            bits = 8
-        elif kind == object:
-            assert bits is None, "bits may not be set for bool or object"
-            assert precision is None, f"Precision may only be specified for float or complex but got {kind}, precision={precision}"
-            bits = int(np.round(np.log2(sys.maxsize))) + 1
-        elif precision is not None:
-            assert bits is None, "Specify either bits or precision when creating a DType but not both."
-            assert kind in [float, complex], f"Precision may only be specified for float or complex but got {kind}, precision={precision}"
-            if kind == float:
-                bits = precision
-            else:
-                bits = precision * 2
-        else:
-            assert isinstance(bits, int)
-        self.kind = kind
-        """ Python class corresponding to the type of data, ignoring precision. One of (bool, int, float, complex, str) """
-        self.bits = bits
-        """ Number of bits used to store a single value of this type. See `DType.itemsize`. """
-
-    @property
-    def precision(self):
-        """ Floating point precision. Only defined if `kind in (float, complex)`. For complex values, returns half of `DType.bits`. """
-        if self.kind == float:
-            return self.bits
-        if self.kind == complex:
-            return self.bits // 2
-        else:
-            return None
-
-    @property
-    def itemsize(self):
-        """ Number of bytes used to storea single value of this type. See `DType.bits`. """
-        assert self.bits % 8 == 0
-        return self.bits // 8
-
-    def __eq__(self, other):
-        return isinstance(other, DType) and self.kind == other.kind and self.bits == other.bits
-
-    def __ne__(self, other):
-        return not self == other
-
-    def __hash__(self):
-        return hash(self.kind) + hash(self.bits)
-
-    def __repr__(self):
-        return f"{self.kind.__name__}{self.bits}"
-
-    @staticmethod
-    def as_dtype(value: Union['DType', tuple, type, None]) -> Union['DType', None]:
-        if isinstance(value, DType):
-            return value
-        elif value is int:
-            return DType(int, 32)
-        elif value is float:
-            from phi.math import get_precision
-            return DType(float, get_precision())
-        elif value is complex:
-            from phi.math import get_precision
-            return DType(complex, 2 * get_precision())
-        elif value is None:
-            return None
-        elif isinstance(value, tuple):
-            return DType(*value)
-        elif value is str:
-            raise ValueError("str DTypes must specify precision")
-        else:
-            return DType(value)  # bool, object
-
-
-# --- NumPy Conversion ---
-
-def to_numpy_dtype(dtype: DType):
-    if dtype in _TO_NUMPY:
-        return _TO_NUMPY[dtype]
-    if dtype.kind == str:
-        bytes_per_char = np.dtype('<U1').itemsize
-        return np.dtype(f'<U{dtype.itemsize // bytes_per_char}')
-    raise KeyError(f"Unsupported dtype: {dtype}")
-
-
-def from_numpy_dtype(np_dtype) -> DType:
-    if np_dtype in _FROM_NUMPY:
-        return _FROM_NUMPY[np_dtype]
-    else:
-        for base_np_dtype, dtype in _FROM_NUMPY.items():
-            if np_dtype == base_np_dtype:
-                return dtype
-        if np_dtype.char == 'U':
-            return DType(str, 8 * np_dtype.itemsize)
-        raise ValueError(np_dtype)
-
-
-_TO_NUMPY = {
-    DType(float, 16): np.float16,
-    DType(float, 32): np.float32,
-    DType(float, 64): np.float64,
-    DType(complex, 64): np.complex64,
-    DType(complex, 128): np.complex128,
-    DType(int, 8): np.int8,
-    DType(int, 16): np.int16,
-    DType(int, 32): np.int32,
-    DType(int, 64): np.int64,
-    DType(bool): np.bool_,
-    DType(object): object,
-}
-_FROM_NUMPY = {np: dtype for dtype, np in _TO_NUMPY.items()}
-_FROM_NUMPY[np.bool_] = DType(bool)
-_FROM_NUMPY[bool] = DType(bool)
-
-
-def combine_types(*dtypes: DType, fp_precision: int = None) -> DType:
-    # all bool?
-    if all(dt.kind == bool for dt in dtypes):
-        return dtypes[0]
-    # all int / bool?
-    if all(dt.kind in (bool, int) for dt in dtypes):
-        largest = max(dtypes, key=lambda dt: dt.bits)
-        return largest
-    # all real?
-    if all(dt.kind in (float, int, bool) for dt in dtypes):
-        if isinstance(fp_precision, int):
-            return DType(float, fp_precision)
-        else:
-            highest_fp = max([dt.precision for dt in dtypes if dt.kind == float])
-            return DType(float, highest_fp)
-    # complex
-    if all(dt.kind in (complex, float, int, bool) for dt in dtypes):
-        if isinstance(fp_precision, int):
-            return DType(complex, 2 * fp_precision)
-        else:
-            highest_fp = max([dt.precision for dt in dtypes if dt.kind in (float, complex)])
-            return DType(complex, highest_fp * 2)
-    # string
-    if any(dt.kind == str for dt in dtypes):
-        largest = max([dt for dt in dtypes if dt.kind == str], key=lambda dt: dt.bits)
-        return largest
-    raise ValueError(dtypes)
+from typing import Union
+
+import numpy as np
+import sys
+
+
+class DType:
+    """
+    Instances of `DType` represent the kind and size of data elements.
+    The data type of a `Tensor` can be obtained via `phi.math.Tensor.dtype`.
+
+    The following kinds of data types are supported:
+
+    * `float` with 32 / 64 bits
+    * `complex` with 64 / 128 bits
+    * `int` with 8 / 16 / 32 / 64 bits
+    * `bool` with 8 bits
+    * `str` with 8*n* bits
+
+    Unlike with many computing libraries, there are no global variables corresponding to the available types.
+    Instead, data types can simply be instantiated as needed.
+    """
+
+    def __init__(self, kind: type, bits: int = None, precision: int = None):
+        """
+        Args:
+            kind: Python type, one of `(bool, int, float, complex, str)`
+            bits: number of bits per element, a multiple of 8.
+        """
+        assert kind in (bool, int, float, complex, str, object)
+        if kind is bool:
+            assert bits is None, "Bits may not be set for bool or object"
+            assert precision is None, f"Precision may only be specified for float or complex but got {kind}, precision={precision}"
+            bits = 8
+        elif kind == object:
+            assert bits is None, "bits may not be set for bool or object"
+            assert precision is None, f"Precision may only be specified for float or complex but got {kind}, precision={precision}"
+            bits = int(np.round(np.log2(sys.maxsize))) + 1
+        elif precision is not None:
+            assert bits is None, "Specify either bits or precision when creating a DType but not both."
+            assert kind in [float, complex], f"Precision may only be specified for float or complex but got {kind}, precision={precision}"
+            if kind == float:
+                bits = precision
+            else:
+                bits = precision * 2
+        else:
+            assert isinstance(bits, int)
+        self.kind = kind
+        """ Python class corresponding to the type of data, ignoring precision. One of (bool, int, float, complex, str) """
+        self.bits = bits
+        """ Number of bits used to store a single value of this type. See `DType.itemsize`. """
+
+    @property
+    def precision(self):
+        """ Floating point precision. Only defined if `kind in (float, complex)`. For complex values, returns half of `DType.bits`. """
+        if self.kind == float:
+            return self.bits
+        if self.kind == complex:
+            return self.bits // 2
+        else:
+            return None
+
+    @property
+    def itemsize(self):
+        """ Number of bytes used to storea single value of this type. See `DType.bits`. """
+        assert self.bits % 8 == 0
+        return self.bits // 8
+
+    def __eq__(self, other):
+        return isinstance(other, DType) and self.kind == other.kind and self.bits == other.bits
+
+    def __ne__(self, other):
+        return not self == other
+
+    def __hash__(self):
+        return hash(self.kind) + hash(self.bits)
+
+    def __repr__(self):
+        return f"{self.kind.__name__}{self.bits}"
+
+    @staticmethod
+    def as_dtype(value: Union['DType', tuple, type, None]) -> Union['DType', None]:
+        if isinstance(value, DType):
+            return value
+        elif value is int:
+            return DType(int, 32)
+        elif value is float:
+            from phi.math import get_precision
+            return DType(float, get_precision())
+        elif value is complex:
+            from phi.math import get_precision
+            return DType(complex, 2 * get_precision())
+        elif value is None:
+            return None
+        elif isinstance(value, tuple):
+            return DType(*value)
+        elif value is str:
+            raise ValueError("str DTypes must specify precision")
+        else:
+            return DType(value)  # bool, object
+
+
+# --- NumPy Conversion ---
+
+def to_numpy_dtype(dtype: DType):
+    if dtype in _TO_NUMPY:
+        return _TO_NUMPY[dtype]
+    if dtype.kind == str:
+        bytes_per_char = np.dtype('<U1').itemsize
+        return np.dtype(f'<U{dtype.itemsize // bytes_per_char}')
+    raise KeyError(f"Unsupported dtype: {dtype}")
+
+
+def from_numpy_dtype(np_dtype) -> DType:
+    if np_dtype in _FROM_NUMPY:
+        return _FROM_NUMPY[np_dtype]
+    else:
+        for base_np_dtype, dtype in _FROM_NUMPY.items():
+            if np_dtype == base_np_dtype:
+                return dtype
+        if np_dtype.char == 'U':
+            return DType(str, 8 * np_dtype.itemsize)
+        raise ValueError(np_dtype)
+
+
+_TO_NUMPY = {
+    DType(float, 16): np.float16,
+    DType(float, 32): np.float32,
+    DType(float, 64): np.float64,
+    DType(complex, 64): np.complex64,
+    DType(complex, 128): np.complex128,
+    DType(int, 8): np.int8,
+    DType(int, 16): np.int16,
+    DType(int, 32): np.int32,
+    DType(int, 64): np.int64,
+    DType(bool): np.bool_,
+    DType(object): object,
+}
+_FROM_NUMPY = {np: dtype for dtype, np in _TO_NUMPY.items()}
+_FROM_NUMPY[np.bool_] = DType(bool)
+_FROM_NUMPY[bool] = DType(bool)
+
+
+def combine_types(*dtypes: DType, fp_precision: int = None) -> DType:
+    # all bool?
+    if all(dt.kind == bool for dt in dtypes):
+        return dtypes[0]
+    # all int / bool?
+    if all(dt.kind in (bool, int) for dt in dtypes):
+        largest = max(dtypes, key=lambda dt: dt.bits)
+        return largest
+    # all real?
+    if all(dt.kind in (float, int, bool) for dt in dtypes):
+        if isinstance(fp_precision, int):
+            return DType(float, fp_precision)
+        else:
+            highest_fp = max([dt.precision for dt in dtypes if dt.kind == float])
+            return DType(float, highest_fp)
+    # complex
+    if all(dt.kind in (complex, float, int, bool) for dt in dtypes):
+        if isinstance(fp_precision, int):
+            return DType(complex, 2 * fp_precision)
+        else:
+            highest_fp = max([dt.precision for dt in dtypes if dt.kind in (float, complex)])
+            return DType(complex, highest_fp * 2)
+    # string
+    if any(dt.kind == str for dt in dtypes):
+        largest = max([dt for dt in dtypes if dt.kind == str], key=lambda dt: dt.bits)
+        return largest
+    return DType(object)
```

### Comparing `phiflow-2.3.4/phi/math/backend/_minimize.py` & `phiflow-2.4.0/phi/math/backend/_minimize.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,178 +1,178 @@
-from threading import Barrier
-
-import numpy
-
-from ._backend import Backend, SolveResult, DType, PHI_LOGGER
-from ._linalg import _max_iter
-
-
-def scipy_minimize(self, method: str, f, x0, atol, max_iter, trj: bool):
-    from scipy.optimize import OptimizeResult, minimize
-    from threading import Thread
-
-    assert self.supports(Backend.jacobian)
-    x0 = self.numpy(x0)
-    assert x0.ndim == 2  # (batch, parameters)
-    atol = self.numpy(atol)
-    batch_size = x0.shape[0]
-    fg = self.jacobian(f, [0], get_output=True, is_f_scalar=True)
-    method_description = f"SciPy {method} with {self.name}"
-
-    iterations = [0] * batch_size
-    function_evaluations = [0] * batch_size
-    xs = [None] * batch_size
-    final_losses = [None] * batch_size
-    converged = [False] * batch_size
-    diverged = [False] * batch_size
-    messages = [""] * batch_size
-
-    f_inputs = [None] * batch_size
-    f_b_losses = None
-    f_b_losses_np = None
-    f_grad_np = None
-    f_input_available = Barrier(batch_size + 1)
-    f_output_available = Barrier(batch_size + 1)
-    finished = [False] * batch_size
-    all_finished = False
-    trajectories = [[] for _ in range(batch_size)] if trj else None
-    threads = []
-
-    for b in range(batch_size):  # Run each independent example as a scipy minimization in a new thread
-
-        def b_thread(b=b):
-            recent_b_losses = []
-
-            def b_fun(x: numpy.ndarray):
-                function_evaluations[b] += 1
-                f_inputs[b] = self.as_tensor(x, convert_external=True)
-                f_input_available.wait()
-                f_output_available.wait()
-                recent_b_losses.append(f_b_losses[b])
-                if final_losses[b] is None:  # first evaluation
-                    final_losses[b] = f_b_losses[b]
-                    if trajectories is not None:
-                        trajectories[b].append(SolveResult(method_description, x0[b], self.numpy(f_b_losses[b]), 0, 1, False, False, ""))
-                return f_b_losses_np[b], f_grad_np[b]
-
-            def callback(x, *args):  # L-BFGS-B only passes x but the documentation says (x, state)
-                iterations[b] += 1
-                loss = min(recent_b_losses)
-                recent_b_losses.clear()
-                final_losses[b] = loss
-                if trajectories is not None:
-                    trajectories[b].append(SolveResult(method_description, x, self.numpy(loss), iterations[b], function_evaluations[b], False, False, ""))
-
-            res = minimize(fun=b_fun, x0=x0[b], jac=True, method=method, tol=atol[b], options={'maxiter': max_iter[b]}, callback=callback)
-            assert isinstance(res, OptimizeResult)
-            # res.nit, res.nfev
-            xs[b] = res.x
-            converged[b] = res.success
-            diverged[b] = res.status not in (0, 1)  # 0=success
-            messages[b] = res.message
-            finished[b] = True
-            while not all_finished:
-                f_input_available.wait()
-                f_output_available.wait()
-
-        b_thread = Thread(target=b_thread)
-        threads.append(b_thread)
-        b_thread.start()
-
-    while True:
-        f_input_available.wait()
-        if all(finished):
-            all_finished = True
-            f_output_available.wait()
-            break
-        f_b_losses, f_grad = fg(self.stack(f_inputs))  # Evaluate function and gradient
-        f_b_losses_np = self.numpy(f_b_losses).astype(numpy.float64)
-        f_grad_np = self.numpy(f_grad).astype(numpy.float64)
-        f_output_available.wait()
-
-    for b_thread in threads:
-        b_thread.join()  # make sure threads exit correctly
-
-    if trj:
-        max_trajectory_length = max([len(t) for t in trajectories])
-        last_points = [SolveResult(method_description, xs[b], self.numpy(final_losses[b]), iterations[b], function_evaluations[b], converged[b], diverged[b], "") for b in range(batch_size)]
-        trajectories = [t[:-1] + [last_point] * (max_trajectory_length - len(t) + 1) for t, last_point in zip(trajectories, last_points)]
-        trajectory = []
-        for states in zip(*trajectories):
-            x = numpy.stack([state.x for state in states])
-            residual = numpy.stack([state.residual for state in states])
-            iterations = [state.iterations for state in states]
-            function_evaluations = [state.function_evaluations for state in states]
-            converged = [state.converged for state in states]
-            diverged = [state.diverged for state in states]
-            trajectory.append(SolveResult(method_description, x, residual, iterations, function_evaluations, converged, diverged, messages))
-        return trajectory
-    else:
-        x = self.stack(xs)
-        residual = self.stack(final_losses)
-        return SolveResult(method_description, x, residual, iterations, function_evaluations, converged, diverged, messages)
-
-
-def gradient_descent(self: Backend, f, x0, atol, max_iter, trj: bool, step_size='adaptive'):
-    assert self.supports(Backend.jacobian)
-    assert len(self.staticshape(x0)) == 2  # (batch, parameters)
-    batch_size = self.staticshape(x0)[0]
-    fg = self.jacobian(f, [0], get_output=True, is_f_scalar=True)
-    method = f"Gradient descent with {self.name}"
-
-    iterations = self.zeros([batch_size], DType(int, 32))
-    function_evaluations = self.ones([batch_size], DType(int, 32))
-
-    adaptive_step_size = step_size == 'adaptive'
-    if adaptive_step_size:
-        step_size = self.zeros([batch_size]) + 0.1
-
-    loss, grad = fg(x0)  # Evaluate function and gradient
-    diverged = self.any(~self.isfinite(x0), axis=(1,))
-    converged = self.zeros([batch_size], DType(bool))
-    trajectory = [SolveResult(method, x0, loss, iterations, function_evaluations, converged, diverged, [""] * batch_size)] if trj else None
-    max_iter_ = self.to_int32(max_iter)
-    continue_ = ~converged & ~diverged & (iterations < max_iter_)
-
-    def gd_step(continue_, x, loss, grad, iterations, function_evaluations, step_size, converged, diverged):
-        prev_loss, prev_grad, prev_x = loss, grad, x
-        continue_1 = self.to_int32(continue_)
-        iterations += continue_1
-        if adaptive_step_size:
-            for i in range(20):
-                dx = - grad * self.expand_dims(step_size * self.to_float(continue_1), -1)
-                next_x = x + dx
-                predicted_loss_decrease = - self.sum(grad * dx, -1)  # >= 0
-                next_loss, next_grad = fg(next_x); function_evaluations += continue_1
-                converged = converged | (self.sum(next_grad ** 2, axis=-1) < atol ** 2)
-                PHI_LOGGER.debug(f"Gradient: {self.numpy(next_grad)} with step_size={self.numpy(step_size)}")
-                actual_loss_decrease = loss - next_loss  # we want > 0
-                # we want actual_loss_decrease to be at least half of predicted_loss_decrease
-                act_pred = self.divide_no_nan(actual_loss_decrease, predicted_loss_decrease)
-                PHI_LOGGER.debug(f"Actual/Predicted: {self.numpy(act_pred)}")
-                step_size_fac = self.clip(self.log(1 + 1.71828182845 * self.exp((act_pred - 0.5) * 2.)), 0.1, 10)
-                PHI_LOGGER.debug(f"step_size *= {self.numpy(step_size_fac)}")
-                step_size *= step_size_fac
-                if self.all((act_pred > 0.4) & (act_pred < 0.9) | converged | diverged):
-                    PHI_LOGGER.debug(f"GD minimization: Finished step_size adjustment after {i + 1} tries\n")
-                    break
-            else:
-                converged = converged | (abs(actual_loss_decrease) < predicted_loss_decrease)
-                PHI_LOGGER.debug("Backend._minimize_gradient_descent(): No step size found!\n")
-            diverged = diverged | (next_loss > loss)
-            x, loss, grad = next_x, next_loss, next_grad
-        else:
-            x -= grad * self.expand_dims(step_size * self.to_float(continue_1), -1)
-            loss, grad = fg(x); function_evaluations += continue_1
-            diverged = self.any(~self.isfinite(x), axis=(1,)) | (loss > prev_loss)
-            converged = ~diverged & (prev_loss - loss < atol)
-        if trj:
-            trajectory.append(SolveResult(method, self.numpy(x), self.numpy(loss), self.numpy(iterations), self.numpy(function_evaluations), self.numpy(diverged), self.numpy(converged), [""] * batch_size))
-        continue_ = ~converged & ~diverged & (iterations < max_iter_)
-        return continue_, x, loss, grad, iterations, function_evaluations, step_size, converged, diverged
-
-    not_converged, x, loss, grad, iterations, function_evaluations, step_size, converged, diverged = self.while_loop(gd_step, (continue_, x0, loss, grad, iterations, function_evaluations, step_size, converged, diverged), int(max(max_iter)))
-    if trj:
-        trajectory.append(SolveResult(method, x, loss, iterations, function_evaluations + 1, converged, diverged, [""] * batch_size))
-        return trajectory
-    else:
-        return SolveResult(method, x, loss, iterations, function_evaluations, converged, diverged, [""] * batch_size)
+from threading import Barrier
+
+import numpy
+
+from ._backend import Backend, SolveResult, DType, PHI_LOGGER
+from ._linalg import _max_iter
+
+
+def scipy_minimize(self, method: str, f, x0, atol, max_iter, trj: bool):
+    from scipy.optimize import OptimizeResult, minimize
+    from threading import Thread
+
+    assert self.supports(Backend.jacobian)
+    x0 = self.numpy(x0)
+    assert x0.ndim == 2  # (batch, parameters)
+    atol = self.numpy(atol)
+    batch_size = x0.shape[0]
+    fg = self.jacobian(f, [0], get_output=True, is_f_scalar=True)
+    method_description = f"SciPy {method} with {self.name}"
+
+    iterations = [0] * batch_size
+    function_evaluations = [0] * batch_size
+    xs = [None] * batch_size
+    final_losses = [None] * batch_size
+    converged = [False] * batch_size
+    diverged = [False] * batch_size
+    messages = [""] * batch_size
+
+    f_inputs = [None] * batch_size
+    f_b_losses = None
+    f_b_losses_np = None
+    f_grad_np = None
+    f_input_available = Barrier(batch_size + 1)
+    f_output_available = Barrier(batch_size + 1)
+    finished = [False] * batch_size
+    all_finished = False
+    trajectories = [[] for _ in range(batch_size)] if trj else None
+    threads = []
+
+    for b in range(batch_size):  # Run each independent example as a scipy minimization in a new thread
+
+        def b_thread(b=b):
+            recent_b_losses = []
+
+            def b_fun(x: numpy.ndarray):
+                function_evaluations[b] += 1
+                f_inputs[b] = self.as_tensor(x, convert_external=True)
+                f_input_available.wait()
+                f_output_available.wait()
+                recent_b_losses.append(f_b_losses[b])
+                if final_losses[b] is None:  # first evaluation
+                    final_losses[b] = f_b_losses[b]
+                    if trajectories is not None:
+                        trajectories[b].append(SolveResult(method_description, x0[b], self.numpy(f_b_losses[b]), 0, 1, False, False, ""))
+                return f_b_losses_np[b], f_grad_np[b]
+
+            def callback(x, *args):  # L-BFGS-B only passes x but the documentation says (x, state)
+                iterations[b] += 1
+                loss = min(recent_b_losses)
+                recent_b_losses.clear()
+                final_losses[b] = loss
+                if trajectories is not None:
+                    trajectories[b].append(SolveResult(method_description, x, self.numpy(loss), iterations[b], function_evaluations[b], False, False, ""))
+
+            res = minimize(fun=b_fun, x0=x0[b], jac=True, method=method, tol=atol[b], options={'maxiter': max_iter[b]}, callback=callback)
+            assert isinstance(res, OptimizeResult)
+            # res.nit, res.nfev
+            xs[b] = res.x
+            converged[b] = res.success
+            diverged[b] = res.status not in (0, 1)  # 0=success
+            messages[b] = res.message
+            finished[b] = True
+            while not all_finished:
+                f_input_available.wait()
+                f_output_available.wait()
+
+        b_thread = Thread(target=b_thread)
+        threads.append(b_thread)
+        b_thread.start()
+
+    while True:
+        f_input_available.wait()
+        if all(finished):
+            all_finished = True
+            f_output_available.wait()
+            break
+        f_b_losses, f_grad = fg(self.stack(f_inputs))  # Evaluate function and gradient
+        f_b_losses_np = self.numpy(f_b_losses).astype(numpy.float64)
+        f_grad_np = self.numpy(f_grad).astype(numpy.float64)
+        f_output_available.wait()
+
+    for b_thread in threads:
+        b_thread.join()  # make sure threads exit correctly
+
+    if trj:
+        max_trajectory_length = max([len(t) for t in trajectories])
+        last_points = [SolveResult(method_description, xs[b], self.numpy(final_losses[b]), iterations[b], function_evaluations[b], converged[b], diverged[b], "") for b in range(batch_size)]
+        trajectories = [t[:-1] + [last_point] * (max_trajectory_length - len(t) + 1) for t, last_point in zip(trajectories, last_points)]
+        trajectory = []
+        for states in zip(*trajectories):
+            x = numpy.stack([state.x for state in states])
+            residual = numpy.stack([state.residual for state in states])
+            iterations = [state.iterations for state in states]
+            function_evaluations = [state.function_evaluations for state in states]
+            converged = [state.converged for state in states]
+            diverged = [state.diverged for state in states]
+            trajectory.append(SolveResult(method_description, x, residual, iterations, function_evaluations, converged, diverged, messages))
+        return trajectory
+    else:
+        x = self.stack(xs)
+        residual = self.stack(final_losses)
+        return SolveResult(method_description, x, residual, iterations, function_evaluations, converged, diverged, messages)
+
+
+def gradient_descent(self: Backend, f, x0, atol, max_iter, trj: bool, step_size='adaptive'):
+    assert self.supports(Backend.jacobian)
+    assert len(self.staticshape(x0)) == 2  # (batch, parameters)
+    batch_size = self.staticshape(x0)[0]
+    fg = self.jacobian(f, [0], get_output=True, is_f_scalar=True)
+    method = f"Gradient descent with {self.name}"
+
+    iterations = self.zeros([batch_size], DType(int, 32))
+    function_evaluations = self.ones([batch_size], DType(int, 32))
+
+    adaptive_step_size = step_size == 'adaptive'
+    if adaptive_step_size:
+        step_size = self.zeros([batch_size]) + 0.1
+
+    loss, grad = fg(x0)  # Evaluate function and gradient
+    diverged = self.any(~self.isfinite(x0), axis=(1,))
+    converged = self.zeros([batch_size], DType(bool))
+    trajectory = [SolveResult(method, x0, loss, iterations, function_evaluations, converged, diverged, [""] * batch_size)] if trj else None
+    max_iter_ = self.to_int32(max_iter)
+    continue_ = ~converged & ~diverged & (iterations < max_iter_)
+
+    def gd_step(continue_, x, loss, grad, iterations, function_evaluations, step_size, converged, diverged):
+        prev_loss, prev_grad, prev_x = loss, grad, x
+        continue_1 = self.to_int32(continue_)
+        iterations += continue_1
+        if adaptive_step_size:
+            for i in range(20):
+                dx = - grad * self.expand_dims(step_size * self.to_float(continue_1), -1)
+                next_x = x + dx
+                predicted_loss_decrease = - self.sum(grad * dx, -1)  # >= 0
+                next_loss, next_grad = fg(next_x); function_evaluations += continue_1
+                converged = converged | (self.sum(next_grad ** 2, axis=-1) < atol ** 2)
+                PHI_LOGGER.debug(f"Gradient: {self.numpy(next_grad)} with step_size={self.numpy(step_size)}")
+                actual_loss_decrease = loss - next_loss  # we want > 0
+                # we want actual_loss_decrease to be at least half of predicted_loss_decrease
+                act_pred = self.divide_no_nan(actual_loss_decrease, predicted_loss_decrease)
+                PHI_LOGGER.debug(f"Actual/Predicted: {self.numpy(act_pred)}")
+                step_size_fac = self.clip(self.log(1 + 1.71828182845 * self.exp((act_pred - 0.5) * 2.)), 0.1, 10)
+                PHI_LOGGER.debug(f"step_size *= {self.numpy(step_size_fac)}")
+                step_size *= step_size_fac
+                if self.all((act_pred > 0.4) & (act_pred < 0.9) | converged | diverged):
+                    PHI_LOGGER.debug(f"GD minimization: Finished step_size adjustment after {i + 1} tries\n")
+                    break
+            else:
+                converged = converged | (abs(actual_loss_decrease) < predicted_loss_decrease)
+                PHI_LOGGER.debug("Backend._minimize_gradient_descent(): No step size found!\n")
+            diverged = diverged | (next_loss > loss)
+            x, loss, grad = next_x, next_loss, next_grad
+        else:
+            x -= grad * self.expand_dims(step_size * self.to_float(continue_1), -1)
+            loss, grad = fg(x); function_evaluations += continue_1
+            diverged = self.any(~self.isfinite(x), axis=(1,)) | (loss > prev_loss)
+            converged = ~diverged & (prev_loss - loss < atol)
+        if trj:
+            trajectory.append(SolveResult(method, self.numpy(x), self.numpy(loss), self.numpy(iterations), self.numpy(function_evaluations), self.numpy(diverged), self.numpy(converged), [""] * batch_size))
+        continue_ = ~converged & ~diverged & (iterations < max_iter_)
+        return continue_, x, loss, grad, iterations, function_evaluations, step_size, converged, diverged
+
+    not_converged, x, loss, grad, iterations, function_evaluations, step_size, converged, diverged = self.while_loop(gd_step, (continue_, x0, loss, grad, iterations, function_evaluations, step_size, converged, diverged), int(max(max_iter)))
+    if trj:
+        trajectory.append(SolveResult(method, x, loss, iterations, function_evaluations + 1, converged, diverged, [""] * batch_size))
+        return trajectory
+    else:
+        return SolveResult(method, x, loss, iterations, function_evaluations, converged, diverged, [""] * batch_size)
```

### Comparing `phiflow-2.3.4/phi/math/backend/_numpy_backend.py` & `phiflow-2.4.0/phi/math/backend/_numpy_backend.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,496 +1,483 @@
-import numbers
-import os
-import sys
-from typing import List, Any, Callable, Union
-
-import numpy as np
-import numpy.random
-import scipy.signal
-import scipy.sparse
-from scipy.sparse import issparse
-from scipy.sparse.linalg import cg, spsolve
-
-from . import Backend, ComputeDevice
-from ._backend import combined_dim, SolveResult, TensorType
-from ._dtype import from_numpy_dtype, to_numpy_dtype, DType
-
-
-class NumPyBackend(Backend):
-    """Core Python Backend using NumPy & SciPy"""
-
-    def __init__(self):
-        if sys.platform != "win32" and sys.platform != "darwin":
-            mem_bytes = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')
-        else:
-            mem_bytes = -1
-        processors = os.cpu_count()
-        cpu = ComputeDevice(self, "CPU", 'CPU', mem_bytes, processors, "", 'CPU')
-        Backend.__init__(self, "NumPy", [cpu], cpu)
-
-    def prefers_channels_last(self) -> bool:
-        return True
-
-    seed = np.random.seed
-    clip = staticmethod(np.clip)
-    minimum = np.minimum
-    maximum = np.maximum
-    ones_like = staticmethod(np.ones_like)
-    zeros_like = staticmethod(np.zeros_like)
-    nonzero = staticmethod(np.argwhere)
-    reshape = staticmethod(np.reshape)
-    concat = staticmethod(np.concatenate)
-    stack = staticmethod(np.stack)
-    tile = staticmethod(np.tile)
-    repeat = staticmethod(np.repeat)
-    transpose = staticmethod(np.transpose)
-    sqrt = np.sqrt
-    exp = np.exp
-    sin = np.sin
-    arcsin = np.arcsin
-    cos = np.cos
-    arccos = np.arccos
-    tan = np.tan
-    arctan = np.arctan
-    arctan2 = staticmethod(np.arctan2)
-    sinh = np.sinh
-    arcsinh = np.arcsinh
-    cosh = np.cosh
-    arccosh = np.arccosh
-    tanh = np.tanh
-    arctanh = np.arctanh
-    log = np.log
-    log2 = np.log2
-    log10 = np.log10
-    isfinite = np.isfinite
-    abs = np.abs
-    sign = np.sign
-    round = staticmethod(np.round)
-    ceil = np.ceil
-    floor = np.floor
-    shape = staticmethod(np.shape)
-    staticshape = staticmethod(np.shape)
-    imag = staticmethod(np.imag)
-    real = staticmethod(np.real)
-    conj = staticmethod(np.conjugate)
-    einsum = staticmethod(np.einsum)
-    cumsum = staticmethod(np.cumsum)
-
-    def as_tensor(self, x, convert_external=True):
-        if self.is_tensor(x, only_native=convert_external):
-            array = x
-        else:
-            array = np.array(x)
-        # --- Enforce Precision ---
-        if not isinstance(array, numbers.Number):
-            if self.dtype(array).kind == float:
-                array = self.to_float(array)
-            elif self.dtype(array).kind == complex:
-                array = self.to_complex(array)
-        return array
-
-    def is_module(self, obj):
-        return False
-
-    def is_tensor(self, x, only_native=False):
-        if isinstance(x, np.ndarray) and x.dtype != object and x.dtype != str:
-            return True
-        if issparse(x):
-            return True
-        if isinstance(x, (np.bool_, np.float32, np.float64, np.float16, np.int8, np.int16, np.int32, np.int64, np.complex128, np.complex64)):
-            return True
-        # --- Above considered native ---
-        if only_native:
-            return False
-        # --- Non-native types
-        if isinstance(x, (numbers.Number, bool)):
-            return True
-        if isinstance(x, (tuple, list)):
-            return all([self.is_tensor(item, False) for item in x])
-        return False
-
-    def is_available(self, tensor):
-        return True
-
-    def numpy(self, tensor):
-        if isinstance(tensor, np.ndarray):
-            return tensor
-        else:
-            return np.array(tensor)
-
-    def copy(self, tensor, only_mutable=False):
-        return np.copy(tensor)
-
-    def get_device(self, tensor) -> ComputeDevice:
-        return self._default_device
-
-    def allocate_on_device(self, tensor: TensorType, device: ComputeDevice) -> TensorType:
-        assert device == self._default_device, f"NumPy Can only allocate on the CPU but got device {device}"
-        return tensor
-
-    def equal(self, x, y):
-        if isinstance(x, np.ndarray) and x.dtype.char == 'U':  # string comparison
-            x = x.astype(object)
-        if isinstance(x, str):
-            x = np.array(x, object)
-        return np.equal(x, y)
-
-    def divide_no_nan(self, x, y):
-        with np.errstate(divide='ignore', invalid='ignore'):
-            result = x / y
-        return np.where(y == 0, 0, result)
-
-    def random_uniform(self, shape, low, high, dtype: Union[DType, None]):
-        dtype = dtype or self.float_type
-        if dtype.kind == float:
-            return np.random.uniform(low, high, shape).astype(to_numpy_dtype(dtype))
-        elif dtype.kind == complex:
-            return (np.random.uniform(low.real, high.real, shape) + 1j * np.random.uniform(low.imag, high.imag, shape)).astype(to_numpy_dtype(dtype))
-        elif dtype.kind == int:
-            return numpy.random.randint(low, high, shape, dtype=to_numpy_dtype(dtype))
-        else:
-            raise ValueError(dtype)
-
-    def random_normal(self, shape, dtype: DType):
-        dtype = dtype or self.float_type
-        return np.random.standard_normal(shape).astype(to_numpy_dtype(dtype))
-
-    def range(self, start, limit=None, delta=1, dtype: DType = DType(int, 32)):
-        if limit is None:
-            start, limit = 0, start
-        return np.arange(start, limit, delta, to_numpy_dtype(dtype))
-
-    def pad(self, value, pad_width, mode='constant', constant_values=0):
-        if mode not in ('constant', 'symmetric', 'periodic', 'reflect', 'boundary'):
-            return NotImplemented
-        if mode == 'constant':
-            return np.pad(value, pad_width, 'constant', constant_values=constant_values)
-        else:
-            if mode in ('periodic', 'boundary'):
-                mode = {'periodic': 'wrap', 'boundary': 'edge'}[mode]
-            return np.pad(value, pad_width, mode)
-
-    def sum(self, value, axis=None, keepdims=False):
-        return np.sum(value, axis=axis, keepdims=keepdims)
-
-    def prod(self, value, axis=None):
-        if not isinstance(value, np.ndarray):
-            value = np.array(value)
-        if value.dtype == bool:
-            return np.all(value, axis=axis)
-        return np.prod(value, axis=axis)
-
-    def where(self, condition, x=None, y=None):
-        if x is None or y is None:
-            return np.argwhere(condition)
-        return np.where(condition, x, y)
-
-    def zeros(self, shape, dtype: DType = None):
-        return np.zeros(shape, dtype=to_numpy_dtype(dtype or self.float_type))
-
-    def ones(self, shape, dtype: DType = None):
-        return np.ones(shape, dtype=to_numpy_dtype(dtype or self.float_type))
-
-    def meshgrid(self, *coordinates):
-        return np.meshgrid(*coordinates, indexing='ij')
-
-    def linspace(self, start, stop, number):
-        return np.linspace(start, stop, number, dtype=to_numpy_dtype(self.float_type))
-
-    def mean(self, value, axis=None, keepdims=False):
-        return np.mean(value, axis, keepdims=keepdims)
-
-    def tensordot(self, a, a_axes: Union[tuple, list], b, b_axes: Union[tuple, list]):
-        return np.tensordot(a, b, (a_axes, b_axes))
-
-    def mul(self, a, b):
-        if scipy.sparse.issparse(a):
-            return a.multiply(b)
-        elif scipy.sparse.issparse(b):
-            return b.multiply(a)
-        else:
-            return Backend.mul(self, a, b)
-
-    def mul_matrix_batched_vector(self, A, b):
-        return np.stack([A.dot(b[i]) for i in range(b.shape[0])])
-
-    def get_diagonal(self, matrices, offset=0):
-        return np.transpose(np.diagonal(matrices, offset=offset, axis1=1, axis2=2), [0, 2, 1])
-
-    def max(self, x, axis=None, keepdims=False):
-        return np.max(x, axis, keepdims=keepdims)
-
-    def min(self, x, axis=None, keepdims=False):
-        return np.min(x, axis, keepdims=keepdims)
-
-    def conv(self, value, kernel, zero_padding=True):
-        assert kernel.shape[0] in (1, value.shape[0])
-        assert value.shape[1] == kernel.shape[2], f"value has {value.shape[1]} channels but kernel has {kernel.shape[2]}"
-        assert value.ndim + 1 == kernel.ndim
-        if zero_padding:
-            result = np.zeros((value.shape[0], kernel.shape[1], *value.shape[2:]), dtype=to_numpy_dtype(self.float_type))
-        else:
-            valid = [value.shape[i + 2] - kernel.shape[i + 3] + 1 for i in range(value.ndim - 2)]
-            result = np.zeros([value.shape[0], kernel.shape[1], *valid], dtype=to_numpy_dtype(self.float_type))
-        mode = 'same' if zero_padding else 'valid'
-        for b in range(value.shape[0]):
-            b_kernel = kernel[min(b, kernel.shape[0] - 1)]
-            for o in range(kernel.shape[1]):
-                for i in range(value.shape[1]):
-                    result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)
-        return result
-
-    def expand_dims(self, a, axis=0, number=1):
-        for _i in range(number):
-            a = np.expand_dims(a, axis)
-        return a
-
-    def cast(self, x, dtype: DType):
-        if self.is_tensor(x, only_native=True) and from_numpy_dtype(x.dtype) == dtype:
-            return x
-        else:
-            return np.array(x, to_numpy_dtype(dtype))
-
-    def gather(self, values, indices, axis: int):
-        slices = [indices if i == axis else slice(None) for i in range(self.ndims(values))]
-        return values[tuple(slices)]
-
-    def batched_gather_nd(self, values, indices):
-        assert indices.shape[-1] == self.ndims(values) - 2
-        batch_size = combined_dim(values.shape[0], indices.shape[0])
-        result = np.empty((batch_size, *indices.shape[1:-1], values.shape[-1],), values.dtype)
-        for b in range(batch_size):
-            b_values = values[min(b, values.shape[0] - 1)]
-            b_indices = self.unstack(indices[min(b, indices.shape[0] - 1)], -1)
-            result[b] = b_values[b_indices]
-        return result
-
-    def std(self, x, axis=None, keepdims=False):
-        return np.std(x, axis, keepdims=keepdims)
-
-    def boolean_mask(self, x, mask, axis=0):
-        slices = [mask if i == axis else slice(None) for i in range(len(x.shape))]
-        return x[tuple(slices)]
-
-    def any(self, boolean_tensor, axis=None, keepdims=False):
-        return np.any(boolean_tensor, axis=axis, keepdims=keepdims)
-
-    def all(self, boolean_tensor, axis=None, keepdims=False):
-        return np.all(boolean_tensor, axis=axis, keepdims=keepdims)
-
-    def scatter(self, base_grid, indices, values, mode: str):
-        assert mode in ('add', 'update')
-        assert isinstance(base_grid, np.ndarray)
-        assert isinstance(indices, (np.ndarray, tuple))
-        assert isinstance(values, np.ndarray)
-        assert indices.ndim == 3
-        assert values.ndim == 3
-        assert base_grid.ndim >= 3
-        batch_size = combined_dim(combined_dim(base_grid.shape[0], indices.shape[0]), values.shape[0])
-        if base_grid.shape[0] == batch_size:
-            result = np.copy(base_grid)
-        else:
-            result = np.tile(base_grid, (batch_size, *[1] * (base_grid.ndim - 1)))
-        if not isinstance(indices, (tuple, list)):
-            indices = self.unstack(indices, axis=-1)
-        if mode == 'add':
-            for b in range(batch_size):
-                np.add.at(result, (b, *[i[min(b, i.shape[0]-1)] for i in indices]), values[min(b, values.shape[0]-1)])
-        else:  # update
-            for b in range(batch_size):
-                result[(b, *[i[min(b, i.shape[0]-1)] for i in indices])] = values[min(b, values.shape[0]-1)]
-        # elif duplicates_handling == 'mean':
-        #     count = np.zeros(shape, np.int32)
-        #     np.add.at(array, tuple(indices), values)
-        #     np.add.at(count, tuple(indices), 1)
-        #     count = np.maximum(1, count)
-        #     return array / count
-        return result
-
-    def quantile(self, x, quantiles):
-        return np.quantile(x, quantiles, axis=-1)
-
-    def fft(self, x, axes: Union[tuple, list]):
-        x = self.to_complex(x)
-        if not axes:
-            return x
-        if len(axes) == 1:
-            return np.fft.fft(x, axis=axes[0]).astype(x.dtype)
-        elif len(axes) == 2:
-            return np.fft.fft2(x, axes=axes).astype(x.dtype)
-        else:
-            return np.fft.fftn(x, axes=axes).astype(x.dtype)
-
-    def ifft(self, k, axes: Union[tuple, list]):
-        if not axes:
-            return k
-        if len(axes) == 1:
-            return np.fft.ifft(k, axis=axes[0]).astype(k.dtype)
-        elif len(axes) == 2:
-            return np.fft.ifft2(k, axes=axes).astype(k.dtype)
-        else:
-            return np.fft.ifftn(k, axes=axes).astype(k.dtype)
-
-    def dtype(self, array) -> DType:
-        if isinstance(array, int):
-            return DType(int, 32)
-        if isinstance(array, float):
-            return DType(float, 64)
-        if isinstance(array, complex):
-            return DType(complex, 128)
-        if not hasattr(array, 'dtype'):
-            array = np.array(array)
-        return from_numpy_dtype(array.dtype)
-
-    def indexed_segment_sum(self, x, indices, axis: int):
-        return np.stack([np.add.reduceat(x[b], indices[b], axis-1) for b in range(x.shape[0])])
-
-    def sparse_coo_tensor(self, indices, values, shape):
-        indices = self.unstack(indices, -1)
-        if len(shape) == 2:
-            return scipy.sparse.coo_matrix((values, indices), shape=shape)
-        else:
-            raise NotImplementedError(f"len(indices) = {len(indices)} not supported. Only (2) allowed.")
-
-    def csr_matrix(self, column_indices, row_pointers, values, shape: tuple):
-        return scipy.sparse.csr_matrix((values, column_indices, row_pointers), shape=shape)
-
-    def mul_csr_dense(self, column_indices, row_pointers, values, shape: tuple, dense):
-        batch_size, nnz, channel_count = values.shape
-        result = []
-        for b in range(batch_size):
-            b_result = []
-            for c in range(channel_count):
-                mat = scipy.sparse.csr_matrix((values[b, :, c], column_indices[b], row_pointers[b]), shape=shape)
-                b_result.append(mat * dense[b, :, c, :])
-            result.append(np.stack(b_result))
-        return np.stack(result)
-
-    def csc_matrix(self, column_pointers, row_indices, values, shape: tuple):
-        return scipy.sparse.csc_matrix((values, row_indices, column_pointers), shape=shape)
-
-    def stop_gradient(self, value):
-        return value
-
-    # def jacobian(self, f, wrt: Union[tuple, list], get_output: bool):
-    #     warnings.warn("NumPy does not support analytic gradients and will use differences instead. This may be slow!", RuntimeWarning)
-    #     eps = {64: 1e-9, 32: 1e-4, 16: 1e-1}[self.precision]
-    #
-    #     def gradient(*args, **kwargs):
-    #         output = f(*args, **kwargs)
-    #         loss = output[0] if isinstance(output, (tuple, list)) else output
-    #         grads = []
-    #         for wrt_ in wrt:
-    #             x = args[wrt_]
-    #             assert isinstance(x, np.ndarray)
-    #             if x.size > 64:
-    #                 raise RuntimeError("NumPy does not support analytic gradients. Use PyTorch, TensorFlow or Jax.")
-    #             grad = np.zeros_like(x).flatten()
-    #             for i in range(x.size):
-    #                 x_flat = x.flatten()  # makes a copy
-    #                 x_flat[i] += eps
-    #                 args_perturbed = list(args)
-    #                 args_perturbed[wrt_] = np.reshape(x_flat, x.shape)
-    #                 output_perturbed = f(*args_perturbed, **kwargs)
-    #                 loss_perturbed = output_perturbed[0] if isinstance(output, (tuple, list)) else output_perturbed
-    #                 grad[i] = (loss_perturbed - loss) / eps
-    #             grads.append(np.reshape(grad, x.shape))
-    #         if get_output:
-    #             return output, grads
-    #         else:
-    #             return grads
-    #     return gradient
-
-    def linear_solve(self, method: str, lin, y, x0, tol_sq, max_iter) -> SolveResult:
-        if method == 'direct':
-            return self.direct_linear_solve(lin, y)
-        elif method == 'CG-native':
-            return self.scipy_iterative_sparse_solve(lin, y, x0, tol_sq, max_iter, scipy_function=scipy.sparse.linalg.cg)
-        elif method == 'GMres':
-            return self.scipy_iterative_sparse_solve(lin, y, x0, tol_sq, max_iter, scipy_function=scipy.sparse.linalg.gmres)
-        elif method == 'biCG':
-            return self.scipy_iterative_sparse_solve(lin, y, x0, tol_sq, max_iter, scipy_function=scipy.sparse.linalg.bicg)
-        elif method == 'CGS':
-            return self.scipy_iterative_sparse_solve(lin, y, x0, tol_sq, max_iter, scipy_function=scipy.sparse.linalg.cgs)
-        elif method == 'lGMres':
-            return self.scipy_iterative_sparse_solve(lin, y, x0, tol_sq, max_iter, scipy_function=scipy.sparse.linalg.lgmres)
-        # elif method == 'minres':
-        #     return self.scipy_iterative_sparse_solve(lin, y, x0, tol_sq, max_iter, scipy_function=scipy.sparse.linalg.minres)
-        elif method == 'QMR':
-            return self.scipy_iterative_sparse_solve(lin, y, x0, tol_sq, max_iter, scipy_function=scipy.sparse.linalg.qmr)
-        elif method == 'GCrotMK':
-            return self.scipy_iterative_sparse_solve(lin, y, x0, tol_sq, max_iter, scipy_function=scipy.sparse.linalg.gcrotmk)
-        elif method == 'auto':
-            return self.conjugate_gradient_adaptive(lin, y, x0, tol_sq, max_iter)
-            # return self.conjugate_gradient(lin, y, x0, tol_sq, max_iter, trj)
-        else:
-            return Backend.linear_solve(self, method, lin, y, x0, tol_sq, max_iter)
-
-    def direct_linear_solve(self, lin, y) -> Any:
-        batch_size = self.staticshape(y)[0]
-        xs = []
-        converged = []
-        if isinstance(lin, (tuple, list)):
-            assert all(issparse(l) for l in lin)
-        else:
-            assert issparse(lin)
-            lin = [lin] * batch_size
-        # Solve each example independently
-        for batch in range(batch_size):
-            # use_umfpack=self.precision == 64
-            x = spsolve(lin[batch], y[batch])  # returns nan when diverges
-            xs.append(x)
-            converged.append(np.all(np.isfinite(x)))
-        x = np.stack(xs)
-        converged = np.stack(converged)
-        diverged = ~converged
-        iterations = [-1] * batch_size  # spsolve does not perform iterations
-        return SolveResult('scipy.sparse.linalg.spsolve', x, None, iterations, iterations, converged, diverged, [""] * batch_size)
-
-    def conjugate_gradient(self, lin, y, x0, tol_sq, max_iter) -> SolveResult:
-        if len(max_iter) > 1 or callable(lin):
-            return Backend.conjugate_gradient(self, lin, y, x0, tol_sq, max_iter)  # generic implementation
-        else:
-            return self.scipy_iterative_sparse_solve(lin, y, x0, tol_sq, max_iter, scipy_function=scipy.sparse.linalg.bicg)  # more stable than cg
-
-    def scipy_iterative_sparse_solve(self, lin, y, x0, tol_sq, max_iter, scipy_function=cg) -> SolveResult:
-        bs_y = self.staticshape(y)[0]
-        bs_x0 = self.staticshape(x0)[0]
-        batch_size = combined_dim(bs_y, bs_x0)
-        # if callable(A):
-        #     A = LinearOperator(dtype=y.dtype, shape=(self.staticshape(y)[-1], self.staticshape(x0)[-1]), matvec=A)
-
-        def count_callback(x_n):  # called after each step, not with x0
-            iterations[b] += 1
-
-        xs = []
-        iterations = [0] * batch_size
-        converged = []
-        diverged = []
-        for b in range(batch_size):
-            lin_b = lin[min(b, len(lin)-1)] if isinstance(lin, (tuple, list)) or (isinstance(lin, np.ndarray) and len(lin.shape) > 2) else lin
-            x, ret_val = scipy_function(lin_b, y[b], x0=x0[b], tol=0, atol=np.sqrt(tol_sq[b]), maxiter=max_iter[-1, b], callback=count_callback)
-            # ret_val: 0=success, >0=not converged, <0=error
-            xs.append(x)
-            converged.append(ret_val == 0)
-            diverged.append(ret_val < 0 or np.any(~np.isfinite(x)))
-        x = np.stack(xs)
-        f_eval = [i + 1 for i in iterations]
-        return SolveResult(f'scipy.sparse.linalg.{scipy_function.__name__}', x, None, iterations, f_eval, converged, diverged, [""] * batch_size)
-
-    def matrix_solve_least_squares(self, matrix: TensorType, rhs: TensorType) -> TensorType:
-        solution, residuals, rank, singular_values = [], [], [], []
-        for b in range(self.shape(rhs)[0]):
-            solution_b, residual_b, rnk_b, s_b = np.linalg.lstsq(matrix[b], rhs[b], rcond=None)
-            solution.append(solution_b)
-            residuals.append(residual_b)
-            rank.append(rnk_b)
-            singular_values.append(s_b)
-        return np.stack(solution), np.stack(residuals), np.stack(rank), np.stack(singular_values)
-
-    def solve_triangular_dense(self, matrix, rhs, lower: bool, unit_diagonal: bool):
-        batch_size, rows, cols = matrix.shape
-        result = []
-        for b in range(batch_size):
-            x = scipy.linalg.solve_triangular(matrix[b, :, :], rhs[b, :], lower=lower, unit_diagonal=unit_diagonal)
-            result.append(x)
-        return np.stack(result)
+import numbers
+import os
+import sys
+from typing import Union, Optional
+
+import numpy as np
+import numpy.random
+import scipy.signal
+import scipy.sparse
+from scipy.sparse import issparse
+from scipy.sparse.linalg import spsolve_triangular
+
+from . import Backend, ComputeDevice
+from ._backend import combined_dim, SolveResult, TensorType
+from ._dtype import from_numpy_dtype, to_numpy_dtype, DType
+
+
+class NumPyBackend(Backend):
+    """Core Python Backend using NumPy & SciPy"""
+
+    def __init__(self):
+        if sys.platform != "win32" and sys.platform != "darwin":
+            mem_bytes = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')
+        else:
+            mem_bytes = -1
+        processors = os.cpu_count()
+        cpu = ComputeDevice(self, "CPU", 'CPU', mem_bytes, processors, "", 'CPU')
+        Backend.__init__(self, "NumPy", [cpu], cpu)
+
+    def prefers_channels_last(self) -> bool:
+        return True
+
+    seed = np.random.seed
+    clip = staticmethod(np.clip)
+    minimum = np.minimum
+    maximum = np.maximum
+    ones_like = staticmethod(np.ones_like)
+    zeros_like = staticmethod(np.zeros_like)
+    nonzero = staticmethod(np.argwhere)
+    reshape = staticmethod(np.reshape)
+    concat = staticmethod(np.concatenate)
+    stack = staticmethod(np.stack)
+    tile = staticmethod(np.tile)
+    transpose = staticmethod(np.transpose)
+    sqrt = np.sqrt
+    exp = np.exp
+    sin = np.sin
+    arcsin = np.arcsin
+    cos = np.cos
+    arccos = np.arccos
+    tan = np.tan
+    arctan = np.arctan
+    arctan2 = staticmethod(np.arctan2)
+    sinh = np.sinh
+    arcsinh = np.arcsinh
+    cosh = np.cosh
+    arccosh = np.arccosh
+    tanh = np.tanh
+    arctanh = np.arctanh
+    log = np.log
+    log2 = np.log2
+    log10 = np.log10
+    isfinite = np.isfinite
+    isnan = np.isnan
+    isinf = np.isinf
+    abs = np.abs
+    sign = np.sign
+    round = staticmethod(np.round)
+    ceil = np.ceil
+    floor = np.floor
+    log_gamma = np.math.lgamma
+    shape = staticmethod(np.shape)
+    staticshape = staticmethod(np.shape)
+    imag = staticmethod(np.imag)
+    real = staticmethod(np.real)
+    conj = staticmethod(np.conjugate)
+    einsum = staticmethod(np.einsum)
+    cumsum = staticmethod(np.cumsum)
+
+    def as_tensor(self, x, convert_external=True):
+        if self.is_tensor(x, only_native=convert_external):
+            array = x
+        else:
+            array = np.array(x)
+        # --- Enforce Precision ---
+        if not isinstance(array, numbers.Number):
+            if self.dtype(array).kind == float:
+                array = self.to_float(array)
+            elif self.dtype(array).kind == complex:
+                array = self.to_complex(array)
+        return array
+
+    def is_module(self, obj):
+        return False
+
+    def is_tensor(self, x, only_native=False):
+        if isinstance(x, np.ndarray) and x.dtype != object and x.dtype != str:
+            return True
+        if issparse(x):
+            return True
+        if isinstance(x, (np.bool_, np.float32, np.float64, np.float16, np.int8, np.int16, np.int32, np.int64, np.complex128, np.complex64)):
+            return True
+        # --- Above considered native ---
+        if only_native:
+            return False
+        # --- Non-native types
+        if isinstance(x, (numbers.Number, bool)):
+            return True
+        if isinstance(x, (tuple, list)):
+            return all([self.is_tensor(item, False) for item in x])
+        return False
+
+    def is_sparse(self, x) -> bool:
+        return issparse(x)
+
+    def is_available(self, tensor):
+        return True
+
+    def numpy(self, tensor):
+        if isinstance(tensor, np.ndarray) or issparse(tensor):
+            return tensor
+        else:
+            return np.array(tensor)
+
+    def copy(self, tensor, only_mutable=False):
+        return np.copy(tensor)
+
+    def get_device(self, tensor) -> ComputeDevice:
+        return self._default_device
+
+    def allocate_on_device(self, tensor: TensorType, device: ComputeDevice) -> TensorType:
+        assert device == self._default_device, f"NumPy Can only allocate on the CPU but got device {device}"
+        return tensor
+
+    def equal(self, x, y):
+        if isinstance(x, np.ndarray) and x.dtype.char == 'U':  # string comparison
+            x = x.astype(object)
+        if isinstance(x, str):
+            x = np.array(x, object)
+        return np.equal(x, y)
+
+    def divide_no_nan(self, x, y):
+        with np.errstate(divide='ignore', invalid='ignore'):
+            result = x / y
+        return np.where(y == 0, 0, result)
+
+    def softplus(self, x):
+        return np.log1p(np.exp(-np.abs(x))) + np.maximum(x, 0)
+
+    def random_uniform(self, shape, low, high, dtype: Optional[DType]):
+        dtype = dtype or self.float_type
+        if dtype.kind == float:
+            return np.random.uniform(low, high, shape).astype(to_numpy_dtype(dtype))
+        elif dtype.kind == complex:
+            return (np.random.uniform(low.real, high.real, shape) + 1j * np.random.uniform(low.imag, high.imag, shape)).astype(to_numpy_dtype(dtype))
+        elif dtype.kind == int:
+            return numpy.random.randint(low, high, shape, dtype=to_numpy_dtype(dtype))
+        else:
+            raise ValueError(dtype)
+
+    def random_normal(self, shape, dtype: DType):
+        dtype = dtype or self.float_type
+        return np.random.standard_normal(shape).astype(to_numpy_dtype(dtype))
+
+    def range(self, start, limit=None, delta=1, dtype: DType = DType(int, 32)):
+        if limit is None:
+            start, limit = 0, start
+        return np.arange(start, limit, delta, to_numpy_dtype(dtype))
+
+    def pad(self, value, pad_width, mode='constant', constant_values=0):
+        if mode not in ('constant', 'symmetric', 'periodic', 'reflect', 'boundary'):
+            return NotImplemented
+        if mode == 'constant':
+            return np.pad(value, pad_width, 'constant', constant_values=constant_values)
+        else:
+            if mode in ('periodic', 'boundary'):
+                mode = {'periodic': 'wrap', 'boundary': 'edge'}[mode]
+            return np.pad(value, pad_width, mode)
+
+    def sum(self, value, axis=None, keepdims=False):
+        return np.sum(value, axis=axis, keepdims=keepdims)
+
+    def prod(self, value, axis=None):
+        if not isinstance(value, np.ndarray):
+            value = np.array(value)
+        if value.dtype == bool:
+            return np.all(value, axis=axis)
+        return np.prod(value, axis=axis)
+
+    def where(self, condition, x=None, y=None):
+        if x is None or y is None:
+            return np.argwhere(condition)
+        return np.where(condition, x, y)
+
+    def zeros(self, shape, dtype: DType = None):
+        return np.zeros(shape, dtype=to_numpy_dtype(dtype or self.float_type))
+
+    def ones(self, shape, dtype: DType = None):
+        return np.ones(shape, dtype=to_numpy_dtype(dtype or self.float_type))
+
+    def meshgrid(self, *coordinates):
+        return np.meshgrid(*coordinates, indexing='ij')
+
+    def linspace(self, start, stop, number):
+        return np.linspace(start, stop, number, dtype=to_numpy_dtype(self.float_type))
+
+    def linspace_without_last(self, start, stop, number):
+        return np.linspace(start, stop, number, dtype=to_numpy_dtype(self.float_type), endpoint=False)
+
+    def mean(self, value, axis=None, keepdims=False):
+        return np.mean(value, axis, keepdims=keepdims)
+
+    def repeat(self, x, repeats, axis: int, new_length=None):
+        return np.repeat(x, repeats, axis)
+
+    def tensordot(self, a, a_axes: Union[tuple, list], b, b_axes: Union[tuple, list]):
+        return np.tensordot(a, b, (a_axes, b_axes))
+
+    def mul(self, a, b):
+        if scipy.sparse.issparse(a):
+            return a.multiply(b)
+        elif scipy.sparse.issparse(b):
+            return b.multiply(a)
+        else:
+            return Backend.mul(self, a, b)
+
+    def mul_matrix_batched_vector(self, A, b):
+        return np.stack([A.dot(b[i]) for i in range(b.shape[0])])
+
+    def get_diagonal(self, matrices, offset=0):
+        return np.transpose(np.diagonal(matrices, offset=offset, axis1=1, axis2=2), [0, 2, 1])
+
+    def max(self, x, axis=None, keepdims=False):
+        return np.max(x, axis, keepdims=keepdims)
+
+    def min(self, x, axis=None, keepdims=False):
+        return np.min(x, axis, keepdims=keepdims)
+
+    def conv(self, value, kernel, zero_padding=True):
+        assert kernel.shape[0] in (1, value.shape[0])
+        assert value.shape[1] == kernel.shape[2], f"value has {value.shape[1]} channels but kernel has {kernel.shape[2]}"
+        assert value.ndim + 1 == kernel.ndim
+        if zero_padding:
+            result = np.zeros((value.shape[0], kernel.shape[1], *value.shape[2:]), dtype=to_numpy_dtype(self.float_type))
+        else:
+            valid = [value.shape[i + 2] - kernel.shape[i + 3] + 1 for i in range(value.ndim - 2)]
+            result = np.zeros([value.shape[0], kernel.shape[1], *valid], dtype=to_numpy_dtype(self.float_type))
+        mode = 'same' if zero_padding else 'valid'
+        for b in range(value.shape[0]):
+            b_kernel = kernel[min(b, kernel.shape[0] - 1)]
+            for o in range(kernel.shape[1]):
+                for i in range(value.shape[1]):
+                    result[b, o, ...] += scipy.signal.correlate(value[b, i, ...], b_kernel[o, i, ...], mode=mode)
+        return result
+
+    def expand_dims(self, a, axis=0, number=1):
+        for _i in range(number):
+            a = np.expand_dims(a, axis)
+        return a
+
+    def cast(self, x, dtype: DType):
+        if self.is_tensor(x, only_native=True) and from_numpy_dtype(x.dtype) == dtype:
+            return x
+        else:
+            return np.array(x, to_numpy_dtype(dtype))
+
+    def unravel_index(self, flat_index, shape):
+        return np.stack(np.unravel_index(flat_index, shape), -1)
+
+    def ravel_multi_index(self, multi_index, shape, mode: Union[str, int] = 'undefined'):
+        mode = mode if isinstance(mode, int) else {'undefined': 'raise', 'periodic': 'wrap', 'clamp': 'clip'}[mode]
+        idx_first = np.transpose(multi_index, (-1,) + tuple(range(self.ndims(multi_index)-1)))
+        result = np.ravel_multi_index(idx_first, shape, mode='wrap' if isinstance(mode, int) else mode)
+        if isinstance(mode, int):
+            outside = self.any((multi_index < 0) | (multi_index >= shape), -1)
+            result = self.where(outside, mode, result)
+        return result
+
+    def gather(self, values, indices, axis: int):
+        slices = [indices if i == axis else slice(None) for i in range(self.ndims(values))]
+        return values[tuple(slices)]
+
+    def batched_gather_nd(self, values, indices):
+        assert indices.shape[-1] == self.ndims(values) - 2
+        batch_size = combined_dim(values.shape[0], indices.shape[0])
+        result = np.empty((batch_size, *indices.shape[1:-1], values.shape[-1],), values.dtype)
+        for b in range(batch_size):
+            b_values = values[min(b, values.shape[0] - 1)]
+            b_indices = self.unstack(indices[min(b, indices.shape[0] - 1)], -1)
+            result[b] = b_values[b_indices]
+        return result
+
+    def std(self, x, axis=None, keepdims=False):
+        return np.std(x, axis, keepdims=keepdims)
+
+    def boolean_mask(self, x, mask, axis=0, new_length=None, fill_value=0):
+        slices = [mask if i == axis else slice(None) for i in range(len(x.shape))]
+        result = x[tuple(slices)]
+        if new_length is not None:
+            if new_length > result.shape[axis]:
+                pad_width = [(0, new_length - result.shape[axis]) if i == axis else (0, 0) for i in range(len(x.shape))]
+                result = np.pad(result, pad_width, mode='constant', constant_values=fill_value)
+            elif new_length < result.shape[axis]:
+                result = result[tuple([slice(new_length) if i == axis else slice(None) for i in range(len(x.shape))])]
+        return result
+
+    def any(self, boolean_tensor, axis=None, keepdims=False):
+        return np.any(boolean_tensor, axis=axis, keepdims=keepdims)
+
+    def all(self, boolean_tensor, axis=None, keepdims=False):
+        return np.all(boolean_tensor, axis=axis, keepdims=keepdims)
+
+    def scatter(self, base_grid, indices, values, mode: str):
+        assert mode in ('add', 'update')
+        assert isinstance(base_grid, np.ndarray)
+        assert isinstance(indices, (np.ndarray, tuple))
+        assert isinstance(values, np.ndarray)
+        assert indices.ndim == 3
+        assert values.ndim == 3
+        assert base_grid.ndim >= 3
+        batch_size = combined_dim(combined_dim(base_grid.shape[0], indices.shape[0]), values.shape[0])
+        if base_grid.shape[0] == batch_size:
+            result = np.copy(base_grid)
+        else:
+            result = np.tile(base_grid, (batch_size, *[1] * (base_grid.ndim - 1)))
+        if not isinstance(indices, (tuple, list)):
+            indices = self.unstack(indices, axis=-1)
+        if mode == 'add':
+            for b in range(batch_size):
+                np.add.at(result, (b, *[i[min(b, i.shape[0]-1)] for i in indices]), values[min(b, values.shape[0]-1)])
+        else:  # update
+            for b in range(batch_size):
+                result[(b, *[i[min(b, i.shape[0]-1)] for i in indices])] = values[min(b, values.shape[0]-1)]
+        # elif duplicates_handling == 'mean':
+        #     count = np.zeros(shape, np.int32)
+        #     np.add.at(array, tuple(indices), values)
+        #     np.add.at(count, tuple(indices), 1)
+        #     count = np.maximum(1, count)
+        #     return array / count
+        return result
+
+    def histogram1d(self, values, weights, bin_edges):
+        batch_size, value_count = self.staticshape(values)
+        result = []
+        for b in range(batch_size):
+            hist, _ = np.histogram(values[b], bins=bin_edges[b], weights=weights[b])
+            result.append(hist)
+        return np.stack(result)
+
+    def bincount(self, x, weights, bins: int):
+        result = np.bincount(x, weights=weights, minlength=bins)
+        assert result.shape[-1] == bins
+        return result
+
+    def quantile(self, x, quantiles):
+        return np.quantile(x, quantiles, axis=-1)
+
+    def argsort(self, x, axis=-1):
+        return np.argsort(x, axis)
+
+    def searchsorted(self, sorted_sequence, search_values, side: str, dtype=DType(int, 32)):
+        if self.ndims(sorted_sequence) == 1:
+            return np.searchsorted(sorted_sequence, search_values, side=side).astype(to_numpy_dtype(dtype))
+        else:
+            return np.stack([self.searchsorted(seq, val, side, dtype) for seq, val in zip(sorted_sequence, search_values)])
+
+    def fft(self, x, axes: Union[tuple, list]):
+        x = self.to_complex(x)
+        if not axes:
+            return x
+        if len(axes) == 1:
+            return np.fft.fft(x, axis=axes[0]).astype(x.dtype)
+        elif len(axes) == 2:
+            return np.fft.fft2(x, axes=axes).astype(x.dtype)
+        else:
+            return np.fft.fftn(x, axes=axes).astype(x.dtype)
+
+    def ifft(self, k, axes: Union[tuple, list]):
+        if not axes:
+            return k
+        if len(axes) == 1:
+            return np.fft.ifft(k, axis=axes[0]).astype(k.dtype)
+        elif len(axes) == 2:
+            return np.fft.ifft2(k, axes=axes).astype(k.dtype)
+        else:
+            return np.fft.ifftn(k, axes=axes).astype(k.dtype)
+
+    def dtype(self, array) -> DType:
+        if isinstance(array, int):
+            return DType(int, 32)
+        if isinstance(array, float):
+            return DType(float, 64)
+        if isinstance(array, complex):
+            return DType(complex, 128)
+        if not hasattr(array, 'dtype'):
+            array = np.array(array)
+        return from_numpy_dtype(array.dtype)
+
+    def indexed_segment_sum(self, x, indices, axis: int):
+        return np.stack([np.add.reduceat(x[b], indices[b], axis-1) for b in range(x.shape[0])])
+
+    def sparse_coo_tensor(self, indices, values, shape):
+        indices = self.unstack(indices, -1)
+        if len(shape) == 2:
+            return scipy.sparse.coo_matrix((values, indices), shape=shape)
+        else:
+            raise NotImplementedError(f"len(indices) = {len(indices)} not supported. Only (2) allowed.")
+
+    def csr_matrix(self, column_indices, row_pointers, values, shape: tuple):
+        return scipy.sparse.csr_matrix((values, column_indices, row_pointers), shape=shape)
+
+    def mul_csr_dense(self, column_indices, row_pointers, values, shape: tuple, dense):
+        batch_size, nnz, channel_count = values.shape
+        result = []
+        for b in range(batch_size):
+            b_result = []
+            for c in range(channel_count):
+                mat = scipy.sparse.csr_matrix((values[b, :, c], column_indices[b], row_pointers[b]), shape=shape)
+                b_result.append(mat * dense[b, :, c, :])
+            result.append(np.stack(b_result))
+        return np.stack(result)
+
+    def csc_matrix(self, column_pointers, row_indices, values, shape: tuple):
+        return scipy.sparse.csc_matrix((values, row_indices, column_pointers), shape=shape)
+
+    def stop_gradient(self, value):
+        return value
+
+    # def jacobian(self, f, wrt: Union[tuple, list], get_output: bool):
+    #     warnings.warn("NumPy does not support analytic gradients and will use differences instead. This may be slow!", RuntimeWarning)
+    #     eps = {64: 1e-9, 32: 1e-4, 16: 1e-1}[self.precision]
+    #
+    #     def gradient(*args, **kwargs):
+    #         output = f(*args, **kwargs)
+    #         loss = output[0] if isinstance(output, (tuple, list)) else output
+    #         grads = []
+    #         for wrt_ in wrt:
+    #             x = args[wrt_]
+    #             assert isinstance(x, np.ndarray)
+    #             if x.size > 64:
+    #                 raise RuntimeError("NumPy does not support analytic gradients. Use PyTorch, TensorFlow or Jax.")
+    #             grad = np.zeros_like(x).flatten()
+    #             for i in range(x.size):
+    #                 x_flat = x.flatten()  # makes a copy
+    #                 x_flat[i] += eps
+    #                 args_perturbed = list(args)
+    #                 args_perturbed[wrt_] = np.reshape(x_flat, x.shape)
+    #                 output_perturbed = f(*args_perturbed, **kwargs)
+    #                 loss_perturbed = output_perturbed[0] if isinstance(output, (tuple, list)) else output_perturbed
+    #                 grad[i] = (loss_perturbed - loss) / eps
+    #             grads.append(np.reshape(grad, x.shape))
+    #         if get_output:
+    #             return output, grads
+    #         else:
+    #             return grads
+    #     return gradient
+
+    def linear_solve(self, method: str, lin, y, x0, rtol, atol, max_iter, pre) -> SolveResult:
+        if method in ['direct', 'CG-native', 'GMres', 'biCG', 'biCG-stab', 'CGS', 'lGMres', 'minres', 'QMR', 'GCrotMK'] and max_iter.shape[0] == 1:
+            from phi.math.backend._linalg import scipy_spsolve
+            return scipy_spsolve(self, method, lin, y, x0, rtol, atol, max_iter, pre)
+        return Backend.linear_solve(self, method, lin, y, x0, rtol, atol, max_iter, pre)
+
+    def matrix_solve_least_squares(self, matrix: TensorType, rhs: TensorType) -> TensorType:
+        solution, residuals, rank, singular_values = [], [], [], []
+        for b in range(self.shape(rhs)[0]):
+            solution_b, residual_b, rnk_b, s_b = np.linalg.lstsq(matrix[b], rhs[b], rcond=None)
+            solution.append(solution_b)
+            residuals.append(residual_b)
+            rank.append(rnk_b)
+            singular_values.append(s_b)
+        return np.stack(solution), np.stack(residuals), np.stack(rank), np.stack(singular_values)
+
+    def solve_triangular_dense(self, matrix, rhs, lower: bool, unit_diagonal: bool):
+        batch_size, rows, cols = matrix.shape
+        result = []
+        for b in range(batch_size):
+            x = scipy.linalg.solve_triangular(matrix[b, :, :], rhs[b, :], lower=lower, unit_diagonal=unit_diagonal)
+            result.append(x)
+        return np.stack(result)
+
+    def solve_triangular_sparse(self, matrix, rhs, lower: bool, unit_diagonal: bool):  # needs to be overridden to indicate this is natively implemented
+        return spsolve_triangular(matrix, rhs.T, lower=lower, unit_diagonal=unit_diagonal).T
```

### Comparing `phiflow-2.3.4/phi/math/backend/_profile.py` & `phiflow-2.4.0/phi/math/backend/_profile.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,563 +1,563 @@
-import inspect
-import json
-from contextlib import contextmanager
-from time import perf_counter
-from typing import Optional, Callable, Union
-
-from ._backend import Backend, BACKENDS, _DEFAULT
-
-
-class BackendCall:
-
-    def __init__(self, start: float, stop: float, backend: 'ProfilingBackend', function_name):
-        self._start = start
-        self._stop = stop
-        self._backend = backend
-        self._function_name = function_name
-        self._args = {"Backend": backend.name}
-
-    def __repr__(self):
-        return f"{1000 * self._duration:.2f} ms  {self._function_name}"
-
-    def print(self, include_parents, depth, min_duration, code_col, code_len):
-        if self._duration >= min_duration:
-            print(f"{'  ' * depth}{1000 * self._duration:.2f} ms  {self._backend}.{self._function_name}")
-
-    @property
-    def _name(self):
-        return repr(self)
-
-    @property
-    def _duration(self):
-        return self._stop - self._start
-
-    def trace_json_events(self, include_parents) -> list:
-        backend_index = self._backend._index
-        name = self._function_name
-        return [
-            {
-                'name': name,
-                'ph': 'X',
-                'pid': 1,
-                'tid': backend_index+1,
-                'ts': int(round(self._start * 1000000)),
-                'dur': int(round((self._stop - self._start) * 1000000)),
-                'args': self._args
-            }
-        ]
-
-    def call_count(self) -> int:
-        return 1
-
-    def add_arg(self, key, value):
-        assert key not in self._args
-        self._args[key] = value
-
-
-class ExtCall:
-    """ Function invocation that is not a Backend method but internally calls Backend methods. """
-
-    def __init__(self,
-                 parent: Union['ExtCall', None],
-                 name: str,
-                 level: int,
-                 function: str,
-                 code_context: Union[list, None],
-                 file_name: str,
-                 line_number: int):
-        """
-        Args:
-            parent: Parent call.
-            name: Name of this call, see `ExtCall.determine_name()`.
-            level: Number of parent stack items including this one.
-        """
-        self._parent = parent
-        if parent is None:
-            self._parents = ()
-        else:
-            self._parents = parent._parents + (parent,)
-        self._children = []  # BackendCalls and ExtCalls
-        self._converted = False
-        self._name = name
-        self._level = level
-        self._function = function
-        self._code_context = code_context
-        self._file_name = file_name
-        self._line_number = line_number
-
-    def common_call(self, stack: list):
-        """ Returns the deepest ExtCall in the hierarchy of this call that contains `stack`. """
-        if self._parent is None:
-            return self
-        if len(stack) < self._level:
-            return self._parent.common_call(stack)
-        for i in range(self._level - 1):
-            if self._parents[i+1]._function != stack[-1-i].function:
-                return self._parents[i]
-        return self
-
-    def add(self, child):
-        self._children.append(child)
-
-    @staticmethod
-    def determine_name(info):
-        fun = info.function
-        if 'self' in info.frame.f_locals:
-            if fun == '__init__':
-                return f"{type(info.frame.f_locals['self']).__name__}()"
-            return f"{type(info.frame.f_locals['self']).__name__}.{fun}"
-        if 'phi/math' in info.filename or 'phi\\math' in info.filename:
-            return f"math.{fun}"
-        else:
-            return fun
-
-    @property
-    def _start(self):
-        return self._children[0]._start
-
-    @property
-    def _stop(self):
-        return self._children[-1]._stop
-
-    @property
-    def _duration(self):
-        return sum(c._duration for c in self._children)
-
-    def call_count(self) -> int:
-        return sum(child.call_count() for child in self._children)
-
-    def __repr__(self):
-        if not self._converted:
-            if self._parent is None:
-                return "/"
-            return f"{self._name} ({self._level})"
-        else:
-            context = self._code_context
-            return f"sum {1000 * self._duration:.2f} ms  {context}"
-
-    def __len__(self):
-        return len(self._children)
-
-    def _empty_parent_count(self):
-        for i, parent in enumerate(reversed(self._parents)):
-            if len(parent._children) > 1:
-                return i
-        return len(self._parents)
-
-    def _eff_parent_count(self):
-        return len([p for p in self._parents if len(p._children) > 1])
-
-    def _closest_non_trivial_parent(self):
-        parent = self._parent
-        while parent._parent is not None:
-            if len(parent._children) > 1:
-                return parent
-            parent = parent._parent
-        return parent
-
-    def _calling_code(self, backtrack=0):
-        if self._level > backtrack + 1:
-            call: ExtCall = self._parents[-backtrack-1]
-            return call._code_context[0].strip(), call._file_name, call._function, call._line_number
-        else:
-            return "", "", "", -1
-
-    def print(self, include_parents=(), depth=0, min_duration=0., code_col=80, code_len=50):
-        if self._duration < min_duration:
-            return
-        if len(self._children) == 1 and isinstance(self._children[0], ExtCall):
-            self._children[0].print(include_parents + ((self,) if self._parent is not None else ()), depth, min_duration, code_col, code_len)
-        else:
-            funcs = [par._name for par in include_parents] + [self._name]
-            text = f"{'. ' * depth}-> {' -> '.join(funcs)} ({1000 * self._duration:.2f} ms)"
-            if self._level > len(include_parents)+1:
-                code = self._calling_code(backtrack=len(include_parents))[0]
-                if len(code) > code_len:
-                    code = code[:code_len-3] + "..."
-                text += " " + "." * max(0, (code_col - len(text))) + " > " + code
-            print(text)
-            for child in self._children:
-                child.print((), depth + 1, min_duration, code_col, code_len)
-
-    def children_to_properties(self) -> dict:
-        result = {}
-        for child in self._children:
-            name = f"{len(result)} {child._name}" if len(self._children) <= 10 else f"{len(result):02d} {child._name}"
-            while isinstance(child, ExtCall) and len(child) == 1:
-                child = child._children[0]
-                name += " -> " + child._name
-            result[name] = child
-            if isinstance(child, ExtCall):
-                child.children_to_properties()
-        # finalize
-        for name, child in result.items():
-            setattr(self, name, child)
-        self._converted = True
-        return result
-
-    def trace_json_events(self, include_parents=()) -> list:
-        if len(self._children) == 1:
-            return self._children[0].trace_json_events(include_parents + (self,))
-        else:
-            name = ' -> '.join([par._name for par in include_parents] + [self._name])
-            eff_parent_count = self._eff_parent_count()
-            calling_code, calling_filename, calling_function, lineno = self._calling_code(backtrack=self._empty_parent_count())
-            result = [
-                {
-                    'name': name,
-                    'ph': "X",  # complete event
-                    'pid': 0,
-                    'tid': eff_parent_count,
-                    'ts': int(self._start * 1000000),
-                    'dur': int((self._stop - self._start) * 1000000),
-                    'args': {
-                        "Calling code snippet": calling_code,
-                        "Called by": f"{calling_function}() in {calling_filename}, line {lineno}",
-                        "Active time (backend calls)": f"{self._duration * 1000:.2f} ms ({round(100 * self._duration / self._closest_non_trivial_parent()._duration):.0f}% of parent, {100 * self._duration / (self._stop - self._start):.1f}% efficiency)",
-                        "Backend calls": f"{self.call_count()} ({round(100 * self.call_count() / self._closest_non_trivial_parent().call_count()):.0f}% of parent)"
-                    }
-                }
-            ]
-            for child in self._children:
-                result.extend(child.trace_json_events(()))
-            return result
-
-
-class Profile:
-    """
-    Stores information about calls to backends and their timing.
-
-    Profile may be created through `profile()` or `profile_function()`.
-
-    Profiles can be printed or saved to disc.
-    """
-
-    def __init__(self, trace: bool, backends: Union[tuple, list], subtract_trace_time: bool):
-        self._start = perf_counter()
-        self._stop = None
-        self._root = ExtCall(None, "", 0, "", "", "", -1)
-        self._last_ext_call = self._root
-        self._messages = []
-        self._trace = trace
-        self._backend_calls = []
-        self._retime_index = -1
-        self._accumulating = False
-        self._backends = backends
-        self._subtract_trace_time = subtract_trace_time
-        self._total_trace_time = 0
-
-    def _add_call(self, backend_call: BackendCall, args: tuple, kwargs: dict, result):
-        if self._retime_index >= 0:
-            prev_call = self._backend_calls[self._retime_index]
-            assert prev_call._function_name == backend_call._function_name
-            if self._accumulating:
-                prev_call._start += backend_call._start
-                prev_call._stop += backend_call._stop
-            else:
-                prev_call._start = backend_call._start
-                prev_call._stop = backend_call._stop
-            self._retime_index = (self._retime_index + 1) % len(self._backend_calls)
-        else:
-            self._backend_calls.append(backend_call)
-            args = {i: arg for i, arg in enumerate(args)}
-            args.update(kwargs)
-            backend_call.add_arg("Inputs", _format_values(args, backend_call._backend))
-            if isinstance(result, (tuple, list)):
-                backend_call.add_arg("Outputs", _format_values({i: res for i, res in enumerate(result)}, backend_call._backend))
-            else:
-                backend_call.add_arg("Outputs", _format_values({0: result}, backend_call._backend))
-            if self._trace:
-                stack = inspect.stack()[2:]
-                call = self._last_ext_call.common_call(stack)
-                for i in range(call._level, len(stack)):
-                    stack_frame = stack[len(stack) - i - 1]
-                    name = ExtCall.determine_name(stack_frame)  # if len(stack) - i > 1 else ""
-                    sub_call = ExtCall(call, name, i + 1, stack_frame.function, stack_frame.code_context, stack_frame.filename, stack_frame.lineno)
-                    call.add(sub_call)
-                    call = sub_call
-                call.add(backend_call)
-                self._last_ext_call = call
-            if self._subtract_trace_time:
-                delta_trace_time = perf_counter() - backend_call._stop
-                backend_call._start -= self._total_trace_time
-                backend_call._stop -= self._total_trace_time
-                self._total_trace_time += delta_trace_time
-
-    def _finish(self):
-        self._stop = perf_counter()
-        self._children_to_properties()
-
-    @property
-    def duration(self) -> float:
-        """ Total time passed from creation of the profile to the end of the last operation. """
-        return self._stop - self._start if self._stop is not None else None
-
-    def print(self, min_duration=1e-3, code_col=80, code_len=50):
-        """
-        Prints this profile to the console.
-
-        Args:
-            min_duration: Hides elements with less time spent on backend calls than `min_duration` (seconds)
-            code_col: Formatting option for where the context code is printed.
-            code_len: Formatting option for cropping the context code
-        """
-        print(f"Profile: {self.duration:.4f} seconds total. Skipping elements shorter than {1000 * min_duration:.2f} ms")
-        if self._messages:
-            print("External profiling:")
-            for message in self._messages:
-                print(f"  {message}")
-            print()
-        self._root.print(min_duration=min_duration, code_col=code_col, code_len=code_len)
-
-    def save(self, json_file: str):
-        """
-        Saves this profile to disc using the *trace event format* described at
-        https://docs.google.com/document/d/1CvAClvFfyA5R-PhYUmn5OOQtYMH4h6I0nSsKchNAySU/edit
-
-        This file can be viewed with external applications such as Google chrome.
-
-        Args:
-            json_file: filename
-        """
-        data = [
-            {'name': "process_name", 'ph': 'M', 'pid': 0, 'tid': 0, "args": {"name": "0 Python calls"}},
-            {'name': "process_name", 'ph': 'M', 'pid': 1, 'tid': 1, "args": {"name": "1 Operations"}},
-        ] + [
-            {'name': "thread_name", 'ph': 'M', 'pid': 1, 'tid': i + 1, "args": {"name": backend.name}}
-            for i, backend in enumerate(self._backends)
-        ]
-        if self._trace:
-            if len(self._root._children) > 0:
-                data.extend(self._root.trace_json_events())
-        else:
-            data.extend(sum([call.trace_json_events(()) for call in self._backend_calls], []))
-        with open(json_file, 'w') as file:
-            json.dump(data, file)
-
-    save_trace = save
-
-    def _children_to_properties(self):
-        children = self._root.children_to_properties()
-        for name, child in children.items():
-            setattr(self, name, child)
-
-    def add_external_message(self, message: str):
-        """ Stores an external message in this profile. External messages are printed in `Profile.print()`. """
-        self._messages.append(message)
-
-    @contextmanager
-    def retime(self):
-        """
-        To be used in `with` statements, `with prof.retime(): ...`.
-
-        Updates this profile by running the same operations again but without tracing.
-        This gives a much better indication of the true timing.
-        The code within the `with` block must perform the same operations as the code that created this profile.
-
-        *Warning:* Internal caching may reduce the number of operations after the first time a function is called.
-        To prevent this, run the function before profiling it, see `warmup` in `profile_function()`.
-        """
-        self._retime_index = 0
-        restore_data = _start_profiling(self, self._backends)
-        try:
-            yield None
-        finally:
-            _stop_profiling(self, *restore_data)
-            assert self._retime_index == 0, f"Number of calls during retime did not match original profile, originally {len(self._backend_calls)}, now {self._retime_index}, "
-            self._retime_index = -1
-
-    @contextmanager
-    def _accumulate_average(self, n):
-        self._retime_index = 0
-        self._accumulating = True
-        restore_data = _start_profiling(self, self._backends)
-        try:
-            yield None
-        finally:
-            _stop_profiling(self, *restore_data)
-            assert self._retime_index == 0, f"Number of calls during retime did not match original profile, originally {len(self._backend_calls)}, now {self._retime_index}, "
-            self._retime_index = -1
-            for call in self._backend_calls:
-                call._start /= n
-                call._stop /= n
-            self._accumulating = False
-
-
-def _format_values(values: dict, backend):
-
-    def format_val(value):
-        if isinstance(value, str):
-            return f'"{value}"'
-        if isinstance(value, (int, float, complex, bool)):
-            return value
-        if isinstance(value, (tuple, list)):
-            return str([format_val(v) for v in value])
-        try:
-            shape = backend.shape(value)
-            dtype = backend.dtype(value)
-            try:
-                shape = (int(dim) if dim is not None else '?' for dim in shape)
-            except Exception:
-                pass
-            return f"{tuple(shape)}, {dtype}"
-        except BaseException:
-            return str(value)
-
-    lines = [f"{key}: {format_val(val)}" for key, val in values.items()]
-    return "\n".join(lines)
-
-
-class ProfilingBackend:
-
-    def __init__(self, prof: Profile, backend: Backend, index: int):
-        self._backend = backend
-        self._profile = prof
-        self._index = index
-        # non-profiling methods
-        self.name = backend.name
-        self.combine_types = backend.combine_types
-        self.auto_cast = backend.auto_cast
-        self.is_tensor = backend.is_tensor
-        self.is_available = backend.is_available
-        self.shape = backend.shape
-        self.staticshape = backend.staticshape
-        self.ndims = backend.ndims
-        self.dtype = backend.dtype
-        self.expand_dims = backend.expand_dims
-        self.reshape = backend.reshape
-        self.supports = backend.supports
-        # TODO strided slice does not go through backend atm
-        # profiling methods
-        for item_name in dir(backend):
-            item = getattr(backend, item_name)
-            if callable(item) and not hasattr(self, item_name):
-                def context(item=item, item_name=item_name, profiling_backend=self):
-                    def call_fun(*args, **kwargs):
-                        start = perf_counter()
-                        result = item(*args, **kwargs)
-                        stop = perf_counter()
-                        prof._add_call(BackendCall(start, stop, profiling_backend, item_name), args, kwargs, result)
-                        return result
-                    return call_fun
-                setattr(self, item_name, context())
-
-    def call(self, f: Callable, *args, name=None):
-        start = perf_counter()
-        result = f(*args)
-        self._backend.block_until_ready(result)
-        stop = perf_counter()
-        self._profile._add_call(BackendCall(start, stop, self, name), args, {}, result)
-        return result
-
-    def __repr__(self):
-        return f"profile[{self._backend}]"
-
-    def __enter__(self):
-        _DEFAULT.append(self)
-
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        _DEFAULT.pop(-1)
-
-    def __eq__(self, other):
-        return other is self or other is self._backend
-
-    def __hash__(self):
-        return hash(self._backend)
-
-
-_PROFILE = []
-
-
-@contextmanager
-def profile(backends=None, trace=True, subtract_trace_time=True, save: Union[str, None] = None) -> Profile:
-    """
-    To be used in `with` statements, `with math.backend.profile() as prof: ...`.
-    Creates a `Profile` for the code executed within the context by tracking calls to the `backends` and optionally tracing the call.
-
-    Args:
-        backends: List of backends to profile, `None` to profile all.
-        trace: Whether to perform a full stack trace for each backend call. If true, groups backend calls by function.
-        subtract_trace_time: If True, subtracts the time it took to trace the call stack from the event times
-        save: (Optional) File path to save the profile to. This will call `Profile.save()`.
-
-    Returns:
-        Created `Profile`
-    """
-    backends = BACKENDS if backends is None else backends
-    prof = Profile(trace, backends, subtract_trace_time)
-    restore_data = _start_profiling(prof, backends)
-    try:
-        yield prof
-    finally:
-        _stop_profiling(prof, *restore_data)
-        if save is not None:
-            prof.save(save)
-
-
-def profile_function(fun: Callable,
-                     args: Union[tuple, list] = (),
-                     kwargs: Union[dict, None] = None,
-                     backends=None,
-                     trace=True,
-                     subtract_trace_time=True,
-                     retime=True,
-                     warmup=1,
-                     call_count=1) -> Profile:
-    """
-    Creates a `Profile` for the function `fun(*args, **kwargs)`.
-
-    Args:
-        fun: Function to be profiled. In case `retime=True`, this function must perform the same operations each time it is called.
-            Use `warmup>0` to ensure that internal caching does not interfere with the operations.
-        args: Arguments to be passed to `fun`.
-        kwargs: Keyword arguments to be passed to `fun`.
-        backends: List of backends to profile, `None` to profile all.
-        trace: Whether to perform a full stack trace for each backend call. If true, groups backend calls by function.
-        subtract_trace_time: If True, subtracts the time it took to trace the call stack from the event times. Has no effect if `retime=True`.
-        retime: If true, calls `fun` another time without tracing the calls and updates the profile.
-            This gives a much better indication of the true timing.
-            See `Profile.retime()`.
-        warmup: Number of times to call `fun` before profiling it.
-        call_count: How often to call the function (excluding retime and warmup). The times will be averaged over multiple runs if `call_count > 1`.
-
-    Returns:
-        Created `Profile` for `fun`.
-    """
-    kwargs = kwargs if isinstance(kwargs, dict) else {}
-    for _ in range(warmup):
-        fun(*args, **kwargs)
-    with profile(backends=backends, trace=trace, subtract_trace_time=subtract_trace_time) as prof:
-        fun(*args, **kwargs)
-    if retime:
-        with prof.retime():
-            fun(*args, **kwargs)
-    if call_count > 1:
-        with prof._accumulate_average(call_count):
-            for _ in range(call_count - 1):
-                fun(*args, **kwargs)
-    return prof
-
-
-def _start_profiling(prof: Profile, backends: Union[tuple, list]):
-    _PROFILE.append(prof)
-    original_default = _DEFAULT[-1]
-    original_backends = tuple(BACKENDS)
-    for i, backend in enumerate(backends):
-        prof_backend = ProfilingBackend(prof, backend, i)
-        BACKENDS[BACKENDS.index(backend)] = prof_backend
-        if _DEFAULT[-1] == backend:
-            _DEFAULT[-1] = prof_backend
-    return original_backends, original_default
-
-
-def _stop_profiling(prof: Profile, original_backends, original_default):
-    prof._finish()
-    _PROFILE.pop(-1)
-    BACKENDS.clear()
-    BACKENDS.extend(original_backends)
-    _DEFAULT[-1] = original_default
-
-
-def get_current_profile() -> Optional[Profile]:
-    """ Returns the currently active `Profile` if one is active. Otherwise returns `None`.  """
-    return _PROFILE[-1] if _PROFILE else None
+import inspect
+import json
+from contextlib import contextmanager
+from time import perf_counter
+from typing import Optional, Callable, Union
+
+from ._backend import Backend, BACKENDS, _DEFAULT
+
+
+class BackendCall:
+
+    def __init__(self, start: float, stop: float, backend: 'ProfilingBackend', function_name):
+        self._start = start
+        self._stop = stop
+        self._backend = backend
+        self._function_name = function_name
+        self._args = {"Backend": backend.name}
+
+    def __repr__(self):
+        return f"{1000 * self._duration:.2f} ms  {self._function_name}"
+
+    def print(self, include_parents, depth, min_duration, code_col, code_len):
+        if self._duration >= min_duration:
+            print(f"{'  ' * depth}{1000 * self._duration:.2f} ms  {self._backend}.{self._function_name}")
+
+    @property
+    def _name(self):
+        return repr(self)
+
+    @property
+    def _duration(self):
+        return self._stop - self._start
+
+    def trace_json_events(self, include_parents) -> list:
+        backend_index = self._backend._index
+        name = self._function_name
+        return [
+            {
+                'name': name,
+                'ph': 'X',
+                'pid': 1,
+                'tid': backend_index+1,
+                'ts': int(round(self._start * 1000000)),
+                'dur': int(round((self._stop - self._start) * 1000000)),
+                'args': self._args
+            }
+        ]
+
+    def call_count(self) -> int:
+        return 1
+
+    def add_arg(self, key, value):
+        assert key not in self._args
+        self._args[key] = value
+
+
+class ExtCall:
+    """ Function invocation that is not a Backend method but internally calls Backend methods. """
+
+    def __init__(self,
+                 parent: Union['ExtCall', None],
+                 name: str,
+                 level: int,
+                 function: str,
+                 code_context: Union[list, None],
+                 file_name: str,
+                 line_number: int):
+        """
+        Args:
+            parent: Parent call.
+            name: Name of this call, see `ExtCall.determine_name()`.
+            level: Number of parent stack items including this one.
+        """
+        self._parent = parent
+        if parent is None:
+            self._parents = ()
+        else:
+            self._parents = parent._parents + (parent,)
+        self._children = []  # BackendCalls and ExtCalls
+        self._converted = False
+        self._name = name
+        self._level = level
+        self._function = function
+        self._code_context = code_context
+        self._file_name = file_name
+        self._line_number = line_number
+
+    def common_call(self, stack: list):
+        """ Returns the deepest ExtCall in the hierarchy of this call that contains `stack`. """
+        if self._parent is None:
+            return self
+        if len(stack) < self._level:
+            return self._parent.common_call(stack)
+        for i in range(self._level - 1):
+            if self._parents[i+1]._function != stack[-1-i].function:
+                return self._parents[i]
+        return self
+
+    def add(self, child):
+        self._children.append(child)
+
+    @staticmethod
+    def determine_name(info):
+        fun = info.function
+        if 'self' in info.frame.f_locals:
+            if fun == '__init__':
+                return f"{type(info.frame.f_locals['self']).__name__}()"
+            return f"{type(info.frame.f_locals['self']).__name__}.{fun}"
+        if 'phi/math' in info.filename or 'phi\\math' in info.filename:
+            return f"math.{fun}"
+        else:
+            return fun
+
+    @property
+    def _start(self):
+        return self._children[0]._start
+
+    @property
+    def _stop(self):
+        return self._children[-1]._stop
+
+    @property
+    def _duration(self):
+        return sum(c._duration for c in self._children)
+
+    def call_count(self) -> int:
+        return sum(child.call_count() for child in self._children)
+
+    def __repr__(self):
+        if not self._converted:
+            if self._parent is None:
+                return "/"
+            return f"{self._name} ({self._level})"
+        else:
+            context = self._code_context
+            return f"sum {1000 * self._duration:.2f} ms  {context}"
+
+    def __len__(self):
+        return len(self._children)
+
+    def _empty_parent_count(self):
+        for i, parent in enumerate(reversed(self._parents)):
+            if len(parent._children) > 1:
+                return i
+        return len(self._parents)
+
+    def _eff_parent_count(self):
+        return len([p for p in self._parents if len(p._children) > 1])
+
+    def _closest_non_trivial_parent(self):
+        parent = self._parent
+        while parent._parent is not None:
+            if len(parent._children) > 1:
+                return parent
+            parent = parent._parent
+        return parent
+
+    def _calling_code(self, backtrack=0):
+        if self._level > backtrack + 1:
+            call: ExtCall = self._parents[-backtrack-1]
+            return call._code_context[0].strip(), call._file_name, call._function, call._line_number
+        else:
+            return "", "", "", -1
+
+    def print(self, include_parents=(), depth=0, min_duration=0., code_col=80, code_len=50):
+        if self._duration < min_duration:
+            return
+        if len(self._children) == 1 and isinstance(self._children[0], ExtCall):
+            self._children[0].print(include_parents + ((self,) if self._parent is not None else ()), depth, min_duration, code_col, code_len)
+        else:
+            funcs = [par._name for par in include_parents] + [self._name]
+            text = f"{'. ' * depth}-> {' -> '.join(funcs)} ({1000 * self._duration:.2f} ms)"
+            if self._level > len(include_parents)+1:
+                code = self._calling_code(backtrack=len(include_parents))[0]
+                if len(code) > code_len:
+                    code = code[:code_len-3] + "..."
+                text += " " + "." * max(0, (code_col - len(text))) + " > " + code
+            print(text)
+            for child in self._children:
+                child.print((), depth + 1, min_duration, code_col, code_len)
+
+    def children_to_properties(self) -> dict:
+        result = {}
+        for child in self._children:
+            name = f"{len(result)} {child._name}" if len(self._children) <= 10 else f"{len(result):02d} {child._name}"
+            while isinstance(child, ExtCall) and len(child) == 1:
+                child = child._children[0]
+                name += " -> " + child._name
+            result[name] = child
+            if isinstance(child, ExtCall):
+                child.children_to_properties()
+        # finalize
+        for name, child in result.items():
+            setattr(self, name, child)
+        self._converted = True
+        return result
+
+    def trace_json_events(self, include_parents=()) -> list:
+        if len(self._children) == 1:
+            return self._children[0].trace_json_events(include_parents + (self,))
+        else:
+            name = ' -> '.join([par._name for par in include_parents] + [self._name])
+            eff_parent_count = self._eff_parent_count()
+            calling_code, calling_filename, calling_function, lineno = self._calling_code(backtrack=self._empty_parent_count())
+            result = [
+                {
+                    'name': name,
+                    'ph': "X",  # complete event
+                    'pid': 0,
+                    'tid': eff_parent_count,
+                    'ts': int(self._start * 1000000),
+                    'dur': int((self._stop - self._start) * 1000000),
+                    'args': {
+                        "Calling code snippet": calling_code,
+                        "Called by": f"{calling_function}() in {calling_filename}, line {lineno}",
+                        "Active time (backend calls)": f"{self._duration * 1000:.2f} ms ({round(100 * self._duration / self._closest_non_trivial_parent()._duration):.0f}% of parent, {100 * self._duration / (self._stop - self._start):.1f}% efficiency)",
+                        "Backend calls": f"{self.call_count()} ({round(100 * self.call_count() / self._closest_non_trivial_parent().call_count()):.0f}% of parent)"
+                    }
+                }
+            ]
+            for child in self._children:
+                result.extend(child.trace_json_events(()))
+            return result
+
+
+class Profile:
+    """
+    Stores information about calls to backends and their timing.
+
+    Profile may be created through `profile()` or `profile_function()`.
+
+    Profiles can be printed or saved to disc.
+    """
+
+    def __init__(self, trace: bool, backends: Union[tuple, list], subtract_trace_time: bool):
+        self._start = perf_counter()
+        self._stop = None
+        self._root = ExtCall(None, "", 0, "", "", "", -1)
+        self._last_ext_call = self._root
+        self._messages = []
+        self._trace = trace
+        self._backend_calls = []
+        self._retime_index = -1
+        self._accumulating = False
+        self._backends = backends
+        self._subtract_trace_time = subtract_trace_time
+        self._total_trace_time = 0
+
+    def _add_call(self, backend_call: BackendCall, args: tuple, kwargs: dict, result):
+        if self._retime_index >= 0:
+            prev_call = self._backend_calls[self._retime_index]
+            assert prev_call._function_name == backend_call._function_name
+            if self._accumulating:
+                prev_call._start += backend_call._start
+                prev_call._stop += backend_call._stop
+            else:
+                prev_call._start = backend_call._start
+                prev_call._stop = backend_call._stop
+            self._retime_index = (self._retime_index + 1) % len(self._backend_calls)
+        else:
+            self._backend_calls.append(backend_call)
+            args = {i: arg for i, arg in enumerate(args)}
+            args.update(kwargs)
+            backend_call.add_arg("Inputs", _format_values(args, backend_call._backend))
+            if isinstance(result, (tuple, list)):
+                backend_call.add_arg("Outputs", _format_values({i: res for i, res in enumerate(result)}, backend_call._backend))
+            else:
+                backend_call.add_arg("Outputs", _format_values({0: result}, backend_call._backend))
+            if self._trace:
+                stack = inspect.stack()[2:]
+                call = self._last_ext_call.common_call(stack)
+                for i in range(call._level, len(stack)):
+                    stack_frame = stack[len(stack) - i - 1]
+                    name = ExtCall.determine_name(stack_frame)  # if len(stack) - i > 1 else ""
+                    sub_call = ExtCall(call, name, i + 1, stack_frame.function, stack_frame.code_context, stack_frame.filename, stack_frame.lineno)
+                    call.add(sub_call)
+                    call = sub_call
+                call.add(backend_call)
+                self._last_ext_call = call
+            if self._subtract_trace_time:
+                delta_trace_time = perf_counter() - backend_call._stop
+                backend_call._start -= self._total_trace_time
+                backend_call._stop -= self._total_trace_time
+                self._total_trace_time += delta_trace_time
+
+    def _finish(self):
+        self._stop = perf_counter()
+        self._children_to_properties()
+
+    @property
+    def duration(self) -> float:
+        """ Total time passed from creation of the profile to the end of the last operation. """
+        return self._stop - self._start if self._stop is not None else None
+
+    def print(self, min_duration=1e-3, code_col=80, code_len=50):
+        """
+        Prints this profile to the console.
+
+        Args:
+            min_duration: Hides elements with less time spent on backend calls than `min_duration` (seconds)
+            code_col: Formatting option for where the context code is printed.
+            code_len: Formatting option for cropping the context code
+        """
+        print(f"Profile: {self.duration:.4f} seconds total. Skipping elements shorter than {1000 * min_duration:.2f} ms")
+        if self._messages:
+            print("External profiling:")
+            for message in self._messages:
+                print(f"  {message}")
+            print()
+        self._root.print(min_duration=min_duration, code_col=code_col, code_len=code_len)
+
+    def save(self, json_file: str):
+        """
+        Saves this profile to disc using the *trace event format* described at
+        https://docs.google.com/document/d/1CvAClvFfyA5R-PhYUmn5OOQtYMH4h6I0nSsKchNAySU/edit
+
+        This file can be viewed with external applications such as Google chrome.
+
+        Args:
+            json_file: filename
+        """
+        data = [
+            {'name': "process_name", 'ph': 'M', 'pid': 0, 'tid': 0, "args": {"name": "0 Python calls"}},
+            {'name': "process_name", 'ph': 'M', 'pid': 1, 'tid': 1, "args": {"name": "1 Operations"}},
+        ] + [
+            {'name': "thread_name", 'ph': 'M', 'pid': 1, 'tid': i + 1, "args": {"name": backend.name}}
+            for i, backend in enumerate(self._backends)
+        ]
+        if self._trace:
+            if len(self._root._children) > 0:
+                data.extend(self._root.trace_json_events())
+        else:
+            data.extend(sum([call.trace_json_events(()) for call in self._backend_calls], []))
+        with open(json_file, 'w') as file:
+            json.dump(data, file)
+
+    save_trace = save
+
+    def _children_to_properties(self):
+        children = self._root.children_to_properties()
+        for name, child in children.items():
+            setattr(self, name, child)
+
+    def add_external_message(self, message: str):
+        """ Stores an external message in this profile. External messages are printed in `Profile.print()`. """
+        self._messages.append(message)
+
+    @contextmanager
+    def retime(self):
+        """
+        To be used in `with` statements, `with prof.retime(): ...`.
+
+        Updates this profile by running the same operations again but without tracing.
+        This gives a much better indication of the true timing.
+        The code within the `with` block must perform the same operations as the code that created this profile.
+
+        *Warning:* Internal caching may reduce the number of operations after the first time a function is called.
+        To prevent this, run the function before profiling it, see `warmup` in `profile_function()`.
+        """
+        self._retime_index = 0
+        restore_data = _start_profiling(self, self._backends)
+        try:
+            yield None
+        finally:
+            _stop_profiling(self, *restore_data)
+            assert self._retime_index == 0, f"Number of calls during retime did not match original profile, originally {len(self._backend_calls)}, now {self._retime_index}, "
+            self._retime_index = -1
+
+    @contextmanager
+    def _accumulate_average(self, n):
+        self._retime_index = 0
+        self._accumulating = True
+        restore_data = _start_profiling(self, self._backends)
+        try:
+            yield None
+        finally:
+            _stop_profiling(self, *restore_data)
+            assert self._retime_index == 0, f"Number of calls during retime did not match original profile, originally {len(self._backend_calls)}, now {self._retime_index}, "
+            self._retime_index = -1
+            for call in self._backend_calls:
+                call._start /= n
+                call._stop /= n
+            self._accumulating = False
+
+
+def _format_values(values: dict, backend):
+
+    def format_val(value):
+        if isinstance(value, str):
+            return f'"{value}"'
+        if isinstance(value, (int, float, complex, bool)):
+            return value
+        if isinstance(value, (tuple, list)):
+            return str([format_val(v) for v in value])
+        try:
+            shape = backend.shape(value)
+            dtype = backend.dtype(value)
+            try:
+                shape = (int(dim) if dim is not None else '?' for dim in shape)
+            except Exception:
+                pass
+            return f"{tuple(shape)}, {dtype}"
+        except BaseException:
+            return str(value)
+
+    lines = [f"{key}: {format_val(val)}" for key, val in values.items()]
+    return "\n".join(lines)
+
+
+class ProfilingBackend:
+
+    def __init__(self, prof: Profile, backend: Backend, index: int):
+        self._backend = backend
+        self._profile = prof
+        self._index = index
+        # non-profiling methods
+        self.name = backend.name
+        self.combine_types = backend.combine_types
+        self.auto_cast = backend.auto_cast
+        self.is_tensor = backend.is_tensor
+        self.is_available = backend.is_available
+        self.shape = backend.shape
+        self.staticshape = backend.staticshape
+        self.ndims = backend.ndims
+        self.dtype = backend.dtype
+        self.expand_dims = backend.expand_dims
+        self.reshape = backend.reshape
+        self.supports = backend.supports
+        # TODO strided slice does not go through backend atm
+        # profiling methods
+        for item_name in dir(backend):
+            item = getattr(backend, item_name)
+            if callable(item) and not hasattr(self, item_name):
+                def context(item=item, item_name=item_name, profiling_backend=self):
+                    def call_fun(*args, **kwargs):
+                        start = perf_counter()
+                        result = item(*args, **kwargs)
+                        stop = perf_counter()
+                        prof._add_call(BackendCall(start, stop, profiling_backend, item_name), args, kwargs, result)
+                        return result
+                    return call_fun
+                setattr(self, item_name, context())
+
+    def call(self, f: Callable, *args, name=None):
+        start = perf_counter()
+        result = f(*args)
+        self._backend.block_until_ready(result)
+        stop = perf_counter()
+        self._profile._add_call(BackendCall(start, stop, self, name), args, {}, result)
+        return result
+
+    def __repr__(self):
+        return f"profile[{self._backend}]"
+
+    def __enter__(self):
+        _DEFAULT.append(self)
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        _DEFAULT.pop(-1)
+
+    def __eq__(self, other):
+        return other is self or other is self._backend
+
+    def __hash__(self):
+        return hash(self._backend)
+
+
+_PROFILE = []
+
+
+@contextmanager
+def profile(backends=None, trace=True, subtract_trace_time=True, save: Union[str, None] = None) -> Profile:
+    """
+    To be used in `with` statements, `with math.backend.profile() as prof: ...`.
+    Creates a `Profile` for the code executed within the context by tracking calls to the `backends` and optionally tracing the call.
+
+    Args:
+        backends: List of backends to profile, `None` to profile all.
+        trace: Whether to perform a full stack trace for each backend call. If true, groups backend calls by function.
+        subtract_trace_time: If True, subtracts the time it took to trace the call stack from the event times
+        save: (Optional) File path to save the profile to. This will call `Profile.save()`.
+
+    Returns:
+        Created `Profile`
+    """
+    backends = BACKENDS if backends is None else backends
+    prof = Profile(trace, backends, subtract_trace_time)
+    restore_data = _start_profiling(prof, backends)
+    try:
+        yield prof
+    finally:
+        _stop_profiling(prof, *restore_data)
+        if save is not None:
+            prof.save(save)
+
+
+def profile_function(fun: Callable,
+                     args: Union[tuple, list] = (),
+                     kwargs: Union[dict, None] = None,
+                     backends=None,
+                     trace=True,
+                     subtract_trace_time=True,
+                     retime=True,
+                     warmup=1,
+                     call_count=1) -> Profile:
+    """
+    Creates a `Profile` for the function `fun(*args, **kwargs)`.
+
+    Args:
+        fun: Function to be profiled. In case `retime=True`, this function must perform the same operations each time it is called.
+            Use `warmup>0` to ensure that internal caching does not interfere with the operations.
+        args: Arguments to be passed to `fun`.
+        kwargs: Keyword arguments to be passed to `fun`.
+        backends: List of backends to profile, `None` to profile all.
+        trace: Whether to perform a full stack trace for each backend call. If true, groups backend calls by function.
+        subtract_trace_time: If True, subtracts the time it took to trace the call stack from the event times. Has no effect if `retime=True`.
+        retime: If true, calls `fun` another time without tracing the calls and updates the profile.
+            This gives a much better indication of the true timing.
+            See `Profile.retime()`.
+        warmup: Number of times to call `fun` before profiling it.
+        call_count: How often to call the function (excluding retime and warmup). The times will be averaged over multiple runs if `call_count > 1`.
+
+    Returns:
+        Created `Profile` for `fun`.
+    """
+    kwargs = kwargs if isinstance(kwargs, dict) else {}
+    for _ in range(warmup):
+        fun(*args, **kwargs)
+    with profile(backends=backends, trace=trace, subtract_trace_time=subtract_trace_time) as prof:
+        fun(*args, **kwargs)
+    if retime:
+        with prof.retime():
+            fun(*args, **kwargs)
+    if call_count > 1:
+        with prof._accumulate_average(call_count):
+            for _ in range(call_count - 1):
+                fun(*args, **kwargs)
+    return prof
+
+
+def _start_profiling(prof: Profile, backends: Union[tuple, list]):
+    _PROFILE.append(prof)
+    original_default = _DEFAULT[-1]
+    original_backends = tuple(BACKENDS)
+    for i, backend in enumerate(backends):
+        prof_backend = ProfilingBackend(prof, backend, i)
+        BACKENDS[BACKENDS.index(backend)] = prof_backend
+        if _DEFAULT[-1] == backend:
+            _DEFAULT[-1] = prof_backend
+    return original_backends, original_default
+
+
+def _stop_profiling(prof: Profile, original_backends, original_default):
+    prof._finish()
+    _PROFILE.pop(-1)
+    BACKENDS.clear()
+    BACKENDS.extend(original_backends)
+    _DEFAULT[-1] = original_default
+
+
+def get_current_profile() -> Optional[Profile]:
+    """ Returns the currently active `Profile` if one is active. Otherwise returns `None`.  """
+    return _PROFILE[-1] if _PROFILE else None
```

### Comparing `phiflow-2.3.4/phi/math/extrapolation.py` & `phiflow-2.4.0/phi/math/extrapolation.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,1455 +1,1455 @@
-"""
-Extrapolations are used for padding tensors and sampling coordinates lying outside the tensor bounds.
-Standard extrapolations are listed as global variables in this module.
-
-Extrapolations are an important part of sampled fields such as grids.
-See the documentation at https://tum-pbs.github.io/PhiFlow/Fields.html#extrapolations .
-"""
-import warnings
-from typing import Union, Dict, Callable, Tuple
-
-from phi.math.backend._backend import get_spatial_derivative_order
-from .backend import choose_backend
-from ._shape import Shape, channel, spatial, EMPTY_SHAPE, merge_shapes
-from ._magic_ops import concat, stack, expand
-from ._tensors import Tensor, NativeTensor, CollapsedTensor, TensorStack, wrap
-from . import _ops as math  # TODO this executes _ops.py, can we avoid this?
-
-
-class Extrapolation:
-    """
-    Extrapolations are used to determine values of grids or other structures outside the sampled bounds.
-    They play a vital role in padding and sampling.
-    """
-
-    def __init__(self, pad_rank):
-        """
-        Args:
-            pad_rank: low-ranking extrapolations are handled first during mixed-extrapolation padding.
-                The typical order is periodic=1, boundary=2, symmetric=3, reflect=4, constant=5.
-        """
-        self.pad_rank = pad_rank
-
-    def to_dict(self) -> dict:
-        """
-        Serialize this extrapolation to a dictionary that is serializable (JSON-writable).
-        
-        Use `from_dict()` to restore the Extrapolation object.
-        """
-        raise NotImplementedError()
-
-    def spatial_gradient(self) -> 'Extrapolation':
-        """
-        Returns the extrapolation for the spatial gradient of a tensor/field with this extrapolation.
-
-        Returns:
-            `Extrapolation` or `NotImplemented`
-        """
-        raise NotImplementedError()
-
-    def valid_outer_faces(self, dim) -> Tuple[bool, bool]:
-        """ `(lower: bool, upper: bool)` indicating whether the values sampled at the outer-most faces of a staggered grid with this extrapolation are valid, i.e. need to be stored and are not redundant. """
-        raise NotImplementedError()
-
-    @property
-    def is_flexible(self) -> bool:
-        """
-        Whether the outside values are affected by the inside values.
-        Only `True` if there are actual outside values, i.e. PERIODIC is not flexible.
-
-        This property is important for pressure solves to determine whether the total divergence is fixed or can be adjusted during the solve.
-        """
-        raise NotImplementedError()
-
-    def pad(self, value: Tensor, widths: dict, **kwargs) -> Tensor:
-        """
-        Pads a tensor using values from `self.pad_values()`.
-
-        If `value` is a linear tracer, assume pad_values() to produce constant values, independent of `value`.
-        To change this behavior, override this method.
-
-        Args:
-            value: `Tensor` to be padded
-            widths: `dict` mapping `dim: str -> (lower: int, upper: int)`
-            kwargs: Additional keyword arguments for padding, passed on to `pad_values()`.
-
-        Returns:
-            Padded `Tensor`
-        """
-        from phi.math._trace import ShiftLinTracer
-        if isinstance(value, ShiftLinTracer):
-            lower = {dim: -lo for dim, (lo, _) in widths.items()}
-            return value.shift(lower, new_shape=value.shape.after_pad(widths), val_fun=lambda v: ZERO.pad(v, widths, **kwargs), bias_fun=lambda b: self.pad(b, widths, **kwargs))
-        already_padded = {}
-        for dim, width in widths.items():
-            assert (w > 0 for w in width), "Negative widths not allowed in Extrapolation.pad(). Use math.pad() instead."
-            values = []
-            if width[False] > 0:
-                values.append(self.pad_values(value, width[False], dim, False, already_padded=already_padded, **kwargs))
-            values.append(value)
-            if width[True] > 0:
-                values.append(self.pad_values(value, width[True], dim, True, already_padded=already_padded, **kwargs))
-            value = concat(values, dim)
-            already_padded[dim] = width
-        return value
-
-    def pad_values(self, value: Tensor, width: int, dim: str, upper_edge: bool, **kwargs) -> Tensor:
-        """
-        Determines the values with which the given tensor would be padded at the specified using this extrapolation.
-
-        Args:
-            value: `Tensor` to be padded.
-            width: `int > 0`: Number of cells to pad along `dimension`.
-            dim: Dimension name as `str`.
-            upper_edge: `True` for upper edge, `False` for lower edge.
-
-        Returns:
-            `Tensor` that can be concatenated to `value` along `dimension`
-        """
-        raise NotImplementedError()
-
-    def transform_coordinates(self, coordinates: Tensor, shape: Shape, **kwargs) -> Tensor:
-        """
-        If `self.is_copy_pad`, transforms outside coordinates to the index from which the value is copied.
-        
-        Otherwise, the grid tensor is assumed to hold the correct boundary values for this extrapolation at the edge.
-        Coordinates are then snapped to the valid index range.
-        This is the default implementation.
-
-        Args:
-            coordinates: integer coordinates in index space
-            shape: tensor shape
-
-        Returns:
-            Transformed coordinates
-        """
-        res = shape.spatial[coordinates.shape.get_item_names('vector')] if 'vector' in coordinates.shape and coordinates.shape.get_item_names('vector') else shape.spatial
-        return math.clip(coordinates, 0, math.wrap(res - 1, channel('vector')))
-
-    def is_copy_pad(self, dim: str, upper_edge: bool):
-        """:return: True if all pad values are copies of existing values in the tensor to be padded"""
-        return False
-
-    @property
-    def native_grid_sample_mode(self) -> Union[str, None]:
-        return None
-
-    def shortest_distance(self, start: Tensor, end: Tensor, domain_size: Tensor):
-        """
-        Computes the shortest distance between two points.
-        Both points are assumed to lie within the domain
-
-        Args:
-            start: Start position.
-            end: End position.
-            domain_size: Domain side lengths as vector.
-
-        Returns:
-            Shortest distance from `start` to `end`.
-        """
-        return end - start
-
-    def __getitem__(self, item):
-        return self
-
-    def _getitem_with_domain(self, item: dict, dim: str, upper_edge: bool, all_dims: tuple):
-        return self[item]
-
-    def __abs__(self):
-        raise NotImplementedError(self.__class__)
-
-    def __neg__(self):
-        raise NotImplementedError(self.__class__)
-
-    def __add__(self, other):
-        raise NotImplementedError(self.__class__)
-
-    def __radd__(self, other):
-        raise NotImplementedError(self.__class__)
-
-    def __sub__(self, other):
-        raise NotImplementedError(self.__class__)
-
-    def __rsub__(self, other):
-        raise NotImplementedError(self.__class__)
-
-    def __mul__(self, other):
-        raise NotImplementedError(self.__class__)
-
-    def __rmul__(self, other):
-        raise NotImplementedError(self.__class__)
-
-    def __truediv__(self, other):
-        raise NotImplementedError(self.__class__)
-
-    def __rtruediv__(self, other):
-        raise NotImplementedError(self.__class__)
-
-
-class ConstantExtrapolation(Extrapolation):
-    """
-    Extrapolate with a constant value.
-    """
-
-    def __init__(self, value: Union[Tensor, float]):
-        Extrapolation.__init__(self, 5)
-        self.value = wrap(value)
-        """ Extrapolation value """
-        assert self.value.dtype.kind in (bool, int, float, complex), f"Numeric value required for constant extrapolation but got '{value}'"
-
-    @property
-    def shape(self):
-        return self.value.shape
-
-    def __repr__(self):
-        return repr(self.value)
-
-    def to_dict(self) -> dict:
-        return {'type': 'constant', 'value': self.value.numpy()}
-
-    def __value_attrs__(self):
-        return 'value',
-
-    def __getitem__(self, item):
-        return ConstantExtrapolation(self.value[item])
-
-    @staticmethod
-    def __stack__(values: tuple, dim: Shape, **kwargs) -> 'ConstantExtrapolation':
-        if all(isinstance(v, ConstantExtrapolation) for v in values):
-            return ConstantExtrapolation(stack([v.value for v in values], dim, **kwargs))
-        else:
-            return NotImplemented
-
-    def spatial_gradient(self):
-        return ZERO
-
-    def valid_outer_faces(self, dim) -> Tuple[bool, bool]:
-        return False, False
-
-    @property
-    def is_flexible(self) -> bool:
-        return False
-
-    def pad(self, value: Tensor, widths: dict, **kwargs):
-        """
-        Pads a tensor using CONSTANT values
-
-        Args:
-          value: tensor to be padded
-          widths: name: str -> (lower: int, upper: int)}
-          value: Tensor: 
-          widths: dict: 
-
-        Returns:
-
-        """
-        derivative = get_spatial_derivative_order()
-        pad_value = self.value if derivative == 0 else math.zeros()
-        value = value._simplify()
-        if isinstance(value, NativeTensor):
-            native = value._native
-            ordered_pad_widths = order_by_shape(value.shape, widths, default=(0, 0))
-            backend = choose_backend(native, pad_value.native())
-            for dim in pad_value.shape.non_batch.names:
-                assert dim in value.shape, f"Cannot pad tensor {value.shape} with extrapolation {pad_value.shape} because non-batch dimension '{dim}' is missing."
-            result_tensor = NotImplemented
-            if pad_value.rank == 0:
-                result_tensor = backend.pad(native, ordered_pad_widths, 'constant', pad_value.native())
-            if result_tensor is NotImplemented:
-                return Extrapolation.pad(self, value, widths, **kwargs)
-            return NativeTensor(result_tensor, value.shape.after_pad(widths))
-        elif isinstance(value, CollapsedTensor):
-            if value._inner.shape.volume > 1 or not math.all_available(pad_value, value) or not math.close(pad_value, value._inner):  # .inner should be safe after _simplify
-                return self.pad(value._cache(), widths)
-            else:  # Stays constant value, only extend shape
-                new_sizes = []
-                for size, dim, *_ in value.shape._dimensions:
-                    if dim not in widths:
-                        new_sizes.append(size)
-                    else:
-                        delta = sum(widths[dim]) if isinstance(widths[dim], (tuple, list)) else 2 * widths[dim]
-                        new_sizes.append(size + int(delta))
-                return expand(value._inner, value.shape.after_pad(widths))
-        elif isinstance(value, TensorStack):
-            if not value.requires_broadcast:
-                return self.pad(value._cache(), widths)
-            inner_widths = {dim: w for dim, w in widths.items() if dim != value._stack_dim.name}
-            tensors = [self[{value._stack_dim.name: i}].pad(t, inner_widths) for i, t in enumerate(value.dimension(value._stack_dim.name))]
-            return TensorStack(tensors, value._stack_dim)
-        else:
-            return Extrapolation.pad(self, value, widths, **kwargs)
-
-    def pad_values(self, value: Tensor, width: int, dim: str, upper_edge: bool, **kwargs) -> Tensor:
-        shape = value.shape.after_gather({dim: slice(0, width)})
-        return math.expand(self.value, shape)
-
-    def __eq__(self, other):
-        return isinstance(other, ConstantExtrapolation) and math.close(self.value, other.value)
-
-    def __hash__(self):
-        return hash(self.__class__)
-
-    def is_zero(self):
-        return self == ZERO
-
-    def is_one(self):
-        return self == ONE
-
-    @property
-    def native_grid_sample_mode(self) -> Union[str, None]:
-        return 'zeros' if self.is_zero() else None
-
-    def __add__(self, other):
-        if isinstance(other, ConstantExtrapolation):
-            return ConstantExtrapolation(self.value + other.value)
-        elif self.is_zero():
-            return other
-        else:
-            return NotImplemented
-
-    __radd__ = __add__
-
-    def __sub__(self, other):
-        if isinstance(other, ConstantExtrapolation):
-            return ConstantExtrapolation(self.value - other.value)
-        elif self.is_zero():
-            return -other
-        else:
-            return NotImplemented
-
-    def __rsub__(self, other):
-        if isinstance(other, ConstantExtrapolation):
-            return ConstantExtrapolation(other.value - self.value)
-        elif self.is_zero():
-            return other
-        else:
-            return NotImplemented
-
-    def __mul__(self, other):
-        if isinstance(other, ConstantExtrapolation):
-            return ConstantExtrapolation(self.value * other.value)
-        elif self.is_one():
-            return other
-        elif self.is_zero():
-            return self
-        else:
-            return NotImplemented
-
-    __rmul__ = __mul__
-
-    def __truediv__(self, other):
-        if isinstance(other, ConstantExtrapolation):
-            return ConstantExtrapolation(self.value / other.value)
-        elif self.is_zero():
-            return self
-        else:
-            return NotImplemented
-
-    def __rtruediv__(self, other):
-        if isinstance(other, ConstantExtrapolation):
-            return ConstantExtrapolation(other.value / self.value)
-        elif self.is_one():
-            return other
-        else:
-            return NotImplemented
-
-    def __lt__(self, other):
-        if isinstance(other, ConstantExtrapolation):
-            return ConstantExtrapolation(self.value < other.value)
-        else:
-            return NotImplemented
-
-    def __gt__(self, other):
-        if isinstance(other, ConstantExtrapolation):
-            return ConstantExtrapolation(self.value > other.value)
-        else:
-            return NotImplemented
-
-    def __abs__(self):
-        return ConstantExtrapolation(abs(self.value))
-
-    def __neg__(self):
-        return ConstantExtrapolation(-self.value)
-
-
-class _CopyExtrapolation(Extrapolation):
-
-    @property
-    def shape(self):
-        return EMPTY_SHAPE
-
-    def is_copy_pad(self, dim: str, upper_edge: bool):
-        return True
-
-    def to_dict(self) -> dict:
-        return {'type': repr(self)}
-
-    def __value_attrs__(self):
-        return ()
-
-    def valid_outer_faces(self, dim) -> Tuple[bool, bool]:
-        return True, True
-
-    def pad(self, value: Tensor, widths: dict, **kwargs) -> Tensor:
-        value = value._simplify()
-        from phi.math._trace import ShiftLinTracer
-        if isinstance(value, NativeTensor):
-            native = value._native
-            ordered_pad_widths = order_by_shape(value.shape, widths, default=(0, 0))
-            result_tensor = choose_backend(native).pad(native, ordered_pad_widths, repr(self))
-            if result_tensor is NotImplemented:
-                return Extrapolation.pad(self, value, widths)
-            return NativeTensor(result_tensor, value.shape.after_pad(widths))
-        elif isinstance(value, CollapsedTensor):
-            inner = value._inner  # should be fine after _simplify
-            inner_widths = {dim: w for dim, w in widths.items() if dim in inner.shape}
-            if len(inner_widths) > 0:
-                inner = self.pad(inner, widths)
-            return expand(inner, value.shape.after_pad(widths))
-        elif isinstance(value, TensorStack):
-            if not value.requires_broadcast:
-                return self.pad(value._cache(), widths)
-            inner_widths = {dim: w for dim, w in widths.items() if dim != value._stack_dim.name}
-            tensors = [self.pad(t, inner_widths) for t in value.dimension(value._stack_dim.name)]
-            return TensorStack(tensors, value._stack_dim)
-        elif isinstance(value, ShiftLinTracer):
-            return self._pad_linear_tracer(value, widths)
-        else:
-            raise NotImplementedError(f'{type(value)} not supported')
-
-    def _pad_linear_tracer(self, value, widths: dict):
-        raise NotImplementedError()
-
-    @property
-    def native_grid_sample_mode(self) -> Union[str, None]:
-        return str(self)
-
-    def __eq__(self, other):
-        return type(other) == type(self)
-
-    def __hash__(self):
-        return hash(self.__class__)
-
-    def _op(self, other, op):
-        if type(other) == type(self):
-            return self
-        if isinstance(other, ConstantExtrapolation):  # some operations can be handled by ConstantExtrapolation, e.g. * 0
-            op = getattr(other, op.__name__)
-            return op(self)
-        else:
-            return NotImplemented
-
-    def __abs__(self):
-        return self  # assume also applied to values
-
-    def __neg__(self):
-        return self  # assume also applied to values
-
-    def __add__(self, other):
-        return self._op(other, ConstantExtrapolation.__add__)
-
-    def __radd__(self, other):
-        return self._op(other, ConstantExtrapolation.__add__)
-
-    def __mul__(self, other):
-        return self._op(other, ConstantExtrapolation.__mul__)
-
-    def __rmul__(self, other):
-        return self._op(other, ConstantExtrapolation.__mul__)
-
-    def __sub__(self, other):
-        return self._op(other, ConstantExtrapolation.__rsub__)
-
-    def __rsub__(self, other):
-        return self._op(other, ConstantExtrapolation.__sub__)
-
-    def __truediv__(self, other):
-        return self._op(other, ConstantExtrapolation.__rtruediv__)
-
-    def __rtruediv__(self, other):
-        return self._op(other, ConstantExtrapolation.__truediv__)
-
-    def __lt__(self, other):
-        return self._op(other, ConstantExtrapolation.__gt__)
-
-    def __gt__(self, other):
-        return self._op(other, ConstantExtrapolation.__lt__)
-
-
-class _BoundaryExtrapolation(_CopyExtrapolation):
-    """Uses the closest defined value for points lying outside the defined region."""
-
-    _CACHED_LOWER_MASKS = {}
-    _CACHED_UPPER_MASKS = {}
-
-    def __repr__(self):
-        return 'boundary'
-
-    def spatial_gradient(self):
-        return ZERO
-
-    @property
-    def is_flexible(self) -> bool:
-        return True
-
-    def pad_values(self, value: Tensor, width: int, dim: str, upper_edge: bool, **kwargs) -> Tensor:
-        if upper_edge:
-            edge = value[{dim: slice(-1, None)}]
-        else:
-            edge = value[{dim: slice(1)}]
-        return concat([edge] * width, value.shape[dim])
-
-    def _pad_linear_tracer(self, value: 'ShiftLinTracer', widths: dict) -> 'ShiftLinTracer':
-        """
-        *Warning*:
-        This implementation discards corners, i.e. values that lie outside the original tensor in more than one dimension.
-        These are typically sliced off in differential operators. Corners are instead assigned the value 0.
-        To take corners into account, call pad() for each axis individually. This is inefficient with ShiftLinTracer.
-
-        Args:
-          value: ShiftLinTracer:
-          widths: dict: 
-
-        Returns:
-
-        """
-        lower = {dim: -lo for dim, (lo, _) in widths.items()}
-        result = value.shift(lower, new_shape=value.shape.after_pad(widths), val_fun=lambda v: ZERO.pad(v, widths), bias_fun=lambda b: ZERO.pad(b, widths))  # inner values  ~half the computation time
-        for bound_dim, (bound_lo, bound_hi) in widths.items():
-            for i in range(bound_lo):  # i=0 means outer
-                # this sets corners to 0
-                lower = {dim: -i if dim == bound_dim else -lo for dim, (lo, _) in widths.items()}
-                mask = self._lower_mask(value.shape.only(result.dependent_dims), widths, bound_dim, bound_lo, bound_hi, i)
-                boundary = value.shift(lower, new_shape=result.shape, val_fun=lambda v: self.pad(v, widths) * mask, bias_fun=lambda b: ZERO.pad(b, widths))
-                result += boundary
-            for i in range(bound_hi):
-                lower = {dim: i - lo - hi if dim == bound_dim else -lo for dim, (lo, hi) in widths.items()}
-                mask = self._upper_mask(value.shape.only(result.dependent_dims), widths, bound_dim, bound_lo, bound_hi, i)
-                boundary = value.shift(lower, new_shape=result.shape, val_fun=lambda v: self.pad(v, widths) * mask, bias_fun=lambda b: ZERO.pad(b, widths))  # ~ half the computation time
-                result += boundary  # this does basically nothing if value is the identity
-        return result
-
-    def _lower_mask(self, shape, widths, bound_dim, bound_lo, bound_hi, i):
-        # key = (shape, tuple(widths.keys()), tuple(widths.values()), bound_dim, bound_lo, bound_hi, i)
-        # if key in _BoundaryExtrapolation._CACHED_LOWER_MASKS:
-        #     result = math.tensor(_BoundaryExtrapolation._CACHED_LOWER_MASKS[key])
-        #     _BoundaryExtrapolation._CACHED_LOWER_MASKS[key] = result
-        #     return result
-        # else:
-            mask = ZERO.pad(math.zeros(shape), {bound_dim: (bound_lo - i - 1, 0)})
-            mask = ONE.pad(mask, {bound_dim: (1, 0)})
-            mask = ZERO.pad(mask, {dim: (i, bound_hi) if dim == bound_dim else (lo, hi) for dim, (lo, hi) in widths.items()})
-            # _BoundaryExtrapolation._CACHED_LOWER_MASKS[key] = mask
-            return mask
-
-    def _upper_mask(self, shape, widths, bound_dim, bound_lo, bound_hi, i):
-        # key = (shape, tuple(widths.keys()), tuple(widths.values()), bound_dim, bound_lo, bound_hi, i)
-        # if key in _BoundaryExtrapolation._CACHED_UPPER_MASKS:
-        #     result = math.tensor(_BoundaryExtrapolation._CACHED_UPPER_MASKS[key])
-        #     _BoundaryExtrapolation._CACHED_UPPER_MASKS[key] = result
-        #     return result
-        # else:
-            mask = ZERO.pad(math.zeros(shape), {bound_dim: (0, bound_hi - i - 1)})
-            mask = ONE.pad(mask, {bound_dim: (0, 1)})
-            mask = ZERO.pad(mask, {dim: (bound_lo, i) if dim == bound_dim else (lo, hi) for dim, (lo, hi) in widths.items()})
-            # _BoundaryExtrapolation._CACHED_UPPER_MASKS[key] = mask
-            return mask
-
-
-class _PeriodicExtrapolation(_CopyExtrapolation):
-    """ Periodic extrapolation in n dimensions. """
-    def __repr__(self):
-        return 'periodic'
-
-    def spatial_gradient(self):
-        return self
-
-    def valid_outer_faces(self, dim) -> Tuple[bool, bool]:
-        return True, False
-
-    @property
-    def is_flexible(self) -> bool:
-        return False
-
-    def transform_coordinates(self, coordinates: Tensor, shape: Shape, **kwargs) -> Tensor:
-        return coordinates % shape.spatial
-
-    def pad_values(self, value: Tensor, width: int, dim: str, upper_edge: bool, **kwargs) -> Tensor:
-        if upper_edge:
-            return value[{dim: slice(width)}]
-        else:
-            return value[{dim: slice(-width, None)}]
-
-    def _pad_linear_tracer(self, value: 'ShiftLinTracer', widths: dict) -> 'ShiftLinTracer':
-        if value.shape.get_sizes(tuple(widths.keys())) != value.source.shape.get_sizes(tuple(widths.keys())):
-            raise NotImplementedError("Periodicity does not match input: %s but input has %s. This can happen when padding an already padded or sliced tensor." % (value.shape.only(tuple(widths.keys())), value.source.shape.only(tuple(widths.keys()))))
-        lower = {dim: -lo for dim, (lo, _) in widths.items()}
-        return value.shift(lower, new_shape=value.shape.after_pad(widths), val_fun=lambda v: self.pad(v, widths), bias_fun=lambda b: ZERO.pad(b, widths))
-
-    def shortest_distance(self, start: Tensor, end: Tensor, domain_size: Tensor):
-        dx = end - start
-        return (dx + domain_size / 2) % domain_size - domain_size / 2
-
-
-class _SymmetricExtrapolation(_CopyExtrapolation):
-    """Mirror with the boundary value occurring twice."""
-
-    def __repr__(self):
-        return 'symmetric'
-
-    def spatial_gradient(self):
-        return -self
-
-    @property
-    def is_flexible(self) -> bool:
-        return True
-
-    def transform_coordinates(self, coordinates: Tensor, shape: Shape, **kwargs) -> Tensor:
-        coordinates = coordinates % (2 * shape)
-        return ((2 * shape - 1) - abs((2 * shape - 1) - 2 * coordinates)) // 2
-
-    def pad_values(self, value: Tensor, width: int, dim: str, upper_edge: bool, **kwargs) -> Tensor:
-        if upper_edge:
-            return value[{dim: slice(-width, None)}].flip(dim)
-        else:
-            return value[{dim: slice(0, width)}].flip(dim)
-
-    def _pad_linear_tracer(self, value: 'ShiftLinTracer', widths: dict) -> 'ShiftLinTracer':
-        """
-        *Warning*:
-        This implementation discards corners, i.e. values that lie outside the original tensor in more than one dimension.
-        These are typically sliced off in differential operators. Corners are instead assigned the value 0.
-        To take corners into account, call pad() for each axis individually. This is inefficient with ShiftLinTracer.
-
-        Args:
-          value: ShiftLinTracer:
-          widths: dict:
-
-        Returns:
-
-        """
-        lower = {dim: -lo for dim, (lo, _) in widths.items()}
-        result = value.shift(lower, new_shape=value.shape.after_pad(widths), val_fun=lambda v: ZERO.pad(v, widths), bias_fun=lambda b: ZERO.pad(b, widths))  # inner values  ~half the computation time
-        for bound_dim, (bound_lo, bound_hi) in widths.items():
-            for i in range(bound_lo):  # i=0 means outer
-                # this sets corners to 0
-                lower = {dim: bound_lo-1-2*i if dim == bound_dim else -lo for dim, (lo, _) in widths.items()}
-                mask = self._lower_mask(value.shape.only(result.dependent_dims), widths, bound_dim, bound_lo, bound_hi, i)
-                boundary = value.shift(lower, new_shape=result.shape, val_fun=lambda v: self.pad(v, widths) * mask, bias_fun=lambda b: ZERO.pad(b, widths))
-                result += boundary
-            for i in range(bound_hi):
-                lower = {dim: -(bound_hi-1-2*i) - bound_lo - bound_hi if dim == bound_dim else -lo for dim, (lo, hi) in widths.items()}
-                mask = self._upper_mask(value.shape.only(result.dependent_dims), widths, bound_dim, bound_lo, bound_hi, i)
-                boundary = value.shift(lower, new_shape=result.shape, val_fun=lambda v: self.pad(v, widths) * mask, bias_fun=lambda b: ZERO.pad(b, widths))  # ~ half the computation time
-                result += boundary  # this does basically nothing if value is the identity
-        return result
-
-    def _lower_mask(self, shape, widths, bound_dim, bound_lo, bound_hi, i):
-        # key = (shape, tuple(widths.keys()), tuple(widths.values()), bound_dim, bound_lo, bound_hi, i)
-        # if key in _BoundaryExtrapolation._CACHED_LOWER_MASKS:
-        #     result = math.tensor(_BoundaryExtrapolation._CACHED_LOWER_MASKS[key])
-        #     _BoundaryExtrapolation._CACHED_LOWER_MASKS[key] = result
-        #     return result
-        # else:
-            mask = ZERO.pad(math.zeros(shape), {bound_dim: (bound_lo - i - 1, 0)})
-            mask = ONE.pad(mask, {bound_dim: (1, 0)})
-            mask = ZERO.pad(mask, {dim: (i, bound_hi) if dim == bound_dim else (lo, hi) for dim, (lo, hi) in widths.items()})
-            # _BoundaryExtrapolation._CACHED_LOWER_MASKS[key] = mask
-            return mask
-
-    def _upper_mask(self, shape, widths, bound_dim, bound_lo, bound_hi, i):
-        # key = (shape, tuple(widths.keys()), tuple(widths.values()), bound_dim, bound_lo, bound_hi, i)
-        # if key in _BoundaryExtrapolation._CACHED_UPPER_MASKS:
-        #     result = math.tensor(_BoundaryExtrapolation._CACHED_UPPER_MASKS[key])
-        #     _BoundaryExtrapolation._CACHED_UPPER_MASKS[key] = result
-        #     return result
-        # else:
-            mask = ZERO.pad(math.zeros(shape), {bound_dim: (0, bound_hi - i - 1)})
-            mask = ONE.pad(mask, {bound_dim: (0, 1)})
-            mask = ZERO.pad(mask, {dim: (bound_lo, i) if dim == bound_dim else (lo, hi) for dim, (lo, hi) in widths.items()})
-            # _BoundaryExtrapolation._CACHED_UPPER_MASKS[key] = mask
-            return mask
-
-
-class _AntiSymmetricExtrapolation(_SymmetricExtrapolation):
-    """Like _SymmetricExtrapolation but symmetric counterparts are negated for padding"""
-
-    def __repr__(self):
-        return 'antisymmetric'
-
-    def pad_values(self, *args, **kwargs) -> Tensor:
-        return -super().pad_values(*args, **kwargs)
-
-    def _pad_linear_tracer(self, value: 'ShiftLinTracer', widths: dict) -> 'ShiftLinTracer':
-        """
-        *Warning*:
-        This implementation discards corners, i.e. values that lie outside the original tensor in more than one dimension.
-        These are typically sliced off in differential operators. Corners are instead assigned the value 0.
-        To take corners into account, call pad() for each axis individually. This is inefficient with ShiftLinTracer.
-
-        Args:
-          value: ShiftLinTracer:
-          widths: dict:
-
-        Returns:
-
-        """
-        lower = {dim: -lo for dim, (lo, _) in widths.items()}
-        result = value.shift(lower, new_shape=value.shape.after_pad(widths), val_fun=lambda v: ZERO.pad(v, widths), bias_fun=lambda b: ZERO.pad(b, widths))  # inner values  ~half the computation time
-        for bound_dim, (bound_lo, bound_hi) in widths.items():
-            for i in range(bound_lo):  # i=0 means outer
-                # this sets corners to 0
-                lower = {dim: bound_lo-1-2*i if dim == bound_dim else -lo for dim, (lo, _) in widths.items()}
-                mask = self._lower_mask(value.shape.only(result.dependent_dims), widths, bound_dim, bound_lo, bound_hi, i)
-                boundary = value.shift(lower, new_shape=result.shape, val_fun=lambda v: self.pad(v, widths) * mask, bias_fun=lambda b: ZERO.pad(b, widths))
-                result -= boundary
-            for i in range(bound_hi):
-                lower = {dim: -(bound_hi-1-2*i) - bound_lo - bound_hi if dim == bound_dim else -lo for dim, (lo, hi) in widths.items()}
-                mask = self._upper_mask(value.shape.only(result.dependent_dims), widths, bound_dim, bound_lo, bound_hi, i)
-                boundary = value.shift(lower, new_shape=result.shape, val_fun=lambda v: self.pad(v, widths) * mask, bias_fun=lambda b: ZERO.pad(b, widths))  # ~ half the computation time
-                result -= boundary  # this does basically nothing if value is the identity
-        return result
-
-
-class _ReflectExtrapolation(_CopyExtrapolation):
-    """Mirror of inner elements. The boundary value is not duplicated."""
-
-    def __repr__(self):
-        return 'reflect'
-
-    def spatial_gradient(self):
-        return -self
-
-    @property
-    def is_flexible(self) -> bool:
-        return True
-
-    def pad_values(self, value: Tensor, width: int, dim: str, upper_edge: bool, **kwargs) -> Tensor:
-        if upper_edge:
-            return value[{dim: slice(-1-width, -1)}].flip(dim)
-        else:
-            return value[{dim: slice(1, width+1)}].flip(dim)
-
-    def transform_coordinates(self, coordinates: Tensor, shape: Shape, **kwargs) -> Tensor:
-        coordinates = coordinates % (2 * shape - 2)
-        return (shape - 1) - math.abs_((shape - 1) - coordinates)
-
-    def _pad_linear_tracer(self, value: 'ShiftLinTracer', widths: dict) -> 'ShiftLinTracer':
-        """
-        *Warning*:
-        This implementation discards corners, i.e. values that lie outside the original tensor in more than one dimension.
-        These are typically sliced off in differential operators. Corners are instead assigned the value 0.
-        To take corners into account, call pad() for each axis individually. This is inefficient with ShiftLinTracer.
-
-        Args:
-          value: ShiftLinTracer:
-          widths: dict:
-
-        Returns:
-
-        """
-        lower = {dim: -lo for dim, (lo, _) in widths.items()}
-        result = value.shift(lower, new_shape=value.shape.after_pad(widths), val_fun=lambda v: ZERO.pad(v, widths), bias_fun=lambda b: ZERO.pad(b, widths))  # inner values  ~half the computation time
-        for bound_dim, (bound_lo, bound_hi) in widths.items():
-            for i in range(bound_lo):  # i=0 means outer
-                # this sets corners to 0
-                lower = {dim: bound_lo-2*i if dim == bound_dim else -lo for dim, (lo, _) in widths.items()}
-                mask = self._lower_mask(value.shape.only(result.dependent_dims), widths, bound_dim, bound_lo, bound_hi, i)
-                boundary = value.shift(lower, new_shape=result.shape, val_fun=lambda v: self.pad(v, widths) * mask, bias_fun=lambda b: ZERO.pad(b, widths))
-                result += boundary
-            for i in range(bound_hi):
-                lower = {dim: -(bound_hi-2*i) - bound_lo - bound_hi if dim == bound_dim else -lo for dim, (lo, hi) in widths.items()}
-                mask = self._upper_mask(value.shape.only(result.dependent_dims), widths, bound_dim, bound_lo, bound_hi, i)
-                boundary = value.shift(lower, new_shape=result.shape, val_fun=lambda v: self.pad(v, widths) * mask, bias_fun=lambda b: ZERO.pad(b, widths))  # ~ half the computation time
-                result += boundary  # this does basically nothing if value is the identity
-        return result
-
-    def _lower_mask(self, shape, widths, bound_dim, bound_lo, bound_hi, i):
-        # key = (shape, tuple(widths.keys()), tuple(widths.values()), bound_dim, bound_lo, bound_hi, i)
-        # if key in _BoundaryExtrapolation._CACHED_LOWER_MASKS:
-        #     result = math.tensor(_BoundaryExtrapolation._CACHED_LOWER_MASKS[key])
-        #     _BoundaryExtrapolation._CACHED_LOWER_MASKS[key] = result
-        #     return result
-        # else:
-            mask = ZERO.pad(math.zeros(shape), {bound_dim: (bound_lo - i - 1, 0)})
-            mask = ONE.pad(mask, {bound_dim: (1, 0)})
-            mask = ZERO.pad(mask, {dim: (i, bound_hi) if dim == bound_dim else (lo, hi) for dim, (lo, hi) in widths.items()})
-            # _BoundaryExtrapolation._CACHED_LOWER_MASKS[key] = mask
-            return mask
-
-    def _upper_mask(self, shape, widths, bound_dim, bound_lo, bound_hi, i):
-        # key = (shape, tuple(widths.keys()), tuple(widths.values()), bound_dim, bound_lo, bound_hi, i)
-        # if key in _BoundaryExtrapolation._CACHED_UPPER_MASKS:
-        #     result = math.tensor(_BoundaryExtrapolation._CACHED_UPPER_MASKS[key])
-        #     _BoundaryExtrapolation._CACHED_UPPER_MASKS[key] = result
-        #     return result
-        # else:
-            mask = ZERO.pad(math.zeros(shape), {bound_dim: (0, bound_hi - i - 1)})
-            mask = ONE.pad(mask, {bound_dim: (0, 1)})
-            mask = ZERO.pad(mask, {dim: (bound_lo, i) if dim == bound_dim else (lo, hi) for dim, (lo, hi) in widths.items()})
-            # _BoundaryExtrapolation._CACHED_UPPER_MASKS[key] = mask
-            return mask
-
-
-class _AntiReflectExtrapolation(_ReflectExtrapolation):
-    """Like _ReflectExtrapolation but symmetric counterparts are negated for padding"""
-
-    def __repr__(self):
-        return 'antireflect'
-
-    def pad_values(self, *args, **kwargs) -> Tensor:
-        return -super().pad_values(*args, **kwargs)
-
-    def _pad_linear_tracer(self, value: 'ShiftLinTracer', widths: dict) -> 'ShiftLinTracer':
-        """
-        *Warning*:
-        This implementation discards corners, i.e. values that lie outside the original tensor in more than one dimension.
-        These are typically sliced off in differential operators. Corners are instead assigned the value 0.
-        To take corners into account, call pad() for each axis individually. This is inefficient with ShiftLinTracer.
-
-        Args:
-          value: ShiftLinTracer:
-          widths: dict:
-
-        Returns:
-
-        """
-        lower = {dim: -lo for dim, (lo, _) in widths.items()}
-        result = value.shift(lower, new_shape=value.shape.after_pad(widths), val_fun=lambda v: ZERO.pad(v, widths), bias_fun=lambda b: ZERO.pad(b, widths))  # inner values  ~half the computation time
-        for bound_dim, (bound_lo, bound_hi) in widths.items():
-            for i in range(bound_lo):  # i=0 means outer
-                # this sets corners to 0
-                lower = {dim: bound_lo-2*i if dim == bound_dim else -lo for dim, (lo, _) in widths.items()}
-                mask = self._lower_mask(value.shape.only(result.dependent_dims), widths, bound_dim, bound_lo, bound_hi, i)
-                boundary = value.shift(lower, new_shape=result.shape, val_fun=lambda v: self.pad(v, widths) * mask, bias_fun=lambda b: ZERO.pad(b, widths))
-                result -= boundary
-            for i in range(bound_hi):
-                lower = {dim: -(bound_hi-2*i) - bound_lo - bound_hi if dim == bound_dim else -lo for dim, (lo, hi) in widths.items()}
-                mask = self._upper_mask(value.shape.only(result.dependent_dims), widths, bound_dim, bound_lo, bound_hi, i)
-                boundary = value.shift(lower, new_shape=result.shape, val_fun=lambda v: self.pad(v, widths) * mask, bias_fun=lambda b: ZERO.pad(b, widths))  # ~ half the computation time
-                result -= boundary  # this does basically nothing if value is the identity
-        return result
-
-
-class _SymmetricGradientExtrapolation(Extrapolation):
-
-    def to_dict(self) -> dict:
-        return {'type': 'symmetric-gradient'}
-
-    def spatial_gradient(self) -> 'Extrapolation':
-        raise NotImplementedError
-
-    def valid_outer_faces(self, dim) -> Tuple[bool, bool]:
-        raise NotImplementedError  # probably return True, True but this hasn't been used on grids yet
-
-    @property
-    def is_flexible(self) -> bool:
-        return True
-
-    def pad_values(self, value: Tensor, width: int, dim: str, upper_edge: bool, **kwargs) -> Tensor:
-        anti_s = ANTIREFLECT.pad_values(value, width, dim, upper_edge, **kwargs)
-        edge = value[{dim: -1}] if upper_edge else value[{dim: 0}]
-        return anti_s + 2 * edge
-
-
-class _NoExtrapolation(Extrapolation):  # singleton
-
-    @property
-    def shape(self):
-        return EMPTY_SHAPE
-
-    def to_dict(self) -> dict:
-        return {'type': 'none'}
-
-    def pad(self, value: Tensor, widths: dict, **kwargs) -> Tensor:
-        return value
-
-    def spatial_gradient(self) -> 'Extrapolation':
-        return self
-
-    def valid_outer_faces(self, dim) -> Tuple[bool, bool]:
-        return True, True
-
-    def __value_attrs__(self):
-        return ()
-
-    @property
-    def is_flexible(self) -> bool:
-        raise AssertionError(f"is_flexible not defined by {self.__class__}")
-
-    def pad_values(self, value: Tensor, width: int, dim: str, upper_edge: bool, **kwargs) -> Tensor:
-        return math.zeros(value.shape._replace_single_size(dim, 0))
-
-    def __repr__(self):
-        return "none"
-
-    def __abs__(self):
-        return self
-
-    def __neg__(self):
-        return self
-
-    def __add__(self, other):
-        return self
-
-    def __radd__(self, other):
-        return self
-
-    def __sub__(self, other):
-        return self
-
-    def __rsub__(self, other):
-        return self
-
-    def __mul__(self, other):
-        return self
-
-    def __rmul__(self, other):
-        return self
-
-    def __truediv__(self, other):
-        return self
-
-    def __rtruediv__(self, other):
-        return self
-
-
-class Undefined(Extrapolation):
-    """
-    The extrapolation is unknown and must be replaced before usage.
-    Any access to outside values will raise an AssertionError.
-    """
-
-    def __init__(self, derived_from: Extrapolation):
-        super().__init__(-1)
-        self.derived_from = derived_from
-
-    @property
-    def shape(self):
-        return EMPTY_SHAPE
-
-    def to_dict(self) -> dict:
-        return {'type': 'undefined', 'derived_from': self.derived_from.to_dict()}
-
-    def pad(self, value: Tensor, widths: dict, **kwargs) -> Tensor:
-        for (lo, up) in widths.items():
-            assert lo == 0 and up == 0, "Undefined extrapolation"
-
-    def spatial_gradient(self) -> 'Extrapolation':
-        return self
-
-    def valid_outer_faces(self, dim) -> Tuple[bool, bool]:
-        return self.derived_from.valid_outer_faces(dim)
-
-    @property
-    def is_flexible(self) -> bool:
-        raise AssertionError("Undefined extrapolation")
-
-    def pad_values(self, value: Tensor, width: int, dim: str, upper_edge: bool, **kwargs) -> Tensor:
-        raise AssertionError("Undefined extrapolation")
-
-    def __repr__(self):
-        return "undefined"
-
-    def __abs__(self):
-        return self
-
-    def __neg__(self):
-        return self
-
-    def __add__(self, other):
-        return self
-
-    def __radd__(self, other):
-        return self
-
-    def __sub__(self, other):
-        return self
-
-    def __rsub__(self, other):
-        return self
-
-    def __mul__(self, other):
-        return self
-
-    def __rmul__(self, other):
-        return self
-
-    def __truediv__(self, other):
-        return self
-
-    def __rtruediv__(self, other):
-        return self
-
-
-ZERO = ConstantExtrapolation(0)
-""" Extrapolates with the constant value 0 (Dirichlet boundary condition). """
-ONE = ConstantExtrapolation(1)
-""" Extrapolates with the constant value 1 (Dirichlet boundary condition). """
-PERIODIC = _PeriodicExtrapolation(1)
-""" Extends a grid by tiling it (Periodic boundary condition). """
-ZERO_GRADIENT = _BoundaryExtrapolation(2)
-""" Extends a grid with its edge values (Neumann boundary condition). The value of a point lying outside the grid is determined by the closest grid value(s). """
-BOUNDARY = ZERO_GRADIENT
-# undocumented, use ZERO_GRADIENT instead
-SYMMETRIC = _SymmetricExtrapolation(3)
-""" Extends a grid by tiling it. Every other copy of the grid is flipped. Edge values occur twice per seam. """
-ANTISYMMETRIC = _AntiSymmetricExtrapolation(3)
-""" Like SYMMETRIC but extends a grid with the negative value of the corresponding counterpart instead. """
-REFLECT = _ReflectExtrapolation(4)
-""" Like SYMMETRIC but the edge values are not copied and only occur once per seam. """
-ANTIREFLECT = _AntiReflectExtrapolation(4)
-""" Like REFLECT but extends a grid with the negative value of the corresponding counterpart instead. """
-SYMMETRIC_GRADIENT = _SymmetricGradientExtrapolation(3)
-""" Extrapolates in a continuous manner. The normal component of the spatial gradient is symmetric at the boundaries. The outer-most valid difference is duplicated. """
-
-NONE = _NoExtrapolation(-1)
-""" Raises AssertionError when used to determine outside values. Padding operations will have no effect with this extrapolation. """
-
-
-_PRIMITIVES = {  # used by as_extrapolation() and from_dict()
-    'periodic': PERIODIC,
-    'zero': ZERO,
-    'one': ONE,
-    'zero-gradient': ZERO_GRADIENT,
-    '∇=0': ZERO_GRADIENT,
-    'boundary': ZERO_GRADIENT,  # deprecated
-    'symmetric': SYMMETRIC,
-    'symmetric-gradient': SYMMETRIC_GRADIENT,
-    'antisymmetric': ANTISYMMETRIC,
-    'reflect': REFLECT,
-    'antireflect': ANTISYMMETRIC,
-}
-
-
-def as_extrapolation(obj) -> Extrapolation:
-    """
-    Creates an `Extrapolation` from a descriptor object.
-
-    Args:
-        obj: Extrapolation specification, one of the following:
-
-            * `Extrapolation`
-            * Primitive name as `str`: periodic, zero, one, zero-gradient, symmetric, symmetric-gradient, antisymmetric, reflect, antireflect
-            * `dict` containing exactly the keys `'normal'` and `'tangential'`
-            * `dict` mapping spatial dimension names to extrapolations
-
-    Returns:
-        `Extrapolation`
-    """
-    if isinstance(obj, Extrapolation):
-        return obj
-    if obj is None:
-        return NONE
-    if isinstance(obj, str):
-        assert obj in _PRIMITIVES, f"Unrecognized extrapolation type: '{obj}'"
-        return _PRIMITIVES[obj]
-    if isinstance(obj, dict):
-        if 'normal' in obj or 'tangential' in obj:
-            assert 'normal' in obj and 'tangential' in obj, f"Normal/tangential dict requires both entries 'normal' and 'tangential' but got {obj}"
-            assert len(obj) == 2, f"Normal/tangential dict must only contain entries 'normal' and 'tangential' but got {obj}"
-            normal = as_extrapolation(obj['normal'])
-            tangential = as_extrapolation(obj['tangential'])
-            return combine_by_direction(normal=normal, tangential=tangential)
-        else:
-            ext = {dim: (as_extrapolation(spec[0]), as_extrapolation(spec[1])) if isinstance(spec, tuple) else as_extrapolation(spec) for dim, spec in obj.items()}
-            return combine_sides(**ext)
-    return ConstantExtrapolation(obj)
-
-
-def combine_sides(**extrapolations: Union[Extrapolation, tuple]) -> Extrapolation:
-    """
-    Specify extrapolations for each side / face of a box.
-
-    Args:
-        **extrapolations: map from dim: str -> `Extrapolation` or `tuple` (lower, upper)
-
-    Returns:
-        `Extrapolation`
-    """
-    values = set()
-    proper_dict = {}
-    for dim, ext in extrapolations.items():
-        if isinstance(ext, Extrapolation):
-            values.add(ext)
-            proper_dict[dim] = (ext, ext)
-        elif isinstance(ext, tuple):
-            assert len(ext) == 2, "Tuple must contain exactly two elements, (lower, upper)"
-            lower = as_extrapolation(ext[0])
-            upper = as_extrapolation(ext[1])
-            values.add(lower)
-            values.add(upper)
-            proper_dict[dim] = (lower, upper)
-        else:
-            proper_ext = as_extrapolation(ext)
-            values.add(proper_ext)
-            proper_dict[dim] = (proper_ext, proper_ext)
-    if len(values) == 1:  # All equal -> return any
-        return next(iter(values))
-    else:
-        return _MixedExtrapolation(proper_dict)
-
-
-class _MixedExtrapolation(Extrapolation):
-
-    def __init__(self, extrapolations: Dict[str, Tuple[Extrapolation, Extrapolation]]):
-        """
-        A mixed extrapolation uses different extrapolations for different sides.
-
-        Args:
-          extrapolations: axis: str -> (lower: Extrapolation, upper: Extrapolation) or Extrapolation
-        """
-        super().__init__(pad_rank=None)
-        self.ext = extrapolations
-
-    @property
-    def shape(self):
-        return merge_shapes(*sum(self.ext.values(), ()))
-
-    def to_dict(self) -> dict:
-        return {
-            'type': 'mixed',
-            'dims': {ax: (es[0].to_dict(), es[1].to_dict()) for ax, es in self.ext.items()}
-        }
-
-    def __value_attrs__(self):
-        return 'ext',
-
-    def __eq__(self, other):
-        if isinstance(other, _MixedExtrapolation):
-            return self.ext == other.ext
-        else:
-            simplified = combine_sides(**self.ext)
-            if not isinstance(simplified, _MixedExtrapolation):
-                return simplified == other
-            else:
-                return False
-
-    def __hash__(self):
-        simplified = combine_sides(**self.ext)
-        if not isinstance(simplified, _MixedExtrapolation):
-            return hash(simplified)
-        else:
-            return hash(frozenset(self.ext.items()))
-
-    def __repr__(self):
-        return repr(self.ext)
-
-    def spatial_gradient(self) -> Extrapolation:
-        return combine_sides(**{ax: (es[0].spatial_gradient(), es[1].spatial_gradient()) for ax, es in self.ext.items()})
-
-    def valid_outer_faces(self, dim) -> Tuple[bool, bool]:
-        e_lower, e_upper = self.ext[dim]
-        return e_lower.valid_outer_faces(dim)[0], e_upper.valid_outer_faces(dim)[1]
-
-    def is_copy_pad(self, dim: str, upper_edge: bool):
-        return self.ext[dim][upper_edge].is_copy_pad(dim, upper_edge)
-
-    @property
-    def is_flexible(self) -> bool:
-        result_by_dim = [lo.is_flexible or up.is_flexible for lo, up in self.ext.values()]
-        return any(result_by_dim)
-
-    def pad(self, value: Tensor, widths: dict, **kwargs) -> Tensor:
-        """
-        Pads a tensor using mixed values
-
-        Args:
-          value: tensor to be padded
-          widths: name: str -> (lower: int, upper: int)}
-          value: Tensor: 
-          widths: dict: 
-
-        Returns:
-
-        """
-        extrapolations = set(sum(self.ext.values(), ()))
-        extrapolations = tuple(sorted(extrapolations, key=lambda e: e.pad_rank))
-        for ext in extrapolations:
-            ext_widths = {ax: (l if self.ext[ax][0] == ext else 0, u if self.ext[ax][1] == ext else 0)
-                          for ax, (l, u) in widths.items()}
-            value = ext.pad(value, ext_widths, **kwargs)
-        return value
-
-    def pad_values(self, value: Tensor, width: int, dim: str, upper_edge: bool, **kwargs) -> Tensor:
-        extrap: Extrapolation = self.ext[dim][upper_edge]
-        return extrap.pad_values(value, width, dim, upper_edge, **kwargs)
-
-    def transform_coordinates(self, coordinates: Tensor, shape: Shape, **kwargs) -> Tensor:
-        assert len(self.ext) == len(shape.spatial) == coordinates.vector.size
-        result = []
-        for dim in shape.spatial.unstack():
-            dim_coords = coordinates[[dim.name]]
-            dim_extrapolations = self.ext[dim.name]
-            if dim_extrapolations[0] == dim_extrapolations[1]:
-                result.append(dim_extrapolations[0].transform_coordinates(dim_coords, dim, **kwargs))
-            else:  # separate boundary for lower and upper face
-                lower = dim_extrapolations[0].transform_coordinates(dim_coords, dim, **kwargs)
-                upper = dim_extrapolations[1].transform_coordinates(dim_coords, dim, **kwargs)
-                result.append(math.where(dim_coords <= 0, lower, upper))
-        if 'vector' in result[0].shape:
-            return concat(result, channel('vector'))
-        else:
-            return stack(result, channel('vector'))
-
-    def __getitem__(self, item):
-        if isinstance(item, dict):
-            all_dims = tuple(self.ext.keys())
-            return combine_sides(**{dim: (e1._getitem_with_domain(item, dim, False, all_dims), e2._getitem_with_domain(item, dim, True, all_dims)) for dim, (e1, e2) in self.ext.items()})
-        else:
-            dim, face = item
-            return self.ext[dim][face]
-
-    def __add__(self, other):
-        return self._op2(other, lambda e1, e2: e1 + e2)
-
-    def __radd__(self, other):
-        return self._op2(other, lambda e1, e2: e2 + e1)
-
-    def __sub__(self, other):
-        return self._op2(other, lambda e1, e2: e1 - e2)
-
-    def __rsub__(self, other):
-        return self._op2(other, lambda e1, e2: e2 - e1)
-
-    def __mul__(self, other):
-        return self._op2(other, lambda e1, e2: e1 * e2)
-
-    def __rmul__(self, other):
-        return self._op2(other, lambda e1, e2: e2 * e1)
-
-    def __truediv__(self, other):
-        return self._op2(other, lambda e1, e2: e1 / e2)
-
-    def __rtruediv__(self, other):
-        return self._op2(other, lambda e1, e2: e2 / e1)
-
-    def _op2(self, other, operator):
-        if isinstance(other, _MixedExtrapolation):
-            assert self.ext.keys() == other.ext.keys()
-            return combine_sides(**{ax: (operator(lo, other.ext[ax][False]), operator(hi, other.ext[ax][True])) for ax, (lo, hi) in self.ext.items()})
-        else:
-            return combine_sides(**{ax: (operator(lo, other), operator(hi, other)) for ax, (lo, hi) in self.ext.items()})
-
-    def __abs__(self):
-        return combine_sides(**{ax: (abs(lo), abs(up)) for ax, (lo, up) in self.ext.items()})
-
-    def __neg__(self):
-        return combine_sides(**{ax: (-lo, -up) for ax, (lo, up) in self.ext.items()})
-
-
-class _NormalTangentialExtrapolation(Extrapolation):
-
-    def __init__(self, normal: Extrapolation, tangential: Extrapolation):
-        super().__init__(pad_rank=min(normal.pad_rank, tangential.pad_rank))
-        self.normal = normal
-        self.tangential = tangential
-
-    @property
-    def shape(self):
-        return merge_shapes(self.normal, self.tangential)
-
-    def to_dict(self) -> dict:
-        return {
-            'type': 'normal-tangential',
-            'normal': self.normal.to_dict(),
-            'tangential': self.tangential.to_dict(),
-        }
-
-    def __value_attrs__(self):
-        return 'normal', 'tangential'
-
-    def __repr__(self):
-        return f"normal={self.normal}, tangential={self.tangential}"
-
-    def spatial_gradient(self) -> 'Extrapolation':
-        return combine_by_direction(self.normal.spatial_gradient(), self.tangential.spatial_gradient())
-
-    def valid_outer_faces(self, dim) -> Tuple[bool, bool]:
-        return self.normal.valid_outer_faces(dim)
-
-    def is_copy_pad(self, dim: str, upper_edge: bool):
-        return False  # normal and tangential might copy from different places, so no.
-
-    @property
-    def is_flexible(self) -> bool:
-        return self.normal.is_flexible
-
-    def pad_values(self, value: Tensor, width: int, dim: str, upper_edge: bool, **kwargs) -> Tensor:
-        if 'vector' not in value.shape:
-            warnings.warn(f'{self} adding a vector dimension to tensor {value.shape}')
-            from phi.math import expand
-            value = expand(value, channel(vector=spatial(value).names))
-        assert value.vector.item_names is not None, "item_names must be present when padding with normal-tangential"
-        result = []
-        for component_name, component in zip(value.vector.item_names, value.vector):
-            ext = self.normal if component_name == dim else self.tangential
-            result.append(ext.pad_values(component, width, dim, upper_edge, **kwargs))
-        from ._magic_ops import stack
-        result = stack(result, value.shape.only('vector'))
-        return result
-
-    def _getitem_with_domain(self, item: dict, dim: str, upper_edge: bool, all_dims: tuple):
-        if 'vector' not in item:
-            return self
-        component = item['vector']
-        assert isinstance(component, str), f"Selecting a component of normal/tangential must be done by dimension name but got {component}"
-        if component == dim:
-            return self.normal
-        else:
-            return self.tangential
-
-    def __eq__(self, other):
-        return isinstance(other, _NormalTangentialExtrapolation) and self.normal == other.normal and self.tangential == other.tangential
-
-    def __hash__(self):
-        return hash(self.normal) + hash(self.tangential)
-
-    def __add__(self, other):
-        return self._op2(other, lambda e1, e2: e1 + e2)
-
-    def __radd__(self, other):
-        return self._op2(other, lambda e1, e2: e2 + e1)
-
-    def __sub__(self, other):
-        return self._op2(other, lambda e1, e2: e1 - e2)
-
-    def __rsub__(self, other):
-        return self._op2(other, lambda e1, e2: e2 - e1)
-
-    def __mul__(self, other):
-        return self._op2(other, lambda e1, e2: e1 * e2)
-
-    def __rmul__(self, other):
-        return self._op2(other, lambda e1, e2: e2 * e1)
-
-    def __truediv__(self, other):
-        return self._op2(other, lambda e1, e2: e1 / e2)
-
-    def __rtruediv__(self, other):
-        return self._op2(other, lambda e1, e2: e2 / e1)
-
-    def _op2(self, other, operator):
-        if isinstance(other, _NormalTangentialExtrapolation):
-            return combine_by_direction(normal=operator(self.normal, other.normal), tangential=operator(self.tangential, other.tangential))
-        else:
-            return combine_by_direction(normal=operator(self.normal, other), tangential=operator(self.tangential, other))
-
-    def __abs__(self):
-        return combine_by_direction(normal=abs(self.normal), tangential=abs(self.tangential))
-
-    def __neg__(self):
-        return combine_by_direction(normal=-self.normal, tangential=-self.tangential)
-
-
-def combine_by_direction(normal: Union[Extrapolation, float, Tensor], tangential: Union[Extrapolation, float, Tensor]) -> Extrapolation:
-    """
-    Use a different extrapolation for the normal component of vector-valued tensors.
-
-    Args:
-        normal: Extrapolation for the component that is orthogonal to the boundary.
-        tangential: Extrapolation for the component that is tangential to the boundary.
-
-    Returns:
-        `Extrapolation`
-    """
-    normal = as_extrapolation(normal)
-    tangential = as_extrapolation(tangential)
-    return normal if normal == tangential else _NormalTangentialExtrapolation(normal, tangential)
-
-
-def from_dict(dictionary: dict) -> Extrapolation:
-    """
-    Loads an `Extrapolation` object from a dictionary that was created using `Extrapolation.to_dict()`.
-
-    Args:
-        dictionary: serializable dictionary holding all extrapolation properties
-
-    Returns:
-        Loaded extrapolation
-    """
-    etype = dictionary['type']
-    if etype in _PRIMITIVES:
-        return _PRIMITIVES[etype]
-    elif etype == 'constant':
-        return ConstantExtrapolation(dictionary['value'])
-    elif etype == 'mixed':
-        dims: Dict[str, tuple] = dictionary['dims']
-        extrapolations = {dim: (from_dict(lo_up[0]), from_dict(lo_up[1])) for dim, lo_up in dims.items()}
-        return _MixedExtrapolation(extrapolations)
-    elif etype == 'normal-tangential':
-        normal = from_dict(dictionary['normal'])
-        tangential = from_dict(dictionary['tangential'])
-        return _NormalTangentialExtrapolation(normal, tangential)
-    elif etype == 'none':
-        return NONE
-    elif etype == 'undefined':
-        derived_from = from_dict(dictionary['derived_from'])
-        return Undefined(derived_from)
-    else:
-        raise ValueError(dictionary)
-
-
-def order_by_shape(shape: Shape, sequence, default=None) -> Union[tuple, list]:
-    """
-    If sequence is a dict with dimension names as keys, orders its values according to this shape.
-
-    Otherwise, the sequence is returned unchanged.
-
-    Args:
-      sequence: Sequence or dict to be ordered
-      default: default value used for dimensions not contained in sequence
-
-    Returns:
-      ordered sequence of values
-    """
-    if isinstance(sequence, dict):
-        result = [sequence.get(name, default) for name in shape.names]
-        return result
-    elif isinstance(sequence, (tuple, list)):
-        assert len(sequence) == shape.rank
-        return sequence
-    else:  # just a constant
-        return sequence
-
-
-def map(f: Callable[[Extrapolation], Extrapolation], extrapolation):
-    """
-    Applies a function to all leaf extrapolations in `extrapolation`.
-    Non-leaves are those created by `combine_sides()` and `combine_by_direction()`.
-
-    The tree will be collapsed if possible.
-
-    Args:
-        f: Function mapping a leaf `Extrapolation` to another `Extrapolation`.
-        extrapolation: Input tree for `f`.
-
-    Returns:
-        `Extrapolation`
-    """
-    if isinstance(extrapolation, _MixedExtrapolation):
-        return combine_sides(**{dim: (map(f, lo), map(f, up)) for dim, (lo, up) in extrapolation.ext.items()})
-    elif isinstance(extrapolation, _NormalTangentialExtrapolation):
-        return combine_by_direction(map(f, extrapolation.normal), map(f, extrapolation.tangential))
-    else:
-        return f(extrapolation)
-
-
-def remove_constant_offset(extrapolation):
-    """
-    Removes all constant offsets from an extrapolation.
-    This also includes `NaN` values in constants (unlike `ext - ext`).
-
-    Args:
-        extrapolation: `Extrapolation` object.
-
-    Returns:
-        `Extrapolation` that has no constant offsets
-    """
-    def const_to_zero(extrapolation):
-        if isinstance(extrapolation, ConstantExtrapolation):
-            return ZERO
-        else:
-            return extrapolation
-    return map(const_to_zero, extrapolation)
-
+"""
+Extrapolations are used for padding tensors and sampling coordinates lying outside the tensor bounds.
+Standard extrapolations are listed as global variables in this module.
+
+Extrapolations are an important part of sampled fields such as grids.
+See the documentation at https://tum-pbs.github.io/PhiFlow/Fields.html#extrapolations .
+"""
+import warnings
+from typing import Union, Dict, Callable, Tuple
+
+from phi.math.backend._backend import get_spatial_derivative_order
+from .backend import choose_backend
+from ._shape import Shape, channel, spatial, EMPTY_SHAPE, merge_shapes
+from ._magic_ops import concat, stack, expand
+from ._tensors import Tensor, NativeTensor, TensorStack, wrap
+from . import _ops as math  # TODO this executes _ops.py, can we avoid this?
+
+
+class Extrapolation:
+    """
+    Extrapolations are used to determine values of grids or other structures outside the sampled bounds.
+    They play a vital role in padding and sampling.
+    """
+
+    def __init__(self, pad_rank):
+        """
+        Args:
+            pad_rank: low-ranking extrapolations are handled first during mixed-extrapolation padding.
+                The typical order is periodic=1, boundary=2, symmetric=3, reflect=4, constant=5.
+        """
+        self.pad_rank = pad_rank
+
+    def to_dict(self) -> dict:
+        """
+        Serialize this extrapolation to a dictionary that is serializable (JSON-writable).
+        
+        Use `from_dict()` to restore the Extrapolation object.
+        """
+        raise NotImplementedError()
+
+    def spatial_gradient(self) -> 'Extrapolation':
+        """
+        Returns the extrapolation for the spatial gradient of a tensor/field with this extrapolation.
+
+        Returns:
+            `Extrapolation` or `NotImplemented`
+        """
+        raise NotImplementedError()
+
+    def valid_outer_faces(self, dim) -> Tuple[bool, bool]:
+        """ `(lower: bool, upper: bool)` indicating whether the values sampled at the outer-most faces of a staggered grid with this extrapolation are valid, i.e. need to be stored and are not redundant. """
+        raise NotImplementedError()
+
+    @property
+    def is_flexible(self) -> bool:
+        """
+        Whether the outside values are affected by the inside values.
+        Only `True` if there are actual outside values, i.e. PERIODIC is not flexible.
+
+        This property is important for pressure solves to determine whether the total divergence is fixed or can be adjusted during the solve.
+        """
+        raise NotImplementedError()
+
+    def pad(self, value: Tensor, widths: dict, **kwargs) -> Tensor:
+        """
+        Pads a tensor using values from `self.pad_values()`.
+
+        If `value` is a linear tracer, assume pad_values() to produce constant values, independent of `value`.
+        To change this behavior, override this method.
+
+        Args:
+            value: `Tensor` to be padded
+            widths: `dict` mapping `dim: str -> (lower: int, upper: int)`
+            kwargs: Additional keyword arguments for padding, passed on to `pad_values()`.
+
+        Returns:
+            Padded `Tensor`
+        """
+        from phi.math._trace import ShiftLinTracer
+        if isinstance(value, ShiftLinTracer):
+            lower = {dim: -lo for dim, (lo, _) in widths.items()}
+            return value.shift(lower, new_shape=value.shape.after_pad(widths), val_fun=lambda v: ZERO.pad(v, widths, **kwargs), bias_fun=lambda b: self.pad(b, widths, **kwargs))
+        already_padded = {}
+        for dim, width in widths.items():
+            assert (w > 0 for w in width), "Negative widths not allowed in Extrapolation.pad(). Use math.pad() instead."
+            values = []
+            if width[False] > 0:
+                values.append(self.pad_values(value, width[False], dim, False, already_padded=already_padded, **kwargs))
+            values.append(value)
+            if width[True] > 0:
+                values.append(self.pad_values(value, width[True], dim, True, already_padded=already_padded, **kwargs))
+            value = concat(values, dim)
+            already_padded[dim] = width
+        return value
+
+    def pad_values(self, value: Tensor, width: int, dim: str, upper_edge: bool, **kwargs) -> Tensor:
+        """
+        Determines the values with which the given tensor would be padded at the specified using this extrapolation.
+
+        Args:
+            value: `Tensor` to be padded.
+            width: `int > 0`: Number of cells to pad along `dimension`.
+            dim: Dimension name as `str`.
+            upper_edge: `True` for upper edge, `False` for lower edge.
+
+        Returns:
+            `Tensor` that can be concatenated to `value` along `dimension`
+        """
+        raise NotImplementedError()
+
+    def transform_coordinates(self, coordinates: Tensor, shape: Shape, **kwargs) -> Tensor:
+        """
+        If `self.is_copy_pad`, transforms outside coordinates to the index from which the value is copied.
+        
+        Otherwise, the grid tensor is assumed to hold the correct boundary values for this extrapolation at the edge.
+        Coordinates are then snapped to the valid index range.
+        This is the default implementation.
+
+        Args:
+            coordinates: integer coordinates in index space
+            shape: tensor shape
+
+        Returns:
+            Transformed coordinates
+        """
+        res = shape.spatial[coordinates.shape.get_item_names('vector')] if 'vector' in coordinates.shape and coordinates.shape.get_item_names('vector') else shape.spatial
+        return math.clip(coordinates, 0, math.wrap(res - 1, channel('vector')))
+
+    def is_copy_pad(self, dim: str, upper_edge: bool):
+        """:return: True if all pad values are copies of existing values in the tensor to be padded"""
+        return False
+
+    @property
+    def native_grid_sample_mode(self) -> Union[str, None]:
+        return None
+
+    def shortest_distance(self, start: Tensor, end: Tensor, domain_size: Tensor):
+        """
+        Computes the shortest distance between two points.
+        Both points are assumed to lie within the domain
+
+        Args:
+            start: Start position.
+            end: End position.
+            domain_size: Domain side lengths as vector.
+
+        Returns:
+            Shortest distance from `start` to `end`.
+        """
+        return end - start
+
+    def __getitem__(self, item):
+        return self
+
+    def _getitem_with_domain(self, item: dict, dim: str, upper_edge: bool, all_dims: tuple):
+        return self[item]
+
+    def __abs__(self):
+        raise NotImplementedError(self.__class__)
+
+    def __neg__(self):
+        raise NotImplementedError(self.__class__)
+
+    def __add__(self, other):
+        raise NotImplementedError(self.__class__)
+
+    def __radd__(self, other):
+        raise NotImplementedError(self.__class__)
+
+    def __sub__(self, other):
+        raise NotImplementedError(self.__class__)
+
+    def __rsub__(self, other):
+        raise NotImplementedError(self.__class__)
+
+    def __mul__(self, other):
+        raise NotImplementedError(self.__class__)
+
+    def __rmul__(self, other):
+        raise NotImplementedError(self.__class__)
+
+    def __truediv__(self, other):
+        raise NotImplementedError(self.__class__)
+
+    def __rtruediv__(self, other):
+        raise NotImplementedError(self.__class__)
+
+
+class ConstantExtrapolation(Extrapolation):
+    """
+    Extrapolate with a constant value.
+    """
+
+    def __init__(self, value: Union[Tensor, float]):
+        Extrapolation.__init__(self, 5)
+        self.value = wrap(value)
+        """ Extrapolation value """
+        assert self.value.dtype.kind in (bool, int, float, complex), f"Numeric value required for constant extrapolation but got '{value}'"
+
+    @property
+    def shape(self):
+        return self.value.shape
+
+    def __repr__(self):
+        return repr(self.value)
+
+    def to_dict(self) -> dict:
+        return {'type': 'constant', 'value': self.value.numpy()}
+
+    def __value_attrs__(self):
+        return 'value',
+
+    def __getitem__(self, item):
+        return ConstantExtrapolation(self.value[item])
+
+    @staticmethod
+    def __stack__(values: tuple, dim: Shape, **kwargs) -> 'ConstantExtrapolation':
+        if all(isinstance(v, ConstantExtrapolation) for v in values):
+            return ConstantExtrapolation(stack([v.value for v in values], dim, **kwargs))
+        else:
+            return NotImplemented
+
+    def spatial_gradient(self):
+        return ZERO
+
+    def valid_outer_faces(self, dim) -> Tuple[bool, bool]:
+        return False, False
+
+    @property
+    def is_flexible(self) -> bool:
+        return False
+
+    def pad(self, value: Tensor, widths: dict, **kwargs):
+        """
+        Pads a tensor using constant values.
+
+        Args:
+          value: `Tensor` to be padded
+          widths: name: str -> (lower: int, upper: int)}
+
+        Returns:
+            Padded `Tensor`
+        """
+        derivative = get_spatial_derivative_order()
+        pad_value = self.value if derivative == 0 else math.wrap(0)
+        value = value._simplify()
+        if isinstance(value, NativeTensor):
+            backend = choose_backend(value._native, pad_value.native())
+            for dim in pad_value.shape.non_batch.names:
+                assert dim in value.shape, f"Cannot pad tensor {value.shape} with extrapolation {pad_value.shape} because non-batch dimension '{dim}' is missing."
+            if pad_value.rank == 0:
+                equal_values = math.all_available(self.value, value) and value._native_shape in self.value.shape and (self.value == value).all
+                if not equal_values:
+                    required_dims = value._shape.only(tuple(widths.keys()))
+                    value = value._cached(required_dims)
+                should_pad_native = any(dim in value._native_shape for dim in widths)
+                if should_pad_native:
+                    ordered_pad_widths = order_by_shape(value._native_shape, widths, default=(0, 0))
+                    result_native = backend.pad(value._native, ordered_pad_widths, 'constant', pad_value.native())
+                else:
+                    result_native = value._native
+                if result_native is not NotImplemented:
+                    return NativeTensor(result_native, value._native_shape.after_pad(widths), value._shape.after_pad(widths))
+            return Extrapolation.pad(self, value, widths, **kwargs)
+        elif isinstance(value, TensorStack):
+            if not value.requires_broadcast:
+                return self.pad(value._cache(), widths)
+            inner_widths = {dim: w for dim, w in widths.items() if dim != value._stack_dim.name}
+            tensors = [self[{value._stack_dim.name: i}].pad(t, inner_widths) for i, t in enumerate(value.dimension(value._stack_dim.name))]
+            return TensorStack(tensors, value._stack_dim)
+        else:
+            return Extrapolation.pad(self, value, widths, **kwargs)
+
+    def pad_values(self, value: Tensor, width: int, dim: str, upper_edge: bool, **kwargs) -> Tensor:
+        shape = value.shape.after_gather({dim: slice(0, width)})
+        return math.expand(self.value, shape)
+
+    def __eq__(self, other):
+        return isinstance(other, ConstantExtrapolation) and math.close(self.value, other.value)
+
+    def __hash__(self):
+        return hash(self.__class__)
+
+    def is_zero(self):
+        return self == ZERO
+
+    def is_one(self):
+        return self == ONE
+
+    @property
+    def native_grid_sample_mode(self) -> Union[str, None]:
+        return 'zeros' if self.is_zero() else None
+
+    def __add__(self, other):
+        if isinstance(other, ConstantExtrapolation):
+            return ConstantExtrapolation(self.value + other.value)
+        elif self.is_zero():
+            return other
+        else:
+            return NotImplemented
+
+    __radd__ = __add__
+
+    def __sub__(self, other):
+        if isinstance(other, ConstantExtrapolation):
+            return ConstantExtrapolation(self.value - other.value)
+        elif self.is_zero():
+            return -other
+        else:
+            return NotImplemented
+
+    def __rsub__(self, other):
+        if isinstance(other, ConstantExtrapolation):
+            return ConstantExtrapolation(other.value - self.value)
+        elif self.is_zero():
+            return other
+        else:
+            return NotImplemented
+
+    def __mul__(self, other):
+        if isinstance(other, ConstantExtrapolation):
+            return ConstantExtrapolation(self.value * other.value)
+        elif self.is_one():
+            return other
+        elif self.is_zero():
+            return self
+        else:
+            return NotImplemented
+
+    __rmul__ = __mul__
+
+    def __truediv__(self, other):
+        if isinstance(other, ConstantExtrapolation):
+            return ConstantExtrapolation(self.value / other.value)
+        elif self.is_zero():
+            return self
+        else:
+            return NotImplemented
+
+    def __rtruediv__(self, other):
+        if isinstance(other, ConstantExtrapolation):
+            return ConstantExtrapolation(other.value / self.value)
+        elif self.is_one():
+            return other
+        else:
+            return NotImplemented
+
+    def __lt__(self, other):
+        if isinstance(other, ConstantExtrapolation):
+            return ConstantExtrapolation(self.value < other.value)
+        else:
+            return NotImplemented
+
+    def __gt__(self, other):
+        if isinstance(other, ConstantExtrapolation):
+            return ConstantExtrapolation(self.value > other.value)
+        else:
+            return NotImplemented
+
+    def __abs__(self):
+        return ConstantExtrapolation(abs(self.value))
+
+    def __neg__(self):
+        return ConstantExtrapolation(-self.value)
+
+
+class _CopyExtrapolation(Extrapolation):
+
+    @property
+    def shape(self):
+        return EMPTY_SHAPE
+
+    def is_copy_pad(self, dim: str, upper_edge: bool):
+        return True
+
+    def to_dict(self) -> dict:
+        return {'type': repr(self)}
+
+    def __value_attrs__(self):
+        return ()
+
+    def valid_outer_faces(self, dim) -> Tuple[bool, bool]:
+        return True, True
+
+    @property
+    def _is_dim_separable(self):
+        """
+        If `True`, the extrapolation values only depend on values of the same row/column.
+        If `False`, collapsed dimensions have to be expanded during padding.
+        """
+        return True
+
+    def pad(self, value: Tensor, widths: dict, **kwargs) -> Tensor:
+        value = value._simplify()
+        from phi.math._trace import ShiftLinTracer
+        if isinstance(value, NativeTensor):
+            if not self._is_dim_separable:
+                required_dims = value._shape.only(tuple(widths.keys()))
+                value = value._cached(required_dims)
+            should_pad_native = any(dim in value._native_shape for dim in widths)
+            if should_pad_native:
+                ordered_pad_widths = order_by_shape(value._native_shape, widths, default=(0, 0))
+                result_native = value.default_backend.pad(value._native, ordered_pad_widths, repr(self))
+            else:
+                result_native = value._native
+            if result_native is not NotImplemented:
+                return NativeTensor(result_native, value._native_shape.after_pad(widths), value._shape.after_pad(widths))
+            return Extrapolation.pad(self, value, widths)
+        elif isinstance(value, TensorStack):
+            if not value.requires_broadcast:
+                return self.pad(value._cache(), widths)
+            inner_widths = {dim: w for dim, w in widths.items() if dim != value._stack_dim.name}
+            tensors = [self.pad(t, inner_widths) for t in value.dimension(value._stack_dim.name)]
+            return TensorStack(tensors, value._stack_dim)
+        elif isinstance(value, ShiftLinTracer):
+            return self._pad_linear_tracer(value, widths)
+        else:
+            raise NotImplementedError(f'{type(value)} not supported')
+
+    def _pad_linear_tracer(self, value, widths: dict):
+        raise NotImplementedError()
+
+    @property
+    def native_grid_sample_mode(self) -> Union[str, None]:
+        return str(self)
+
+    def __eq__(self, other):
+        return type(other) == type(self)
+
+    def __hash__(self):
+        return hash(self.__class__)
+
+    def _op(self, other, op):
+        if type(other) == type(self):
+            return self
+        if isinstance(other, ConstantExtrapolation):  # some operations can be handled by ConstantExtrapolation, e.g. * 0
+            op = getattr(other, op.__name__)
+            return op(self)
+        else:
+            return NotImplemented
+
+    def __abs__(self):
+        return self  # assume also applied to values
+
+    def __neg__(self):
+        return self  # assume also applied to values
+
+    def __add__(self, other):
+        return self._op(other, ConstantExtrapolation.__add__)
+
+    def __radd__(self, other):
+        return self._op(other, ConstantExtrapolation.__add__)
+
+    def __mul__(self, other):
+        return self._op(other, ConstantExtrapolation.__mul__)
+
+    def __rmul__(self, other):
+        return self._op(other, ConstantExtrapolation.__mul__)
+
+    def __sub__(self, other):
+        return self._op(other, ConstantExtrapolation.__rsub__)
+
+    def __rsub__(self, other):
+        return self._op(other, ConstantExtrapolation.__sub__)
+
+    def __truediv__(self, other):
+        return self._op(other, ConstantExtrapolation.__rtruediv__)
+
+    def __rtruediv__(self, other):
+        return self._op(other, ConstantExtrapolation.__truediv__)
+
+    def __lt__(self, other):
+        return self._op(other, ConstantExtrapolation.__gt__)
+
+    def __gt__(self, other):
+        return self._op(other, ConstantExtrapolation.__lt__)
+
+
+class _BoundaryExtrapolation(_CopyExtrapolation):
+    """Uses the closest defined value for points lying outside the defined region."""
+
+    _CACHED_LOWER_MASKS = {}
+    _CACHED_UPPER_MASKS = {}
+
+    def __repr__(self):
+        return 'boundary'
+
+    def spatial_gradient(self):
+        return ZERO
+
+    @property
+    def is_flexible(self) -> bool:
+        return True
+
+    def pad_values(self, value: Tensor, width: int, dim: str, upper_edge: bool, **kwargs) -> Tensor:
+        if upper_edge:
+            edge = value[{dim: slice(-1, None)}]
+        else:
+            edge = value[{dim: slice(1)}]
+        return concat([edge] * width, value.shape[dim])
+
+    def _pad_linear_tracer(self, value: 'ShiftLinTracer', widths: dict) -> 'ShiftLinTracer':
+        """
+        *Warning*:
+        This implementation discards corners, i.e. values that lie outside the original tensor in more than one dimension.
+        These are typically sliced off in differential operators. Corners are instead assigned the value 0.
+        To take corners into account, call pad() for each axis individually. This is inefficient with ShiftLinTracer.
+
+        Args:
+          value: ShiftLinTracer:
+          widths: dict: 
+
+        Returns:
+
+        """
+        lower = {dim: -lo for dim, (lo, _) in widths.items()}
+        result = value.shift(lower, new_shape=value.shape.after_pad(widths), val_fun=lambda v: ZERO.pad(v, widths), bias_fun=lambda b: ZERO.pad(b, widths))  # inner values  ~half the computation time
+        for bound_dim, (bound_lo, bound_hi) in widths.items():
+            for i in range(bound_lo):  # i=0 means outer
+                # this sets corners to 0
+                lower = {dim: -i if dim == bound_dim else -lo for dim, (lo, _) in widths.items()}
+                mask = self._lower_mask(value.shape.only(result.dependent_dims), widths, bound_dim, bound_lo, bound_hi, i)
+                boundary = value.shift(lower, new_shape=result.shape, val_fun=lambda v: self.pad(v, widths) * mask, bias_fun=lambda b: ZERO.pad(b, widths))
+                result += boundary
+            for i in range(bound_hi):
+                lower = {dim: i - lo - hi if dim == bound_dim else -lo for dim, (lo, hi) in widths.items()}
+                mask = self._upper_mask(value.shape.only(result.dependent_dims), widths, bound_dim, bound_lo, bound_hi, i)
+                boundary = value.shift(lower, new_shape=result.shape, val_fun=lambda v: self.pad(v, widths) * mask, bias_fun=lambda b: ZERO.pad(b, widths))  # ~ half the computation time
+                result += boundary  # this does basically nothing if value is the identity
+        return result
+
+    def _lower_mask(self, shape, widths, bound_dim, bound_lo, bound_hi, i):
+        # key = (shape, tuple(widths.keys()), tuple(widths.values()), bound_dim, bound_lo, bound_hi, i)
+        # if key in _BoundaryExtrapolation._CACHED_LOWER_MASKS:
+        #     result = math.tensor(_BoundaryExtrapolation._CACHED_LOWER_MASKS[key])
+        #     _BoundaryExtrapolation._CACHED_LOWER_MASKS[key] = result
+        #     return result
+        # else:
+            mask = ZERO.pad(math.zeros(shape), {bound_dim: (bound_lo - i - 1, 0)})
+            mask = ONE.pad(mask, {bound_dim: (1, 0)})
+            mask = ZERO.pad(mask, {dim: (i, bound_hi) if dim == bound_dim else (lo, hi) for dim, (lo, hi) in widths.items()})
+            # _BoundaryExtrapolation._CACHED_LOWER_MASKS[key] = mask
+            return mask
+
+    def _upper_mask(self, shape, widths, bound_dim, bound_lo, bound_hi, i):
+        # key = (shape, tuple(widths.keys()), tuple(widths.values()), bound_dim, bound_lo, bound_hi, i)
+        # if key in _BoundaryExtrapolation._CACHED_UPPER_MASKS:
+        #     result = math.tensor(_BoundaryExtrapolation._CACHED_UPPER_MASKS[key])
+        #     _BoundaryExtrapolation._CACHED_UPPER_MASKS[key] = result
+        #     return result
+        # else:
+            mask = ZERO.pad(math.zeros(shape), {bound_dim: (0, bound_hi - i - 1)})
+            mask = ONE.pad(mask, {bound_dim: (0, 1)})
+            mask = ZERO.pad(mask, {dim: (bound_lo, i) if dim == bound_dim else (lo, hi) for dim, (lo, hi) in widths.items()})
+            # _BoundaryExtrapolation._CACHED_UPPER_MASKS[key] = mask
+            return mask
+
+
+class _PeriodicExtrapolation(_CopyExtrapolation):
+    """ Periodic extrapolation in n dimensions. """
+    def __repr__(self):
+        return 'periodic'
+
+    def spatial_gradient(self):
+        return self
+
+    def valid_outer_faces(self, dim) -> Tuple[bool, bool]:
+        return True, False
+
+    @property
+    def is_flexible(self) -> bool:
+        return False
+
+    def transform_coordinates(self, coordinates: Tensor, shape: Shape, **kwargs) -> Tensor:
+        return coordinates % shape.spatial
+
+    def pad_values(self, value: Tensor, width: int, dim: str, upper_edge: bool, **kwargs) -> Tensor:
+        if upper_edge:
+            return value[{dim: slice(width)}]
+        else:
+            return value[{dim: slice(-width, None)}]
+
+    def _pad_linear_tracer(self, value: 'ShiftLinTracer', widths: dict) -> 'ShiftLinTracer':
+        if value.shape.get_sizes(tuple(widths.keys())) != value.source.shape.get_sizes(tuple(widths.keys())):
+            raise NotImplementedError("Periodicity does not match input: %s but input has %s. This can happen when padding an already padded or sliced tensor." % (value.shape.only(tuple(widths.keys())), value.source.shape.only(tuple(widths.keys()))))
+        lower = {dim: -lo for dim, (lo, _) in widths.items()}
+        return value.shift(lower, new_shape=value.shape.after_pad(widths), val_fun=lambda v: self.pad(v, widths), bias_fun=lambda b: ZERO.pad(b, widths))
+
+    def shortest_distance(self, start: Tensor, end: Tensor, domain_size: Tensor):
+        dx = end - start
+        return (dx + domain_size / 2) % domain_size - domain_size / 2
+
+
+class _SymmetricExtrapolation(_CopyExtrapolation):
+    """Mirror with the boundary value occurring twice."""
+
+    def __repr__(self):
+        return 'symmetric'
+
+    def spatial_gradient(self):
+        return -self
+
+    @property
+    def is_flexible(self) -> bool:
+        return True
+
+    def transform_coordinates(self, coordinates: Tensor, shape: Shape, **kwargs) -> Tensor:
+        coordinates = coordinates % (2 * shape)
+        return ((2 * shape - 1) - abs((2 * shape - 1) - 2 * coordinates)) // 2
+
+    def pad_values(self, value: Tensor, width: int, dim: str, upper_edge: bool, **kwargs) -> Tensor:
+        if upper_edge:
+            return value[{dim: slice(-width, None)}].flip(dim)
+        else:
+            return value[{dim: slice(0, width)}].flip(dim)
+
+    def _pad_linear_tracer(self, value: 'ShiftLinTracer', widths: dict) -> 'ShiftLinTracer':
+        """
+        *Warning*:
+        This implementation discards corners, i.e. values that lie outside the original tensor in more than one dimension.
+        These are typically sliced off in differential operators. Corners are instead assigned the value 0.
+        To take corners into account, call pad() for each axis individually. This is inefficient with ShiftLinTracer.
+
+        Args:
+          value: ShiftLinTracer:
+          widths: dict:
+
+        Returns:
+
+        """
+        lower = {dim: -lo for dim, (lo, _) in widths.items()}
+        result = value.shift(lower, new_shape=value.shape.after_pad(widths), val_fun=lambda v: ZERO.pad(v, widths), bias_fun=lambda b: ZERO.pad(b, widths))  # inner values  ~half the computation time
+        for bound_dim, (bound_lo, bound_hi) in widths.items():
+            for i in range(bound_lo):  # i=0 means outer
+                # this sets corners to 0
+                lower = {dim: bound_lo-1-2*i if dim == bound_dim else -lo for dim, (lo, _) in widths.items()}
+                mask = self._lower_mask(value.shape.only(result.dependent_dims), widths, bound_dim, bound_lo, bound_hi, i)
+                boundary = value.shift(lower, new_shape=result.shape, val_fun=lambda v: self.pad(v, widths) * mask, bias_fun=lambda b: ZERO.pad(b, widths))
+                result += boundary
+            for i in range(bound_hi):
+                lower = {dim: -(bound_hi-1-2*i) - bound_lo - bound_hi if dim == bound_dim else -lo for dim, (lo, hi) in widths.items()}
+                mask = self._upper_mask(value.shape.only(result.dependent_dims), widths, bound_dim, bound_lo, bound_hi, i)
+                boundary = value.shift(lower, new_shape=result.shape, val_fun=lambda v: self.pad(v, widths) * mask, bias_fun=lambda b: ZERO.pad(b, widths))  # ~ half the computation time
+                result += boundary  # this does basically nothing if value is the identity
+        return result
+
+    def _lower_mask(self, shape, widths, bound_dim, bound_lo, bound_hi, i):
+        # key = (shape, tuple(widths.keys()), tuple(widths.values()), bound_dim, bound_lo, bound_hi, i)
+        # if key in _BoundaryExtrapolation._CACHED_LOWER_MASKS:
+        #     result = math.tensor(_BoundaryExtrapolation._CACHED_LOWER_MASKS[key])
+        #     _BoundaryExtrapolation._CACHED_LOWER_MASKS[key] = result
+        #     return result
+        # else:
+            mask = ZERO.pad(math.zeros(shape), {bound_dim: (bound_lo - i - 1, 0)})
+            mask = ONE.pad(mask, {bound_dim: (1, 0)})
+            mask = ZERO.pad(mask, {dim: (i, bound_hi) if dim == bound_dim else (lo, hi) for dim, (lo, hi) in widths.items()})
+            # _BoundaryExtrapolation._CACHED_LOWER_MASKS[key] = mask
+            return mask
+
+    def _upper_mask(self, shape, widths, bound_dim, bound_lo, bound_hi, i):
+        # key = (shape, tuple(widths.keys()), tuple(widths.values()), bound_dim, bound_lo, bound_hi, i)
+        # if key in _BoundaryExtrapolation._CACHED_UPPER_MASKS:
+        #     result = math.tensor(_BoundaryExtrapolation._CACHED_UPPER_MASKS[key])
+        #     _BoundaryExtrapolation._CACHED_UPPER_MASKS[key] = result
+        #     return result
+        # else:
+            mask = ZERO.pad(math.zeros(shape), {bound_dim: (0, bound_hi - i - 1)})
+            mask = ONE.pad(mask, {bound_dim: (0, 1)})
+            mask = ZERO.pad(mask, {dim: (bound_lo, i) if dim == bound_dim else (lo, hi) for dim, (lo, hi) in widths.items()})
+            # _BoundaryExtrapolation._CACHED_UPPER_MASKS[key] = mask
+            return mask
+
+
+class _AntiSymmetricExtrapolation(_SymmetricExtrapolation):
+    """Like _SymmetricExtrapolation but symmetric counterparts are negated for padding"""
+
+    def __repr__(self):
+        return 'antisymmetric'
+
+    def pad_values(self, *args, **kwargs) -> Tensor:
+        return -super().pad_values(*args, **kwargs)
+
+    def _pad_linear_tracer(self, value: 'ShiftLinTracer', widths: dict) -> 'ShiftLinTracer':
+        """
+        *Warning*:
+        This implementation discards corners, i.e. values that lie outside the original tensor in more than one dimension.
+        These are typically sliced off in differential operators. Corners are instead assigned the value 0.
+        To take corners into account, call pad() for each axis individually. This is inefficient with ShiftLinTracer.
+
+        Args:
+          value: ShiftLinTracer:
+          widths: dict:
+
+        Returns:
+
+        """
+        lower = {dim: -lo for dim, (lo, _) in widths.items()}
+        result = value.shift(lower, new_shape=value.shape.after_pad(widths), val_fun=lambda v: ZERO.pad(v, widths), bias_fun=lambda b: ZERO.pad(b, widths))  # inner values  ~half the computation time
+        for bound_dim, (bound_lo, bound_hi) in widths.items():
+            for i in range(bound_lo):  # i=0 means outer
+                # this sets corners to 0
+                lower = {dim: bound_lo-1-2*i if dim == bound_dim else -lo for dim, (lo, _) in widths.items()}
+                mask = self._lower_mask(value.shape.only(result.dependent_dims), widths, bound_dim, bound_lo, bound_hi, i)
+                boundary = value.shift(lower, new_shape=result.shape, val_fun=lambda v: self.pad(v, widths) * mask, bias_fun=lambda b: ZERO.pad(b, widths))
+                result -= boundary
+            for i in range(bound_hi):
+                lower = {dim: -(bound_hi-1-2*i) - bound_lo - bound_hi if dim == bound_dim else -lo for dim, (lo, hi) in widths.items()}
+                mask = self._upper_mask(value.shape.only(result.dependent_dims), widths, bound_dim, bound_lo, bound_hi, i)
+                boundary = value.shift(lower, new_shape=result.shape, val_fun=lambda v: self.pad(v, widths) * mask, bias_fun=lambda b: ZERO.pad(b, widths))  # ~ half the computation time
+                result -= boundary  # this does basically nothing if value is the identity
+        return result
+
+
+class _ReflectExtrapolation(_CopyExtrapolation):
+    """Mirror of inner elements. The boundary value is not duplicated."""
+
+    def __repr__(self):
+        return 'reflect'
+
+    def spatial_gradient(self):
+        return -self
+
+    @property
+    def is_flexible(self) -> bool:
+        return True
+
+    def pad_values(self, value: Tensor, width: int, dim: str, upper_edge: bool, **kwargs) -> Tensor:
+        if upper_edge:
+            return value[{dim: slice(-1-width, -1)}].flip(dim)
+        else:
+            return value[{dim: slice(1, width+1)}].flip(dim)
+
+    def transform_coordinates(self, coordinates: Tensor, shape: Shape, **kwargs) -> Tensor:
+        coordinates = coordinates % (2 * shape - 2)
+        return (shape - 1) - math.abs_((shape - 1) - coordinates)
+
+    def _pad_linear_tracer(self, value: 'ShiftLinTracer', widths: dict) -> 'ShiftLinTracer':
+        """
+        *Warning*:
+        This implementation discards corners, i.e. values that lie outside the original tensor in more than one dimension.
+        These are typically sliced off in differential operators. Corners are instead assigned the value 0.
+        To take corners into account, call pad() for each axis individually. This is inefficient with ShiftLinTracer.
+
+        Args:
+          value: ShiftLinTracer:
+          widths: dict:
+
+        Returns:
+
+        """
+        lower = {dim: -lo for dim, (lo, _) in widths.items()}
+        result = value.shift(lower, new_shape=value.shape.after_pad(widths), val_fun=lambda v: ZERO.pad(v, widths), bias_fun=lambda b: ZERO.pad(b, widths))  # inner values  ~half the computation time
+        for bound_dim, (bound_lo, bound_hi) in widths.items():
+            for i in range(bound_lo):  # i=0 means outer
+                # this sets corners to 0
+                lower = {dim: bound_lo-2*i if dim == bound_dim else -lo for dim, (lo, _) in widths.items()}
+                mask = self._lower_mask(value.shape.only(result.dependent_dims), widths, bound_dim, bound_lo, bound_hi, i)
+                boundary = value.shift(lower, new_shape=result.shape, val_fun=lambda v: self.pad(v, widths) * mask, bias_fun=lambda b: ZERO.pad(b, widths))
+                result += boundary
+            for i in range(bound_hi):
+                lower = {dim: -(bound_hi-2*i) - bound_lo - bound_hi if dim == bound_dim else -lo for dim, (lo, hi) in widths.items()}
+                mask = self._upper_mask(value.shape.only(result.dependent_dims), widths, bound_dim, bound_lo, bound_hi, i)
+                boundary = value.shift(lower, new_shape=result.shape, val_fun=lambda v: self.pad(v, widths) * mask, bias_fun=lambda b: ZERO.pad(b, widths))  # ~ half the computation time
+                result += boundary  # this does basically nothing if value is the identity
+        return result
+
+    def _lower_mask(self, shape, widths, bound_dim, bound_lo, bound_hi, i):
+        # key = (shape, tuple(widths.keys()), tuple(widths.values()), bound_dim, bound_lo, bound_hi, i)
+        # if key in _BoundaryExtrapolation._CACHED_LOWER_MASKS:
+        #     result = math.tensor(_BoundaryExtrapolation._CACHED_LOWER_MASKS[key])
+        #     _BoundaryExtrapolation._CACHED_LOWER_MASKS[key] = result
+        #     return result
+        # else:
+            mask = ZERO.pad(math.zeros(shape), {bound_dim: (bound_lo - i - 1, 0)})
+            mask = ONE.pad(mask, {bound_dim: (1, 0)})
+            mask = ZERO.pad(mask, {dim: (i, bound_hi) if dim == bound_dim else (lo, hi) for dim, (lo, hi) in widths.items()})
+            # _BoundaryExtrapolation._CACHED_LOWER_MASKS[key] = mask
+            return mask
+
+    def _upper_mask(self, shape, widths, bound_dim, bound_lo, bound_hi, i):
+        # key = (shape, tuple(widths.keys()), tuple(widths.values()), bound_dim, bound_lo, bound_hi, i)
+        # if key in _BoundaryExtrapolation._CACHED_UPPER_MASKS:
+        #     result = math.tensor(_BoundaryExtrapolation._CACHED_UPPER_MASKS[key])
+        #     _BoundaryExtrapolation._CACHED_UPPER_MASKS[key] = result
+        #     return result
+        # else:
+            mask = ZERO.pad(math.zeros(shape), {bound_dim: (0, bound_hi - i - 1)})
+            mask = ONE.pad(mask, {bound_dim: (0, 1)})
+            mask = ZERO.pad(mask, {dim: (bound_lo, i) if dim == bound_dim else (lo, hi) for dim, (lo, hi) in widths.items()})
+            # _BoundaryExtrapolation._CACHED_UPPER_MASKS[key] = mask
+            return mask
+
+
+class _AntiReflectExtrapolation(_ReflectExtrapolation):
+    """Like _ReflectExtrapolation but symmetric counterparts are negated for padding"""
+
+    def __repr__(self):
+        return 'antireflect'
+
+    def pad_values(self, *args, **kwargs) -> Tensor:
+        return -super().pad_values(*args, **kwargs)
+
+    def _pad_linear_tracer(self, value: 'ShiftLinTracer', widths: dict) -> 'ShiftLinTracer':
+        """
+        *Warning*:
+        This implementation discards corners, i.e. values that lie outside the original tensor in more than one dimension.
+        These are typically sliced off in differential operators. Corners are instead assigned the value 0.
+        To take corners into account, call pad() for each axis individually. This is inefficient with ShiftLinTracer.
+
+        Args:
+          value: ShiftLinTracer:
+          widths: dict:
+
+        Returns:
+
+        """
+        lower = {dim: -lo for dim, (lo, _) in widths.items()}
+        result = value.shift(lower, new_shape=value.shape.after_pad(widths), val_fun=lambda v: ZERO.pad(v, widths), bias_fun=lambda b: ZERO.pad(b, widths))  # inner values  ~half the computation time
+        for bound_dim, (bound_lo, bound_hi) in widths.items():
+            for i in range(bound_lo):  # i=0 means outer
+                # this sets corners to 0
+                lower = {dim: bound_lo-2*i if dim == bound_dim else -lo for dim, (lo, _) in widths.items()}
+                mask = self._lower_mask(value.shape.only(result.dependent_dims), widths, bound_dim, bound_lo, bound_hi, i)
+                boundary = value.shift(lower, new_shape=result.shape, val_fun=lambda v: self.pad(v, widths) * mask, bias_fun=lambda b: ZERO.pad(b, widths))
+                result -= boundary
+            for i in range(bound_hi):
+                lower = {dim: -(bound_hi-2*i) - bound_lo - bound_hi if dim == bound_dim else -lo for dim, (lo, hi) in widths.items()}
+                mask = self._upper_mask(value.shape.only(result.dependent_dims), widths, bound_dim, bound_lo, bound_hi, i)
+                boundary = value.shift(lower, new_shape=result.shape, val_fun=lambda v: self.pad(v, widths) * mask, bias_fun=lambda b: ZERO.pad(b, widths))  # ~ half the computation time
+                result -= boundary  # this does basically nothing if value is the identity
+        return result
+
+
+class _SymmetricGradientExtrapolation(Extrapolation):
+
+    def to_dict(self) -> dict:
+        return {'type': 'symmetric-gradient'}
+
+    def spatial_gradient(self) -> 'Extrapolation':
+        raise NotImplementedError
+
+    def valid_outer_faces(self, dim) -> Tuple[bool, bool]:
+        raise NotImplementedError  # probably return True, True but this hasn't been used on grids yet
+
+    @property
+    def is_flexible(self) -> bool:
+        return True
+
+    def pad_values(self, value: Tensor, width: int, dim: str, upper_edge: bool, **kwargs) -> Tensor:
+        anti_s = ANTIREFLECT.pad_values(value, width, dim, upper_edge, **kwargs)
+        edge = value[{dim: -1}] if upper_edge else value[{dim: 0}]
+        return anti_s + 2 * edge
+
+
+class _NoExtrapolation(Extrapolation):  # singleton
+
+    @property
+    def shape(self):
+        return EMPTY_SHAPE
+
+    def to_dict(self) -> dict:
+        return {'type': 'none'}
+
+    def pad(self, value: Tensor, widths: dict, **kwargs) -> Tensor:
+        return value
+
+    def spatial_gradient(self) -> 'Extrapolation':
+        return self
+
+    def valid_outer_faces(self, dim) -> Tuple[bool, bool]:
+        return True, True
+
+    def __value_attrs__(self):
+        return ()
+
+    @property
+    def is_flexible(self) -> bool:
+        raise AssertionError(f"is_flexible not defined by {self.__class__}")
+
+    def pad_values(self, value: Tensor, width: int, dim: str, upper_edge: bool, **kwargs) -> Tensor:
+        return math.zeros(value.shape._replace_single_size(dim, 0))
+
+    def __repr__(self):
+        return "none"
+
+    def __abs__(self):
+        return self
+
+    def __neg__(self):
+        return self
+
+    def __add__(self, other):
+        return self
+
+    def __radd__(self, other):
+        return self
+
+    def __sub__(self, other):
+        return self
+
+    def __rsub__(self, other):
+        return self
+
+    def __mul__(self, other):
+        return self
+
+    def __rmul__(self, other):
+        return self
+
+    def __truediv__(self, other):
+        return self
+
+    def __rtruediv__(self, other):
+        return self
+
+
+class Undefined(Extrapolation):
+    """
+    The extrapolation is unknown and must be replaced before usage.
+    Any access to outside values will raise an AssertionError.
+    """
+
+    def __init__(self, derived_from: Extrapolation):
+        super().__init__(-1)
+        self.derived_from = derived_from
+
+    @property
+    def shape(self):
+        return EMPTY_SHAPE
+
+    def to_dict(self) -> dict:
+        return {'type': 'undefined', 'derived_from': self.derived_from.to_dict()}
+
+    def pad(self, value: Tensor, widths: dict, **kwargs) -> Tensor:
+        for (lo, up) in widths.items():
+            assert lo == 0 and up == 0, "Undefined extrapolation"
+
+    def spatial_gradient(self) -> 'Extrapolation':
+        return self
+
+    def valid_outer_faces(self, dim) -> Tuple[bool, bool]:
+        return self.derived_from.valid_outer_faces(dim)
+
+    @property
+    def is_flexible(self) -> bool:
+        raise AssertionError("Undefined extrapolation")
+
+    def pad_values(self, value: Tensor, width: int, dim: str, upper_edge: bool, **kwargs) -> Tensor:
+        raise AssertionError("Undefined extrapolation")
+
+    def __repr__(self):
+        return "undefined"
+
+    def __abs__(self):
+        return self
+
+    def __neg__(self):
+        return self
+
+    def __add__(self, other):
+        return self
+
+    def __radd__(self, other):
+        return self
+
+    def __sub__(self, other):
+        return self
+
+    def __rsub__(self, other):
+        return self
+
+    def __mul__(self, other):
+        return self
+
+    def __rmul__(self, other):
+        return self
+
+    def __truediv__(self, other):
+        return self
+
+    def __rtruediv__(self, other):
+        return self
+
+
+ZERO = ConstantExtrapolation(0)
+""" Extrapolates with the constant value 0 (Dirichlet boundary condition). """
+ONE = ConstantExtrapolation(1)
+""" Extrapolates with the constant value 1 (Dirichlet boundary condition). """
+PERIODIC = _PeriodicExtrapolation(1)
+""" Extends a grid by tiling it (Periodic boundary condition). """
+ZERO_GRADIENT = _BoundaryExtrapolation(2)
+""" Extends a grid with its edge values (Neumann boundary condition). The value of a point lying outside the grid is determined by the closest grid value(s). """
+BOUNDARY = ZERO_GRADIENT
+# undocumented, use ZERO_GRADIENT instead
+SYMMETRIC = _SymmetricExtrapolation(3)
+""" Extends a grid by tiling it. Every other copy of the grid is flipped. Edge values occur twice per seam. """
+ANTISYMMETRIC = _AntiSymmetricExtrapolation(3)
+""" Like SYMMETRIC but extends a grid with the negative value of the corresponding counterpart instead. """
+REFLECT = _ReflectExtrapolation(4)
+""" Like SYMMETRIC but the edge values are not copied and only occur once per seam. """
+ANTIREFLECT = _AntiReflectExtrapolation(4)
+""" Like REFLECT but extends a grid with the negative value of the corresponding counterpart instead. """
+SYMMETRIC_GRADIENT = _SymmetricGradientExtrapolation(3)
+""" Extrapolates in a continuous manner. The normal component of the spatial gradient is symmetric at the boundaries. The outer-most valid difference is duplicated. """
+
+NONE = _NoExtrapolation(-1)
+""" Raises AssertionError when used to determine outside values. Padding operations will have no effect with this extrapolation. """
+
+
+_PRIMITIVES = {  # used by as_extrapolation() and from_dict()
+    'periodic': PERIODIC,
+    'zero': ZERO,
+    'one': ONE,
+    'zero-gradient': ZERO_GRADIENT,
+    '∇=0': ZERO_GRADIENT,
+    'boundary': ZERO_GRADIENT,  # deprecated
+    'symmetric': SYMMETRIC,
+    'symmetric-gradient': SYMMETRIC_GRADIENT,
+    'antisymmetric': ANTISYMMETRIC,
+    'reflect': REFLECT,
+    'antireflect': ANTISYMMETRIC,
+}
+
+
+def as_extrapolation(obj) -> Extrapolation:
+    """
+    Creates an `Extrapolation` from a descriptor object.
+
+    Args:
+        obj: Extrapolation specification, one of the following:
+
+            * `Extrapolation`
+            * Primitive name as `str`: periodic, zero, one, zero-gradient, symmetric, symmetric-gradient, antisymmetric, reflect, antireflect
+            * `dict` containing exactly the keys `'normal'` and `'tangential'`
+            * `dict` mapping spatial dimension names to extrapolations
+
+    Returns:
+        `Extrapolation`
+    """
+    if isinstance(obj, Extrapolation):
+        return obj
+    if obj is None:
+        return NONE
+    if isinstance(obj, str):
+        assert obj in _PRIMITIVES, f"Unrecognized extrapolation type: '{obj}'"
+        return _PRIMITIVES[obj]
+    if isinstance(obj, dict):
+        if 'normal' in obj or 'tangential' in obj:
+            assert 'normal' in obj and 'tangential' in obj, f"Normal/tangential dict requires both entries 'normal' and 'tangential' but got {obj}"
+            assert len(obj) == 2, f"Normal/tangential dict must only contain entries 'normal' and 'tangential' but got {obj}"
+            normal = as_extrapolation(obj['normal'])
+            tangential = as_extrapolation(obj['tangential'])
+            return combine_by_direction(normal=normal, tangential=tangential)
+        else:
+            ext = {dim: (as_extrapolation(spec[0]), as_extrapolation(spec[1])) if isinstance(spec, tuple) else as_extrapolation(spec) for dim, spec in obj.items()}
+            return combine_sides(**ext)
+    return ConstantExtrapolation(obj)
+
+
+def combine_sides(**extrapolations: Union[Extrapolation, tuple]) -> Extrapolation:
+    """
+    Specify extrapolations for each side / face of a box.
+
+    Args:
+        **extrapolations: map from dim: str -> `Extrapolation` or `tuple` (lower, upper)
+
+    Returns:
+        `Extrapolation`
+    """
+    values = set()
+    proper_dict = {}
+    for dim, ext in extrapolations.items():
+        if isinstance(ext, Extrapolation):
+            values.add(ext)
+            proper_dict[dim] = (ext, ext)
+        elif isinstance(ext, tuple):
+            assert len(ext) == 2, "Tuple must contain exactly two elements, (lower, upper)"
+            lower = as_extrapolation(ext[0])
+            upper = as_extrapolation(ext[1])
+            values.add(lower)
+            values.add(upper)
+            proper_dict[dim] = (lower, upper)
+        else:
+            proper_ext = as_extrapolation(ext)
+            values.add(proper_ext)
+            proper_dict[dim] = (proper_ext, proper_ext)
+    if len(values) == 1:  # All equal -> return any
+        return next(iter(values))
+    else:
+        return _MixedExtrapolation(proper_dict)
+
+
+class _MixedExtrapolation(Extrapolation):
+
+    def __init__(self, extrapolations: Dict[str, Tuple[Extrapolation, Extrapolation]]):
+        """
+        A mixed extrapolation uses different extrapolations for different sides.
+
+        Args:
+          extrapolations: axis: str -> (lower: Extrapolation, upper: Extrapolation) or Extrapolation
+        """
+        super().__init__(pad_rank=None)
+        self.ext = extrapolations
+
+    @property
+    def shape(self):
+        return merge_shapes(*sum(self.ext.values(), ()))
+
+    def to_dict(self) -> dict:
+        return {
+            'type': 'mixed',
+            'dims': {ax: (es[0].to_dict(), es[1].to_dict()) for ax, es in self.ext.items()}
+        }
+
+    def __value_attrs__(self):
+        return 'ext',
+
+    def __eq__(self, other):
+        if isinstance(other, _MixedExtrapolation):
+            return self.ext == other.ext
+        else:
+            simplified = combine_sides(**self.ext)
+            if not isinstance(simplified, _MixedExtrapolation):
+                return simplified == other
+            else:
+                return False
+
+    def __hash__(self):
+        simplified = combine_sides(**self.ext)
+        if not isinstance(simplified, _MixedExtrapolation):
+            return hash(simplified)
+        else:
+            return hash(frozenset(self.ext.items()))
+
+    def __repr__(self):
+        return repr(self.ext)
+
+    def spatial_gradient(self) -> Extrapolation:
+        return combine_sides(**{ax: (es[0].spatial_gradient(), es[1].spatial_gradient()) for ax, es in self.ext.items()})
+
+    def valid_outer_faces(self, dim) -> Tuple[bool, bool]:
+        e_lower, e_upper = self.ext[dim]
+        return e_lower.valid_outer_faces(dim)[0], e_upper.valid_outer_faces(dim)[1]
+
+    def is_copy_pad(self, dim: str, upper_edge: bool):
+        return self.ext[dim][upper_edge].is_copy_pad(dim, upper_edge)
+
+    @property
+    def is_flexible(self) -> bool:
+        result_by_dim = [lo.is_flexible or up.is_flexible for lo, up in self.ext.values()]
+        return any(result_by_dim)
+
+    def pad(self, value: Tensor, widths: dict, **kwargs) -> Tensor:
+        """
+        Pads a tensor using mixed values
+
+        Args:
+          value: tensor to be padded
+          widths: name: str -> (lower: int, upper: int)}
+          value: Tensor: 
+          widths: dict: 
+
+        Returns:
+
+        """
+        extrapolations = set(sum(self.ext.values(), ()))
+        extrapolations = tuple(sorted(extrapolations, key=lambda e: e.pad_rank))
+        for ext in extrapolations:
+            ext_widths = {ax: (l if self.ext[ax][0] == ext else 0, u if self.ext[ax][1] == ext else 0)
+                          for ax, (l, u) in widths.items()}
+            value = ext.pad(value, ext_widths, **kwargs)
+        return value
+
+    def pad_values(self, value: Tensor, width: int, dim: str, upper_edge: bool, **kwargs) -> Tensor:
+        extrap: Extrapolation = self.ext[dim][upper_edge]
+        return extrap.pad_values(value, width, dim, upper_edge, **kwargs)
+
+    def transform_coordinates(self, coordinates: Tensor, shape: Shape, **kwargs) -> Tensor:
+        assert len(self.ext) == len(shape.spatial) == coordinates.vector.size
+        result = []
+        for dim in shape.spatial.unstack():
+            dim_coords = coordinates[[dim.name]]
+            dim_extrapolations = self.ext[dim.name]
+            if dim_extrapolations[0] == dim_extrapolations[1]:
+                result.append(dim_extrapolations[0].transform_coordinates(dim_coords, dim, **kwargs))
+            else:  # separate boundary for lower and upper face
+                lower = dim_extrapolations[0].transform_coordinates(dim_coords, dim, **kwargs)
+                upper = dim_extrapolations[1].transform_coordinates(dim_coords, dim, **kwargs)
+                result.append(math.where(dim_coords <= 0, lower, upper))
+        if 'vector' in result[0].shape:
+            return concat(result, channel('vector'))
+        else:
+            return stack(result, channel('vector'))
+
+    def __getitem__(self, item):
+        if isinstance(item, dict):
+            all_dims = tuple(self.ext.keys())
+            return combine_sides(**{dim: (e1._getitem_with_domain(item, dim, False, all_dims), e2._getitem_with_domain(item, dim, True, all_dims)) for dim, (e1, e2) in self.ext.items()})
+        else:
+            dim, face = item
+            return self.ext[dim][face]
+
+    def __add__(self, other):
+        return self._op2(other, lambda e1, e2: e1 + e2)
+
+    def __radd__(self, other):
+        return self._op2(other, lambda e1, e2: e2 + e1)
+
+    def __sub__(self, other):
+        return self._op2(other, lambda e1, e2: e1 - e2)
+
+    def __rsub__(self, other):
+        return self._op2(other, lambda e1, e2: e2 - e1)
+
+    def __mul__(self, other):
+        return self._op2(other, lambda e1, e2: e1 * e2)
+
+    def __rmul__(self, other):
+        return self._op2(other, lambda e1, e2: e2 * e1)
+
+    def __truediv__(self, other):
+        return self._op2(other, lambda e1, e2: e1 / e2)
+
+    def __rtruediv__(self, other):
+        return self._op2(other, lambda e1, e2: e2 / e1)
+
+    def _op2(self, other, operator):
+        if isinstance(other, _MixedExtrapolation):
+            assert self.ext.keys() == other.ext.keys()
+            return combine_sides(**{ax: (operator(lo, other.ext[ax][False]), operator(hi, other.ext[ax][True])) for ax, (lo, hi) in self.ext.items()})
+        else:
+            return combine_sides(**{ax: (operator(lo, other), operator(hi, other)) for ax, (lo, hi) in self.ext.items()})
+
+    def __abs__(self):
+        return combine_sides(**{ax: (abs(lo), abs(up)) for ax, (lo, up) in self.ext.items()})
+
+    def __neg__(self):
+        return combine_sides(**{ax: (-lo, -up) for ax, (lo, up) in self.ext.items()})
+
+
+class _NormalTangentialExtrapolation(Extrapolation):
+
+    def __init__(self, normal: Extrapolation, tangential: Extrapolation):
+        super().__init__(pad_rank=min(normal.pad_rank, tangential.pad_rank))
+        self.normal = normal
+        self.tangential = tangential
+
+    @property
+    def shape(self):
+        return merge_shapes(self.normal, self.tangential)
+
+    def to_dict(self) -> dict:
+        return {
+            'type': 'normal-tangential',
+            'normal': self.normal.to_dict(),
+            'tangential': self.tangential.to_dict(),
+        }
+
+    def __value_attrs__(self):
+        return 'normal', 'tangential'
+
+    def __repr__(self):
+        return f"normal={self.normal}, tangential={self.tangential}"
+
+    def spatial_gradient(self) -> 'Extrapolation':
+        return combine_by_direction(self.normal.spatial_gradient(), self.tangential.spatial_gradient())
+
+    def valid_outer_faces(self, dim) -> Tuple[bool, bool]:
+        return self.normal.valid_outer_faces(dim)
+
+    def is_copy_pad(self, dim: str, upper_edge: bool):
+        return False  # normal and tangential might copy from different places, so no.
+
+    @property
+    def is_flexible(self) -> bool:
+        return self.normal.is_flexible
+
+    def pad_values(self, value: Tensor, width: int, dim: str, upper_edge: bool, **kwargs) -> Tensor:
+        if 'vector' not in value.shape:
+            warnings.warn(f'{self} adding a vector dimension to tensor {value.shape}')
+            from phi.math import expand
+            value = expand(value, channel(vector=spatial(value).names))
+        assert value.vector.item_names is not None, "item_names must be present when padding with normal-tangential"
+        result = []
+        for component_name, component in zip(value.vector.item_names, value.vector):
+            ext = self.normal if component_name == dim else self.tangential
+            result.append(ext.pad_values(component, width, dim, upper_edge, **kwargs))
+        from ._magic_ops import stack
+        result = stack(result, value.shape.only('vector'))
+        return result
+
+    def _getitem_with_domain(self, item: dict, dim: str, upper_edge: bool, all_dims: tuple):
+        if 'vector' not in item:
+            return self
+        component = item['vector']
+        assert isinstance(component, str), f"Selecting a component of normal/tangential must be done by dimension name but got {component}"
+        if component == dim:
+            return self.normal
+        else:
+            return self.tangential
+
+    def __eq__(self, other):
+        return isinstance(other, _NormalTangentialExtrapolation) and self.normal == other.normal and self.tangential == other.tangential
+
+    def __hash__(self):
+        return hash(self.normal) + hash(self.tangential)
+
+    def __add__(self, other):
+        return self._op2(other, lambda e1, e2: e1 + e2)
+
+    def __radd__(self, other):
+        return self._op2(other, lambda e1, e2: e2 + e1)
+
+    def __sub__(self, other):
+        return self._op2(other, lambda e1, e2: e1 - e2)
+
+    def __rsub__(self, other):
+        return self._op2(other, lambda e1, e2: e2 - e1)
+
+    def __mul__(self, other):
+        return self._op2(other, lambda e1, e2: e1 * e2)
+
+    def __rmul__(self, other):
+        return self._op2(other, lambda e1, e2: e2 * e1)
+
+    def __truediv__(self, other):
+        return self._op2(other, lambda e1, e2: e1 / e2)
+
+    def __rtruediv__(self, other):
+        return self._op2(other, lambda e1, e2: e2 / e1)
+
+    def _op2(self, other, operator):
+        if isinstance(other, _NormalTangentialExtrapolation):
+            return combine_by_direction(normal=operator(self.normal, other.normal), tangential=operator(self.tangential, other.tangential))
+        else:
+            return combine_by_direction(normal=operator(self.normal, other), tangential=operator(self.tangential, other))
+
+    def __abs__(self):
+        return combine_by_direction(normal=abs(self.normal), tangential=abs(self.tangential))
+
+    def __neg__(self):
+        return combine_by_direction(normal=-self.normal, tangential=-self.tangential)
+
+
+def combine_by_direction(normal: Union[Extrapolation, float, Tensor], tangential: Union[Extrapolation, float, Tensor]) -> Extrapolation:
+    """
+    Use a different extrapolation for the normal component of vector-valued tensors.
+
+    Args:
+        normal: Extrapolation for the component that is orthogonal to the boundary.
+        tangential: Extrapolation for the component that is tangential to the boundary.
+
+    Returns:
+        `Extrapolation`
+    """
+    normal = as_extrapolation(normal)
+    tangential = as_extrapolation(tangential)
+    return normal if normal == tangential else _NormalTangentialExtrapolation(normal, tangential)
+
+
+def from_dict(dictionary: dict) -> Extrapolation:
+    """
+    Loads an `Extrapolation` object from a dictionary that was created using `Extrapolation.to_dict()`.
+
+    Args:
+        dictionary: serializable dictionary holding all extrapolation properties
+
+    Returns:
+        Loaded extrapolation
+    """
+    etype = dictionary['type']
+    if etype in _PRIMITIVES:
+        return _PRIMITIVES[etype]
+    elif etype == 'constant':
+        return ConstantExtrapolation(dictionary['value'])
+    elif etype == 'mixed':
+        dims: Dict[str, tuple] = dictionary['dims']
+        extrapolations = {dim: (from_dict(lo_up[0]), from_dict(lo_up[1])) for dim, lo_up in dims.items()}
+        return _MixedExtrapolation(extrapolations)
+    elif etype == 'normal-tangential':
+        normal = from_dict(dictionary['normal'])
+        tangential = from_dict(dictionary['tangential'])
+        return _NormalTangentialExtrapolation(normal, tangential)
+    elif etype == 'none':
+        return NONE
+    elif etype == 'undefined':
+        derived_from = from_dict(dictionary['derived_from'])
+        return Undefined(derived_from)
+    else:
+        raise ValueError(dictionary)
+
+
+def order_by_shape(shape: Shape, sequence, default=None) -> Union[tuple, list]:
+    """
+    If sequence is a dict with dimension names as keys, orders its values according to this shape.
+
+    Otherwise, the sequence is returned unchanged.
+
+    Args:
+      sequence: Sequence or dict to be ordered
+      default: default value used for dimensions not contained in sequence
+
+    Returns:
+      ordered sequence of values
+    """
+    if isinstance(sequence, dict):
+        result = [sequence.get(dim, default) for dim in shape.names]
+        return result
+    elif isinstance(sequence, (tuple, list)):
+        assert len(sequence) == shape.rank
+        return sequence
+    else:  # just a constant
+        return sequence
+
+
+def map(f: Callable[[Extrapolation], Extrapolation], extrapolation):
+    """
+    Applies a function to all leaf extrapolations in `extrapolation`.
+    Non-leaves are those created by `combine_sides()` and `combine_by_direction()`.
+
+    The tree will be collapsed if possible.
+
+    Args:
+        f: Function mapping a leaf `Extrapolation` to another `Extrapolation`.
+        extrapolation: Input tree for `f`.
+
+    Returns:
+        `Extrapolation`
+    """
+    if isinstance(extrapolation, _MixedExtrapolation):
+        return combine_sides(**{dim: (map(f, lo), map(f, up)) for dim, (lo, up) in extrapolation.ext.items()})
+    elif isinstance(extrapolation, _NormalTangentialExtrapolation):
+        return combine_by_direction(map(f, extrapolation.normal), map(f, extrapolation.tangential))
+    else:
+        return f(extrapolation)
+
+
+def remove_constant_offset(extrapolation):
+    """
+    Removes all constant offsets from an extrapolation.
+    This also includes `NaN` values in constants (unlike `ext - ext`).
+
+    Args:
+        extrapolation: `Extrapolation` object.
+
+    Returns:
+        `Extrapolation` that has no constant offsets
+    """
+    def const_to_zero(extrapolation):
+        if isinstance(extrapolation, ConstantExtrapolation):
+            return ZERO
+        else:
+            return extrapolation
+    return map(const_to_zero, extrapolation)
+
```

### Comparing `phiflow-2.3.4/phi/math/magic.py` & `phiflow-2.4.0/phi/math/magic.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,746 +1,779 @@
-"""
-Magic methods allow custom classes to be compatible with various functions defined in `phi.math`, analogous to how implementing `__hash__` allows objects to be used with `hash()`.
-The magic methods are grouped into purely declarative classes (interfaces) by what functionality they provide.
-
-* `Shaped` objects have a `phi.math.Shape`.
-* `Sliceable` objects can be sliced along dimensions.
-* `Shapable` objects can additionally be reshaped.
-* `PhiTreeNode` objects can be disassembled into tensors.
-
-All of these magic classes declared here define a custom instance checks and should not be used as superclasses.
-
-An object implements one of the types defined here by implementing one or more of the related magic methods.
-Instance checks can be performed via `isinstance(obj, <MagicClass>)`.
-
-This is analogous to interfaces defined in the built-in `collections` package, such as `Sized, Iterable, Hashable, Callable`.
-To check whether `len(obj)` can be performed, you check `isinstance(obj, Sized)`.
-"""
-import warnings
-from typing import Tuple, Callable, Union
-
-import dataclasses
-
-from ._shape import Shape, shape, channel, non_batch, batch, spatial, instance, concat_shapes, dual
-from .backend._dtype import DType
-
-
-class _ShapedType(type):
-    def __instancecheck__(self, instance):
-        try:
-            shape(instance)
-            return True
-        except ValueError:
-            return False
-
-    def __subclasscheck__(self, subclass):
-        return True
-
-
-class Shaped(metaclass=_ShapedType):
-    """
-    To be considered shaped, an object must either implement the magic method `__shape__()` or have a valid `shape` property.
-    In either case, the returned shape must be an instance of `phi.math.Shape`.
-
-    To check whether an object is `Shaped`, use `isinstance(obj, Shaped)`.
-
-    **Usage in `phi.math`:**
-
-    The functions `phi.math.shape` as well as dimension filters, such as `phi.math.spatial` or `phi.math.non_batch` can be called on all shaped objects.
-
-    See Also:
-        `Sliceable`, `Shapable`
-    """
-
-    def __shape__(self) -> 'Shape':
-        """
-        Returns the shape of this object.
-
-        Alternatively, the shape can be declared via the property `shape`.
-
-        Returns:
-            `phi.math.Shape`
-        """
-        raise NotImplementedError
-
-    @property
-    def shape(self) -> 'Shape':
-        """
-        Alternative form of `__shape__()`.
-        Implement either to be considered `Shaped`.
-
-        Returns:
-            `phi.math.Shape`
-        """
-        raise NotImplementedError
-
-
-class _SliceableType(type):
-    def __instancecheck__(self, instance):
-        return isinstance(instance, Shaped) and hasattr(instance, '__getitem__')
-
-    def __subclasscheck__(self, subclass):
-        return hasattr(subclass, '__getitem__')
-
-
-class Sliceable(metaclass=_SliceableType):
-    """
-    Objects are considered sliceable if they are `Shaped` and implement `__getitem__` as defined below.
-
-    To enable the slicing syntax `obj.dim[slice]`, implement the `__getattr__` method as defined below.
-
-    Classes implementing `Sliceable` should override `__getattr__` to enable the special slicing syntax defined in `BoundDim`.
-
-    **Usage in `phi.math`:**
-
-    In addition to slicing, sliceable objects can be unstacked along one or multiple dimensions using `phi.math.unstack`.
-
-    See Also
-        `Shapable`, `Shaped`
-    """
-
-    def __getitem__(self, item) -> 'Sliceable':
-        """
-        Slice this object along one or multiple existing or non-existing dimensions.
-
-        When overriding this function, make sure to first call `slicing_dict(self, item)` to sort slices by dimension.
-
-        Args:
-            item: `dict` mapping dimension names to the corresponding selections.
-                Selections can be slices, indices, tuples, item names, bool tensors, int tensors or other custom types.
-                All Sliceable object must support indexing by `int`, `slice`, `tuple`, `list`, `str`.
-
-        Returns:
-            Instance of the same class (or a compatible class) as `self`.
-        """
-        raise NotImplementedError
-
-    def __unstack__(self, dims: Tuple[str, ...]) -> Tuple['Sliceable', ...]:
-        """
-        Un-stack this object along one or multiple dimensions.
-        Un-stacking along multiple dimensions is equal to first packing the dimensions and then unstacking along the packed dimension.
-
-        Implementing this magic method is optional but the default implementation may be slow.
-
-        Args:
-            dims: Ordered `tuple` of dimension names along which to unstack this object.
-
-        Returns:
-            `tuple` of slices along `dims` or `NotImplemented` to revert to default behavior for this object.
-        """
-        raise NotImplementedError
-
-
-class _ShapableType(type):
-    def __instancecheck__(self, instance):
-        return isinstance(instance, Sliceable) and isinstance(instance, Shaped) and\
-               (hasattr(instance, '__stack__') or (hasattr(instance, '__concat__') and hasattr(instance, '__expand__')) or isinstance(instance, PhiTreeNode))
-
-    def __subclasscheck__(self, subclass):
-        return issubclass(subclass, Sliceable) and\
-               (hasattr(subclass, '__stack__') or (hasattr(subclass, '__concat__') and hasattr(subclass, '__expand__')) or issubclass(subclass, PhiTreeNode))
-
-
-class Shapable(metaclass=_ShapableType):
-    """
-    Shapable objects can be stacked, concatenated and reshaped.
-
-    To be considered `Shapable`, objects must be `Sliceable` and `Shaped` and implement
-
-    * `__stack__()` or
-    * `__concat__()` and `__expand__()`.
-
-    Objects should additionally implement the other magic methods for performance reasons.
-
-    **Usage in `phi.math`:**
-
-    Shapable objects can be used with the following functions in addition to what they inherit from being `Sliceable` and `Shaped`:
-
-    * `phi.math.stack`
-    * `phi.math.concat`
-    * `phi.math.expand`
-    * `phi.math.rename_dims`
-    * `phi.math.pack_dims`
-    * `phi.math.unpack_dim`
-    * `phi.math.flatten`
-
-    Additionally, the `phi.math.BoundDim` syntax for dimension renaming and retyping is enabled, e.g. `obj.dim.as_channel('vector')`.
-    """
-    @staticmethod
-    def __stack__(values: tuple, dim: Shape, **kwargs) -> 'Shapable':
-        """
-        Stack all `values` into a single instance along the new dimension `dim`.
-
-        This method can be implemented as a bound method or as a `staticmethod` (without the `self` argument).
-
-        Args:
-            values: `tuple` of `Shapable` objects to be stacked. `self` is included in that list at least once.
-            dim: Single-dimension `Shape`. This dimension must not be present with any of the `values`.
-                The dimension fulfills the condition `dim.size == len(values)`.
-            **kwargs: Additional keyword arguments required by specific implementations.
-                Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
-                Adding batch dimensions must always work without keyword arguments.
-
-        Returns:
-            New instance of `Shapable` representing the stacked slices.
-            Its shape includes `dim` in addition to the dimensions present in `values`.
-            If such a representation cannot be created because some values in `values` are not supported, returns `NotImplemented`.
-        """
-        raise NotImplementedError
-
-    @staticmethod
-    def __concat__(values: tuple, dim: str, **kwargs) -> 'Shapable':
-        """
-        Concatenate `values` along `dim`.
-
-        This method can be implemented as a bound method or as a `staticmethod` (without the `self` argument).
-
-        Args:
-            values: Values to concatenate. `self` is included in that list at least once.
-            dim: Dimension nams as `str`, must be present in all `values`.
-            **kwargs: Additional keyword arguments required by specific implementations.
-                Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
-                Adding batch dimensions must always work without keyword arguments.
-
-        Returns:
-            New instance of `Shapable` representing the concatenated values or `NotImplemented` to revert to default behavior for this object.
-            When returning a valid object, the size of `dim` must be equal to the sum of all `dim` sizes in `values`.
-            If such a representation cannot be created because some values in `values` are not supported, returns `NotImplemented`.
-        """
-        raise NotImplementedError
-
-    def __expand__(self, dims: Shape, **kwargs) -> 'Shapable':
-        """
-        Adds new dimensions to this object.
-        The value of this object is constant along the new dimensions.
-
-        Args:
-            dims: Dimensions to add.
-                They are guaranteed to not already be present in `shape(self)`.
-            **kwargs: Additional keyword arguments required by specific implementations.
-                Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
-                Adding batch dimensions must always work without keyword arguments.
-
-        Returns:
-            New instance of `Shapable` or `NotImplemented` to revert to default behavior for this object.
-        """
-        raise NotImplementedError
-
-    def __replace_dims__(self, dims: Tuple[str, ...], new_dims: Shape, **kwargs) -> 'Shapable':
-        """
-        Exchange existing dimensions.
-        This can be used to rename dimensions, change dimension types or change item names.
-
-        Args:
-            dims: Dimensions to be replaced.
-            new_dims: Replacement dimensions as `Shape` with `rank == len(dims)`.
-            **kwargs: Additional keyword arguments required by specific implementations.
-                Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
-                Adding batch dimensions must always work without keyword arguments.
-
-        Returns:
-            New instance of `Shapable` or `NotImplemented` to revert to default behavior for this object.
-        """
-        raise NotImplementedError
-
-    def __pack_dims__(self, dims: Tuple[str, ...], packed_dim: Shape, pos: Union[int, None], **kwargs) -> 'Shapable':
-        """
-        Compresses multiple dimensions into a single dimension by concatenating the elements.
-        Elements along the new dimensions are laid out according to the order of `dims`.
-
-        The type of the new dimension will be equal to the types of `dims`.
-        If `dims` have varying types, the new dimension will be a batch dimension.
-
-        Args:
-            dims: Dimensions to be compressed in the specified order.
-            packed_dim: Single-dimension `Shape`.
-            pos: Index of new dimension. `None` for automatic, `-1` for last, `0` for first.
-            **kwargs: Additional keyword arguments required by specific implementations.
-                Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
-                Adding batch dimensions must always work without keyword arguments.
-
-        Returns:
-            New instance of `Shapable` or `NotImplemented` to revert to default behavior for this object.
-        """
-        raise NotImplementedError
-
-    def __unpack_dim__(self, dim: str, unpacked_dims: Shape, **kwargs) -> 'Shapable':
-        """
-        Decompresses a tensor dimension by unstacking the elements along it.
-        The compressed dimension `dim` is assumed to contain elements laid out according to the order of `unpacked_dims`.
-
-        Args:
-            dim: Dimension to be decompressed.
-            unpacked_dims: `Shape`: Ordered dimensions to replace `dim`, fulfilling `unpacked_dims.volume == shape(self)[dim].rank`.
-            **kwargs: Additional keyword arguments required by specific implementations.
-                Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
-                Adding batch dimensions must always work without keyword arguments.
-
-        Returns:
-            New instance of `Shapable` or `NotImplemented` to revert to default behavior for this object.
-        """
-        raise NotImplementedError
-
-    def __flatten__(self, flat_dim: Shape, flatten_batch: bool, **kwargs) -> 'Shapable':
-        """
-        Lays out all elements along a single dimension.
-        This is equivalent to packing all dimensions.
-
-        Args:
-            flat_dim: Single dimension as `Shape`.
-            flatten_batch: Whether to flatten batch dimensions as well.
-            If `False`, batch dimensions are kept, only onn-batch dimensions are flattened.
-            **kwargs: Additional keyword arguments required by specific implementations.
-                Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
-                Adding batch dimensions must always work without keyword arguments.
-
-        Returns:
-            New instance of `Shapable` or `NotImplemented` to revert to default behavior for this object.
-        """
-        raise NotImplementedError
-
-
-class _PhiTreeNodeType(type):
-
-    def __instancecheck__(self, instance):
-        from ._tensors import Tensor, MISSING_TENSOR, NATIVE_TENSOR, Dict
-        if isinstance(instance, Tensor):
-            return True
-        if instance is MISSING_TENSOR or instance is NATIVE_TENSOR:
-            return True
-        if instance is None or isinstance(instance, Tensor):
-            return True
-        elif isinstance(instance, (tuple, list)):
-            return all(isinstance(item, PhiTreeNode) for item in instance)
-        elif isinstance(instance, Dict):
-            return True
-        elif isinstance(instance, dict):
-            return all(isinstance(name, str) for name in instance.keys()) and all(isinstance(val, PhiTreeNode) for val in instance.values())
-        elif dataclasses.is_dataclass(instance):
-            return True
-        else:
-            return hasattr(instance, '__variable_attrs__') or hasattr(instance, '__value_attrs__')
-
-    def __subclasscheck__(self, subclass):
-        from ._tensors import Tensor, Dict
-        if issubclass(subclass, Tensor):
-            return True
-        if subclass in (tuple, list, dict):
-            return True
-        elif issubclass(subclass, Dict):
-            return True
-        elif dataclasses.is_dataclass(subclass):
-            return True
-        else:
-            return hasattr(subclass, '__variable_attrs__') or hasattr(subclass, '__value_attrs__')
-
-
-class PhiTreeNode(metaclass=_PhiTreeNodeType):
-    """
-    Φ-tree nodes can be iterated over and disassembled or flattened into elementary objects, such as tensors.
-    `phi.math.Tensor` instances as well as PyTree nodes (`tuple`, `list`, `dict` with `str` keys) are Φ-tree nodes.
-    All data classes are also considered PhiTreeNodes as of version 2.3.
-
-    For custom classes to be considered Φ-tree nodes, they have to be a dataclass or implement one of the following magic methods:
-
-    * `__variable_attrs__()`
-    * `__value_attrs__()`
-
-    Dataclasses may also implement these functions to specify which attributes should be considered value / variable properties.
-
-    Additionally, Φ-tree nodes must override `__eq__()` to allow comparison of data-stripped (key) instances.
-
-    To check whether an object is a Φ-tree node, use `isinstance(obj, PhiTreeNode)`.
-
-    **Usage in `phi.math`:**
-
-    Φ-tree nodes can be used as keys, for example in `jit_compile()`.
-    They are converted to keys by stripping all variable tensors and replacing them by a placeholder object.
-    In key mode, `__eq__()` compares all non-variable properties that might invalidate a trace when changed.
-
-    Disassembly and assembly of Φ-tree nodes uses `phi.math.copy_with` which will call `__with_attrs__` if implemented.
-    """
-
-    def __value_attrs__(self) -> Tuple[str, ...]:
-        """
-        Returns all `Tensor` or `PhiTreeNode` attribute names of `self` that should be transformed by single-operand math operations,
-        such as `sin()`, `exp()`.
-
-        Returns:
-            `tuple` of `str` attributes.
-                Calling `getattr(self, attr)` must return a `Tensor` or `PhiTreeNode` for all returned attributes.
-        """
-        raise NotImplementedError
-
-    def __variable_attrs__(self) -> Tuple[str, ...]:
-        """
-        Returns all `Tensor` or `PhiTreeNode` attribute names of `self` whose values are variable.
-        Variables denote values that can change from one function call to the next or for which gradients can be recorded.
-        If this method is not implemented, all attributes returned by `__value_attrs__()` are considered variable.
-
-        The returned properties are used by the following functions:
-
-        - `jit_compile()`
-        - `jit_compile_linear()`
-        - `stop_gradient()`
-        - `jacobian()`
-        - `custom_gradient()`
-
-        Returns:
-            `tuple` of `str` attributes.
-                Calling `getattr(self, attr)` must return a `Tensor` or `PhiTreeNode` for all returned attributes.
-        """
-        raise NotImplementedError
-
-    def __with_attrs__(self, **attrs):
-        """
-        Used by `phi.math.copy_with`.
-        Create a copy of this object which has the `Tensor` or `PhiTreeNode` attributes contained in `attrs` replaced.
-        If this method is not implemented, tensor attributes are replaced using `setattr()`.
-
-        Args:
-            **attrs: `dict` mapping `str` attribute names to `Tensor` or `PhiTreeNode`.
-
-        Returns:
-            Altered copy of `self`
-        """
-        raise NotImplementedError
-
-    def __eq__(self, other):
-        raise NotImplementedError
-
-
-
-class BoundDim:
-    """
-    Represents a dimension of a sliceable object to make slicing, renaming and retyping prettier.
-    Any instance of `BoundDim` is bound to the sliceable object and is immutable.
-    All operations upon the dim affect return a copy of the sliceable object.
-
-    `BoundDim` objects are generally created by and for objects that are `Sliceable` (and therefore also `Shaped`).
-    These objects should declare the following method to support the `.dim` syntax:
-
-    ```python
-    from phi.math.magic import BoundDim
-
-    class MyClass:
-
-        def __getattr__(self, name: str) -> BoundDim:
-            return BoundDim(self, name)
-    ```
-
-    **Usage**
-
-    * `obj.dim.size` returns the dimension size.
-    * `obj.dim.item_names` returns the dimension item names.
-    * `obj.dim.exists` checks whether a dimension is listed in the shape of the bound object.
-    * `obj.dim[0]` picks the first element along `dim`. The shape of the result will not contain `dim`.
-    * `obj.dim[1:-1]` discards the first and last element along `dim`.
-    * `obj.dim.rename('new_name')` renames `dim` to `new_name`.
-    * `obj.dim.as_channel()` changes the type of `dim` to *channel*.
-    * `obj.dim.unstack()` un-stacks the bound value along `dim`.
-    * `for slice in obj.dim` loops over all slices of `dim`.
-
-    Multiple dimensions can also be chained together using `obj.dim1.dim2...`.
-    This supports the following operations:
-
-    * `obj.dim1.dim2...volume` returns the product of the sizes
-    * `obj.dim1.dim2...[0, -1]` takes the first element along `dim1` and the last element along `dim2`
-    * `obj.dim1.dim2...pack(new_dim)` packs the dimensions into a new dimension with size equal to their volume
-    * `obj.dim1.dim2...unstack()` un-stacks `obj` along multiple dimensions
-    * `obj.dim1.dim2...retype(type)` Changes the type of all selected dimensions
-    * `for slice in obj.dim1.dim2...` loops over all slices as if unstacking first
-    """
-
-    def __init__(self, obj, name: str):
-        """
-        Args:
-            obj: `Sliceable` bound object.
-            name: Dimension name as `str`.
-        """
-        if name.startswith('_') or ',' in name or ' ' in name:
-            raise AttributeError(f"'{type(self)}' object has no attribute '{name}'")
-        if name == 'shape':
-            raise AttributeError(f"{type(obj)} has no shape")
-        assert isinstance(obj, Sliceable) and isinstance(obj, Shaped), f"Cannot create BoundDim for {type(obj).__name__}. Objects must be Sliceable and Shaped, see https://tum-pbs.github.io/PhiFlow/phi/math/magic.html"
-        self.obj = obj
-        self.name = name
-
-    @property
-    def dual(self):
-        return BoundDim(self.obj, '~' + self.name)
-
-    @property
-    def exists(self):
-        """ Whether the dimension is listed in the `Shape` of the object. """
-        return self.name in shape(self.obj)
-
-    def __repr__(self):
-        if self.name not in shape(self.obj):
-            return f"{type(self.obj).__name__}.{self.name} (non-existent)"
-        items = self.item_names
-        if items is not None:
-            if len(items) <= 4:
-                size_repr = ",".join(items)
-            else:
-                size_repr = f"{self.size}:{items[0]}..{items[-1]}"
-        else:
-            size_repr = self.size
-        from ._shape import TYPE_ABBR
-        return f"{type(self.obj).__name__}.{self.name}{TYPE_ABBR.get(self.type.__name__, '?')}={size_repr}"
-
-    @property
-    def size(self):
-        """ Length of this dimension as listed in the `Shape` of the bound object. """
-        return shape(self.obj).get_size(self.name) if self.exists else None
-
-    volume = size
-
-    @property
-    def size_or_1(self):
-        return shape(self.obj).get_size(self.name) if self.exists else 1
-
-    @property
-    def type(self) -> Callable:
-        """
-        The dimension type of this bound dimension. Must be one of `batch`, `spatial`, `instance`, `channel`.
-
-        Returns:
-
-        """
-        return shape(self.obj).get_dim_type(self.name)
-
-    @property
-    def item_names(self):
-        return shape(self.obj).get_item_names(self.name)
-
-    def __getitem__(self, item):
-        return self.obj[{self.name: item}]
-
-    def __setitem__(self, key, value):
-        self.obj[{self.name: key}] = value
-
-    def __getattr__(self, item):
-        return _BoundDims(self.obj, (self.name, item))
-
-    def unstack(self, size: Union[int, None] = None) -> tuple:
-        """
-        Lists the slices along this dimension as a `tuple`.
-
-        Args:
-            size: (optional) If given as `int`, this dimension can be unstacked even if it is not present on the object.
-                In that case, `size` copies of the object are returned.
-
-        Returns:
-            `tuple` of `Sliceable`
-        """
-        from ._magic_ops import unstack
-        if size is None:
-            return unstack(self.obj, self.name)
-        else:
-            if self.exists:
-                unstacked = unstack(self.obj, self.name)
-                assert len(unstacked) == size, f"Size of dimension {self.name} does not match {size}."
-                return unstacked
-            else:
-                return (self.obj,) * size
-
-    def __iter__(self):
-        """ Iterate over slices along this dim """
-        if self.exists:
-            return iter(self.unstack())
-        else:
-            return iter([self.obj])
-
-    def __call__(self, *args, **kwargs):
-        raise TypeError(f"Method {type(self.obj).__name__}.{self.name}() does not exist.")
-
-    def rename(self, name: str, **kwargs):
-        """
-        Returns a shallow copy of the `Tensor` where this dimension has the specified name.
-
-        See Also:
-            `phi.math.rename_dims()`
-        """
-        if not self.exists:
-            return self.obj
-        from ._magic_ops import rename_dims
-        return rename_dims(self.obj, self.name, name, **kwargs)
-
-    def retype(self, dim_type: Callable, **kwargs):
-        """
-        Returns a shallow copy of the `Tensor` where this dimension has the specified type.
-
-        See Also:
-            `phi.math.rename_dims()`
-        """
-        new_dim = dim_type(**{self.name: self.item_names or self.size})
-        from ._magic_ops import rename_dims
-        return rename_dims(self.obj, self.name, new_dim, **kwargs)
-
-    def as_batch(self, name: str = None):
-        """ Returns a shallow copy of the `Tensor` where the type of this dimension is *batch*. """
-        return self.retype(batch) if name is None else self.replace(batch(name=self.item_names or self.size))
-
-    def as_spatial(self, name: str = None):
-        """ Returns a shallow copy of the `Tensor` where the type of this dimension is *spatial*. """
-        return self.retype(spatial) if name is None else self.replace(spatial(name=self.item_names or self.size))
-
-    def as_channel(self, name: str = None):
-        """ Returns a shallow copy of the `Tensor` where the type of this dimension is *channel*. """
-        return self.retype(channel) if name is None else self.replace(channel(name=self.item_names or self.size))
-
-    def as_instance(self, name: str = None):
-        """ Returns a shallow copy of the `Tensor` where the type of this dimension is *instance*. """
-        return self.retype(instance) if name is None else self.replace(instance(name=self.item_names or self.size))
-
-    def as_dual(self, name: str = None):
-        """ Returns a shallow copy of the `Tensor` where the type of this dimension is *instance*. """
-        return self.retype(dual) if name is None else self.replace(dual(name=self.item_names or self.size))
-
-    def replace(self, dim: Shape, **kwargs):
-        """
-        Returns a shallow copy of the `Tensor` where this dimension has been replaced by `dim`.
-
-        See Also:
-            `phi.math.rename_dims()`
-        """
-        from ._magic_ops import rename_dims
-        return rename_dims(self.obj, self.name, dim, **kwargs)
-
-    def unpack(self, *dims: Shape, **kwargs):
-        """
-        Returns a shallow copy of the `Tensor` where this dimension has been unpacked into `dims`.
-
-        See Also:
-            `phi.math.unpack_dim()`
-        """
-        from ._magic_ops import unpack_dim
-        return unpack_dim(self.obj, self.name, *dims, **kwargs)
-
-
-class _BoundDims:
-
-    def __init__(self, obj, dims: Tuple[str, ...]):
-        self.obj = obj
-        self.dims = dims
-
-    @property
-    def dual(self):
-        last_dual = "~" + self.dims[-1]
-        return _BoundDims(self.obj, self.dims[:-1] + (last_dual,))
-
-    def __getitem__(self, item):
-        assert isinstance(item, tuple), f"A tuple of slices is required for slicing multiple dimensions at once but got {type(item)}"
-        assert len(item) == len(self.dims), f"Number of slices must equal number of dimensions but got {len(item)} for dims {self.dims}"
-        return self.obj[{dim: i for dim, i in zip(self.dims, item)}]
-
-    def __getattr__(self, item):
-        if len(self.dims) > 10:  # to avoid recursion limit
-            raise RuntimeError("Maximum BoundDim length reached")
-        return _BoundDims(self.obj, self.dims + (item,))
-
-    def __len__(self):
-        return self.volume
-
-    @property
-    def size(self):
-        raise SyntaxError("dim.size only exists for single dimensions. Use .volume for multiple dimensions")
-
-    @property
-    def volume(self):
-        return shape(self.obj).only(self.dims).volume
-
-    def pack(self, packed_dim: Shape, pos=None, **kwargs):
-        from ._magic_ops import pack_dims
-        return pack_dims(self.obj, self.dims, packed_dim, pos=pos, **kwargs)
-
-    def unstack(self) -> tuple:
-        from ._magic_ops import unstack
-        return unstack(self.obj, self.dims)
-
-    def __iter__(self):
-        """ Iterate over slices along this dim """
-        return iter(self.unstack())
-
-    def retype(self, dim_type: Callable, **kwargs):
-        """
-        Returns a shallow copy of the `Tensor` where this dimension has the specified type.
-
-        See Also:
-            `phi.math.rename_dims()`
-        """
-        s = shape(self.obj)
-        new_dims = concat_shapes(*[dim_type(**{dim: s.get_item_names(dim) or s.get_size(dim)}) for dim in self.dims])
-        from ._magic_ops import rename_dims
-        return rename_dims(self.obj, self.dims, new_dims, **kwargs)
-
-    def as_batch(self):
-        """ Returns a shallow copy of the `Tensor` where the type of this dimension is *batch*. """
-        return self.retype(batch)
-
-    def as_spatial(self):
-        """ Returns a shallow copy of the `Tensor` where the type of this dimension is *spatial*. """
-        return self.retype(spatial)
-
-    def as_channel(self):
-        """ Returns a shallow copy of the `Tensor` where the type of this dimension is *channel*. """
-        return self.retype(channel)
-
-    def as_instance(self):
-        """ Returns a shallow copy of the `Tensor` where the type of this dimension is *instance*. """
-        return self.retype(instance)
-
-    def as_dual(self):
-        """ Returns a shallow copy of the `Tensor` where the type of this dimension is *instance*. """
-        return self.retype(dual)
-
-
-def slicing_dict(obj, item) -> dict:
-    """
-    Creates a slicing `dict` from `item` where `item` is an arbitrary value passed to `__getitem__()`.
-
-    `Sliceable` objects should call this function inside `__getitem__()`, passing `self` and `item`.
-
-    Args:
-        obj: Object to be sliced.
-        item: Slices.
-
-    Returns:
-        `dict` mapping dimension names to slices.
-    """
-    if isinstance(item, dict):
-        assert all(isinstance(key, str) for key in item.keys()), f"All slice dimensions must be given as str but got keys {tuple(item.keys())}"
-        return item
-    if isinstance(item, tuple):
-        if item[0] == Ellipsis:
-            assert len(item) - 1 == shape(obj).channel_rank
-            return {name: selection for name, selection in zip(channel(obj).names, item[1:])}
-        elif len(item) == shape(obj).channel_rank:
-            if len(item) > 1:
-                warnings.warn("NumPy-style slicing for more than one channel dimension is highly discouraged. Use a dict or the special slicing syntax value.dim[slice] instead. See https://tum-pbs.github.io/PhiFlow/Math.html", SyntaxWarning, stacklevel=3)
-            return {name: selection for name, selection in zip(channel(obj).names, item)}
-        elif shape(obj).channel_rank == 1 and all(isinstance(e, str) for e in item):
-            return {channel(obj).name: item}
-        else:
-            raise AssertionError(f"Cannot slice {obj}[{item}]. Use a dict or the special slicing syntax value.dim[slice] instead. See https://tum-pbs.github.io/PhiFlow/Math.html")
-    else:
-        if shape(obj).channel_rank == 1:
-            return {channel(obj).name: item}
-        elif non_batch(obj).rank == 1:
-            return {non_batch(obj).name: item}
-        else:
-            raise AssertionError(f"Slicing {type(obj).__name__}[{type(item).__name__}] is only supported for 1D values (excluding batch dimensions) but shape is {shape(obj)}")
-
-
-class OtherMagicFunctions:
-
-    def __cast__(self, dtype: DType):
-        raise NotImplementedError
-
-
-__pdoc__ = {}  # Show all magic functions in pdoc3
-for cls_name, cls in dict(globals()).items():
-    if isinstance(cls, type) and type(cls) != type and not cls_name.startswith('_'):
-        for magic_function in dir(cls):
-            if magic_function.startswith('__') and magic_function.endswith('__') and not hasattr(object, magic_function) and magic_function != '__weakref__':
-                __pdoc__[f'{cls_name}.{magic_function}'] = True
+"""
+Magic methods allow custom classes to be compatible with various functions defined in `phi.math`, analogous to how implementing `__hash__` allows objects to be used with `hash()`.
+The magic methods are grouped into purely declarative classes (interfaces) by what functionality they provide.
+
+* `Shaped` objects have a `phi.math.Shape`.
+* `Sliceable` objects can be sliced along dimensions.
+* `Shapable` objects can additionally be reshaped.
+* `PhiTreeNode` objects can be disassembled into tensors.
+
+All of these magic classes declared here define a custom instance checks and should not be used as superclasses.
+
+An object implements one of the types defined here by implementing one or more of the related magic methods.
+Instance checks can be performed via `isinstance(obj, <MagicClass>)`.
+
+This is analogous to interfaces defined in the built-in `collections` package, such as `Sized, Iterable, Hashable, Callable`.
+To check whether `len(obj)` can be performed, you check `isinstance(obj, Sized)`.
+"""
+import warnings
+from typing import Tuple, Callable, Union
+
+import dataclasses
+
+from ._shape import Shape, shape, channel, non_batch, batch, spatial, instance, concat_shapes, dual
+from .backend._dtype import DType
+
+
+class _ShapedType(type):
+    def __instancecheck__(self, instance):
+        try:
+            shape(instance)
+            return True
+        except ValueError:
+            return False
+
+    def __subclasscheck__(self, subclass):
+        return True
+
+
+class Shaped(metaclass=_ShapedType):
+    """
+    To be considered shaped, an object must either implement the magic method `__shape__()` or have a valid `shape` property.
+    In either case, the returned shape must be an instance of `phi.math.Shape`.
+
+    To check whether an object is `Shaped`, use `isinstance(obj, Shaped)`.
+
+    **Usage in `phi.math`:**
+
+    The functions `phi.math.shape` as well as dimension filters, such as `phi.math.spatial` or `phi.math.non_batch` can be called on all shaped objects.
+
+    See Also:
+        `Sliceable`, `Shapable`
+    """
+
+    def __shape__(self) -> 'Shape':
+        """
+        Returns the shape of this object.
+
+        Alternatively, the shape can be declared via the property `shape`.
+
+        Returns:
+            `phi.math.Shape`
+        """
+        raise NotImplementedError
+
+    @property
+    def shape(self) -> 'Shape':
+        """
+        Alternative form of `__shape__()`.
+        Implement either to be considered `Shaped`.
+
+        Returns:
+            `phi.math.Shape`
+        """
+        raise NotImplementedError
+
+
+class _SliceableType(type):
+    def __instancecheck__(self, instance):
+        if isinstance(instance, str):
+            return False
+        return isinstance(instance, Shaped) and hasattr(instance, '__getitem__')
+
+    def __subclasscheck__(self, subclass):
+        if subclass == str:
+            return False
+        return hasattr(subclass, '__getitem__')
+
+
+class Sliceable(metaclass=_SliceableType):
+    """
+    Objects are considered sliceable if they are `Shaped` and implement `__getitem__` as defined below.
+
+    To enable the slicing syntax `obj.dim[slice]`, implement the `__getattr__` method as defined below.
+
+    Classes implementing `Sliceable` should override `__getattr__` to enable the special slicing syntax defined in `BoundDim`.
+
+    **Usage in `phi.math`:**
+
+    In addition to slicing, sliceable objects can be unstacked along one or multiple dimensions using `phi.math.unstack`.
+
+    See Also
+        `Shapable`, `Shaped`
+    """
+
+    def __getitem__(self, item) -> 'Sliceable':
+        """
+        Slice this object along one or multiple existing or non-existing dimensions.
+
+        When overriding this function, make sure to first call `slicing_dict(self, item)` to sort slices by dimension.
+
+        Args:
+            item: `dict` mapping dimension names to the corresponding selections.
+                Selections can be slices, indices, tuples, item names, bool tensors, int tensors or other custom types.
+                All Sliceable object must support indexing by `int`, `slice`, `tuple`, `list`, `str`.
+
+        Returns:
+            Instance of the same class (or a compatible class) as `self`.
+        """
+        raise NotImplementedError
+
+    def __unstack__(self, dims: Tuple[str, ...]) -> Tuple['Sliceable', ...]:
+        """
+        Un-stack this object along one or multiple dimensions.
+        Un-stacking along multiple dimensions is equal to first packing the dimensions and then unstacking along the packed dimension.
+
+        Implementing this magic method is optional but the default implementation may be slow.
+
+        Args:
+            dims: Ordered `tuple` of dimension names along which to unstack this object.
+
+        Returns:
+            `tuple` of slices along `dims` or `NotImplemented` to revert to default behavior for this object.
+        """
+        raise NotImplementedError
+
+
+class _ShapableType(type):
+    def __instancecheck__(self, instance):
+        return isinstance(instance, Sliceable) and isinstance(instance, Shaped) and\
+               (hasattr(instance, '__stack__') or (hasattr(instance, '__concat__') and hasattr(instance, '__expand__')) or isinstance(instance, PhiTreeNode))
+
+    def __subclasscheck__(self, subclass):
+        return issubclass(subclass, Sliceable) and\
+               (hasattr(subclass, '__stack__') or (hasattr(subclass, '__concat__') and hasattr(subclass, '__expand__')) or issubclass(subclass, PhiTreeNode))
+
+
+class Shapable(metaclass=_ShapableType):
+    """
+    Shapable objects can be stacked, concatenated and reshaped.
+
+    To be considered `Shapable`, objects must be `Sliceable` and `Shaped` and implement
+
+    * `__stack__()` or
+    * `__concat__()` and `__expand__()`.
+
+    Objects should additionally implement the other magic methods for performance reasons.
+
+    **Usage in `phi.math`:**
+
+    Shapable objects can be used with the following functions in addition to what they inherit from being `Sliceable` and `Shaped`:
+
+    * `phi.math.stack`
+    * `phi.math.concat`
+    * `phi.math.expand`
+    * `phi.math.rename_dims`
+    * `phi.math.pack_dims`
+    * `phi.math.unpack_dim`
+    * `phi.math.flatten`
+
+    Additionally, the `phi.math.BoundDim` syntax for dimension renaming and retyping is enabled, e.g. `obj.dim.as_channel('vector')`.
+    """
+    @staticmethod
+    def __stack__(values: tuple, dim: Shape, **kwargs) -> 'Shapable':
+        """
+        Stack all `values` into a single instance along the new dimension `dim`.
+
+        This method can be implemented as a bound method or as a `staticmethod` (without the `self` argument).
+
+        Args:
+            values: `tuple` of `Shapable` objects to be stacked. `self` is included in that list at least once.
+            dim: Single-dimension `Shape`. This dimension must not be present with any of the `values`.
+                The dimension fulfills the condition `dim.size == len(values)`.
+            **kwargs: Additional keyword arguments required by specific implementations.
+                Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
+                Adding batch dimensions must always work without keyword arguments.
+
+        Returns:
+            New instance of `Shapable` representing the stacked slices.
+            Its shape includes `dim` in addition to the dimensions present in `values`.
+            If such a representation cannot be created because some values in `values` are not supported, returns `NotImplemented`.
+        """
+        raise NotImplementedError
+
+    @staticmethod
+    def __concat__(values: tuple, dim: str, **kwargs) -> 'Shapable':
+        """
+        Concatenate `values` along `dim`.
+
+        This method can be implemented as a bound method or as a `staticmethod` (without the `self` argument).
+
+        Args:
+            values: Values to concatenate. `self` is included in that list at least once.
+            dim: Dimension nams as `str`, must be present in all `values`.
+            **kwargs: Additional keyword arguments required by specific implementations.
+                Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
+                Adding batch dimensions must always work without keyword arguments.
+
+        Returns:
+            New instance of `Shapable` representing the concatenated values or `NotImplemented` to revert to default behavior for this object.
+            When returning a valid object, the size of `dim` must be equal to the sum of all `dim` sizes in `values`.
+            If such a representation cannot be created because some values in `values` are not supported, returns `NotImplemented`.
+        """
+        raise NotImplementedError
+
+    def __expand__(self, dims: Shape, **kwargs) -> 'Shapable':
+        """
+        Adds new dimensions to this object.
+        The value of this object is constant along the new dimensions.
+
+        Args:
+            dims: Dimensions to add.
+                They are guaranteed to not already be present in `shape(self)`.
+            **kwargs: Additional keyword arguments required by specific implementations.
+                Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
+                Adding batch dimensions must always work without keyword arguments.
+
+        Returns:
+            New instance of `Shapable` or `NotImplemented` to revert to default behavior for this object.
+        """
+        raise NotImplementedError
+
+    def __replace_dims__(self, dims: Tuple[str, ...], new_dims: Shape, **kwargs) -> 'Shapable':
+        """
+        Exchange existing dimensions.
+        This can be used to rename dimensions, change dimension types or change item names.
+
+        Args:
+            dims: Dimensions to be replaced.
+            new_dims: Replacement dimensions as `Shape` with `rank == len(dims)`.
+            **kwargs: Additional keyword arguments required by specific implementations.
+                Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
+                Adding batch dimensions must always work without keyword arguments.
+
+        Returns:
+            New instance of `Shapable` or `NotImplemented` to revert to default behavior for this object.
+        """
+        raise NotImplementedError
+
+    def __pack_dims__(self, dims: Tuple[str, ...], packed_dim: Shape, pos: Union[int, None], **kwargs) -> 'Shapable':
+        """
+        Compresses multiple dimensions into a single dimension by concatenating the elements.
+        Elements along the new dimensions are laid out according to the order of `dims`.
+
+        The type of the new dimension will be equal to the types of `dims`.
+        If `dims` have varying types, the new dimension will be a batch dimension.
+
+        Args:
+            dims: Dimensions to be compressed in the specified order.
+            packed_dim: Single-dimension `Shape`.
+            pos: Index of new dimension. `None` for automatic, `-1` for last, `0` for first.
+            **kwargs: Additional keyword arguments required by specific implementations.
+                Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
+                Adding batch dimensions must always work without keyword arguments.
+
+        Returns:
+            New instance of `Shapable` or `NotImplemented` to revert to default behavior for this object.
+        """
+        raise NotImplementedError
+
+    def __unpack_dim__(self, dim: str, unpacked_dims: Shape, **kwargs) -> 'Shapable':
+        """
+        Decompresses a tensor dimension by unstacking the elements along it.
+        The compressed dimension `dim` is assumed to contain elements laid out according to the order of `unpacked_dims`.
+
+        Args:
+            dim: Dimension to be decompressed.
+            unpacked_dims: `Shape`: Ordered dimensions to replace `dim`, fulfilling `unpacked_dims.volume == shape(self)[dim].rank`.
+            **kwargs: Additional keyword arguments required by specific implementations.
+                Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
+                Adding batch dimensions must always work without keyword arguments.
+
+        Returns:
+            New instance of `Shapable` or `NotImplemented` to revert to default behavior for this object.
+        """
+        raise NotImplementedError
+
+    def __flatten__(self, flat_dim: Shape, flatten_batch: bool, **kwargs) -> 'Shapable':
+        """
+        Lays out all elements along a single dimension.
+        This is equivalent to packing all dimensions.
+
+        Args:
+            flat_dim: Single dimension as `Shape`.
+            flatten_batch: Whether to flatten batch dimensions as well.
+            If `False`, batch dimensions are kept, only onn-batch dimensions are flattened.
+            **kwargs: Additional keyword arguments required by specific implementations.
+                Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
+                Adding batch dimensions must always work without keyword arguments.
+
+        Returns:
+            New instance of `Shapable` or `NotImplemented` to revert to default behavior for this object.
+        """
+        raise NotImplementedError
+
+
+class _PhiTreeNodeType(type):
+
+    def __instancecheck__(self, instance):
+        from ._tensors import Tensor, MISSING_TENSOR, NATIVE_TENSOR, Dict
+        if isinstance(instance, Tensor):
+            return True
+        if instance is MISSING_TENSOR or instance is NATIVE_TENSOR:
+            return True
+        if instance is None or isinstance(instance, Tensor):
+            return True
+        elif isinstance(instance, (tuple, list)):
+            return all(isinstance(item, PhiTreeNode) for item in instance)
+        elif isinstance(instance, Dict):
+            return True
+        elif isinstance(instance, dict):
+            return all(isinstance(name, str) for name in instance.keys()) and all(isinstance(val, (Shaped, PhiTreeNode)) for val in instance.values())
+        elif dataclasses.is_dataclass(instance):
+            return True
+        else:
+            return hasattr(instance, '__variable_attrs__') or hasattr(instance, '__value_attrs__')
+
+    def __subclasscheck__(self, subclass):
+        from ._tensors import Tensor, Dict
+        if issubclass(subclass, Tensor):
+            return True
+        if subclass in (tuple, list, dict):
+            return True
+        elif issubclass(subclass, Dict):
+            return True
+        elif dataclasses.is_dataclass(subclass):
+            return True
+        else:
+            return hasattr(subclass, '__variable_attrs__') or hasattr(subclass, '__value_attrs__')
+
+
+class PhiTreeNode(metaclass=_PhiTreeNodeType):
+    """
+    Φ-tree nodes can be iterated over and disassembled or flattened into elementary objects, such as tensors.
+    `phi.math.Tensor` instances as well as PyTree nodes (`tuple`, `list`, `dict` with `str` keys) are Φ-tree nodes.
+    All data classes are also considered PhiTreeNodes as of version 2.3.
+
+    For custom classes to be considered Φ-tree nodes, they have to be a dataclass or implement one of the following magic methods:
+
+    * `__variable_attrs__()`
+    * `__value_attrs__()`
+
+    Dataclasses may also implement these functions to specify which attributes should be considered value / variable properties.
+
+    Additionally, Φ-tree nodes must override `__eq__()` to allow comparison of data-stripped (key) instances.
+
+    To check whether an object is a Φ-tree node, use `isinstance(obj, PhiTreeNode)`.
+
+    **Usage in `phi.math`:**
+
+    Φ-tree nodes can be used as keys, for example in `jit_compile()`.
+    They are converted to keys by stripping all variable tensors and replacing them by a placeholder object.
+    In key mode, `__eq__()` compares all non-variable properties that might invalidate a trace when changed.
+
+    Disassembly and assembly of Φ-tree nodes uses `phi.math.copy_with` which will call `__with_attrs__` if implemented.
+    """
+
+    def __value_attrs__(self) -> Tuple[str, ...]:
+        """
+        Returns all `Tensor` or `PhiTreeNode` attribute names of `self` that should be transformed by single-operand math operations,
+        such as `sin()`, `exp()`.
+
+        Returns:
+            `tuple` of `str` attributes.
+                Calling `getattr(self, attr)` must return a `Tensor` or `PhiTreeNode` for all returned attributes.
+        """
+        raise NotImplementedError
+
+    def __variable_attrs__(self) -> Tuple[str, ...]:
+        """
+        Returns all `Tensor` or `PhiTreeNode` attribute names of `self` whose values are variable.
+        Variables denote values that can change from one function call to the next or for which gradients can be recorded.
+        If this method is not implemented, all attributes returned by `__value_attrs__()` are considered variable.
+
+        The returned properties are used by the following functions:
+
+        - `jit_compile()`
+        - `jit_compile_linear()`
+        - `stop_gradient()`
+        - `jacobian()`
+        - `custom_gradient()`
+
+        Returns:
+            `tuple` of `str` attributes.
+                Calling `getattr(self, attr)` must return a `Tensor` or `PhiTreeNode` for all returned attributes.
+        """
+        raise NotImplementedError
+
+    def __with_attrs__(self, **attrs):
+        """
+        Used by `phi.math.copy_with`.
+        Create a copy of this object which has the `Tensor` or `PhiTreeNode` attributes contained in `attrs` replaced.
+        If this method is not implemented, tensor attributes are replaced using `setattr()`.
+
+        Args:
+            **attrs: `dict` mapping `str` attribute names to `Tensor` or `PhiTreeNode`.
+
+        Returns:
+            Altered copy of `self`
+        """
+        raise NotImplementedError
+
+    def __eq__(self, other):
+        raise NotImplementedError
+
+
+
+class BoundDim:
+    """
+    Represents a dimension of a sliceable object to make slicing, renaming and retyping prettier.
+    Any instance of `BoundDim` is bound to the sliceable object and is immutable.
+    All operations upon the dim affect return a copy of the sliceable object.
+
+    `BoundDim` objects are generally created by and for objects that are `Sliceable` (and therefore also `Shaped`).
+    These objects should declare the following method to support the `.dim` syntax:
+
+    ```python
+    from phi.math.magic import BoundDim
+
+    class MyClass:
+
+        def __getattr__(self, name: str) -> BoundDim:
+            return BoundDim(self, name)
+    ```
+
+    **Usage**
+
+    * `obj.dim.size` returns the dimension size.
+    * `obj.dim.item_names` returns the dimension item names.
+    * `obj.dim.exists` checks whether a dimension is listed in the shape of the bound object.
+    * `obj.dim[0]` picks the first element along `dim`. The shape of the result will not contain `dim`.
+    * `obj.dim[1:-1]` discards the first and last element along `dim`.
+    * `obj.dim.rename('new_name')` renames `dim` to `new_name`.
+    * `obj.dim.as_channel()` changes the type of `dim` to *channel*.
+    * `obj.dim.unstack()` un-stacks the bound value along `dim`.
+    * `for slice in obj.dim` loops over all slices of `dim`.
+
+    Multiple dimensions can also be chained together using `obj.dim1.dim2...`.
+    This supports the following operations:
+
+    * `obj.dim1.dim2...volume` returns the product of the sizes
+    * `obj.dim1.dim2...[0, -1]` takes the first element along `dim1` and the last element along `dim2`
+    * `obj.dim1.dim2...pack(new_dim)` packs the dimensions into a new dimension with size equal to their volume
+    * `obj.dim1.dim2...unstack()` un-stacks `obj` along multiple dimensions
+    * `obj.dim1.dim2...retype(type)` Changes the type of all selected dimensions
+    * `for slice in obj.dim1.dim2...` loops over all slices as if unstacking first
+    """
+
+    def __init__(self, obj, name: str):
+        """
+        Args:
+            obj: `Sliceable` bound object.
+            name: Dimension name as `str`.
+        """
+        if name.startswith('_') or ',' in name or ' ' in name:
+            raise AttributeError(f"'{type(self)}' object has no attribute '{name}'")
+        if name == 'shape':
+            raise AttributeError(f"{type(obj)} has no shape")
+        assert isinstance(obj, Sliceable) and isinstance(obj, Shaped), f"Cannot create BoundDim for {type(obj).__name__}. Objects must be Sliceable and Shaped, see https://tum-pbs.github.io/PhiFlow/phi/math/magic.html"
+        self.obj = obj
+        self.name = name
+
+    @property
+    def dual(self):
+        return BoundDim(self.obj, '~' + self.name)
+
+    @property
+    def exists(self):
+        """ Whether the dimension is listed in the `Shape` of the object. """
+        return self.name in shape(self.obj)
+
+    def __repr__(self):
+        if self.name not in shape(self.obj):
+            return f"{type(self.obj).__name__}.{self.name} (non-existent)"
+        items = self.item_names
+        if items is not None:
+            if len(items) <= 4:
+                size_repr = ",".join(items)
+            else:
+                size_repr = f"{self.size}:{items[0]}..{items[-1]}"
+        else:
+            size_repr = self.size
+        from ._shape import TYPE_ABBR
+        return f"{type(self.obj).__name__}.{self.name}{TYPE_ABBR.get(self.type.__name__, '?')}={size_repr}"
+
+    @property
+    def size(self):
+        """ Length of this dimension as listed in the `Shape` of the bound object. """
+        return shape(self.obj).get_size(self.name) if self.exists else None
+
+    volume = size
+
+    @property
+    def size_or_1(self):
+        return shape(self.obj).get_size(self.name) if self.exists else 1
+
+    @property
+    def type(self) -> Callable:
+        """
+        The dimension type of this bound dimension. Must be one of `batch`, `spatial`, `instance`, `channel`.
+
+        Returns:
+
+        """
+        return shape(self.obj).get_dim_type(self.name)
+
+    @property
+    def item_names(self):
+        return shape(self.obj).get_item_names(self.name)
+
+    def __getitem__(self, item):
+        return self.obj[{self.name: item}]
+
+    def __setitem__(self, key, value):
+        self.obj[{self.name: key}] = value
+
+    def __getattr__(self, item):
+        return _BoundDims(self.obj, (self.name, item))
+
+    def keys(self):
+        """
+        Allows unstacking with item names as `dict(**obj.dim)`.
+
+        Returns:
+            Sequence of item names or indices.
+        """
+        if not self.exists:
+            raise SyntaxError(f"Cannot get keys of nonexistent dimension {self}")
+        if self.item_names is not None:
+            return self.item_names
+        else:
+            return range(self.size)
+
+    def unstack(self, size: Union[int, None] = None) -> tuple:
+        """
+        Lists the slices along this dimension as a `tuple`.
+
+        Args:
+            size: (optional) If given as `int`, this dimension can be unstacked even if it is not present on the object.
+                In that case, `size` copies of the object are returned.
+
+        Returns:
+            `tuple` of `Sliceable`
+        """
+        from ._magic_ops import unstack
+        if size is None:
+            return unstack(self.obj, self.name)
+        else:
+            if self.exists:
+                unstacked = unstack(self.obj, self.name)
+                assert len(unstacked) == size, f"Size of dimension {self.name} does not match {size}."
+                return unstacked
+            else:
+                return (self.obj,) * size
+
+    def __iter__(self):
+        """ Iterate over slices along this dim """
+        if self.exists:
+            return iter(self.unstack())
+        else:
+            return iter([self.obj])
+
+    def __call__(self, *args, **kwargs):
+        raise TypeError(f"Method {type(self.obj).__name__}.{self.name}() does not exist.")
+
+    def rename(self, name: str, **kwargs):
+        """
+        Returns a shallow copy of the `Tensor` where this dimension has the specified name.
+
+        See Also:
+            `phi.math.rename_dims()`
+        """
+        if not self.exists:
+            return self.obj
+        from ._magic_ops import rename_dims
+        return rename_dims(self.obj, self.name, name, **kwargs)
+
+    def retype(self, dim_type: Callable, **kwargs):
+        """
+        Returns a shallow copy of the `Tensor` where this dimension has the specified type.
+
+        See Also:
+            `phi.math.rename_dims()`
+        """
+        new_dim = dim_type(**{self.name: self.item_names or self.size})
+        from ._magic_ops import rename_dims
+        return rename_dims(self.obj, self.name, new_dim, **kwargs)
+
+    def as_batch(self, name: str = None):
+        """ Returns a shallow copy of the `Tensor` where the type of this dimension is *batch*. """
+        return self.retype(batch) if name is None else self.replace(batch(name=self.item_names or self.size))
+
+    def as_spatial(self, name: str = None):
+        """ Returns a shallow copy of the `Tensor` where the type of this dimension is *spatial*. """
+        return self.retype(spatial) if name is None else self.replace(spatial(name=self.item_names or self.size))
+
+    def as_channel(self, name: str = None):
+        """ Returns a shallow copy of the `Tensor` where the type of this dimension is *channel*. """
+        return self.retype(channel) if name is None else self.replace(channel(name=self.item_names or self.size))
+
+    def as_instance(self, name: str = None):
+        """ Returns a shallow copy of the `Tensor` where the type of this dimension is *instance*. """
+        return self.retype(instance) if name is None else self.replace(instance(name=self.item_names or self.size))
+
+    def as_dual(self, name: str = None):
+        """ Returns a shallow copy of the `Tensor` where the type of this dimension is *instance*. """
+        return self.retype(dual) if name is None else self.replace(dual(name=self.item_names or self.size))
+
+    def replace(self, dim: Shape, **kwargs):
+        """
+        Returns a shallow copy of the `Tensor` where this dimension has been replaced by `dim`.
+
+        See Also:
+            `phi.math.rename_dims()`
+        """
+        from ._magic_ops import rename_dims
+        return rename_dims(self.obj, self.name, dim, **kwargs)
+
+    def unpack(self, *dims: Shape, **kwargs):
+        """
+        Returns a shallow copy of the `Tensor` where this dimension has been unpacked into `dims`.
+
+        See Also:
+            `phi.math.unpack_dim()`
+        """
+        from ._magic_ops import unpack_dim
+        return unpack_dim(self.obj, self.name, *dims, **kwargs)
+
+    def __bool__(self):
+        raise SyntaxError(f"{self} cannot be converted to bool. The property you want to access likely does not exist.")
+
+
+class _BoundDims:
+
+    def __init__(self, obj, dims: Tuple[str, ...]):
+        self.obj = obj
+        self.dims = dims
+
+    @property
+    def dual(self):
+        last_dual = "~" + self.dims[-1]
+        return _BoundDims(self.obj, self.dims[:-1] + (last_dual,))
+
+    def __getitem__(self, item):
+        assert isinstance(item, tuple), f"A tuple of slices is required for slicing multiple dimensions at once but got {type(item)}"
+        assert len(item) == len(self.dims), f"Number of slices must equal number of dimensions but got {len(item)} for dims {self.dims}"
+        return self.obj[{dim: i for dim, i in zip(self.dims, item)}]
+
+    def __getattr__(self, item):
+        if len(self.dims) > 10:  # to avoid recursion limit
+            raise RuntimeError("Maximum BoundDim length reached")
+        return _BoundDims(self.obj, self.dims + (item,))
+
+    def __len__(self):
+        return self.volume
+
+    @property
+    def size(self):
+        raise SyntaxError("dim.size only exists for single dimensions. Use .volume for multiple dimensions")
+
+    @property
+    def volume(self):
+        return shape(self.obj).only(self.dims).volume
+
+    def pack(self, packed_dim: Shape, pos=None, **kwargs):
+        from ._magic_ops import pack_dims
+        return pack_dims(self.obj, self.dims, packed_dim, pos=pos, **kwargs)
+
+    def unstack(self) -> tuple:
+        from ._magic_ops import unstack
+        return unstack(self.obj, self.dims)
+
+    def __iter__(self):
+        """ Iterate over slices along this dim """
+        return iter(self.unstack())
+
+    def __call__(self, *args, **kwargs):
+        raise TypeError(f"Method {type(self.obj).__name__}.{self.name}() does not exist.")
+
+    def __repr__(self):
+        parts = [type(self.obj).__name__, *self.dims]
+        return ".".join(parts)
+
+    def retype(self, dim_type: Callable, **kwargs):
+        """
+        Returns a shallow copy of the `Tensor` where this dimension has the specified type.
+
+        See Also:
+            `phi.math.rename_dims()`
+        """
+        s = shape(self.obj)
+        new_dims = concat_shapes(*[dim_type(**{dim: s.get_item_names(dim) or s.get_size(dim)}) for dim in self.dims])
+        from ._magic_ops import rename_dims
+        return rename_dims(self.obj, self.dims, new_dims, **kwargs)
+
+    def as_batch(self):
+        """ Returns a shallow copy of the `Tensor` where the type of this dimension is *batch*. """
+        return self.retype(batch)
+
+    def as_spatial(self):
+        """ Returns a shallow copy of the `Tensor` where the type of this dimension is *spatial*. """
+        return self.retype(spatial)
+
+    def as_channel(self):
+        """ Returns a shallow copy of the `Tensor` where the type of this dimension is *channel*. """
+        return self.retype(channel)
+
+    def as_instance(self):
+        """ Returns a shallow copy of the `Tensor` where the type of this dimension is *instance*. """
+        return self.retype(instance)
+
+    def as_dual(self):
+        """ Returns a shallow copy of the `Tensor` where the type of this dimension is *instance*. """
+        return self.retype(dual)
+
+    def __bool__(self):
+        raise SyntaxError(f"{self} cannot be converted to bool. The property you want to access likely does not exist.")
+
+
+def slicing_dict(obj, item) -> dict:
+    """
+    Creates a slicing `dict` from `item` where `item` is an arbitrary value passed to `__getitem__()`.
+
+    `Sliceable` objects should call this function inside `__getitem__()`, passing `self` and `item`.
+
+    Args:
+        obj: Object to be sliced.
+        item: Slices.
+
+    Returns:
+        `dict` mapping dimension names to slices.
+    """
+    if isinstance(item, dict):
+        assert all(isinstance(key, str) for key in item.keys()), f"All slice dimensions must be given as str but got keys {tuple(item.keys())}"
+        return item
+    if isinstance(item, tuple):
+        if item[0] == Ellipsis:
+            assert len(item) - 1 == shape(obj).channel_rank
+            return {name: selection for name, selection in zip(channel(obj).names, item[1:])}
+        elif len(item) == shape(obj).channel_rank:
+            if len(item) > 1:
+                warnings.warn("NumPy-style slicing for more than one channel dimension is highly discouraged. Use a dict or the special slicing syntax value.dim[slice] instead. See https://tum-pbs.github.io/PhiFlow/Math.html", SyntaxWarning, stacklevel=3)
+            return {name: selection for name, selection in zip(channel(obj).names, item)}
+        elif shape(obj).channel_rank == 1 and all(isinstance(e, str) for e in item):
+            return {channel(obj).name: item}
+        else:
+            raise AssertionError(f"Cannot slice {obj}[{item}]. Use a dict or the special slicing syntax value.dim[slice] instead. See https://tum-pbs.github.io/PhiFlow/Math.html")
+    else:
+        if shape(obj).channel_rank == 1:
+            return {channel(obj).name: item}
+        elif non_batch(obj).rank == 1:
+            return {non_batch(obj).name: item}
+        else:
+            from ._tensors import Tensor
+            class_name = "Tensor" if isinstance(obj, Tensor) else type(obj).__name__
+            raise AssertionError(f"Slicing {class_name}[{type(item).__name__}] is only supported for 1D values (excluding batch dimensions) but shape is {shape(obj)}")
+
+
+class OtherMagicFunctions:
+
+    def __cast__(self, dtype: DType):
+        raise NotImplementedError
+
+
+__pdoc__ = {}  # Show all magic functions in pdoc3
+for cls_name, cls in dict(globals()).items():
+    if isinstance(cls, type) and type(cls) != type and not cls_name.startswith('_'):
+        for magic_function in dir(cls):
+            if magic_function.startswith('__') and magic_function.endswith('__') and not hasattr(object, magic_function) and magic_function != '__weakref__':
+                __pdoc__[f'{cls_name}.{magic_function}'] = True
```

### Comparing `phiflow-2.3.4/phi/physics/advect.py` & `phiflow-2.4.0/phi/physics/advect.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,197 +1,199 @@
-"""
-Container for different advection schemes for grids and particles.
-
-Examples:
-
-* semi_lagrangian (grid)
-* mac_cormack (grid)
-* runge_kutta_4 (particle)
-"""
-from typing import Union
-
-from phi.math import Solve, channel
-
-from phi import math
-from phi.field import SampledField, Field, PointCloud, Grid, sample, reduce_sample, spatial_gradient, unstack, stack, CenteredGrid, StaggeredGrid
-from phi.field._field import FieldType
-from phi.field._field_math import GridType
-from phi.geom import Geometry
-
-
-def euler(elements: Geometry, velocity: Field, dt: float, v0: math.Tensor = None) -> Geometry:
-    """ Euler integrator. """
-    if v0 is None:
-        v0 = sample(velocity, elements)
-    return elements.shifted(v0 * dt)
-
-
-def rk4(elements: Geometry, velocity: Field, dt: float, v0: math.Tensor = None) -> Geometry:
-    """ Runge-Kutta-4 integrator. """
-    if v0 is None:
-        v0 = sample(velocity, elements)
-    vel_half = sample(velocity, elements.shifted(0.5 * dt * v0))
-    vel_half2 = sample(velocity, elements.shifted(0.5 * dt * vel_half))
-    vel_full = sample(velocity, elements.shifted(dt * vel_half2))
-    vel_rk4 = (1 / 6.) * (v0 + 2 * (vel_half + vel_half2) + vel_full)
-    return elements.shifted(dt * vel_rk4)
-
-
-def finite_rk4(elements: Geometry, velocity: Grid, dt: float, v0: math.Tensor = None) -> Geometry:
-    """ Runge-Kutta-4 integrator with Euler fallback where velocity values are NaN. """
-    v0 = sample(velocity, elements)
-    vel_half = sample(velocity, elements.shifted(0.5 * dt * v0))
-    vel_half2 = sample(velocity, elements.shifted(0.5 * dt * vel_half))
-    vel_full = sample(velocity, elements.shifted(dt * vel_half2))
-    vel_rk4 = (1 / 6.) * (v0 + 2 * (vel_half + vel_half2) + vel_full)
-    vel_nan = math.where(math.is_finite(vel_rk4), vel_rk4, v0)
-    return elements.shifted(dt * vel_nan)
-
-
-
-def advect(field: SampledField,
-           velocity: Field,
-           dt: Union[float, math.Tensor],
-           integrator=euler) -> FieldType:
-    """
-    Advect `field` along the `velocity` vectors using the specified integrator.
-
-    The behavior depends on the type of `field`:
-
-    * `phi.field.PointCloud`: Points are advected forward, see `points`.
-    * `phi.field.Grid`: Sample points are traced backward, see `semi_lagrangian`.
-
-    Args:
-        field: Field to be advected as `phi.field.SampledField`.
-        velocity: Any `phi.field.Field` that can be sampled in the elements of `field`.
-        dt: Time increment
-        integrator: ODE integrator for solving the movement.
-
-    Returns:
-        Advected field of same type as `field`
-    """
-    if isinstance(field, PointCloud):
-        return points(field, velocity, dt=dt, integrator=integrator)
-    elif isinstance(field, Grid):
-        return semi_lagrangian(field, velocity, dt=dt, integrator=integrator)
-    raise NotImplementedError(field)
-
-
-def finite_difference(grid: Grid,
-                      velocity: Field,
-                      order=2,
-                      implicit: Solve = None) -> Field:
-
-    """
-    Finite difference advection using the differentiation Scheme indicated by `scheme` and a simple Euler step
-
-    Args:
-        grid: Grid to be advected
-        velocity: `Grid` that can be sampled in the elements of `grid`.
-        order: Spatial order of accuracy.
-            Higher orders entail larger stencils and more computation time but result in more accurate results assuming a large enough resolution.
-            Supported: 2 explicit, 4 explicit, 6 implicit (inherited from `phi.field.spatial_gradient()` and resampling).
-            Passing order=4 currently uses 2nd-order resampling. This is work-in-progress.
-        implicit: When a `Solve` object is passed, performs an implicit operation with the specified solver and tolerances.
-            Otherwise, an explicit stencil is used.
-
-    Returns:
-        Advected grid of same type as `grid`
-    """
-    if isinstance(grid, StaggeredGrid):
-        grad_list = [spatial_gradient(field_component, stack_dim=channel('gradient'), order=order, implicit=implicit) for field_component in grid.vector]
-        grad_grid = grid.with_values(math.stack([component.values for component in grad_list], channel('vector')))
-        if order == 4:
-            amounts = [grad * vel.at(grad, order=2) for grad, vel in zip(grad_grid.gradient, velocity.vector)]  # ToDo resampling does not yet support order=4
-        else:
-            amounts = [grad * vel.at(grad, order=order, implicit=implicit) for grad, vel in zip(grad_grid.gradient, velocity.vector)]
-        amount = sum(amounts)
-    else:
-        assert isinstance(grid, CenteredGrid), f"grid must be CenteredGrid or StaggeredGrid but got {type(grid)}"
-        grad = spatial_gradient(grid, stack_dim=channel('gradient'), order=order, implicit=implicit)
-        velocity = stack(unstack(velocity, dim='vector'), dim=channel('gradient'))
-        amounts = velocity * grad
-        amount = sum(amounts.gradient)
-    return - amount
-
-
-def points(field: PointCloud, velocity: Field, dt: float, integrator=euler):
-    """
-    Advects the sample points of a point cloud using a simple Euler step.
-    Each point moves by an amount equal to the local velocity times `dt`.
-
-    Args:
-        field: point cloud to be advected
-        velocity: velocity sampled at the same points as the point cloud
-        dt: Euler step time increment
-        integrator: ODE integrator for solving the movement.
-
-    Returns:
-        Advected point cloud
-    """
-    new_elements = integrator(field.elements, velocity, dt)
-    return field.with_elements(new_elements)
-
-
-def semi_lagrangian(field: GridType,
-                    velocity: Field,
-                    dt: float,
-                    integrator=euler) -> GridType:
-    """
-    Semi-Lagrangian advection with simple backward lookup.
-    
-    This method samples the `velocity` at the grid points of `field`
-    to determine the lookup location for each grid point by walking backwards along the velocity vectors.
-    The new values are then determined by sampling `field` at these lookup locations.
-
-    Args:
-        field: quantity to be advected, stored on a grid (CenteredGrid or StaggeredGrid)
-        velocity: vector field, need not be compatible with with `field`.
-        dt: time increment
-        integrator: ODE integrator for solving the movement.
-
-    Returns:
-        Field with same sample points as `field`
-
-    """
-    lookup = integrator(field.elements, velocity, -dt)
-    interpolated = reduce_sample(field, lookup)
-    return field.with_values(interpolated)
-
-
-def mac_cormack(field: GridType,
-                velocity: Field,
-                dt: float,
-                correction_strength=1.0,
-                integrator=euler) -> GridType:
-    """
-    MacCormack advection uses a forward and backward lookup to determine the first-order error of semi-Lagrangian advection.
-    It then uses that error estimate to correct the field values.
-    To avoid overshoots, the resulting value is bounded by the neighbouring grid cells of the backward lookup.
-
-    Args:
-        field: Field to be advected, one of `(CenteredGrid, StaggeredGrid)`
-        velocity: Vector field, need not be sampled at same locations as `field`.
-        dt: Time increment
-        correction_strength: The estimated error is multiplied by this factor before being applied.
-            The case correction_strength=0 equals semi-lagrangian advection. Set lower than 1.0 to avoid oscillations.
-        integrator: ODE integrator for solving the movement.
-
-    Returns:
-        Advected field of type `type(field)`
-
-    """
-    v0 = sample(velocity, field.elements)
-    points_bwd = integrator(field.elements, velocity, -dt, v0=v0)
-    points_fwd = integrator(field.elements, velocity, dt, v0=v0)
-    # Semi-Lagrangian advection
-    field_semi_la = field.with_values(reduce_sample(field, points_bwd))
-    # Inverse semi-Lagrangian advection
-    field_inv_semi_la = field.with_values(reduce_sample(field_semi_la, points_fwd))
-    # correction
-    new_field = field_semi_la + correction_strength * 0.5 * (field - field_inv_semi_la)
-    # Address overshoots
-    limits = field.closest_values(points_bwd)
-    lower_limit = math.min(limits, [f'closest_{dim}' for dim in field.shape.spatial.names])
-    upper_limit = math.max(limits, [f'closest_{dim}' for dim in field.shape.spatial.names])
-    values_clamped = math.clip(new_field.values, lower_limit, upper_limit)
-    return new_field.with_values(values_clamped)
+"""
+Container for different advection schemes for grids and particles.
+
+Examples:
+
+* semi_lagrangian (grid)
+* mac_cormack (grid)
+* runge_kutta_4 (particle)
+"""
+from typing import Union
+
+from phi.math import Solve, channel
+
+from phi import math
+from phi.field import SampledField, Field, PointCloud, Grid, sample, reduce_sample, spatial_gradient, unstack, stack, CenteredGrid, StaggeredGrid
+from phi.field._field import FieldType
+from phi.field._field_math import GridType
+from phi.geom import Geometry
+
+
+def euler(elements: Geometry, velocity: Field, dt: float, v0: math.Tensor = None) -> Geometry:
+    """ Euler integrator. """
+    if v0 is None:
+        v0 = sample(velocity, elements)
+    return elements.shifted(v0 * dt)
+
+
+def rk4(elements: Geometry, velocity: Field, dt: float, v0: math.Tensor = None) -> Geometry:
+    """ Runge-Kutta-4 integrator. """
+    if v0 is None:
+        v0 = sample(velocity, elements)
+    vel_half = sample(velocity, elements.shifted(0.5 * dt * v0))
+    vel_half2 = sample(velocity, elements.shifted(0.5 * dt * vel_half))
+    vel_full = sample(velocity, elements.shifted(dt * vel_half2))
+    vel_rk4 = (1 / 6.) * (v0 + 2 * (vel_half + vel_half2) + vel_full)
+    return elements.shifted(dt * vel_rk4)
+
+
+def finite_rk4(elements: Geometry, velocity: Grid, dt: float, v0: math.Tensor = None) -> Geometry:
+    """ Runge-Kutta-4 integrator with Euler fallback where velocity values are NaN. """
+    v0 = sample(velocity, elements)
+    vel_half = sample(velocity, elements.shifted(0.5 * dt * v0))
+    vel_half2 = sample(velocity, elements.shifted(0.5 * dt * vel_half))
+    vel_full = sample(velocity, elements.shifted(dt * vel_half2))
+    vel_rk4 = (1 / 6.) * (v0 + 2 * (vel_half + vel_half2) + vel_full)
+    vel_nan = math.where(math.is_finite(vel_rk4), vel_rk4, v0)
+    return elements.shifted(dt * vel_nan)
+
+
+
+def advect(field: SampledField,
+           velocity: Field,
+           dt: Union[float, math.Tensor],
+           integrator=euler) -> FieldType:
+    """
+    Advect `field` along the `velocity` vectors using the specified integrator.
+
+    The behavior depends on the type of `field`:
+
+    * `phi.field.PointCloud`: Points are advected forward, see `points`.
+    * `phi.field.Grid`: Sample points are traced backward, see `semi_lagrangian`.
+
+    Args:
+        field: Field to be advected as `phi.field.SampledField`.
+        velocity: Any `phi.field.Field` that can be sampled in the elements of `field`.
+        dt: Time increment
+        integrator: ODE integrator for solving the movement.
+
+    Returns:
+        Advected field of same type as `field`
+    """
+    if isinstance(field, PointCloud):
+        return points(field, velocity, dt=dt, integrator=integrator)
+    elif isinstance(field, Grid):
+        return semi_lagrangian(field, velocity, dt=dt, integrator=integrator)
+    raise NotImplementedError(field)
+
+
+def finite_difference(grid: Grid,
+                      velocity: Field,
+                      order=2,
+                      implicit: Solve = None) -> Field:
+
+    """
+    Finite difference advection using the differentiation Scheme indicated by `scheme` and a simple Euler step
+
+    Args:
+        grid: Grid to be advected
+        velocity: `Grid` that can be sampled in the elements of `grid`.
+        order: Spatial order of accuracy.
+            Higher orders entail larger stencils and more computation time but result in more accurate results assuming a large enough resolution.
+            Supported: 2 explicit, 4 explicit, 6 implicit (inherited from `phi.field.spatial_gradient()` and resampling).
+            Passing order=4 currently uses 2nd-order resampling. This is work-in-progress.
+        implicit: When a `Solve` object is passed, performs an implicit operation with the specified solver and tolerances.
+            Otherwise, an explicit stencil is used.
+
+    Returns:
+        Advected grid of same type as `grid`
+    """
+    if isinstance(grid, StaggeredGrid):
+        grad_list = [spatial_gradient(field_component, stack_dim=channel('gradient'), order=order, implicit=implicit) for field_component in grid.vector]
+        grad_grid = grid.with_values(math.stack([component.values for component in grad_list], channel(velocity)))
+        if order == 4:
+            amounts = [grad * vel.at(grad, order=2) for grad, vel in zip(grad_grid.gradient, velocity.vector)]  # ToDo resampling does not yet support order=4
+        else:
+            grad_grid.gradient[0].elements
+            velocity.vector[0].at(grad_grid.gradient[0], order=order, implicit=implicit)
+            amounts = [grad * vel.at(grad, order=order, implicit=implicit) for grad, vel in zip(grad_grid.gradient, velocity.vector)]
+        amount = sum(amounts)
+    else:
+        assert isinstance(grid, CenteredGrid), f"grid must be CenteredGrid or StaggeredGrid but got {type(grid)}"
+        grad = spatial_gradient(grid, stack_dim=channel('gradient'), order=order, implicit=implicit)
+        velocity = stack(unstack(velocity, dim='vector'), dim=channel('gradient'))
+        amounts = velocity * grad
+        amount = sum(amounts.gradient)
+    return - amount
+
+
+def points(field: PointCloud, velocity: Field, dt: float, integrator=euler):
+    """
+    Advects the sample points of a point cloud using a simple Euler step.
+    Each point moves by an amount equal to the local velocity times `dt`.
+
+    Args:
+        field: point cloud to be advected
+        velocity: velocity sampled at the same points as the point cloud
+        dt: Euler step time increment
+        integrator: ODE integrator for solving the movement.
+
+    Returns:
+        Advected point cloud
+    """
+    new_elements = integrator(field.elements, velocity, dt)
+    return field.with_elements(new_elements)
+
+
+def semi_lagrangian(field: GridType,
+                    velocity: Field,
+                    dt: float,
+                    integrator=euler) -> GridType:
+    """
+    Semi-Lagrangian advection with simple backward lookup.
+    
+    This method samples the `velocity` at the grid points of `field`
+    to determine the lookup location for each grid point by walking backwards along the velocity vectors.
+    The new values are then determined by sampling `field` at these lookup locations.
+
+    Args:
+        field: quantity to be advected, stored on a grid (CenteredGrid or StaggeredGrid)
+        velocity: vector field, need not be compatible with with `field`.
+        dt: time increment
+        integrator: ODE integrator for solving the movement.
+
+    Returns:
+        Field with same sample points as `field`
+
+    """
+    lookup = integrator(field.elements, velocity, -dt)
+    interpolated = reduce_sample(field, lookup)
+    return field.with_values(interpolated)
+
+
+def mac_cormack(field: GridType,
+                velocity: Field,
+                dt: float,
+                correction_strength=1.0,
+                integrator=euler) -> GridType:
+    """
+    MacCormack advection uses a forward and backward lookup to determine the first-order error of semi-Lagrangian advection.
+    It then uses that error estimate to correct the field values.
+    To avoid overshoots, the resulting value is bounded by the neighbouring grid cells of the backward lookup.
+
+    Args:
+        field: Field to be advected, one of `(CenteredGrid, StaggeredGrid)`
+        velocity: Vector field, need not be sampled at same locations as `field`.
+        dt: Time increment
+        correction_strength: The estimated error is multiplied by this factor before being applied.
+            The case correction_strength=0 equals semi-lagrangian advection. Set lower than 1.0 to avoid oscillations.
+        integrator: ODE integrator for solving the movement.
+
+    Returns:
+        Advected field of type `type(field)`
+
+    """
+    v0 = sample(velocity, field.elements)
+    points_bwd = integrator(field.elements, velocity, -dt, v0=v0)
+    points_fwd = integrator(field.elements, velocity, dt, v0=v0)
+    # Semi-Lagrangian advection
+    field_semi_la = field.with_values(reduce_sample(field, points_bwd))
+    # Inverse semi-Lagrangian advection
+    field_inv_semi_la = field.with_values(reduce_sample(field_semi_la, points_fwd))
+    # correction
+    new_field = field_semi_la + correction_strength * 0.5 * (field - field_inv_semi_la)
+    # Address overshoots
+    limits = field.closest_values(points_bwd)
+    lower_limit = math.min(limits, [f'closest_{dim}' for dim in field.shape.spatial.names])
+    upper_limit = math.max(limits, [f'closest_{dim}' for dim in field.shape.spatial.names])
+    values_clamped = math.clip(new_field.values, lower_limit, upper_limit)
+    return new_field.with_values(values_clamped)
```

### Comparing `phiflow-2.3.4/phi/physics/diffuse.py` & `phiflow-2.4.0/phi/physics/diffuse.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,119 +1,119 @@
-"""
-Functions to simulate diffusion processes on `phi.field.Field` objects.
-"""
-from typing import Union
-
-from phi import math
-from phi.field import Grid, Field, laplace, solve_linear, jit_compile_linear
-from phi.field._field import FieldType
-from phi.field._grid import GridType
-from phi.math import copy_with, shape, Solve
-
-
-def explicit(field: FieldType,
-             diffusivity: Union[float, math.Tensor, Field],
-             dt: Union[float, math.Tensor],
-             substeps: int = 1) -> FieldType:
-    """
-    Simulate a finite-time diffusion process of the form dF/dt = α · ΔF on a given `Field` FieldType with diffusion coefficient α.
-
-    Args:
-        field: CenteredGrid, StaggeredGrid or ConstantField
-        diffusivity: Diffusion per time. `diffusion_amount = diffusivity * dt`
-            Can be a number, `phi.math.Tensor` or `phi.field.Field`.
-            If a channel dimension is present, it will be interpreted as non-isotropic diffusion.
-        dt: Time interval. `diffusion_amount = diffusivity * dt`
-        substeps: number of iterations to use (Default value = 1)
-
-    Returns:
-        Diffused field of same type as `field`.
-    """
-    amount = diffusivity * dt / substeps
-    if isinstance(amount, Field):
-        amount = amount.at(field)
-    ext = field.extrapolation
-    for i in range(substeps):
-        delta = laplace(field, weights=amount) if 'vector' in shape(amount) else amount * laplace(field)
-        field = (field + delta.with_extrapolation(ext)).with_extrapolation(ext)
-    return field
-
-
-def implicit(field: FieldType,
-             diffusivity: Union[float, math.Tensor, Field],
-             dt: Union[float, math.Tensor],
-             order: int = 1,
-             solve=Solve('CG')) -> FieldType:
-    """
-    Diffusion by solving a linear system of equations.
-
-    Args:
-        order: Order of method, 1=first order. This translates to `substeps` for the explicit sharpening.
-        field: `phi.field.Field` to diffuse.
-        diffusivity: Diffusion per time. `diffusion_amount = diffusivity * dt`
-        dt: Time interval. `diffusion_amount = diffusivity * dt`
-        solve:
-
-    Returns:
-        Diffused field of same type as `field`.
-    """
-    @jit_compile_linear
-    def sharpen(x):
-        return explicit(x, diffusivity, -dt, substeps=order)
-
-    if not solve.x0:
-        solve = copy_with(solve, x0=field)
-    return solve_linear(sharpen, y=field, solve=solve)
-
-
-def finite_difference(grid: Grid,
-                      diffusivity: Union[float, math.Tensor, Field],
-                      order: int,
-                      implicit: math.Solve) -> FieldType:
-
-    """
-    Diffusion by using a finite difference scheme.
-    In contrast to `explicit` and `implicit` accuracy can be increased by using stencils of higher-order rather than calculating substeps.
-    This is controlled by the `scheme` passed.
-
-    Args:
-        grid: CenteredGrid or StaggeredGrid
-        diffusivity: Diffusion per time. `diffusion_amount = diffusivity * dt`
-        order: Spatial order of accuracy.
-            Higher orders entail larger stencils and more computation time but result in more accurate results assuming a large enough resolution.
-            Supported: 2 explicit, 4 explicit, 6 implicit (inherited from `phi.field.laplace()`).
-        implicit: When a `Solve` object is passed, performs an implicit operation with the specified solver and tolerances.
-            Otherwise, an explicit stencil is used.
-
-    Returns:
-        Diffused grid of same type as `grid`.
-    """
-    diffusivity = diffusivity.at(grid) if isinstance(diffusivity, Field) else diffusivity
-    return diffusivity * laplace(grid, order=order, implicit=implicit).with_extrapolation(grid.extrapolation)
-
-
-def fourier(field: GridType,
-            diffusivity: Union[float, math.Tensor],
-            dt: Union[float, math.Tensor]) -> FieldType:
-    """
-    Exact diffusion of a periodic field in frequency space.
-
-    For non-periodic fields or non-constant diffusivity, use another diffusion function such as `explicit()`.
-
-    Args:
-        field:
-        diffusivity: Diffusion per time. `diffusion_amount = diffusivity * dt`
-        dt: Time interval. `diffusion_amount = diffusivity * dt`
-
-    Returns:
-        Diffused field of same type as `field`.
-    """
-    assert isinstance(field, Grid), "Cannot diffuse field of type '%s'" % type(field)
-    assert field.extrapolation == math.extrapolation.PERIODIC, "Fourier diffusion can only be applied to periodic fields."
-    amount = diffusivity * dt
-    k = math.fftfreq(field.resolution)
-    k2 = math.vec_squared(k)
-    fft_laplace = -(2 * math.PI) ** 2 * k2
-    diffuse_kernel = math.exp(fft_laplace * amount)
-    result_k = math.fft(field.values) * diffuse_kernel
-    result_values = math.real(math.ifft(result_k))
-    return field.with_values(result_values)
+"""
+Functions to simulate diffusion processes on `phi.field.Field` objects.
+"""
+from typing import Union
+
+from phi import math
+from phi.field import Grid, Field, laplace, solve_linear, jit_compile_linear
+from phi.field._field import FieldType
+from phi.field._grid import GridType
+from phi.math import copy_with, shape, Solve
+
+
+def explicit(field: FieldType,
+             diffusivity: Union[float, math.Tensor, Field],
+             dt: Union[float, math.Tensor],
+             substeps: int = 1) -> FieldType:
+    """
+    Simulate a finite-time diffusion process of the form dF/dt = α · ΔF on a given `Field` FieldType with diffusion coefficient α.
+
+    Args:
+        field: CenteredGrid, StaggeredGrid or ConstantField
+        diffusivity: Diffusion per time. `diffusion_amount = diffusivity * dt`
+            Can be a number, `phi.math.Tensor` or `phi.field.Field`.
+            If a channel dimension is present, it will be interpreted as non-isotropic diffusion.
+        dt: Time interval. `diffusion_amount = diffusivity * dt`
+        substeps: number of iterations to use (Default value = 1)
+
+    Returns:
+        Diffused field of same type as `field`.
+    """
+    amount = diffusivity * dt / substeps
+    if isinstance(amount, Field):
+        amount = amount.at(field)
+    ext = field.extrapolation
+    for i in range(substeps):
+        delta = laplace(field, weights=amount) if 'vector' in shape(amount) else amount * laplace(field)
+        field = (field + delta.with_extrapolation(ext)).with_extrapolation(ext)
+    return field
+
+
+def implicit(field: FieldType,
+             diffusivity: Union[float, math.Tensor, Field],
+             dt: Union[float, math.Tensor],
+             order: int = 1,
+             solve=Solve('CG')) -> FieldType:
+    """
+    Diffusion by solving a linear system of equations.
+
+    Args:
+        order: Order of method, 1=first order. This translates to `substeps` for the explicit sharpening.
+        field: `phi.field.Field` to diffuse.
+        diffusivity: Diffusion per time. `diffusion_amount = diffusivity * dt`
+        dt: Time interval. `diffusion_amount = diffusivity * dt`
+        solve:
+
+    Returns:
+        Diffused field of same type as `field`.
+    """
+    @jit_compile_linear
+    def sharpen(x):
+        return explicit(x, diffusivity, -dt, substeps=order)
+
+    if not solve.x0:
+        solve = copy_with(solve, x0=field)
+    return solve_linear(sharpen, y=field, solve=solve)
+
+
+def finite_difference(grid: Grid,
+                      diffusivity: Union[float, math.Tensor, Field],
+                      order: int,
+                      implicit: math.Solve) -> FieldType:
+
+    """
+    Diffusion by using a finite difference scheme.
+    In contrast to `explicit` and `implicit` accuracy can be increased by using stencils of higher-order rather than calculating substeps.
+    This is controlled by the `scheme` passed.
+
+    Args:
+        grid: CenteredGrid or StaggeredGrid
+        diffusivity: Diffusion per time. `diffusion_amount = diffusivity * dt`
+        order: Spatial order of accuracy.
+            Higher orders entail larger stencils and more computation time but result in more accurate results assuming a large enough resolution.
+            Supported: 2 explicit, 4 explicit, 6 implicit (inherited from `phi.field.laplace()`).
+        implicit: When a `Solve` object is passed, performs an implicit operation with the specified solver and tolerances.
+            Otherwise, an explicit stencil is used.
+
+    Returns:
+        Diffused grid of same type as `grid`.
+    """
+    diffusivity = diffusivity.at(grid) if isinstance(diffusivity, Field) else diffusivity
+    return diffusivity * laplace(grid, order=order, implicit=implicit).with_extrapolation(grid.extrapolation)
+
+
+def fourier(field: GridType,
+            diffusivity: Union[float, math.Tensor],
+            dt: Union[float, math.Tensor]) -> FieldType:
+    """
+    Exact diffusion of a periodic field in frequency space.
+
+    For non-periodic fields or non-constant diffusivity, use another diffusion function such as `explicit()`.
+
+    Args:
+        field:
+        diffusivity: Diffusion per time. `diffusion_amount = diffusivity * dt`
+        dt: Time interval. `diffusion_amount = diffusivity * dt`
+
+    Returns:
+        Diffused field of same type as `field`.
+    """
+    assert isinstance(field, Grid), "Cannot diffuse field of type '%s'" % type(field)
+    assert field.extrapolation == math.extrapolation.PERIODIC, "Fourier diffusion can only be applied to periodic fields."
+    amount = diffusivity * dt
+    k = math.fftfreq(field.resolution)
+    k2 = math.vec_squared(k)
+    fft_laplace = -(2 * math.PI) ** 2 * k2
+    diffuse_kernel = math.exp(fft_laplace * amount)
+    result_k = math.fft(field.values) * diffuse_kernel
+    result_values = math.real(math.ifft(result_k))
+    return field.with_values(result_values)
```

### Comparing `phiflow-2.3.4/phi/physics/fluid.py` & `phiflow-2.4.0/phi/physics/fluid.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,281 +1,281 @@
-"""
-Functions for simulating incompressible fluids, both grid-based and particle-based.
-
-The main function for incompressible fluids (Eulerian as well as FLIP / PIC) is `make_incompressible()` which removes the divergence of a velocity field.
-"""
-import warnings
-from typing import Tuple, Callable, Union
-
-from phi import math, field
-from phi.math import wrap, channel, Solve
-from phi.field import AngularVelocity, Grid, divergence, spatial_gradient, where, CenteredGrid, PointCloud, Field, resample
-from phi.geom import union, Geometry
-from ..field._embed import FieldEmbedding
-from ..field._grid import GridType, StaggeredGrid
-from ..math import extrapolation, NUMPY, batch, shape, non_channel, expand
-from ..math._magic_ops import copy_with
-from ..math.extrapolation import combine_sides, Extrapolation
-
-
-class Obstacle:
-    """
-    An obstacle defines boundary conditions inside a geometry.
-    It can also have a linear and angular velocity.
-    """
-
-    def __init__(self, geometry, velocity=0, angular_velocity=0):
-        """
-        Args:
-            geometry: Physical shape and size of the obstacle.
-            velocity: Linear velocity vector of the obstacle.
-            angular_velocity: Rotation speed of the obstacle. Scalar value in 2D, vector in 3D.
-        """
-        self.geometry = geometry
-        self.velocity = wrap(velocity, channel(geometry)) if isinstance(velocity, (tuple, list)) else velocity
-        self.angular_velocity = angular_velocity
-        self.shape = shape(geometry) & non_channel(self.velocity) & non_channel(angular_velocity)
-
-    @property
-    def is_stationary(self):
-        """ Test whether the obstacle is completely still. """
-        return isinstance(self.velocity, (int, float)) and self.velocity == 0 and isinstance(self.angular_velocity, (int, float)) and self.angular_velocity == 0
-
-    def copied_with(self, **kwargs):
-        warnings.warn("Obstacle.copied_with is deprecated. Use math.copy_with instead.", DeprecationWarning, stacklevel=2)
-        return math.copy_with(self, **kwargs)
-
-    def __variable_attrs__(self) -> Tuple[str, ...]:
-        return 'geometry', 'velocity', 'angular_velocity'
-
-    def __eq__(self, other):
-        if not isinstance(other, Obstacle):
-            return False
-        return self.geometry == other.geometry and self.velocity == other.velocity and self.angular_velocity == other.angular_velocity
-
-
-def _get_obstacles_for(obstacles, space: Field):
-    obstacles = [obstacles] if isinstance(obstacles, (Obstacle, Geometry)) else obstacles
-    assert isinstance(obstacles, (tuple, list)), f"obstacles must be an Obstacle or Geometry or a tuple/list thereof but got {type(obstacles)}"
-    obstacles = [Obstacle(o) if isinstance(o, Geometry) else o for o in obstacles]
-    for obstacle in obstacles:
-        assert obstacle.geometry.vector.item_names == space.vector.item_names, f"Obstacles must live in the same physical space as the velocity field {space.vector.item_names} but got {type(obstacle.geometry).__name__} obstacle with order {obstacle.geometry.vector.item_names}"
-    return obstacles
-
-
-def make_incompressible(velocity: GridType,
-                        obstacles: Union[Obstacle, Geometry, tuple, list] = (),
-                        solve: Solve = Solve(),
-                        active: CenteredGrid = None,
-                        order: int = 2) -> Tuple[GridType, CenteredGrid]:
-    """
-    Projects the given velocity field by solving for the pressure and subtracting its spatial_gradient.
-    
-    This method is similar to :func:`field.divergence_free()` but differs in how the boundary conditions are specified.
-
-    Args:
-        velocity: Vector field sampled on a grid.
-        obstacles: `Obstacle` or `phi.geom.Geometry` or tuple/list thereof to specify boundary conditions inside the domain.
-        solve: `Solve` object specifying method and tolerances for the implicit pressure solve.
-        active: (Optional) Mask for which cells the pressure should be solved.
-            If given, the velocity may take `NaN` values where it does not contribute to the pressure.
-            Also, the total divergence will never be subtracted if active is given, even if all values are 1.
-        order: spatial order for derivative computations.
-            For Higher-order schemes, the laplace operation is not conducted with a stencil exactly corresponding to the one used in divergence calculations but a smaller one instead.
-            While this disrupts the formal correctness of the method it only induces insignificant errors and yields considerable performance gains.
-            supported: explicit 2/4th order - implicit 6th order (obstacles are only supported with explicit 2nd order)
-
-    Returns:
-        velocity: divergence-free velocity of type `type(velocity)`
-        pressure: solved pressure field, `CenteredGrid`
-    """
-    obstacles = _get_obstacles_for(obstacles, velocity)
-    assert order == 2 or len(obstacles) == 0, f"obstacles are not supported with higher order schemes"
-    input_velocity = velocity
-    # --- Create masks ---
-    accessible_extrapolation = _accessible_extrapolation(input_velocity.extrapolation)
-    with NUMPY:
-        accessible = CenteredGrid(~union([obs.geometry for obs in obstacles]), accessible_extrapolation, velocity.bounds, velocity.resolution)
-        hard_bcs = field.stagger(accessible, math.minimum, input_velocity.extrapolation, type=type(velocity))
-    all_active = active is None
-    if active is None:
-        active = accessible.with_extrapolation(extrapolation.NONE)
-    else:
-        active *= accessible  # no pressure inside obstacles
-    # --- Linear solve ---
-    velocity = apply_boundary_conditions(velocity, obstacles)
-    div = divergence(velocity, order=order) * active
-    if not all_active:  # NaN in velocity allowed
-        div = field.where(field.is_finite(div), div, 0)
-    if not input_velocity.extrapolation.is_flexible and all_active:
-        solve = solve.with_preprocessing(_balance_divergence, active)
-    if solve.x0 is None:
-        pressure_extrapolation = _pressure_extrapolation(input_velocity.extrapolation)
-        solve = copy_with(solve, x0=CenteredGrid(0, pressure_extrapolation, div.bounds, div.resolution))
-    if batch(math.merge_shapes(*obstacles)).without(batch(solve.x0)):  # The initial pressure guess must contain all batch dimensions
-        solve = copy_with(solve, x0=expand(solve.x0, batch(math.merge_shapes(*obstacles))))
-    pressure = math.solve_linear(masked_laplace, div, solve, hard_bcs, active, order=order)
-    # --- Subtract grad p ---
-    grad_pressure = field.spatial_gradient(pressure, input_velocity.extrapolation, type=type(velocity), order=order) * hard_bcs
-    velocity = (velocity - grad_pressure).with_extrapolation(input_velocity.extrapolation)
-    return velocity, pressure
-
-
-@math.jit_compile_linear(auxiliary_args='hard_bcs,active,order,implicit', forget_traces=True)  # jit compilation is required for boundary conditions that add a constant offset solving Ax + b = y
-def masked_laplace(pressure: CenteredGrid, hard_bcs: Grid, active: CenteredGrid, order=2, implicit: Solve = None) -> CenteredGrid:
-    """
-    Computes the laplace of `pressure` in the presence of obstacles.
-
-    Args:
-        pressure: Pressure field.
-        hard_bcs: Mask encoding which cells are connected to each other.
-            One between fluid cells, zero inside and at the boundary of obstacles.
-            This should be of the same type as the velocity, i.e. `StaggeredGrid` or `CenteredGrid`.
-        active: Mask indicating for which cells the pressure value is valid.
-            Linear solves will only determine the pressure for these cells.
-            This is generally zero inside obstacles and in non-simulated regions.
-        order: Spatial order of accuracy.
-            Higher orders entail larger stencils and more computation time but result in more accurate results assuming a large enough resolution.
-            Supported: 2 explicit, 4 explicit, 6 implicit (inherited from `phi.field.laplace()`).
-        implicit: When a `Solve` object is passed, performs an implicit operation with the specified solver and tolerances.
-            Otherwise, an explicit stencil is used.
-
-    Returns:
-        `CenteredGrid`
-    """
-    if order == 2 and not implicit:
-        grad = spatial_gradient(pressure, hard_bcs.extrapolation, type=type(hard_bcs))
-        valid_grad = grad * hard_bcs
-        valid_grad = valid_grad.with_extrapolation(extrapolation.remove_constant_offset(valid_grad.extrapolation))
-        div = divergence(valid_grad)
-        laplace = where(active, div, pressure)
-    else:
-        laplace = field.laplace(pressure, order=order, implicit=implicit)
-    return laplace
-
-
-def _balance_divergence(div, active):
-    return div - active * (field.mean(div) / field.mean(active))
-
-
-def apply_boundary_conditions(velocity: Union[Grid, PointCloud], obstacles: Union[Obstacle, Geometry, tuple, list]):
-    """
-    Enforces velocities boundary conditions on a velocity grid.
-    Cells inside obstacles will get their velocity from the obstacle movement.
-    Cells outside far away will be unaffected.
-
-    Args:
-      velocity: Velocity `Grid`.
-        obstacles: `Obstacle` or `phi.geom.Geometry` or tuple/list thereof to specify boundary conditions inside the domain.
-
-    Returns:
-        Velocity of same type as `velocity`
-    """
-    obstacles = _get_obstacles_for(obstacles, velocity)
-    # velocity = field.bake_extrapolation(velocity)  # TODO we should bake only for divergence but keep correct extrapolation for velocity. However, obstacles should override extrapolation.
-    for obstacle in obstacles:
-        if isinstance(obstacle, Geometry):
-            obstacle = Obstacle(obstacle)
-        assert isinstance(obstacle, Obstacle)
-        obs_mask = resample(obstacle.geometry, velocity, soft=True, balance=1)
-        if obstacle.is_stationary:
-            velocity = (1 - obs_mask) * velocity
-        else:
-            angular_velocity = AngularVelocity(location=obstacle.geometry.center, strength=obstacle.angular_velocity, falloff=None) @ velocity
-            velocity = (1 - obs_mask) * velocity + obs_mask * (angular_velocity + obstacle.velocity)
-    return velocity
-
-
-def boundary_push(particles: PointCloud, obstacles: Union[tuple, list], offset: float = 0.5) -> PointCloud:
-    """
-    Enforces boundary conditions by correcting possible errors of the advection step and shifting particles out of
-    obstacles or back into the domain.
-
-    Args:
-        particles: PointCloud holding particle positions as elements
-        obstacles: List of `Obstacle` or `Geometry` objects where any particles inside should get shifted outwards
-        offset: Minimum distance between particles and domain boundary / obstacle surface after particles have been shifted.
-
-    Returns:
-        PointCloud where all particles are inside the domain / outside of obstacles.
-    """
-    pos = particles.elements.center
-    for obj in obstacles:
-        geometry = obj.geometry if isinstance(obj, Obstacle) else obj
-        assert isinstance(geometry, Geometry), f"obstacles must be a list of Obstacle or Geometry objects but got {type(obj)}"
-        pos = geometry.push(pos, shift_amount=offset)
-    return particles.with_elements(particles.elements @ pos)
-
-
-def _pressure_extrapolation(vext: Extrapolation):
-    if vext == extrapolation.PERIODIC:
-        return extrapolation.PERIODIC
-    elif vext == extrapolation.BOUNDARY:
-        return extrapolation.ZERO
-    elif isinstance(vext, extrapolation.ConstantExtrapolation):
-        return extrapolation.BOUNDARY
-    else:
-        return extrapolation.map(_pressure_extrapolation, vext)
-
-
-def _accessible_extrapolation(vext: Extrapolation):
-    """ Determine whether outside cells are accessible based on the velocity extrapolation. """
-    if vext == extrapolation.PERIODIC:
-        return extrapolation.PERIODIC
-    elif vext == extrapolation.BOUNDARY:
-        return extrapolation.ONE
-    elif isinstance(vext, extrapolation.ConstantExtrapolation):
-        return extrapolation.ZERO
-    elif isinstance(vext, FieldEmbedding):
-        return extrapolation.ONE
-    elif isinstance(vext, extrapolation._MixedExtrapolation):
-        return combine_sides(**{dim: (_accessible_extrapolation(lo), _accessible_extrapolation(hi)) for dim, (lo, hi) in vext.ext.items()})
-    elif isinstance(vext, extrapolation._NormalTangentialExtrapolation):
-        return _accessible_extrapolation(vext.normal)
-    else:
-        raise ValueError(f"Unsupported extrapolation: {type(vext)}")
-
-
-def incompressible_rk4(pde: Callable, velocity: GridType, pressure: CenteredGrid, dt, pressure_order=4, pressure_solve=Solve('CG'), **pde_aux_kwargs):
-    """
-    Implements the 4th-order Runge-Kutta time advancement scheme for incompressible vector fields.
-    This approach is inspired by [Kampanis et. al., 2006](https://www.sciencedirect.com/science/article/pii/S0021999105005061) and incorporates the pressure treatment into the time step.
-
-    Args:
-        pde: Momentum equation. Function that computes all PDE terms not related to pressure, e.g. diffusion, advection, external forces.
-        velocity: Velocity grid at time `t`.
-        pressure: Pressure at time `t`.
-        dt: Time increment to integrate.
-        pressure_order: spatial order for derivative computations.
-            For Higher-order schemes, the laplace operation is not conducted with a stencil exactly corresponding to the one used in divergence calculations but a smaller one instead.
-            While this disrupts the formal correctness of the method it only induces insignificant errors and yields considerable performance gains.
-            supported: explicit 2/4th order - implicit 6th order (obstacles are only supported with explicit 2nd order)
-        pressure_solve: `Solve` object specifying method and tolerances for the implicit pressure solve.
-        **pde_aux_kwargs: Auxiliary arguments for `pde`. These are considered constant over time.
-
-    Returns:
-        velocity: Velocity at time `t+dt`, same type as `velocity`.
-        pressure: Pressure grid at time `t+dt`, `CenteredGrid`.
-    """
-    v_1, p_1 = velocity, pressure
-    # PDE at current point
-    rhs_1 = pde(v_1, **pde_aux_kwargs) - field.spatial_gradient(p_1, type=StaggeredGrid, order=pressure_order)
-    v_2_old = velocity + (dt / 2) * rhs_1
-    v_2, delta_p = make_incompressible(v_2_old, solve=pressure_solve, order=pressure_order)
-    p_2 = p_1 + delta_p / dt
-    # PDE at half-point
-    rhs_2 = pde(v_2, **pde_aux_kwargs) - field.spatial_gradient(p_2, type=StaggeredGrid, order=pressure_order)
-    v_3_old = velocity + (dt / 2) * rhs_2
-    v_3, delta_p = make_incompressible(v_3_old, solve=pressure_solve, order=pressure_order)
-    p_3 = p_2 + delta_p / dt
-    # PDE at corrected half-point
-    rhs_3 = pde(v_3, **pde_aux_kwargs) - field.spatial_gradient(p_3, type=StaggeredGrid, order=pressure_order)
-    v_4_old = velocity + dt * rhs_2
-    v_4, delta_p = make_incompressible(v_4_old, solve=pressure_solve, order=pressure_order)
-    p_4 = p_3 + delta_p / dt
-    # PDE at RK4 point
-    rhs_4 = pde(v_4, **pde_aux_kwargs) - field.spatial_gradient(p_4, type=StaggeredGrid, order=pressure_order)
-    v_p1_old = velocity + (dt / 6) * (rhs_1 + 2 * rhs_2 + 2 * rhs_3 + rhs_4)
-    p_p1_old = (1 / 6) * (p_1 + 2 * p_2 + 2 * p_3 + p_4)
-    v_p1, delta_p = make_incompressible(v_p1_old, solve=pressure_solve, order=pressure_order)
-    p_p1 = p_p1_old + delta_p / dt
-    return v_p1, p_p1
+"""
+Functions for simulating incompressible fluids, both grid-based and particle-based.
+
+The main function for incompressible fluids (Eulerian as well as FLIP / PIC) is `make_incompressible()` which removes the divergence of a velocity field.
+"""
+import warnings
+from typing import Tuple, Callable, Union
+
+from phi import math, field
+from phi.math import wrap, channel, Solve
+from phi.field import AngularVelocity, Grid, divergence, spatial_gradient, where, CenteredGrid, PointCloud, Field, resample
+from phi.geom import union, Geometry
+from ..field._embed import FieldEmbedding
+from ..field._grid import GridType, StaggeredGrid
+from ..math import extrapolation, NUMPY, batch, shape, non_channel, expand
+from ..math._magic_ops import copy_with
+from ..math.extrapolation import combine_sides, Extrapolation
+
+
+class Obstacle:
+    """
+    An obstacle defines boundary conditions inside a geometry.
+    It can also have a linear and angular velocity.
+    """
+
+    def __init__(self, geometry, velocity=0, angular_velocity=0):
+        """
+        Args:
+            geometry: Physical shape and size of the obstacle.
+            velocity: Linear velocity vector of the obstacle.
+            angular_velocity: Rotation speed of the obstacle. Scalar value in 2D, vector in 3D.
+        """
+        self.geometry = geometry
+        self.velocity = wrap(velocity, channel(geometry)) if isinstance(velocity, (tuple, list)) else velocity
+        self.angular_velocity = angular_velocity
+        self.shape = shape(geometry) & non_channel(self.velocity) & non_channel(angular_velocity)
+
+    @property
+    def is_stationary(self):
+        """ Test whether the obstacle is completely still. """
+        return isinstance(self.velocity, (int, float)) and self.velocity == 0 and isinstance(self.angular_velocity, (int, float)) and self.angular_velocity == 0
+
+    def copied_with(self, **kwargs):
+        warnings.warn("Obstacle.copied_with is deprecated. Use math.copy_with instead.", DeprecationWarning, stacklevel=2)
+        return math.copy_with(self, **kwargs)
+
+    def __variable_attrs__(self) -> Tuple[str, ...]:
+        return 'geometry', 'velocity', 'angular_velocity'
+
+    def __eq__(self, other):
+        if not isinstance(other, Obstacle):
+            return False
+        return self.geometry == other.geometry and self.velocity == other.velocity and self.angular_velocity == other.angular_velocity
+
+
+def _get_obstacles_for(obstacles, space: Field):
+    obstacles = [obstacles] if isinstance(obstacles, (Obstacle, Geometry)) else obstacles
+    assert isinstance(obstacles, (tuple, list)), f"obstacles must be an Obstacle or Geometry or a tuple/list thereof but got {type(obstacles)}"
+    obstacles = [Obstacle(o) if isinstance(o, Geometry) else o for o in obstacles]
+    for obstacle in obstacles:
+        assert obstacle.geometry.vector.item_names == space.vector.item_names, f"Obstacles must live in the same physical space as the velocity field {space.vector.item_names} but got {type(obstacle.geometry).__name__} obstacle with order {obstacle.geometry.vector.item_names}"
+    return obstacles
+
+
+def make_incompressible(velocity: GridType,
+                        obstacles: Union[Obstacle, Geometry, tuple, list] = (),
+                        solve: Solve = Solve(),
+                        active: CenteredGrid = None,
+                        order: int = 2) -> Tuple[GridType, CenteredGrid]:
+    """
+    Projects the given velocity field by solving for the pressure and subtracting its spatial_gradient.
+    
+    This method is similar to :func:`field.divergence_free()` but differs in how the boundary conditions are specified.
+
+    Args:
+        velocity: Vector field sampled on a grid.
+        obstacles: `Obstacle` or `phi.geom.Geometry` or tuple/list thereof to specify boundary conditions inside the domain.
+        solve: `Solve` object specifying method and tolerances for the implicit pressure solve.
+        active: (Optional) Mask for which cells the pressure should be solved.
+            If given, the velocity may take `NaN` values where it does not contribute to the pressure.
+            Also, the total divergence will never be subtracted if active is given, even if all values are 1.
+        order: spatial order for derivative computations.
+            For Higher-order schemes, the laplace operation is not conducted with a stencil exactly corresponding to the one used in divergence calculations but a smaller one instead.
+            While this disrupts the formal correctness of the method it only induces insignificant errors and yields considerable performance gains.
+            supported: explicit 2/4th order - implicit 6th order (obstacles are only supported with explicit 2nd order)
+
+    Returns:
+        velocity: divergence-free velocity of type `type(velocity)`
+        pressure: solved pressure field, `CenteredGrid`
+    """
+    obstacles = _get_obstacles_for(obstacles, velocity)
+    assert order == 2 or len(obstacles) == 0, f"obstacles are not supported with higher order schemes"
+    input_velocity = velocity
+    # --- Create masks ---
+    accessible_extrapolation = _accessible_extrapolation(input_velocity.extrapolation)
+    with NUMPY:
+        accessible = CenteredGrid(~union([obs.geometry for obs in obstacles]), accessible_extrapolation, velocity.bounds, velocity.resolution)
+        hard_bcs = field.stagger(accessible, math.minimum, input_velocity.extrapolation, type=type(velocity))
+    all_active = active is None
+    if active is None:
+        active = accessible.with_extrapolation(extrapolation.NONE)
+    else:
+        active *= accessible  # no pressure inside obstacles
+    # --- Linear solve ---
+    velocity = apply_boundary_conditions(velocity, obstacles)
+    div = divergence(velocity, order=order) * active
+    if not all_active:  # NaN in velocity allowed
+        div = field.where(field.is_finite(div), div, 0)
+    if not input_velocity.extrapolation.is_flexible and all_active:
+        solve = solve.with_preprocessing(_balance_divergence, active)
+    if solve.x0 is None:
+        pressure_extrapolation = _pressure_extrapolation(input_velocity.extrapolation)
+        solve = copy_with(solve, x0=CenteredGrid(0, pressure_extrapolation, div.bounds, div.resolution))
+    if batch(math.merge_shapes(*obstacles)).without(batch(solve.x0)):  # The initial pressure guess must contain all batch dimensions
+        solve = copy_with(solve, x0=expand(solve.x0, batch(math.merge_shapes(*obstacles))))
+    pressure = math.solve_linear(masked_laplace, div, solve, hard_bcs, active, order=order)
+    # --- Subtract grad p ---
+    grad_pressure = field.spatial_gradient(pressure, input_velocity.extrapolation, type=type(velocity), order=order) * hard_bcs
+    velocity = (velocity - grad_pressure).with_extrapolation(input_velocity.extrapolation)
+    return velocity, pressure
+
+
+@math.jit_compile_linear(auxiliary_args='hard_bcs,active,order,implicit', forget_traces=True)  # jit compilation is required for boundary conditions that add a constant offset solving Ax + b = y
+def masked_laplace(pressure: CenteredGrid, hard_bcs: Grid, active: CenteredGrid, order=2, implicit: Solve = None) -> CenteredGrid:
+    """
+    Computes the laplace of `pressure` in the presence of obstacles.
+
+    Args:
+        pressure: Pressure field.
+        hard_bcs: Mask encoding which cells are connected to each other.
+            One between fluid cells, zero inside and at the boundary of obstacles.
+            This should be of the same type as the velocity, i.e. `StaggeredGrid` or `CenteredGrid`.
+        active: Mask indicating for which cells the pressure value is valid.
+            Linear solves will only determine the pressure for these cells.
+            This is generally zero inside obstacles and in non-simulated regions.
+        order: Spatial order of accuracy.
+            Higher orders entail larger stencils and more computation time but result in more accurate results assuming a large enough resolution.
+            Supported: 2 explicit, 4 explicit, 6 implicit (inherited from `phi.field.laplace()`).
+        implicit: When a `Solve` object is passed, performs an implicit operation with the specified solver and tolerances.
+            Otherwise, an explicit stencil is used.
+
+    Returns:
+        `CenteredGrid`
+    """
+    if order == 2 and not implicit:
+        grad = spatial_gradient(pressure, hard_bcs.extrapolation, type=type(hard_bcs))
+        valid_grad = grad * hard_bcs
+        valid_grad = valid_grad.with_extrapolation(extrapolation.remove_constant_offset(valid_grad.extrapolation))
+        div = divergence(valid_grad)
+        laplace = where(active, div, pressure)
+    else:
+        laplace = field.laplace(pressure, order=order, implicit=implicit)
+    return laplace
+
+
+def _balance_divergence(div, active):
+    return div - active * (field.mean(div) / field.mean(active))
+
+
+def apply_boundary_conditions(velocity: Union[Grid, PointCloud], obstacles: Union[Obstacle, Geometry, tuple, list]):
+    """
+    Enforces velocities boundary conditions on a velocity grid.
+    Cells inside obstacles will get their velocity from the obstacle movement.
+    Cells outside far away will be unaffected.
+
+    Args:
+      velocity: Velocity `Grid`.
+        obstacles: `Obstacle` or `phi.geom.Geometry` or tuple/list thereof to specify boundary conditions inside the domain.
+
+    Returns:
+        Velocity of same type as `velocity`
+    """
+    obstacles = _get_obstacles_for(obstacles, velocity)
+    # velocity = field.bake_extrapolation(velocity)  # TODO we should bake only for divergence but keep correct extrapolation for velocity. However, obstacles should override extrapolation.
+    for obstacle in obstacles:
+        if isinstance(obstacle, Geometry):
+            obstacle = Obstacle(obstacle)
+        assert isinstance(obstacle, Obstacle)
+        obs_mask = resample(obstacle.geometry, velocity, soft=True, balance=1)
+        if obstacle.is_stationary:
+            velocity = (1 - obs_mask) * velocity
+        else:
+            angular_velocity = AngularVelocity(location=obstacle.geometry.center, strength=obstacle.angular_velocity, falloff=None) @ velocity
+            velocity = (1 - obs_mask) * velocity + obs_mask * (angular_velocity + obstacle.velocity)
+    return velocity
+
+
+def boundary_push(particles: PointCloud, obstacles: Union[tuple, list], offset: float = 0.5) -> PointCloud:
+    """
+    Enforces boundary conditions by correcting possible errors of the advection step and shifting particles out of
+    obstacles or back into the domain.
+
+    Args:
+        particles: PointCloud holding particle positions as elements
+        obstacles: List of `Obstacle` or `Geometry` objects where any particles inside should get shifted outwards
+        offset: Minimum distance between particles and domain boundary / obstacle surface after particles have been shifted.
+
+    Returns:
+        PointCloud where all particles are inside the domain / outside of obstacles.
+    """
+    pos = particles.elements.center
+    for obj in obstacles:
+        geometry = obj.geometry if isinstance(obj, Obstacle) else obj
+        assert isinstance(geometry, Geometry), f"obstacles must be a list of Obstacle or Geometry objects but got {type(obj)}"
+        pos = geometry.push(pos, shift_amount=offset)
+    return particles.with_elements(particles.elements @ pos)
+
+
+def _pressure_extrapolation(vext: Extrapolation):
+    if vext == extrapolation.PERIODIC:
+        return extrapolation.PERIODIC
+    elif vext == extrapolation.BOUNDARY:
+        return extrapolation.ZERO
+    elif isinstance(vext, extrapolation.ConstantExtrapolation):
+        return extrapolation.BOUNDARY
+    else:
+        return extrapolation.map(_pressure_extrapolation, vext)
+
+
+def _accessible_extrapolation(vext: Extrapolation):
+    """ Determine whether outside cells are accessible based on the velocity extrapolation. """
+    if vext == extrapolation.PERIODIC:
+        return extrapolation.PERIODIC
+    elif vext == extrapolation.BOUNDARY:
+        return extrapolation.ONE
+    elif isinstance(vext, extrapolation.ConstantExtrapolation):
+        return extrapolation.ZERO
+    elif isinstance(vext, FieldEmbedding):
+        return extrapolation.ONE
+    elif isinstance(vext, extrapolation._MixedExtrapolation):
+        return combine_sides(**{dim: (_accessible_extrapolation(lo), _accessible_extrapolation(hi)) for dim, (lo, hi) in vext.ext.items()})
+    elif isinstance(vext, extrapolation._NormalTangentialExtrapolation):
+        return _accessible_extrapolation(vext.normal)
+    else:
+        raise ValueError(f"Unsupported extrapolation: {type(vext)}")
+
+
+def incompressible_rk4(pde: Callable, velocity: GridType, pressure: CenteredGrid, dt, pressure_order=4, pressure_solve=Solve('CG'), **pde_aux_kwargs):
+    """
+    Implements the 4th-order Runge-Kutta time advancement scheme for incompressible vector fields.
+    This approach is inspired by [Kampanis et. al., 2006](https://www.sciencedirect.com/science/article/pii/S0021999105005061) and incorporates the pressure treatment into the time step.
+
+    Args:
+        pde: Momentum equation. Function that computes all PDE terms not related to pressure, e.g. diffusion, advection, external forces.
+        velocity: Velocity grid at time `t`.
+        pressure: Pressure at time `t`.
+        dt: Time increment to integrate.
+        pressure_order: spatial order for derivative computations.
+            For Higher-order schemes, the laplace operation is not conducted with a stencil exactly corresponding to the one used in divergence calculations but a smaller one instead.
+            While this disrupts the formal correctness of the method it only induces insignificant errors and yields considerable performance gains.
+            supported: explicit 2/4th order - implicit 6th order (obstacles are only supported with explicit 2nd order)
+        pressure_solve: `Solve` object specifying method and tolerances for the implicit pressure solve.
+        **pde_aux_kwargs: Auxiliary arguments for `pde`. These are considered constant over time.
+
+    Returns:
+        velocity: Velocity at time `t+dt`, same type as `velocity`.
+        pressure: Pressure grid at time `t+dt`, `CenteredGrid`.
+    """
+    v_1, p_1 = velocity, pressure
+    # PDE at current point
+    rhs_1 = pde(v_1, **pde_aux_kwargs) - field.spatial_gradient(p_1, type=StaggeredGrid, order=pressure_order)
+    v_2_old = velocity + (dt / 2) * rhs_1
+    v_2, delta_p = make_incompressible(v_2_old, solve=pressure_solve, order=pressure_order)
+    p_2 = p_1 + delta_p / dt
+    # PDE at half-point
+    rhs_2 = pde(v_2, **pde_aux_kwargs) - field.spatial_gradient(p_2, type=StaggeredGrid, order=pressure_order)
+    v_3_old = velocity + (dt / 2) * rhs_2
+    v_3, delta_p = make_incompressible(v_3_old, solve=pressure_solve, order=pressure_order)
+    p_3 = p_2 + delta_p / dt
+    # PDE at corrected half-point
+    rhs_3 = pde(v_3, **pde_aux_kwargs) - field.spatial_gradient(p_3, type=StaggeredGrid, order=pressure_order)
+    v_4_old = velocity + dt * rhs_2
+    v_4, delta_p = make_incompressible(v_4_old, solve=pressure_solve, order=pressure_order)
+    p_4 = p_3 + delta_p / dt
+    # PDE at RK4 point
+    rhs_4 = pde(v_4, **pde_aux_kwargs) - field.spatial_gradient(p_4, type=StaggeredGrid, order=pressure_order)
+    v_p1_old = velocity + (dt / 6) * (rhs_1 + 2 * rhs_2 + 2 * rhs_3 + rhs_4)
+    p_p1_old = (1 / 6) * (p_1 + 2 * p_2 + 2 * p_3 + p_4)
+    v_p1, delta_p = make_incompressible(v_p1_old, solve=pressure_solve, order=pressure_order)
+    p_p1 = p_p1_old + delta_p / dt
+    return v_p1, p_p1
```

### Comparing `phiflow-2.3.4/phi/tf/__init__.py` & `phiflow-2.4.0/phi/tf/__init__.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,36 +1,36 @@
-"""
-TensorFlow integration.
-
-Importing this module registers the TensorFlow backend with `phi.math`.
-Without this, TensorFlow tensors cannot be handled by `phi.math` functions.
-
-To make TensorFlow the default backend, import `phi.tf.flow`.
-"""
-import platform as _platform
-import warnings as _warnings
-
-from phi import math as _math
-import os
-import tensorflow as _tf
-
-from ..math.backend import PHI_LOGGER as _LOGGER
-
-if _tf.__version__.startswith('1.'):
-    raise ImportError(f"PhiFlow 2.x and newer requires TensorFlow 2 but found TensorFlow {_tf.__version__}")
-
-os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # only errors
-if _platform.system().lower() == 'windows':  # prevent Blas GEMM launch failed on Windows
-    for i, device in enumerate(_tf.config.list_physical_devices('GPU')):
-        _tf.config.experimental.set_memory_growth(device, True)
-        _LOGGER.info(f"phi.tf: Setting memory_growth on GPU {i} to True to prevent Blas errors")
-
-from ._compile_cuda import compile_cuda_ops
-
-from ._tf_backend import TFBackend as _TFBackend
-
-TENSORFLOW = _TFBackend()
-"""Backend for TensorFlow operations."""
-
-_math.backend.BACKENDS.append(TENSORFLOW)
-
-__all__ = [key for key in globals().keys() if not key.startswith('_')]
+"""
+TensorFlow integration.
+
+Importing this module registers the TensorFlow backend with `phi.math`.
+Without this, TensorFlow tensors cannot be handled by `phi.math` functions.
+
+To make TensorFlow the default backend, import `phi.tf.flow`.
+"""
+import platform as _platform
+import warnings as _warnings
+
+from phi import math as _math
+import os
+import tensorflow as _tf
+
+from ..math.backend import PHI_LOGGER as _LOGGER
+
+if _tf.__version__.startswith('1.'):
+    raise ImportError(f"PhiFlow 2.x and newer requires TensorFlow 2 but found TensorFlow {_tf.__version__}")
+
+os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # only errors
+if _platform.system().lower() == 'windows':  # prevent Blas GEMM launch failed on Windows
+    for i, device in enumerate(_tf.config.list_physical_devices('GPU')):
+        _tf.config.experimental.set_memory_growth(device, True)
+        _LOGGER.info(f"phi.tf: Setting memory_growth on GPU {i} to True to prevent Blas errors")
+
+from ._compile_cuda import compile_cuda_ops
+
+from ._tf_backend import TFBackend as _TFBackend
+
+TENSORFLOW = _TFBackend()
+"""Backend for TensorFlow operations."""
+
+_math.backend.BACKENDS.append(TENSORFLOW)
+
+__all__ = [key for key in globals().keys() if not key.startswith('_')]
```

### Comparing `phiflow-2.3.4/phi/tf/_compile_cuda.py` & `phiflow-2.4.0/phi/tf/_compile_cuda.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,106 +1,106 @@
-import os
-from os.path import isfile, isdir, abspath, join, dirname
-import subprocess
-
-
-def compile_cuda_ops(gcc: str = None,
-                     nvcc: str = None,
-                     cuda_lib: str = None):
-    tf_gcc = check_tf_cuda_compatibility()
-    if gcc is None:
-        gcc = tf_gcc if isfile(tf_gcc) else 'gcc'
-    if nvcc is None:
-        nvcc = '/usr/local/cuda/bin/nvcc' if isfile('/usr/local/cuda/bin/nvcc') else 'nvcc'
-    if cuda_lib is None:
-        cuda_lib = '/usr/local/cuda/lib64/'
-
-    phi_tf_path = abspath(dirname(__file__))
-    src_path = join(phi_tf_path, 'cuda', 'src')
-    build_path = join(phi_tf_path, 'cuda', 'build')
-    logfile_path = join(phi_tf_path, 'cuda', 'log.txt')
-    print("Source Path:\t" + src_path)
-    print("Build Path:\t" + build_path)
-    print("GCC:\t\t" + gcc)
-    print("NVCC:\t\t" + nvcc)
-    print("CUDA lib:\t" + cuda_lib)
-    print("----------------------------")
-    # Remove old build files
-    if isdir(build_path):
-        print('Removing old build files from %s' % build_path)
-        for file in os.listdir(build_path):
-            os.remove(join(build_path, file))
-    else:
-        print('Creating build directory at %s' % build_path)
-        os.mkdir(build_path)
-    print('Compiling CUDA code...')
-    with open(logfile_path, "w") as logfile:
-        try:
-            compile_cuda('resample', nvcc, src_path, build_path, logfile=logfile)
-            compile_gcc('resample', gcc, src_path, build_path, cuda_lib, logfile=logfile)
-            compile_cuda('resample_gradient', nvcc, src_path, build_path, logfile=logfile)
-            compile_gcc('resample_gradient', gcc, src_path, build_path, cuda_lib, logfile=logfile)
-            # compile_cuda('bicgstab_ilu_linear_solve_op', self.nvcc, src_path, build_path, logfile=logfile)
-            # compile_gcc('bicgstab_ilu_linear_solve_op', self.gcc, src_path, build_path, self.cuda_lib, logfile=logfile)
-        except BaseException as err:
-            print(f"Compilation failed. See {logfile_path} for details.")
-            raise err
-    print(f"Compilation complete. See {logfile_path} for details.")
-
-
-def check_tf_cuda_compatibility():
-    import tensorflow
-    build = tensorflow.sysconfig.get_build_info()  # is_rocm_build, cuda_compute_capabilities
-    tf_gcc = build['cpu_compiler']
-    is_cuda_build = build['is_cuda_build']
-    print(f"TensorFlow compiler: {tf_gcc}.")
-    if not is_cuda_build:
-        raise AssertionError("Your TensorFlow build does not support CUDA.")
-    else:
-        cuda_version = build['cuda_version']
-        cudnn_version = build['cudnn_version']
-        print(f"TensorFlow was compiled against CUDA {cuda_version} and cuDNN {cudnn_version}.")
-        return tf_gcc
-
-
-def compile_cuda(file_names, nvcc, source_dir, target_dir, logfile):
-    import tensorflow
-    tf_cflags = tensorflow.sysconfig.get_compile_flags()
-    command = [
-            nvcc,
-            join(source_dir, f'{file_names}.cu.cc'),
-            '-o', join(target_dir, f'{file_names}.cu.o'),
-            '-std=c++11',
-            '-c',
-            '-D GOOGLE_CUDA=1',
-            '-x', 'cu',
-            '-Xcompiler',
-            '-fPIC',
-            '--expt-relaxed-constexpr',
-            '-DNDEBUG',
-            '-O3'
-        ] + tf_cflags
-    print(f"nvcc {file_names}")
-    logfile.writelines(["\n", " ".join(command), "\n"])
-    subprocess.check_call(command, stdout=logfile, stderr=logfile)
-
-
-def compile_gcc(file_names, gcc, source_dir, target_dir, cuda_lib, logfile):
-    import tensorflow
-    tf_cflags = tensorflow.sysconfig.get_compile_flags()
-    tf_lflags = tensorflow.sysconfig.get_link_flags()
-    link_cuda_lib = '-L' + cuda_lib
-    command = [
-                gcc,
-                join(source_dir, f'{file_names}.cc'),
-                join(target_dir, f'{file_names}.cu.o'),
-                '-o', join(target_dir, f'{file_names}.so'),
-                '-std=c++11',
-                '-shared',
-                '-fPIC',
-                '-lcudart',
-                '-O3',
-                link_cuda_lib
-            ] + tf_cflags + tf_lflags
-    print(f"gcc {file_names}")
-    logfile.writelines(["\n", " ".join(command), "\n"])
-    subprocess.check_call(command, stdout=logfile, stderr=logfile)
+import os
+from os.path import isfile, isdir, abspath, join, dirname
+import subprocess
+
+
+def compile_cuda_ops(gcc: str = None,
+                     nvcc: str = None,
+                     cuda_lib: str = None):
+    tf_gcc = check_tf_cuda_compatibility()
+    if gcc is None:
+        gcc = tf_gcc if isfile(tf_gcc) else 'gcc'
+    if nvcc is None:
+        nvcc = '/usr/local/cuda/bin/nvcc' if isfile('/usr/local/cuda/bin/nvcc') else 'nvcc'
+    if cuda_lib is None:
+        cuda_lib = '/usr/local/cuda/lib64/'
+
+    phi_tf_path = abspath(dirname(__file__))
+    src_path = join(phi_tf_path, 'cuda', 'src')
+    build_path = join(phi_tf_path, 'cuda', 'build')
+    logfile_path = join(phi_tf_path, 'cuda', 'log.txt')
+    print("Source Path:\t" + src_path)
+    print("Build Path:\t" + build_path)
+    print("GCC:\t\t" + gcc)
+    print("NVCC:\t\t" + nvcc)
+    print("CUDA lib:\t" + cuda_lib)
+    print("----------------------------")
+    # Remove old build files
+    if isdir(build_path):
+        print('Removing old build files from %s' % build_path)
+        for file in os.listdir(build_path):
+            os.remove(join(build_path, file))
+    else:
+        print('Creating build directory at %s' % build_path)
+        os.mkdir(build_path)
+    print('Compiling CUDA code...')
+    with open(logfile_path, "w") as logfile:
+        try:
+            compile_cuda('resample', nvcc, src_path, build_path, logfile=logfile)
+            compile_gcc('resample', gcc, src_path, build_path, cuda_lib, logfile=logfile)
+            compile_cuda('resample_gradient', nvcc, src_path, build_path, logfile=logfile)
+            compile_gcc('resample_gradient', gcc, src_path, build_path, cuda_lib, logfile=logfile)
+            # compile_cuda('bicgstab_ilu_linear_solve_op', self.nvcc, src_path, build_path, logfile=logfile)
+            # compile_gcc('bicgstab_ilu_linear_solve_op', self.gcc, src_path, build_path, self.cuda_lib, logfile=logfile)
+        except BaseException as err:
+            print(f"Compilation failed. See {logfile_path} for details.")
+            raise err
+    print(f"Compilation complete. See {logfile_path} for details.")
+
+
+def check_tf_cuda_compatibility():
+    import tensorflow
+    build = tensorflow.sysconfig.get_build_info()  # is_rocm_build, cuda_compute_capabilities
+    tf_gcc = build['cpu_compiler']
+    is_cuda_build = build['is_cuda_build']
+    print(f"TensorFlow compiler: {tf_gcc}.")
+    if not is_cuda_build:
+        raise AssertionError("Your TensorFlow build does not support CUDA.")
+    else:
+        cuda_version = build['cuda_version']
+        cudnn_version = build['cudnn_version']
+        print(f"TensorFlow was compiled against CUDA {cuda_version} and cuDNN {cudnn_version}.")
+        return tf_gcc
+
+
+def compile_cuda(file_names, nvcc, source_dir, target_dir, logfile):
+    import tensorflow
+    tf_cflags = tensorflow.sysconfig.get_compile_flags()
+    command = [
+            nvcc,
+            join(source_dir, f'{file_names}.cu.cc'),
+            '-o', join(target_dir, f'{file_names}.cu.o'),
+            '-std=c++11',
+            '-c',
+            '-D GOOGLE_CUDA=1',
+            '-x', 'cu',
+            '-Xcompiler',
+            '-fPIC',
+            '--expt-relaxed-constexpr',
+            '-DNDEBUG',
+            '-O3'
+        ] + tf_cflags
+    print(f"nvcc {file_names}")
+    logfile.writelines(["\n", " ".join(command), "\n"])
+    subprocess.check_call(command, stdout=logfile, stderr=logfile)
+
+
+def compile_gcc(file_names, gcc, source_dir, target_dir, cuda_lib, logfile):
+    import tensorflow
+    tf_cflags = tensorflow.sysconfig.get_compile_flags()
+    tf_lflags = tensorflow.sysconfig.get_link_flags()
+    link_cuda_lib = '-L' + cuda_lib
+    command = [
+                gcc,
+                join(source_dir, f'{file_names}.cc'),
+                join(target_dir, f'{file_names}.cu.o'),
+                '-o', join(target_dir, f'{file_names}.so'),
+                '-std=c++11',
+                '-shared',
+                '-fPIC',
+                '-lcudart',
+                '-O3',
+                link_cuda_lib
+            ] + tf_cflags + tf_lflags
+    print(f"gcc {file_names}")
+    logfile.writelines(["\n", " ".join(command), "\n"])
+    subprocess.check_call(command, stdout=logfile, stderr=logfile)
```

### Comparing `phiflow-2.3.4/phi/tf/_profiling.py` & `phiflow-2.4.0/phi/tf/_profiling.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,60 +1,60 @@
-import json
-import os
-import threading
-
-import tensorflow as tf
-from tensorflow.python.client import timeline
-
-
-class Timeliner:
-
-    # _timeline_dict = None
-    # options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
-    # run_metadata = tf.RunMetadata()
-
-    def update_timeline(self, chrome_trace):
-        # convert chrome trace to python dict
-        chrome_trace_dict = json.loads(chrome_trace)
-        # for first run store full trace
-        if self._timeline_dict is None:
-            self._timeline_dict = chrome_trace_dict
-        # for other - update only time consumption, not definitions
-        else:
-            for event in chrome_trace_dict['traceEvents']:
-                # events time consumption started with 'ts' prefix
-                if 'ts' in event:
-                    self._timeline_dict['traceEvents'].append(event)
-
-    def save(self, f_name):
-        os.path.isdir(os.path.dirname(f_name)) or os.makedirs(os.path.dirname(f_name))
-        with open(f_name, 'w') as f:
-            json.dump(self._timeline_dict, f)
-
-    def add_run(self, run_metadata=None):
-        if run_metadata is None:
-            run_metadata = self.run_metadata
-        fetched_timeline = timeline.Timeline(run_metadata.step_stats)
-        chrome_trace = fetched_timeline.generate_chrome_trace_format()
-        self.update_timeline(chrome_trace)
-
-
-def launch_tensorboard(log_dir, same_process=False, port=6006):
-    if port is None:
-        port = 6006
-    if same_process:
-        from tensorboard import main as tb
-        tf.flags.FLAGS.logdir = log_dir
-        tf.flags.FLAGS.reload_interval = 1
-        tf.flags.FLAGS.port = port
-        threading.Thread(target=tb.main).start()
-    else:
-        def run_tb():
-            os.system('tensorboard --logdir=%s --port=%d' % (log_dir,port))
-        threading.Thread(target=run_tb).start()
-    try:
-        import phi.local.hostname
-        host = phi.local.hostname.hostname
-    except (ImportError, AttributeError):
-        host = 'localhost'  # socket.gethostname()
-    url = "http://%s:%d/" % (host,port)
-    return url
+import json
+import os
+import threading
+
+import tensorflow as tf
+from tensorflow.python.client import timeline
+
+
+class Timeliner:
+
+    # _timeline_dict = None
+    # options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
+    # run_metadata = tf.RunMetadata()
+
+    def update_timeline(self, chrome_trace):
+        # convert chrome trace to python dict
+        chrome_trace_dict = json.loads(chrome_trace)
+        # for first run store full trace
+        if self._timeline_dict is None:
+            self._timeline_dict = chrome_trace_dict
+        # for other - update only time consumption, not definitions
+        else:
+            for event in chrome_trace_dict['traceEvents']:
+                # events time consumption started with 'ts' prefix
+                if 'ts' in event:
+                    self._timeline_dict['traceEvents'].append(event)
+
+    def save(self, f_name):
+        os.path.isdir(os.path.dirname(f_name)) or os.makedirs(os.path.dirname(f_name))
+        with open(f_name, 'w') as f:
+            json.dump(self._timeline_dict, f)
+
+    def add_run(self, run_metadata=None):
+        if run_metadata is None:
+            run_metadata = self.run_metadata
+        fetched_timeline = timeline.Timeline(run_metadata.step_stats)
+        chrome_trace = fetched_timeline.generate_chrome_trace_format()
+        self.update_timeline(chrome_trace)
+
+
+def launch_tensorboard(log_dir, same_process=False, port=6006):
+    if port is None:
+        port = 6006
+    if same_process:
+        from tensorboard import main as tb
+        tf.flags.FLAGS.logdir = log_dir
+        tf.flags.FLAGS.reload_interval = 1
+        tf.flags.FLAGS.port = port
+        threading.Thread(target=tb.main).start()
+    else:
+        def run_tb():
+            os.system('tensorboard --logdir=%s --port=%d' % (log_dir,port))
+        threading.Thread(target=run_tb).start()
+    try:
+        import phi.local.hostname
+        host = phi.local.hostname.hostname
+    except (ImportError, AttributeError):
+        host = 'localhost'  # socket.gethostname()
+    url = "http://%s:%d/" % (host,port)
+    return url
```

### Comparing `phiflow-2.3.4/phi/tf/_tf_backend.py` & `phiflow-2.4.0/phi/tf/_tf_backend.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,751 +1,817 @@
-import numbers
-from contextlib import contextmanager
-from functools import wraps, partial
-from typing import List, Callable, Tuple, Union
-
-import keras
-import numpy as np
-import os
-import tensorflow as tf
-from tensorflow.python.client import device_lib
-from tensorflow.python.framework.errors_impl import NotFoundError
-
-from ..math.backend._backend import combined_dim, TensorType
-from ..math.backend._dtype import DType, to_numpy_dtype, from_numpy_dtype
-from phi.math.backend import Backend, ComputeDevice, NUMPY
-from ._tf_cuda_resample import resample_cuda, use_cuda
-
-
-class TFBackend(Backend):
-
-    def __init__(self):
-        devices = [ComputeDevice(self, device.name, simple_device_type(device.device_type), device.memory_limit, -1, str(device), device.name) for device in device_lib.list_local_devices()]
-        # Example refs: '/device:CPU:0'
-        default_device_ref = '/' + os.path.basename(tf.zeros(()).device)
-        default_device = None
-        for device in devices:
-            if device.ref == default_device_ref:
-                default_device = device
-        assert default_device is not None
-        Backend.__init__(self, "TensorFlow", devices, default_device)
-
-    def prefers_channels_last(self) -> bool:
-        return True
-
-    def _device_for(self, *values):
-        devices = set(v.device for v in values if hasattr(v, 'device'))
-        if len(devices) == 0:
-            return tf.device(self._default_device.ref)
-        elif len(devices) == 1:
-            return tf.device(next(iter(devices)))
-        else:
-            return tf.device(self._default_device.ref)
-
-    def seed(self, seed: int):
-        tf.random.set_seed(seed)
-
-    def is_module(self, obj):
-        return isinstance(obj, keras.Model)
-
-    def is_tensor(self, x, only_native=False):
-        is_tf_tensor = tf.is_tensor(x) is True  # tf.is_tensor() can return non-bool values which indicates not a Tensor
-        if only_native:
-            return is_tf_tensor
-        else:
-            return is_tf_tensor or NUMPY.is_tensor(x, only_native=False)
-
-    def as_tensor(self, x, convert_external=True):
-        with tf.device(self._default_device.ref):
-            if self.is_tensor(x, only_native=convert_external):
-                return tf.identity(x)
-            tensor = tf.convert_to_tensor(x)
-            # --- Enforce Precision ---
-            if not isinstance(tensor, numbers.Number):
-                if isinstance(tensor, np.ndarray):
-                    tensor = NUMPY.as_tensor(tensor)
-                elif tensor.dtype.is_floating:
-                    tensor = self.to_float(tensor)
-            return tensor
-
-    def is_available(self, tensor) -> bool:
-        if self.is_tensor(tensor, only_native=True):
-            return tf.executing_eagerly()
-        else:
-            return True
-
-    def numpy(self, tensor):
-        if tf.is_tensor(tensor):
-            return tensor.numpy()
-        return NUMPY.numpy(tensor)
-
-    def to_dlpack(self, tensor):
-        from tensorflow import experimental
-        return experimental.dlpack.to_dlpack(tensor)
-
-    def from_dlpack(self, capsule):
-        from tensorflow import experimental
-        with tf.device(self._default_device.ref):
-            return experimental.dlpack.from_dlpack(capsule)
-
-    def copy(self, tensor, only_mutable=False):
-        if not only_mutable or tf.executing_eagerly():
-            with tf.device(tensor.device):
-                return tf.identity(tensor)
-        else:
-            return tensor
-
-    def get_device(self, tensor: TensorType) -> ComputeDevice:
-        device_name = '/' + os.path.basename(tensor.device)
-        return self.get_device_by_ref(device_name)
-
-    def allocate_on_device(self, tensor: TensorType, device: ComputeDevice) -> TensorType:
-        with tf.device(device.ref):
-            result = tf.identity(tensor)
-            assert self.get_device(result) == device
-            return result
-
-    def jit_compile(self, f: Callable) -> Callable:
-        compiled = tf.function(f)
-        return lambda *args: self.as_registered.call(compiled, *args, name=f"run jit-compiled '{f.__name__}'")
-
-    def custom_gradient(self, f: Callable, gradient: Callable, get_external_cache: Callable = None, on_call_skipped: Callable = None) -> Callable:
-        @tf.custom_gradient
-        def tf_function(*args, **kwargs):
-            def grad(*grad_args):
-                return gradient(args, y, grad_args)
-            y = f(*args, **kwargs)
-            return y, grad
-        return tf_function
-
-    def transpose(self, tensor, axes):
-        with tf.device(tensor.device):
-            return tf.transpose(tensor, perm=axes)
-
-    def equal(self, x, y):
-        with self._device_for(x, y):
-            x, y = self.auto_cast(x, y)
-            return tf.equal(x, y)
-
-    def divide_no_nan(self, x, y):
-        with self._device_for(x, y):
-            x, y = self.auto_cast(x, y)
-            return tf.math.divide_no_nan(x, y)
-
-    def random_uniform(self, shape, low, high, dtype: Union[DType, None]):
-        dtype = dtype or self.float_type
-        tdt = to_numpy_dtype(dtype)
-        with tf.device(self._default_device.ref):
-            if dtype.kind != complex:
-                return tf.random.uniform(shape, low, high, dtype=tdt)
-            else:
-                real = tf.cast(tf.random.uniform(shape, low.real, high.real, dtype=to_numpy_dtype(DType(float, dtype.precision))), tdt)
-                imag = tf.cast(tf.random.uniform(shape, low.imag, high.imag, dtype=to_numpy_dtype(DType(float, dtype.precision))), tdt)
-                return real + 1j * imag
-
-    def random_normal(self, shape, dtype: DType):
-        with tf.device(self._default_device.ref):
-            return tf.random.normal(shape, dtype=to_numpy_dtype(dtype or self.float_type))
-
-    def range(self, start, limit=None, delta=1, dtype: DType = DType(int, 32)):
-        with tf.device(self._default_device.ref):
-            return tf.range(start, limit, delta, to_numpy_dtype(dtype))
-
-    def tile(self, value, multiples):
-        with tf.device(value.device):
-            if isinstance(multiples, (tuple, list)) and self.ndims(value) < len(multiples):
-                value = self.expand_dims(value, axis=0, number=len(multiples) - self.ndims(value))
-            return tf.tile(value, multiples)
-
-    def repeat(self, x, repeats, axis: int):
-        x = self.as_tensor(x)
-        with tf.device(x.device):
-            return tf.repeat(x, repeats, axis)
-
-    def stack(self, values, axis=0):
-        with self._device_for(*values):
-            return tf.stack(values, axis=axis)
-
-    def concat(self, values, axis):
-        with self._device_for(*values):
-            values = self.auto_cast(*values)
-            return tf.concat(values, axis)
-
-    def pad(self, value, pad_width, mode='constant', constant_values=0):
-        if mode == 'boundary' and np.all(np.array(pad_width) <= 1):
-            mode = 'symmetric'
-        if mode in ('constant', 'symmetric', 'reflect'):
-            with tf.device(value.device):
-                constant_values = tf.cast(constant_values, value.dtype)
-                return tf.pad(value, pad_width, mode.upper(), constant_values=constant_values)
-        else:
-            return NotImplemented
-
-    def reshape(self, value, shape):
-        with tf.device(value.device):
-            return tf.reshape(value, shape)
-
-    def sum(self, value, axis=None, keepdims=False):
-        with tf.device(value.device):
-            if self.dtype(value).kind == bool:
-                value = self.to_int32(value)
-            if axis is not None:
-                if not isinstance(axis, int):
-                    axis = list(axis)
-            if isinstance(value, tf.SparseTensor):
-                return tf.sparse.reduce_sum(value, axis=axis, keepdims=keepdims, output_is_sparse=False)
-            if isinstance(value, (tuple, list)) and any([isinstance(x, tf.SparseTensor) for x in value]):
-                result = value[0]
-                for v in value[1:]:
-                    result = tf.sparse.add(result, v, threshold=0)
-                return result
-            return tf.reduce_sum(value, axis=axis, keepdims=keepdims)
-
-    def prod(self, value, axis=None):
-        with tf.device(value.device):
-            if axis is not None:
-                if not isinstance(axis, int):
-                    axis = list(axis)
-            if value.dtype == bool:
-                return tf.reduce_all(value, axis=axis)
-            return tf.reduce_prod(value, axis=axis)
-
-    def where(self, condition, x=None, y=None):
-        with self._device_for(condition, x, y):
-            x, y = self.auto_cast(x, y)
-            condition = tf.cast(condition, tf.bool)
-            return tf.where(condition, x, y)
-
-    def nonzero(self, values):
-        with tf.device(values.device):
-            return tf.where(tf.not_equal(values, 0))
-
-    def mean(self, value, axis=None, keepdims=False):
-        with tf.device(value.device):
-            if self.dtype(value).kind not in (float, complex):
-                value = self.to_float(value)
-            if axis is not None:
-                if not isinstance(axis, int):
-                    axis = list(axis)
-            return tf.reduce_mean(value, axis, keepdims=keepdims)
-
-    def grid_sample(self, grid, coordinates, extrapolation: str):
-        assert extrapolation in ('undefined', 'zeros', 'boundary', 'periodic', 'symmetric', 'reflect'), extrapolation
-        if use_cuda(grid):
-            if self.staticshape(grid)[0] > self.staticshape(coordinates)[0]:
-                assert self.staticshape(coordinates)[0] == 1
-                coordinates = self.tile(coordinates, [self.staticshape(grid)[0], *[1] * (self.ndims(coordinates) - 1)])
-            return resample_cuda(grid, coordinates, extrapolation)
-        else:
-            return NotImplemented
-
-    def zeros(self, shape, dtype: DType = None):
-        with tf.device(self._default_device.ref):
-            return tf.zeros(shape, dtype=to_numpy_dtype(dtype or self.float_type))
-
-    def zeros_like(self, tensor):
-        with tf.device(self._default_device.ref):
-            return tf.zeros_like(tensor)
-
-    def ones(self, shape, dtype: DType = None):
-        with tf.device(self._default_device.ref):
-            return tf.ones(shape, dtype=to_numpy_dtype(dtype or self.float_type))
-
-    def ones_like(self, tensor):
-        with tf.device(self._default_device.ref):
-            return tf.ones_like(tensor)
-
-    def meshgrid(self, *coordinates):
-        with tf.device(self._default_device.ref):
-            result = tf.meshgrid(*coordinates, indexing='ij')
-            return result
-
-    def linspace(self, start, stop, number):
-        with tf.device(self._default_device.ref):
-            return self.to_float(tf.linspace(start, stop, number))
-
-    def tensordot(self, a, a_axes: Union[tuple, list], b, b_axes: Union[tuple, list]):
-        with self._device_for(a, b):
-            a, b = self.auto_cast(a, b, bool_to_int=True)
-            return tf.tensordot(a, b, (a_axes, b_axes))
-
-    def mul_matrix_batched_vector(self, A, b):
-        with self._device_for(A, b):
-            if isinstance(A, tf.SparseTensor):
-                result_T = tf.sparse.sparse_dense_matmul(A, tf.transpose(b))  # result shape contains unknown size
-                result = tf.transpose(result_T)
-                result.set_shape(tf.TensorShape([b.shape[0], A.shape[0]]))
-                return result
-            else:
-                return tf.transpose(tf.matmul(A, b, transpose_b=True))
-
-    def einsum(self, equation, *tensors):
-        with self._device_for(*tensors):
-            return tf.einsum(equation, *tensors)
-
-    def cumsum(self, x, axis: int):
-        with tf.device(x.device):
-            return tf.cumsum(x, axis=axis, exclusive=False)
-
-    def while_loop(self, loop: Callable, values: tuple, max_iter: Union[int, Tuple[int, ...], List[int]]):
-        with self._device_for(*values):
-            if isinstance(max_iter, (tuple, list)):  # stack traced trajectory, unroll until max_iter
-                values = self.stop_gradient_tree(values)
-                trj = [values] if 0 in max_iter else []
-                for i in range(1, max(max_iter) + 1):
-                    values = loop(*values)
-                    if i in max_iter:
-                        trj.append(values)  # values are not mutable so no need to copy
-                    condition = values[0]
-                    if self.is_available(condition) and not self.any(values[0]):
-                        break
-                trj.extend([trj[-1]] * (len(max_iter) - len(trj)))  # fill trj with final values
-                return self.stop_gradient_tree(self.stack_leaves(trj))
-            else:
-                cond = lambda c, *vals: tf.reduce_any(tf.cast(c, tf.bool))
-                return self.stop_gradient_tree(tf.while_loop(cond, loop, values, maximum_iterations=max_iter))
-
-    def stop_gradient_tree(self, tree):
-        return tf.nest.map_structure(tf.stop_gradient, tree)
-
-    def abs(self, x):
-        with tf.device(x.device):
-            return tf.abs(x)
-
-    def sign(self, x):
-        with tf.device(x.device):
-            return tf.sign(x)
-
-    def round(self, x):
-        with tf.device(x.device):
-            return tf.round(x)
-
-    def ceil(self, x):
-        with tf.device(x.device):
-            return tf.math.ceil(x)
-
-    def floor(self, x):
-        with tf.device(x.device):
-            return tf.floor(x)
-
-    def max(self, x, axis=None, keepdims=False):
-        with tf.device(x.device):
-            if isinstance(x, (tuple, list)):
-                x = tf.stack(x)
-            if x.dtype == tf.bool:
-                return tf.cast(tf.reduce_max(tf.cast(x, tf.uint8), axis=axis, keepdims=keepdims), tf.bool)  # reduce_max allows no bool
-            return tf.reduce_max(x, axis=axis, keepdims=keepdims)
-
-    def min(self, x, axis=None, keepdims=False):
-        with tf.device(x.device):
-            if isinstance(x, (tuple, list)):
-                x = tf.stack(x)
-            if x.dtype == tf.bool:
-                return tf.cast(tf.reduce_min(tf.cast(x, tf.uint8), axis=axis, keepdims=keepdims), tf.bool)  # reduce_min allows no bool
-            return tf.reduce_min(x, axis=axis, keepdims=keepdims)
-
-    def maximum(self, a, b):
-        with self._device_for(a, b):
-            a, b = self.auto_cast(a, b)
-            return tf.maximum(a, b)
-
-    def minimum(self, a, b):
-        with self._device_for(a, b):
-            a, b = self.auto_cast(a, b)
-            return tf.minimum(a, b)
-
-    def clip(self, x, minimum, maximum):
-        with self._device_for(x, minimum, maximum):
-            x, minimum, maximum = self.auto_cast(x, minimum, maximum)
-            return tf.clip_by_value(x, minimum, maximum)
-
-    def sqrt(self, x):
-        with tf.device(x.device):
-            return tf.sqrt(x)
-
-    def exp(self, x):
-        with tf.device(x.device):
-            return tf.exp(x)
-
-    def conv(self, value, kernel, zero_padding=True):
-        with self._device_for(value, kernel):
-            value = self.to_float(value)
-            kernel = self.to_float(kernel)  # should use auto_cast but TensorFlow only supports DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32
-            if zero_padding:
-                value_padding = [[0, 0]] * 2 + [[s // 2, (s - 1) // 2] for s in kernel.shape[3:]]
-                value = tf.pad(value, value_padding)
-            convf = {3: partial(tf.nn.conv1d, stride=1),
-                     4: partial(tf.nn.conv2d, strides=[1, 1, 1, 1]),
-                     5: partial(tf.nn.conv3d, strides=[1, 1, 1, 1, 1])}[len(value.shape)]
-            value = tf.transpose(value, [0, *range(2, self.ndims(value)), 1])  # could use data_format='NC...' but it's supported neither on CPU and for int tensors
-            kernel = tf.transpose(kernel, [0, *range(3, self.ndims(kernel)), 2, 1])
-            if kernel.shape[0] == 1:
-                result = convf(value, kernel[0, ...], padding='VALID')
-            else:
-                result = []
-                for b in range(kernel.shape[0]):
-                    result.append(convf(value[b:b+1, ...], kernel[b], padding='VALID'))
-                result = tf.concat(result, 0)
-            result = tf.transpose(result, [0, self.ndims(result) - 1, *range(1, self.ndims(result) - 1)])
-            return result
-
-    def expand_dims(self, a, axis=0, number=1):
-        a = self.as_tensor(a)
-        with tf.device(a.device):
-            if number == 0:
-                return a
-            for _i in range(number):
-                a = tf.expand_dims(a, axis)
-            return a
-
-    def shape(self, tensor):
-        if isinstance(tensor, np.ndarray):
-            return tensor.shape
-        else:
-            with tf.device(tensor.device):
-                return tf.shape(tensor)
-
-    def staticshape(self, tensor):
-        if self.is_tensor(tensor, only_native=True):
-            return tuple(tensor.shape.as_list())
-        else:
-            return np.shape(tensor)
-
-    def gather(self, values, indices, axis: int):
-        with self._device_for(values, indices):
-            return tf.gather(values, indices, axis=axis)
-
-    def batched_gather_nd(self, values, indices):
-        with self._device_for(values, indices):
-            values_shape = self.staticshape(values)
-            if values_shape[0] == 1 and self.staticshape(indices)[0] > 1:
-                result = tf.gather_nd(values[0, ...], indices, batch_dims=0)
-                return result
-            if values_shape[0] > 1 and self.staticshape(indices)[0] == 1:
-                indices = tf.tile(indices, [values_shape[0]] + [1] * (len(values_shape) - 1))
-            return tf.gather_nd(values, indices, batch_dims=1)
-
-    def unstack(self, tensor, axis=0, keepdims=False):
-        with tf.device(tensor.device):
-            unstacked = tf.unstack(tensor, axis=axis)
-            if keepdims:
-                unstacked = [self.expand_dims(c, axis=axis) for c in unstacked]
-            return unstacked
-
-    def std(self, x, axis=None, keepdims=False):
-        with tf.device(x.device):
-            if self.dtype(x).kind not in (float, complex):
-                x = self.to_float(x)
-            _mean, var = tf.nn.moments(x, axis, keepdims=keepdims)
-            return tf.sqrt(var)
-
-    def boolean_mask(self, x, mask, axis=0):
-        with self._device_for(x, mask):
-            return tf.boolean_mask(x, mask, axis=axis)
-
-    def isfinite(self, x):
-        with tf.device(x.device):
-            return tf.math.is_finite(x)
-
-    def any(self, boolean_tensor, axis=None, keepdims=False):
-        with tf.device(boolean_tensor.device):
-            if self.dtype(boolean_tensor).kind != bool:
-                boolean_tensor = tf.not_equal(boolean_tensor, 0)
-            return tf.reduce_any(boolean_tensor, axis=axis, keepdims=keepdims)
-
-    def all(self, boolean_tensor, axis=None, keepdims=False):
-        with tf.device(boolean_tensor.device):
-            if self.dtype(boolean_tensor).kind != bool:
-                boolean_tensor = tf.not_equal(boolean_tensor, 0)
-            return tf.reduce_all(boolean_tensor, axis=axis, keepdims=keepdims)
-
-    def quantile(self, x, quantiles):
-        import tensorflow_probability as tfp
-        with tf.device(x.device):
-            x = self.to_float(x)
-            result = tfp.stats.percentile(x, quantiles * 100, axis=-1, interpolation='linear')
-            return result
-
-    def scatter(self, base_grid, indices, values, mode: str):
-        with self._device_for(base_grid, indices, values):
-            base_grid, values = self.auto_cast(base_grid, values)
-            indices = self.as_tensor(indices)
-            batch_size = combined_dim(combined_dim(indices.shape[0], values.shape[0]), base_grid.shape[0])
-            scatter = tf.tensor_scatter_nd_add if mode == 'add' else tf.tensor_scatter_nd_update
-            result = []
-            for b in range(batch_size):
-                b_grid = base_grid[b, ...]
-                b_indices = indices[min(b, indices.shape[0] - 1), ...]
-                b_values = values[min(b, values.shape[0] - 1), ...]
-                result.append(scatter(b_grid, b_indices, b_values))
-            return self.stack(result, axis=0)
-
-    def fft(self, x, axes: Union[tuple, list]):
-        if not axes:
-            return x
-        x = self.to_complex(x)
-        perm = (*[i for i in range(self.ndims(x)) if i not in axes], *axes)
-        iperm = np.argsort(perm)
-        with tf.device(x.device):
-            if len(axes) == 1:
-                return tf.transpose(tf.signal.fft(tf.transpose(x, perm)), iperm)
-            elif len(axes) == 2:
-                return tf.transpose(tf.signal.fft2d(tf.transpose(x, perm)), iperm)
-            elif len(axes) == 3:
-                return tf.transpose(tf.signal.fft3d(tf.transpose(x, perm)), iperm)
-            else:
-                for axis in axes:
-                    x = self.fft(x, [axis])
-                return x
-
-    def ifft(self, k, axes: Union[tuple, list]):
-        if not axes:
-            return k
-        k = self.to_complex(k)
-        perm = (*[i for i in range(self.ndims(k)) if i not in axes], *axes)
-        iperm = np.argsort(perm)
-        with tf.device(k.device):
-            if len(axes) == 1:
-                return tf.transpose(tf.signal.ifft(tf.transpose(k, perm)), iperm)
-            elif len(axes) == 2:
-                return tf.transpose(tf.signal.ifft2d(tf.transpose(k, perm)), iperm)
-            elif len(axes) == 3:
-                return tf.transpose(tf.signal.ifft3d(tf.transpose(k, perm)), iperm)
-            else:
-                for axis in axes:
-                    k = self.ifft(k, [axis])
-                return k
-
-    def imag(self, x):
-        with tf.device(x.device):
-            return tf.math.imag(x)
-
-    def real(self, x):
-        with tf.device(x.device):
-            return tf.math.real(x)
-
-    def conj(self, x):
-        with tf.device(x.device):
-            return tf.math.conj(x)
-
-    def cast(self, x, dtype: DType):
-        if not self.is_tensor(x, only_native=True):
-            x = self.as_tensor(x, convert_external=True)
-        if self.dtype(x) == dtype:
-            return x
-        else:
-            with tf.device(x.device):
-                return tf.cast(x, to_numpy_dtype(dtype))
-
-    def sin(self, x):
-        with tf.device(x.device):
-            return tf.math.sin(x)
-
-    def arcsin(self, x):
-        with tf.device(x.device):
-            return tf.math.asin(x)
-
-    def cos(self, x):
-        with tf.device(x.device):
-            return tf.math.cos(x)
-
-    def arccos(self, x):
-        with tf.device(x.device):
-            return tf.math.acos(x)
-
-    def tan(self, x):
-        with tf.device(x.device):
-            return tf.math.tan(x)
-
-    def arctan(self, x):
-        with tf.device(x.device):
-            return tf.math.atan(x)
-
-    def arctan2(self, y, x):
-        y, x = self.auto_cast(y, x)
-        with tf.device(x.device):
-            return tf.math.atan2(y, x)
-
-    def sinh(self, x):
-        with tf.device(x.device):
-            return tf.math.sinh(x)
-
-    def arcsinh(self, x):
-        with tf.device(x.device):
-            return tf.math.asinh(x)
-
-    def cosh(self, x):
-        with tf.device(x.device):
-            return tf.math.cosh(x)
-
-    def arccosh(self, x):
-        with tf.device(x.device):
-            return tf.math.acosh(x)
-
-    def tanh(self, x):
-        with tf.device(x.device):
-            return tf.math.tanh(x)
-
-    def arctanh(self, x):
-        with tf.device(x.device):
-            return tf.math.atanh(x)
-
-    def log(self, x):
-        with tf.device(x.device):
-            return tf.math.log(x)
-
-    def sigmoid(self, x):
-        with tf.device(x.device):
-            return tf.math.sigmoid(x)
-
-    def log2(self, x):
-        with tf.device(x.device):
-            return tf.math.log(x) / 0.6931471805599453094  # log(x) / log(2)
-
-    def log10(self, x):
-        with tf.device(x.device):
-            return tf.math.log(x) / 2.3025850929940456840  # log(x) / log(10)
-
-    def dtype(self, array) -> DType:
-        if tf.is_tensor(array):
-            dt = array.dtype.as_numpy_dtype
-            return from_numpy_dtype(dt)
-        else:
-            return NUMPY.dtype(array)
-
-    def sparse_coo_tensor(self, indices, values, shape):
-        with self._device_for(indices, values):
-            return tf.SparseTensor(indices=self.to_int64(indices), values=values, dense_shape=shape)
-
-    def mul_coo_dense(self, indices, values, shape, dense):
-        values, dense = self.auto_cast(values, dense)
-        batch_size, nnz, channel_count = self.staticshape(values)
-        if batch_size > 1:
-            return Backend.mul_coo_dense(self, indices, values, shape, dense)
-        indices = tf.cast(indices, np.int64)
-        result = []
-        for b in range(batch_size):
-            b_result = []
-            for c in range(channel_count):
-                matrix = tf.SparseTensor(indices=indices[b], values=values[b, :, c], dense_shape=shape)
-                try:
-                    b_result.append(tf.sparse.sparse_dense_matmul(matrix, dense[b, :, c, :]))
-                except NotFoundError:  # These data types are probably not supported by TensorFlow
-                    return Backend.mul_coo_dense(self, indices, values, shape, dense)
-            result.append(tf.stack(b_result))
-        return tf.stack(result)
-
-    def not_equal(self, x, y):
-        with self._device_for(x, y):
-            return ~self.equal(x, y)
-
-    def greater_than(self, x, y):
-        with self._device_for(x, y):
-            x, y = self.auto_cast(x, y)
-            return x > y
-
-    def greater_or_equal(self, x, y):
-        with self._device_for(x, y):
-            x, y = self.auto_cast(x, y)
-            return x >= y
-
-    def add(self, a, b):
-        with self._device_for(a, b):
-            if isinstance(a, tf.SparseTensor) or isinstance(b, tf.SparseTensor):
-                return tf.sparse.add(a, b, threshold=1e-5)
-            else:
-                return Backend.add(self, a, b)
-
-    def sub(self, a, b):
-        with self._device_for(a, b):
-            a, b = self.auto_cast(a, b)
-            return a - b
-
-    def mul(self, a, b):
-        with self._device_for(a, b):
-            a, b = self.auto_cast(a, b)
-            return a * b
-
-    def div(self, numerator, denominator):
-        with self._device_for(numerator, denominator):
-            numerator, denominator = self.auto_cast(numerator, denominator)
-            return numerator / denominator
-
-    def pow(self, base, exp):
-        with self._device_for(base, exp):
-            base, exp = self.auto_cast(base, exp)
-            return base ** exp
-
-    def mod(self, dividend, divisor):
-        with self._device_for(divisor, dividend):
-            dividend, divisor = self.auto_cast(dividend, divisor)
-            return dividend % divisor
-
-    def and_(self, a, b):
-        with self._device_for(a, b):
-            a, b = self.auto_cast(a, b)
-            return a & b
-
-    def or_(self, a, b):
-        with self._device_for(a, b):
-            a, b = self.auto_cast(a, b)
-            return a | b
-
-    def xor(self, a, b):
-        with self._device_for(a, b):
-            a, b = self.auto_cast(a, b)
-            return a ^ b
-
-    def floordiv(self, a, b):
-        with self._device_for(a, b):
-            a, b = self.auto_cast(a, b)
-            return a // b
-
-    def jacobian(self, f, wrt: Union[tuple, list], get_output: bool, is_f_scalar: bool):
-        @wraps(f)
-        def eval_grad(*args):
-            args = [self.as_tensor(arg, True) if i in wrt else arg for i, arg in enumerate(args)]
-            args = [self.to_float(arg) if self.dtype(arg).kind in (bool, int) else arg for arg in args]
-            wrt_args = [arg for i, arg in enumerate(args) if i in wrt]
-            with tf.GradientTape(watch_accessed_variables=False) as tape:
-                for arg in wrt_args:
-                    assert arg.dtype in (tf.float16, tf.float32, tf.float64, tf.complex64, tf.complex128), f"Gradients can only be computed for float or complex tensors but got {arg.dtype} for argument with shape {arg.shape}"
-                    tape.watch(arg)
-                loss, output = f(*args)
-            if self.prod(tf.shape(loss)) == 1:
-                grads = list(self.as_registered.call(tape.gradient, loss, wrt_args, name=f"Backpropagation"))
-            else:
-                grads = list(self.as_registered.call(tape.jacobian, loss, wrt_args, name=f"Backpropagation"))
-            assert None not in grads, f"Gradient could not be computed for wrt argument {grads.index(None)} (argument {wrt[grads.index(None)]}) with shape {wrt_args[grads.index(None)].shape}. TensorFlow returned gradient=None."
-            return (*output, *grads) if get_output else grads
-        return eval_grad
-
-    def stop_gradient(self, value):
-        with self._device_for(value):
-            return tf.stop_gradient(value)
-
-    def matrix_solve_least_squares(self, matrix: TensorType, rhs: TensorType) -> Tuple[TensorType, TensorType, TensorType, TensorType]:
-        with self._device_for(matrix, rhs):
-            solution = tf.linalg.lstsq(matrix, rhs)
-        return solution, None, None, None
-
-    def solve_triangular_dense(self, matrix, rhs, lower: bool, unit_diagonal: bool):
-        matrix, rhs = self.auto_cast(matrix, rhs, int_to_float=True, bool_to_int=True)
-        rhs = self.expand_dims(rhs, -1)
-        if unit_diagonal:
-            diag = np.diag(np.ones((self.staticshape(matrix)[-1],)))
-            matrix = self.where(diag, diag, matrix)
-        result = tf.linalg.triangular_solve(matrix, rhs, lower=lower)
-        return result[..., 0]
-
-    def get_diagonal(self, matrices, offset=0):
-        with self._device_for(matrices):
-            matrices = tf.transpose(matrices, [0, 3, 1, 2])
-            result = tf.linalg.diag_part(matrices, k=offset)
-            return tf.transpose(result, [0, 2, 1])
-
-
-_TAPES = []
-
-
-def simple_device_type(t: str):
-    return t[len('XLA_'):] if t.startswith('XLA_') else t
+import numbers
+from contextlib import contextmanager
+from functools import wraps, partial
+from typing import List, Callable, Tuple, Union
+
+import keras
+import numpy as np
+import os
+import tensorflow as tf
+from tensorflow.python.client import device_lib
+from tensorflow.python.framework.errors_impl import NotFoundError
+
+from ..math.backend._backend import combined_dim, TensorType
+from ..math.backend._dtype import DType, to_numpy_dtype, from_numpy_dtype
+from phi.math.backend import Backend, ComputeDevice, NUMPY
+from ._tf_cuda_resample import resample_cuda, use_cuda
+
+
+class TFBackend(Backend):
+
+    def __init__(self):
+        devices = [ComputeDevice(self, device.name, simple_device_type(device.device_type), device.memory_limit, -1, str(device), device.name) for device in device_lib.list_local_devices()]
+        # Example refs: '/device:CPU:0'
+        default_device_ref = '/' + os.path.basename(tf.zeros(()).device)
+        default_device = None
+        for device in devices:
+            if device.ref == default_device_ref:
+                default_device = device
+        assert default_device is not None
+        Backend.__init__(self, "TensorFlow", devices, default_device)
+
+    def prefers_channels_last(self) -> bool:
+        return True
+
+    def _device_for(self, *values):
+        devices = set(v.device for v in values if hasattr(v, 'device'))
+        if len(devices) == 0:
+            return tf.device(self._default_device.ref)
+        elif len(devices) == 1:
+            return tf.device(next(iter(devices)))
+        else:
+            return tf.device(self._default_device.ref)
+
+    def seed(self, seed: int):
+        tf.random.set_seed(seed)
+
+    def is_module(self, obj):
+        return isinstance(obj, keras.Model)
+
+    def is_tensor(self, x, only_native=False):
+        is_tf_tensor = tf.is_tensor(x) is True  # tf.is_tensor() can return non-bool values which indicates not a Tensor
+        if only_native:
+            return is_tf_tensor
+        else:
+            return is_tf_tensor or NUMPY.is_tensor(x, only_native=False)
+
+    def is_sparse(self, x) -> bool:
+        return isinstance(x, tf.SparseTensor)
+
+    def as_tensor(self, x, convert_external=True):
+        with tf.device(self._default_device.ref):
+            if self.is_tensor(x, only_native=convert_external):
+                return tf.identity(x)
+            tensor = tf.convert_to_tensor(x)
+            # --- Enforce Precision ---
+            if not isinstance(tensor, numbers.Number):
+                if isinstance(tensor, np.ndarray):
+                    tensor = NUMPY.as_tensor(tensor)
+                elif tensor.dtype.is_floating:
+                    tensor = self.to_float(tensor)
+            return tensor
+
+    def is_available(self, tensor) -> bool:
+        if self.is_tensor(tensor, only_native=True):
+            return tf.executing_eagerly()
+        else:
+            return True
+
+    def numpy(self, tensor):
+        if self.is_sparse(tensor):
+            indices = np.array(tensor.indices)
+            values = np.array(tensor.values)
+            indices = indices[..., 0], indices[..., 1]
+            from scipy.sparse import coo_matrix
+            return coo_matrix((values, indices), shape=self.staticshape(tensor))
+        if tf.is_tensor(tensor):
+            return tensor.numpy()
+        return NUMPY.numpy(tensor)
+
+    def to_dlpack(self, tensor):
+        from tensorflow import experimental
+        return experimental.dlpack.to_dlpack(tensor)
+
+    def from_dlpack(self, capsule):
+        from tensorflow import experimental
+        with tf.device(self._default_device.ref):
+            return experimental.dlpack.from_dlpack(capsule)
+
+    def copy(self, tensor, only_mutable=False):
+        if not only_mutable or tf.executing_eagerly():
+            with tf.device(tensor.device):
+                return tf.identity(tensor)
+        else:
+            return tensor
+
+    def get_device(self, tensor: TensorType) -> ComputeDevice:
+        device_name = '/' + os.path.basename(tensor.device)
+        return self.get_device_by_ref(device_name)
+
+    def allocate_on_device(self, tensor: TensorType, device: ComputeDevice) -> TensorType:
+        with tf.device(device.ref):
+            result = tf.identity(tensor)
+            assert self.get_device(result) == device
+            return result
+
+    def vectorized_call(self, f, *args, output_dtypes=None, **aux_args):
+        batch_size = self.determine_size(args, 0)
+        args = [self.tile_to(t, 0, batch_size) for t in args]
+        if output_dtypes is None:
+            output0 = f(*[t[0] for t in args], **aux_args)  # Call f to determine its output signature.
+            output_dtypes = tf.nest.map_structure(lambda x: x.dtype, output0)
+        else:
+            output_dtypes = tf.nest.map_structure(lambda dtype: to_numpy_dtype(dtype), output_dtypes)
+        return tf.map_fn(lambda vals: f(*vals, **aux_args), tuple(args), fn_output_signature=output_dtypes)
+
+    def jit_compile(self, f: Callable) -> Callable:
+        compiled = tf.function(f)
+        return lambda *args: self.as_registered.call(compiled, *args, name=f"run jit-compiled '{f.__name__}'")
+
+    def custom_gradient(self, f: Callable, gradient: Callable, get_external_cache: Callable = None, on_call_skipped: Callable = None) -> Callable:
+        @tf.custom_gradient
+        def tf_function(*args, **kwargs):
+            def grad(*grad_args):
+                return gradient(args, y, grad_args)
+            y = f(*args, **kwargs)
+            return y, grad
+        return tf_function
+
+    def transpose(self, tensor, axes):
+        with tf.device(tensor.device):
+            return tf.transpose(tensor, perm=axes)
+
+    def equal(self, x, y):
+        with self._device_for(x, y):
+            x, y = self.auto_cast(x, y)
+            return tf.equal(x, y)
+
+    def divide_no_nan(self, x, y):
+        with self._device_for(x, y):
+            x, y = self.auto_cast(x, y)
+            return tf.math.divide_no_nan(x, y)
+
+    def random_uniform(self, shape, low, high, dtype: Union[DType, None]):
+        dtype = dtype or self.float_type
+        tdt = to_numpy_dtype(dtype)
+        with tf.device(self._default_device.ref):
+            if dtype.kind != complex:
+                return tf.random.uniform(shape, low, high, dtype=tdt)
+            else:
+                real = tf.cast(tf.random.uniform(shape, low.real, high.real, dtype=to_numpy_dtype(DType(float, dtype.precision))), tdt)
+                imag = tf.cast(tf.random.uniform(shape, low.imag, high.imag, dtype=to_numpy_dtype(DType(float, dtype.precision))), tdt)
+                return real + 1j * imag
+
+    def random_normal(self, shape, dtype: DType):
+        with tf.device(self._default_device.ref):
+            return tf.random.normal(shape, dtype=to_numpy_dtype(dtype or self.float_type))
+
+    def range(self, start, limit=None, delta=1, dtype: DType = DType(int, 32)):
+        with tf.device(self._default_device.ref):
+            return tf.range(start, limit, delta, to_numpy_dtype(dtype))
+
+    def tile(self, value, multiples):
+        with tf.device(value.device):
+            if isinstance(multiples, (tuple, list)) and self.ndims(value) < len(multiples):
+                value = self.expand_dims(value, axis=0, number=len(multiples) - self.ndims(value))
+            return tf.tile(value, multiples)
+
+    def repeat(self, x, repeats, axis: int, new_length=None):
+        x = self.as_tensor(x)
+        with tf.device(x.device):
+            return tf.repeat(x, repeats, axis)
+
+    def stack(self, values, axis=0):
+        with self._device_for(*values):
+            return tf.stack(values, axis=axis)
+
+    def concat(self, values, axis):
+        with self._device_for(*values):
+            values = self.auto_cast(*values)
+            return tf.concat(values, axis)
+
+    def pad(self, value, pad_width, mode='constant', constant_values=0):
+        if mode == 'boundary' and np.all(np.array(pad_width) <= 1):
+            mode = 'symmetric'
+        if mode in ('constant', 'symmetric', 'reflect'):
+            with tf.device(value.device):
+                constant_values = tf.cast(constant_values, value.dtype)
+                return tf.pad(value, pad_width, mode.upper(), constant_values=constant_values)
+        else:
+            return NotImplemented
+
+    def reshape(self, value, shape):
+        with tf.device(value.device):
+            return tf.reshape(value, shape)
+
+    def sum(self, value, axis=None, keepdims=False):
+        with tf.device(value.device):
+            if self.dtype(value).kind == bool:
+                value = self.to_int32(value)
+            if axis is not None:
+                if not isinstance(axis, int):
+                    axis = list(axis)
+            if isinstance(value, tf.SparseTensor):
+                return tf.sparse.reduce_sum(value, axis=axis, keepdims=keepdims, output_is_sparse=False)
+            if isinstance(value, (tuple, list)) and any([isinstance(x, tf.SparseTensor) for x in value]):
+                result = value[0]
+                for v in value[1:]:
+                    result = tf.sparse.add(result, v, threshold=0)
+                return result
+            return tf.reduce_sum(value, axis=axis, keepdims=keepdims)
+
+    def prod(self, value, axis=None):
+        with tf.device(value.device):
+            if axis is not None:
+                if not isinstance(axis, int):
+                    axis = list(axis)
+            if value.dtype == bool:
+                return tf.reduce_all(value, axis=axis)
+            return tf.reduce_prod(value, axis=axis)
+
+    def where(self, condition, x=None, y=None):
+        with self._device_for(condition, x, y):
+            x, y = self.auto_cast(x, y)
+            condition = tf.cast(condition, tf.bool)
+            return tf.where(condition, x, y)
+
+    def nonzero(self, values):
+        with tf.device(values.device):
+            return tf.where(tf.not_equal(values, 0))
+
+    def mean(self, value, axis=None, keepdims=False):
+        with tf.device(value.device):
+            if self.dtype(value).kind not in (float, complex):
+                value = self.to_float(value)
+            if axis is not None:
+                if not isinstance(axis, int):
+                    axis = list(axis)
+            return tf.reduce_mean(value, axis, keepdims=keepdims)
+
+    def grid_sample(self, grid, coordinates, extrapolation: str):
+        assert extrapolation in ('undefined', 'zeros', 'boundary', 'periodic', 'symmetric', 'reflect'), extrapolation
+        if use_cuda(grid):
+            if self.staticshape(grid)[0] > self.staticshape(coordinates)[0]:
+                assert self.staticshape(coordinates)[0] == 1
+                coordinates = self.tile(coordinates, [self.staticshape(grid)[0], *[1] * (self.ndims(coordinates) - 1)])
+            return resample_cuda(grid, coordinates, extrapolation)
+        else:
+            return NotImplemented
+
+    def zeros(self, shape, dtype: DType = None):
+        with tf.device(self._default_device.ref):
+            return tf.zeros(shape, dtype=to_numpy_dtype(dtype or self.float_type))
+
+    def zeros_like(self, tensor):
+        with tf.device(self._default_device.ref):
+            return tf.zeros_like(tensor)
+
+    def ones(self, shape, dtype: DType = None):
+        with tf.device(self._default_device.ref):
+            return tf.ones(shape, dtype=to_numpy_dtype(dtype or self.float_type))
+
+    def ones_like(self, tensor):
+        with tf.device(self._default_device.ref):
+            return tf.ones_like(tensor)
+
+    def meshgrid(self, *coordinates):
+        with tf.device(self._default_device.ref):
+            result = tf.meshgrid(*coordinates, indexing='ij')
+            return result
+
+    def linspace(self, start, stop, number):
+        with tf.device(self._default_device.ref):
+            return self.to_float(tf.linspace(start, stop, number))
+
+    def tensordot(self, a, a_axes: Union[tuple, list], b, b_axes: Union[tuple, list]):
+        with self._device_for(a, b):
+            a, b = self.auto_cast(a, b, bool_to_int=True)
+            return tf.tensordot(a, b, (a_axes, b_axes))
+
+    def mul_matrix_batched_vector(self, A, b):
+        with self._device_for(A, b):
+            if isinstance(A, tf.SparseTensor):
+                result_T = tf.sparse.sparse_dense_matmul(A, tf.transpose(b))  # result shape contains unknown size
+                result = tf.transpose(result_T)
+                result.set_shape(tf.TensorShape([b.shape[0], A.shape[0]]))
+                return result
+            else:
+                return tf.transpose(tf.matmul(A, b, transpose_b=True))
+
+    def einsum(self, equation, *tensors):
+        with self._device_for(*tensors):
+            return tf.einsum(equation, *tensors)
+
+    def cumsum(self, x, axis: int):
+        with tf.device(x.device):
+            return tf.cumsum(x, axis=axis, exclusive=False)
+
+    def while_loop(self, loop: Callable, values: tuple, max_iter: Union[int, Tuple[int, ...], List[int]]):
+        with self._device_for(*values):
+            if isinstance(max_iter, (tuple, list)):  # stack traced trajectory, unroll until max_iter
+                values = self.stop_gradient_tree(values)
+                trj = [values] if 0 in max_iter else []
+                for i in range(1, max(max_iter) + 1):
+                    values = loop(*values)
+                    if i in max_iter:
+                        trj.append(values)  # values are not mutable so no need to copy
+                    condition = values[0]
+                    if self.is_available(condition) and not self.any(values[0]):
+                        break
+                trj.extend([trj[-1]] * (len(max_iter) - len(trj)))  # fill trj with final values
+                return self.stop_gradient_tree(self.stack_leaves(trj))
+            else:
+                cond = lambda c, *vals: tf.reduce_any(tf.cast(c, tf.bool))
+                return self.stop_gradient_tree(tf.while_loop(cond, loop, values, maximum_iterations=max_iter))
+
+    def stop_gradient_tree(self, tree):
+        return tf.nest.map_structure(tf.stop_gradient, tree)
+
+    def abs(self, x):
+        with tf.device(x.device):
+            return tf.abs(x)
+
+    def sign(self, x):
+        with tf.device(x.device):
+            return tf.sign(x)
+
+    def round(self, x):
+        with tf.device(x.device):
+            return tf.round(x)
+
+    def ceil(self, x):
+        with tf.device(x.device):
+            return tf.math.ceil(x)
+
+    def floor(self, x):
+        with tf.device(x.device):
+            return tf.floor(x)
+
+    def max(self, x, axis=None, keepdims=False):
+        with tf.device(x.device):
+            if isinstance(x, (tuple, list)):
+                x = tf.stack(x)
+            if x.dtype == tf.bool:
+                return tf.cast(tf.reduce_max(tf.cast(x, tf.uint8), axis=axis, keepdims=keepdims), tf.bool)  # reduce_max allows no bool
+            return tf.reduce_max(x, axis=axis, keepdims=keepdims)
+
+    def min(self, x, axis=None, keepdims=False):
+        with tf.device(x.device):
+            if isinstance(x, (tuple, list)):
+                x = tf.stack(x)
+            if x.dtype == tf.bool:
+                return tf.cast(tf.reduce_min(tf.cast(x, tf.uint8), axis=axis, keepdims=keepdims), tf.bool)  # reduce_min allows no bool
+            return tf.reduce_min(x, axis=axis, keepdims=keepdims)
+
+    def maximum(self, a, b):
+        with self._device_for(a, b):
+            a, b = self.auto_cast(a, b)
+            return tf.maximum(a, b)
+
+    def minimum(self, a, b):
+        with self._device_for(a, b):
+            a, b = self.auto_cast(a, b)
+            return tf.minimum(a, b)
+
+    def clip(self, x, minimum, maximum):
+        with self._device_for(x, minimum, maximum):
+            x, minimum, maximum = self.auto_cast(x, minimum, maximum)
+            return tf.clip_by_value(x, minimum, maximum)
+
+    def sqrt(self, x):
+        with tf.device(x.device):
+            return tf.sqrt(x)
+
+    def exp(self, x):
+        with tf.device(x.device):
+            return tf.exp(x)
+
+    def softplus(self, x):
+        with tf.device(x.device):
+            return tf.math.softplus(x)
+
+    def log_gamma(self, x):
+        with tf.device(x.device):
+            return tf.math.lgamma(self.to_float(x))
+
+    def conv(self, value, kernel, zero_padding=True):
+        with self._device_for(value, kernel):
+            value = self.to_float(value)
+            kernel = self.to_float(kernel)  # should use auto_cast but TensorFlow only supports DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32
+            if zero_padding:
+                value_padding = [[0, 0]] * 2 + [[s // 2, (s - 1) // 2] for s in kernel.shape[3:]]
+                value = tf.pad(value, value_padding)
+            convf = {3: partial(tf.nn.conv1d, stride=1),
+                     4: partial(tf.nn.conv2d, strides=[1, 1, 1, 1]),
+                     5: partial(tf.nn.conv3d, strides=[1, 1, 1, 1, 1])}[len(value.shape)]
+            value = tf.transpose(value, [0, *range(2, self.ndims(value)), 1])  # could use data_format='NC...' but it's supported neither on CPU and for int tensors
+            kernel = tf.transpose(kernel, [0, *range(3, self.ndims(kernel)), 2, 1])
+            if kernel.shape[0] == 1:
+                result = convf(value, kernel[0, ...], padding='VALID')
+            else:
+                result = []
+                for b in range(kernel.shape[0]):
+                    result.append(convf(value[b:b+1, ...], kernel[b], padding='VALID'))
+                result = tf.concat(result, 0)
+            result = tf.transpose(result, [0, self.ndims(result) - 1, *range(1, self.ndims(result) - 1)])
+            return result
+
+    def expand_dims(self, a, axis=0, number=1):
+        a = self.as_tensor(a)
+        with tf.device(a.device):
+            if number == 0:
+                return a
+            for _i in range(number):
+                a = tf.expand_dims(a, axis)
+            return a
+
+    def shape(self, tensor):
+        if isinstance(tensor, np.ndarray):
+            return tensor.shape
+        else:
+            with tf.device(tensor.device):
+                return tf.shape(tensor)
+
+    def staticshape(self, tensor):
+        if self.is_tensor(tensor, only_native=True):
+            return tuple(tensor.shape.as_list())
+        else:
+            return np.shape(tensor)
+
+    def gather(self, values, indices, axis: int):
+        with self._device_for(values, indices):
+            indices = indices % self.cast(self.shape(values)[axis], self.dtype(indices))
+            return tf.gather(values, indices, axis=axis)
+
+    def gather_by_component_indices(self, values, *component_indices):
+        indices = self.stack(component_indices, -1)
+        return tf.gather_nd(values, indices)
+
+    def batched_gather_nd(self, values, indices):
+        with self._device_for(values, indices):
+            values_shape = self.staticshape(values)
+            if values_shape[0] == 1 and self.staticshape(indices)[0] > 1:
+                result = tf.gather_nd(values[0, ...], indices, batch_dims=0)
+                return result
+            if values_shape[0] > 1 and self.staticshape(indices)[0] == 1:
+                indices = tf.tile(indices, [values_shape[0]] + [1] * (len(values_shape) - 1))
+            return tf.gather_nd(values, indices, batch_dims=1)
+
+    def unstack(self, tensor, axis=0, keepdims=False):
+        with tf.device(tensor.device):
+            unstacked = tf.unstack(tensor, axis=axis)
+            if keepdims:
+                unstacked = [self.expand_dims(c, axis=axis) for c in unstacked]
+            return unstacked
+
+    def std(self, x, axis=None, keepdims=False):
+        with tf.device(x.device):
+            if self.dtype(x).kind not in (float, complex):
+                x = self.to_float(x)
+            _mean, var = tf.nn.moments(x, axis, keepdims=keepdims)
+            return tf.sqrt(var)
+
+    def boolean_mask(self, x, mask, axis=0, new_length=None, fill_value=0):
+        with self._device_for(x, mask):
+            return tf.boolean_mask(x, mask, axis=axis)
+
+    def isfinite(self, x):
+        if self.dtype(x).kind in (bool, int):
+            return self.ones(self.shape(x), dtype=DType(bool))
+        with tf.device(x.device):
+            return tf.math.is_finite(x)
+
+    def isnan(self, x):
+        if self.dtype(x).kind in (bool, int):
+            return self.zeros(self.shape(x), dtype=DType(bool))
+        with tf.device(x.device):
+            return tf.math.is_nan(x)
+
+    def isinf(self, x):
+        if self.dtype(x).kind in (bool, int):
+            return self.zeros(self.shape(x), dtype=DType(bool))
+        with tf.device(x.device):
+            return tf.math.is_inf(x)
+
+    def any(self, boolean_tensor, axis=None, keepdims=False):
+        with tf.device(boolean_tensor.device):
+            if self.dtype(boolean_tensor).kind != bool:
+                boolean_tensor = tf.not_equal(boolean_tensor, 0)
+            return tf.reduce_any(boolean_tensor, axis=axis, keepdims=keepdims)
+
+    def all(self, boolean_tensor, axis=None, keepdims=False):
+        with tf.device(boolean_tensor.device):
+            if self.dtype(boolean_tensor).kind != bool:
+                boolean_tensor = tf.not_equal(boolean_tensor, 0)
+            return tf.reduce_all(boolean_tensor, axis=axis, keepdims=keepdims)
+
+    def quantile(self, x, quantiles):
+        import tensorflow_probability as tfp
+        with tf.device(x.device):
+            x = self.to_float(x)
+            result = tfp.stats.percentile(x, quantiles * 100, axis=-1, interpolation='linear')
+            return result
+
+    def argsort(self, x, axis=-1):
+        return tf.argsort(x, axis)
+
+    def searchsorted(self, sorted_sequence, search_values, side: str, dtype=DType(int, 32)):
+        return tf.searchsorted(sorted_sequence, search_values, side=side, out_type=to_numpy_dtype(dtype))
+
+    def scatter(self, base_grid, indices, values, mode: str):
+        with self._device_for(base_grid, indices, values):
+            base_grid, values = self.auto_cast(base_grid, values)
+            indices = self.as_tensor(indices)
+            batch_size = combined_dim(combined_dim(indices.shape[0], values.shape[0]), base_grid.shape[0])
+            scatter = tf.tensor_scatter_nd_add if mode == 'add' else tf.tensor_scatter_nd_update
+            result = []
+            for b in range(batch_size):
+                b_grid = base_grid[b, ...]
+                b_indices = indices[min(b, indices.shape[0] - 1), ...]
+                b_values = values[min(b, values.shape[0] - 1), ...]
+                result.append(scatter(b_grid, b_indices, b_values))
+            return self.stack(result, axis=0)
+
+    def histogram1d(self, values, weights, bin_edges):
+        with self._device_for(values, weights, bin_edges):
+            bin_count = self.staticshape(bin_edges)[-1] - 1
+            bin_indices = tf.minimum(tf.searchsorted(bin_edges, values, side='right') - 1, bin_count - 1)  # ToDo this includes values outside
+            hist = tf.math.bincount(bin_indices, weights=weights, minlength=bin_count, maxlength=bin_count, axis=-1)
+            return hist
+
+    def bincount(self, x, weights, bins: int):
+        return tf.math.bincount(x, weights=weights, minlength=bins, maxlength=bins)
+
+    def fft(self, x, axes: Union[tuple, list]):
+        if not axes:
+            return x
+        x = self.to_complex(x)
+        perm = (*[i for i in range(self.ndims(x)) if i not in axes], *axes)
+        iperm = np.argsort(perm)
+        with tf.device(x.device):
+            if len(axes) == 1:
+                return tf.transpose(tf.signal.fft(tf.transpose(x, perm)), iperm)
+            elif len(axes) == 2:
+                return tf.transpose(tf.signal.fft2d(tf.transpose(x, perm)), iperm)
+            elif len(axes) == 3:
+                return tf.transpose(tf.signal.fft3d(tf.transpose(x, perm)), iperm)
+            else:
+                for axis in axes:
+                    x = self.fft(x, [axis])
+                return x
+
+    def ifft(self, k, axes: Union[tuple, list]):
+        if not axes:
+            return k
+        k = self.to_complex(k)
+        perm = (*[i for i in range(self.ndims(k)) if i not in axes], *axes)
+        iperm = np.argsort(perm)
+        with tf.device(k.device):
+            if len(axes) == 1:
+                return tf.transpose(tf.signal.ifft(tf.transpose(k, perm)), iperm)
+            elif len(axes) == 2:
+                return tf.transpose(tf.signal.ifft2d(tf.transpose(k, perm)), iperm)
+            elif len(axes) == 3:
+                return tf.transpose(tf.signal.ifft3d(tf.transpose(k, perm)), iperm)
+            else:
+                for axis in axes:
+                    k = self.ifft(k, [axis])
+                return k
+
+    def imag(self, x):
+        with tf.device(x.device):
+            return tf.math.imag(x)
+
+    def real(self, x):
+        with tf.device(x.device):
+            return tf.math.real(x)
+
+    def conj(self, x):
+        with tf.device(x.device):
+            return tf.math.conj(x)
+
+    def cast(self, x, dtype: DType):
+        if not self.is_tensor(x, only_native=True):
+            x = self.as_tensor(x, convert_external=True)
+        if self.dtype(x) == dtype:
+            return x
+        else:
+            with tf.device(x.device):
+                return tf.cast(x, to_numpy_dtype(dtype))
+
+    def unravel_index(self, flat_index, shape):
+        idx_first = tf.unravel_index(flat_index, shape)
+        return tf.transpose(idx_first, perm=tuple(range(1, self.ndims(flat_index)+1)) + (0,))
+
+    def sin(self, x):
+        with tf.device(x.device):
+            return tf.math.sin(x)
+
+    def arcsin(self, x):
+        with tf.device(x.device):
+            return tf.math.asin(x)
+
+    def cos(self, x):
+        with tf.device(x.device):
+            return tf.math.cos(x)
+
+    def arccos(self, x):
+        with tf.device(x.device):
+            return tf.math.acos(x)
+
+    def tan(self, x):
+        with tf.device(x.device):
+            return tf.math.tan(x)
+
+    def arctan(self, x):
+        with tf.device(x.device):
+            return tf.math.atan(x)
+
+    def arctan2(self, y, x):
+        y, x = self.auto_cast(y, x)
+        with tf.device(x.device):
+            return tf.math.atan2(y, x)
+
+    def sinh(self, x):
+        with tf.device(x.device):
+            return tf.math.sinh(x)
+
+    def arcsinh(self, x):
+        with tf.device(x.device):
+            return tf.math.asinh(x)
+
+    def cosh(self, x):
+        with tf.device(x.device):
+            return tf.math.cosh(x)
+
+    def arccosh(self, x):
+        with tf.device(x.device):
+            return tf.math.acosh(x)
+
+    def tanh(self, x):
+        with tf.device(x.device):
+            return tf.math.tanh(x)
+
+    def arctanh(self, x):
+        with tf.device(x.device):
+            return tf.math.atanh(x)
+
+    def log(self, x):
+        with tf.device(x.device):
+            return tf.math.log(x)
+
+    def sigmoid(self, x):
+        with tf.device(x.device):
+            return tf.math.sigmoid(x)
+
+    def log2(self, x):
+        with tf.device(x.device):
+            return tf.math.log(x) / 0.6931471805599453094  # log(x) / log(2)
+
+    def log10(self, x):
+        with tf.device(x.device):
+            return tf.math.log(x) / 2.3025850929940456840  # log(x) / log(10)
+
+    def dtype(self, array) -> DType:
+        if tf.is_tensor(array):
+            dt = array.dtype.as_numpy_dtype
+            return from_numpy_dtype(dt)
+        else:
+            return NUMPY.dtype(array)
+
+    def sparse_coo_tensor(self, indices, values, shape):
+        with self._device_for(indices, values):
+            return tf.SparseTensor(indices=self.to_int64(indices), values=values, dense_shape=shape)
+
+    def mul_coo_dense(self, indices, values, shape, dense):
+        values, dense = self.auto_cast(values, dense)
+        batch_size, nnz, channel_count = self.staticshape(values)
+        if batch_size > 1:
+            return Backend.mul_coo_dense(self, indices, values, shape, dense)
+        indices = tf.cast(indices, np.int64)
+        result = []
+        for b in range(batch_size):
+            b_result = []
+            for c in range(channel_count):
+                matrix = tf.SparseTensor(indices=indices[b], values=values[b, :, c], dense_shape=shape)
+                try:
+                    b_result.append(tf.sparse.sparse_dense_matmul(matrix, dense[b, :, c, :]))
+                except NotFoundError:  # These data types are probably not supported by TensorFlow
+                    return Backend.mul_coo_dense(self, indices, values, shape, dense)
+            result.append(tf.stack(b_result))
+        return tf.stack(result)
+
+    def not_equal(self, x, y):
+        with self._device_for(x, y):
+            return ~self.equal(x, y)
+
+    def greater_than(self, x, y):
+        with self._device_for(x, y):
+            x, y = self.auto_cast(x, y)
+            return x > y
+
+    def greater_or_equal(self, x, y):
+        with self._device_for(x, y):
+            x, y = self.auto_cast(x, y)
+            return x >= y
+
+    def add(self, a, b):
+        with self._device_for(a, b):
+            if isinstance(a, tf.SparseTensor) or isinstance(b, tf.SparseTensor):
+                return tf.sparse.add(a, b, threshold=1e-5)
+            else:
+                return Backend.add(self, a, b)
+
+    def sub(self, a, b):
+        with self._device_for(a, b):
+            a, b = self.auto_cast(a, b)
+            return a - b
+
+    def mul(self, a, b):
+        with self._device_for(a, b):
+            a, b = self.auto_cast(a, b)
+            return a * b
+
+    def div(self, numerator, denominator):
+        with self._device_for(numerator, denominator):
+            numerator, denominator = self.auto_cast(numerator, denominator)
+            return numerator / denominator
+
+    def pow(self, base, exp):
+        with self._device_for(base, exp):
+            base, exp = self.auto_cast(base, exp)
+            return base ** exp
+
+    def mod(self, dividend, divisor):
+        with self._device_for(divisor, dividend):
+            dividend, divisor = self.auto_cast(dividend, divisor)
+            return dividend % divisor
+
+    def and_(self, a, b):
+        with self._device_for(a, b):
+            a, b = self.auto_cast(a, b)
+            return a & b
+
+    def or_(self, a, b):
+        with self._device_for(a, b):
+            a, b = self.auto_cast(a, b)
+            return a | b
+
+    def xor(self, a, b):
+        with self._device_for(a, b):
+            a, b = self.auto_cast(a, b)
+            return a ^ b
+
+    def floordiv(self, a, b):
+        with self._device_for(a, b):
+            a, b = self.auto_cast(a, b)
+            return a // b
+
+    def jacobian(self, f, wrt: Union[tuple, list], get_output: bool, is_f_scalar: bool):
+        @wraps(f)
+        def eval_grad(*args):
+            args = [self.as_tensor(arg, True) if i in wrt else arg for i, arg in enumerate(args)]
+            args = [self.to_float(arg) if self.dtype(arg).kind in (bool, int) else arg for arg in args]
+            wrt_args = [arg for i, arg in enumerate(args) if i in wrt]
+            with tf.GradientTape(watch_accessed_variables=False) as tape:
+                for arg in wrt_args:
+                    assert arg.dtype in (tf.float16, tf.float32, tf.float64, tf.complex64, tf.complex128), f"Gradients can only be computed for float or complex tensors but got {arg.dtype} for argument with shape {arg.shape}"
+                    tape.watch(arg)
+                loss, output = f(*args)
+            if self.prod(tf.shape(loss)) == 1:
+                grads = list(self.as_registered.call(tape.gradient, loss, wrt_args, name=f"Backpropagation"))
+            else:
+                grads = list(self.as_registered.call(tape.jacobian, loss, wrt_args, name=f"Backpropagation"))
+            assert None not in grads, f"Gradient could not be computed for wrt argument {grads.index(None)} (argument {wrt[grads.index(None)]}) with shape {wrt_args[grads.index(None)].shape}. TensorFlow returned gradient=None."
+            return (*output, *grads) if get_output else grads
+        return eval_grad
+
+    def stop_gradient(self, value):
+        with self._device_for(value):
+            return tf.stop_gradient(value)
+
+    def matrix_solve_least_squares(self, matrix: TensorType, rhs: TensorType) -> Tuple[TensorType, TensorType, TensorType, TensorType]:
+        with self._device_for(matrix, rhs):
+            solution = tf.linalg.lstsq(matrix, rhs)
+        return solution, None, None, None
+
+    def solve_triangular_dense(self, matrix, rhs, lower: bool, unit_diagonal: bool):
+        matrix, rhs = self.auto_cast(matrix, rhs, int_to_float=True, bool_to_int=True)
+        rhs = self.expand_dims(rhs, -1)
+        if unit_diagonal:
+            diag = np.diag(np.ones((self.staticshape(matrix)[-1],)))
+            matrix = self.where(diag, diag, matrix)
+        result = tf.linalg.triangular_solve(matrix, rhs, lower=lower)
+        return result[..., 0]
+
+    def get_diagonal(self, matrices, offset=0):
+        with self._device_for(matrices):
+            matrices = tf.transpose(matrices, [0, 3, 1, 2])
+            result = tf.linalg.diag_part(matrices, k=offset)
+            return tf.transpose(result, [0, 2, 1])
+
+
+_TAPES = []
+
+
+def simple_device_type(t: str):
+    return t[len('XLA_'):] if t.startswith('XLA_') else t
```

### Comparing `phiflow-2.3.4/phi/tf/_tf_cuda_resample.py` & `phiflow-2.4.0/phi/tf/_tf_cuda_resample.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,93 +1,93 @@
-import os
-import numpy as np
-import tensorflow as tf
-from tensorflow.python.framework import ops
-from tensorflow.python.framework.errors_impl import NotFoundError
-
-# Load Custom Ops
-librariesLoaded = False
-try:
-    current_dir = os.path.dirname(os.path.realpath(__file__))
-    resample_op_path = os.path.join(current_dir, "cuda/build/resample.so")
-    resample_gradient_op_path = os.path.join(
-        current_dir, "cuda/build/resample_gradient.so"
-    )
-    assert os.path.isfile(resample_op_path), (
-        'CUDA binaries not found at %s. Run "python setup.py tf_cuda" to compile them'
-        % resample_op_path
-    )
-    assert os.path.isfile(resample_gradient_op_path), (
-        'CUDA binaries not found at %s. Run "python setup.py tf_cuda" to '
-        "compile them" % resample_gradient_op_path
-    )
-    resample_op = tf.load_op_library(resample_op_path)
-    resample_gradient_op = tf.load_op_library(resample_gradient_op_path)
-    librariesLoaded = True
-except (RuntimeError, AssertionError) as e:
-    librariesLoaded = False
-except NotFoundError as e:
-    # e.g.: tensorflow.python.framework.errors_impl.NotFoundError: libcudart.so.10.0: cannot open shared object file: No such file or directory
-    librariesLoaded = False
-    print(f"Could not find resample library.  {e}")
-except Exception as e:
-    print(e)
-    librariesLoaded = False
-
-# Register spatial_gradient
-
-
-@ops.RegisterGradient("Resample")
-def _resample_gradient(op, gradient):
-    gradients = resample_gradient_op.resample_gradient(
-        gradient, op.inputs[0], op.inputs[1], op.inputs[2]
-    )
-    return [gradients[0], gradients[1], None]
-
-
-def use_cuda(inputs):
-    if not librariesLoaded:
-        return False
-    if not tf.test.is_gpu_available(True, (3, 0)):
-        return False
-    shape = inputs.shape
-    dims = len(shape) - 2
-    components = shape[len(shape) - 1]
-    if dims > 3 or components > 4:
-        return False
-    if dims == 1 and shape[1] > 8192:
-        return False
-    if dims == 2 and (shape[1] > 32768 or shape[2] > 65536):
-        return False
-    if dims == 3 and (shape[1] > 2048 or shape[2] > 2048 or shape[3] > 2048):
-        return False
-    return True
-
-
-EXT_ID = {'undefined': 0, 'zeros': 0, 'boundary': 1, 'periodic': 2, 'symmetric': 3, 'reflect': 4}
-
-
-def resample_cuda(inputs, sample_coords, extrapolation):
-    shape = inputs.shape
-    dims = len(shape) - 2
-    boundary_array = np.zeros((dims, 2), np.uint32)
-    for i in range(dims):
-        for j in range(2):
-            current_extrapolation = collapsed_gather_nd(extrapolation, [i, j]).lower()
-            if current_extrapolation in EXT_ID:
-                boundary_array[i, j] = EXT_ID[current_extrapolation]
-            else:
-                return NotImplemented
-    return resample_op.resample(inputs, sample_coords, boundary_array)
-
-
-def collapsed_gather_nd(collapsed, nd_index, leaf_condition=None):
-    if isinstance(collapsed, (tuple, list, np.ndarray)):
-        if leaf_condition is not None and leaf_condition(collapsed):
-            return collapsed
-        # collapsed = np.array(collapsed)
-        if len(nd_index) == 1:
-            return collapsed[nd_index[0]]
-        else:
-            return collapsed_gather_nd(collapsed[nd_index[0]], nd_index[1:])
-    else:
-        return collapsed
+import os
+import numpy as np
+import tensorflow as tf
+from tensorflow.python.framework import ops
+from tensorflow.python.framework.errors_impl import NotFoundError
+
+# Load Custom Ops
+librariesLoaded = False
+try:
+    current_dir = os.path.dirname(os.path.realpath(__file__))
+    resample_op_path = os.path.join(current_dir, "cuda/build/resample.so")
+    resample_gradient_op_path = os.path.join(
+        current_dir, "cuda/build/resample_gradient.so"
+    )
+    assert os.path.isfile(resample_op_path), (
+        'CUDA binaries not found at %s. Run "python setup.py tf_cuda" to compile them'
+        % resample_op_path
+    )
+    assert os.path.isfile(resample_gradient_op_path), (
+        'CUDA binaries not found at %s. Run "python setup.py tf_cuda" to '
+        "compile them" % resample_gradient_op_path
+    )
+    resample_op = tf.load_op_library(resample_op_path)
+    resample_gradient_op = tf.load_op_library(resample_gradient_op_path)
+    librariesLoaded = True
+except (RuntimeError, AssertionError) as e:
+    librariesLoaded = False
+except NotFoundError as e:
+    # e.g.: tensorflow.python.framework.errors_impl.NotFoundError: libcudart.so.10.0: cannot open shared object file: No such file or directory
+    librariesLoaded = False
+    print(f"Could not find resample library.  {e}")
+except Exception as e:
+    print(e)
+    librariesLoaded = False
+
+# Register spatial_gradient
+
+
+@ops.RegisterGradient("Resample")
+def _resample_gradient(op, gradient):
+    gradients = resample_gradient_op.resample_gradient(
+        gradient, op.inputs[0], op.inputs[1], op.inputs[2]
+    )
+    return [gradients[0], gradients[1], None]
+
+
+def use_cuda(inputs):
+    if not librariesLoaded:
+        return False
+    if not tf.test.is_gpu_available(True, (3, 0)):
+        return False
+    shape = inputs.shape
+    dims = len(shape) - 2
+    components = shape[len(shape) - 1]
+    if dims > 3 or components > 4:
+        return False
+    if dims == 1 and shape[1] > 8192:
+        return False
+    if dims == 2 and (shape[1] > 32768 or shape[2] > 65536):
+        return False
+    if dims == 3 and (shape[1] > 2048 or shape[2] > 2048 or shape[3] > 2048):
+        return False
+    return True
+
+
+EXT_ID = {'undefined': 0, 'zeros': 0, 'boundary': 1, 'periodic': 2, 'symmetric': 3, 'reflect': 4}
+
+
+def resample_cuda(inputs, sample_coords, extrapolation):
+    shape = inputs.shape
+    dims = len(shape) - 2
+    boundary_array = np.zeros((dims, 2), np.uint32)
+    for i in range(dims):
+        for j in range(2):
+            current_extrapolation = collapsed_gather_nd(extrapolation, [i, j]).lower()
+            if current_extrapolation in EXT_ID:
+                boundary_array[i, j] = EXT_ID[current_extrapolation]
+            else:
+                return NotImplemented
+    return resample_op.resample(inputs, sample_coords, boundary_array)
+
+
+def collapsed_gather_nd(collapsed, nd_index, leaf_condition=None):
+    if isinstance(collapsed, (tuple, list, np.ndarray)):
+        if leaf_condition is not None and leaf_condition(collapsed):
+            return collapsed
+        # collapsed = np.array(collapsed)
+        if len(nd_index) == 1:
+            return collapsed[nd_index[0]]
+        else:
+            return collapsed_gather_nd(collapsed[nd_index[0]], nd_index[1:])
+    else:
+        return collapsed
```

### Comparing `phiflow-2.3.4/phi/tf/flow.py` & `phiflow-2.4.0/phi/tf/flow.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,32 +1,32 @@
-# pylint: disable-msg = wildcard-import, unused-wildcard-import, unused-import
-"""
-Standard import for TensorFlow mode.
-
-Extends the import `from phi.flow import *` by TensorFlow-related functions and modules.
-
-The following TensorFlow modules are included: `tensorflow` / `tf`, `keras`, `layers`.
-
-Importing this module registers the TensorFlow backend as the default backend unless called within a backend context.
-New tensors created via `phi.math` functions will be backed by TensorFlow tensors.
-
-See `phi.flow`, `phi.torch.flow`, `phi.jax.flow`.
-"""
-
-from phi.flow import *
-from . import TENSORFLOW
-
-from . import nets
-from .nets import parameter_count, get_parameters, dense_net, u_net, save_state, load_state, update_weights, adam, conv_net, res_net, sgd, sgd as SGD, adagrad, rmsprop, conv_classifier, invertible_net, fno
-
-import tensorflow
-from tensorflow import keras
-from tensorflow.keras import layers
-
-from ..math.backend import PHI_LOGGER as _LOGGER
-
-tf = tensorflow
-
-if not backend.context_backend():
-    backend.set_global_default_backend(TENSORFLOW)
-else:
+# pylint: disable-msg = wildcard-import, unused-wildcard-import, unused-import
+"""
+Standard import for TensorFlow mode.
+
+Extends the import `from phi.flow import *` by TensorFlow-related functions and modules.
+
+The following TensorFlow modules are included: `tensorflow` / `tf`, `keras`, `layers`.
+
+Importing this module registers the TensorFlow backend as the default backend unless called within a backend context.
+New tensors created via `phi.math` functions will be backed by TensorFlow tensors.
+
+See `phi.flow`, `phi.torch.flow`, `phi.jax.flow`.
+"""
+
+from phi.flow import *
+from . import TENSORFLOW
+
+from . import nets
+from .nets import parameter_count, get_parameters, dense_net, u_net, save_state, load_state, update_weights, adam, conv_net, res_net, sgd, sgd as SGD, adagrad, rmsprop, conv_classifier, invertible_net, fno
+
+import tensorflow
+from tensorflow import keras
+from tensorflow.keras import layers
+
+from ..math.backend import PHI_LOGGER as _LOGGER
+
+tf = tensorflow
+
+if not backend.context_backend():
+    backend.set_global_default_backend(TENSORFLOW)
+else:
     _LOGGER.warn(f"Importing '{__name__}' within a backend context will not set the default backend.")
```

### Comparing `phiflow-2.3.4/phi/tf/nets.py` & `phiflow-2.4.0/phi/tf/nets.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,806 +1,803 @@
-"""
-Jax implementation of the unified machine learning API.
-Equivalent functions also exist for the other frameworks.
-
-For API documentation, see https://tum-pbs.github.io/PhiFlow/Network_API .
-"""
-import pickle
-from typing import Callable
-from typing import Union, Sequence
-
-import numpy
-import numpy as np
-import tensorflow as tf
-from tensorflow import Tensor
-from tensorflow import keras
-from tensorflow.keras import layers as kl
-
-from .. import math
-
-
-def parameter_count(model: keras.Model):
-    """
-    Counts the number of parameters in a model.
-
-    Args:
-        model: Keras model
-
-    Returns:
-        `int`
-    """
-    total = 0
-    for parameter in model.trainable_weights:
-        total += numpy.prod(parameter.shape)
-    return int(total)
-
-
-def get_parameters(model: keras.Model, wrap=True) -> dict:
-    result = {}
-    for var in model.trainable_weights:
-        name: str = var.name
-        layer = name[:name.index('/')].replace('_', '').replace('dense', 'linear')
-        try:
-            int(layer[-1:])
-        except ValueError:
-            layer += '0'
-        prop = name[name.index('/') + 1:].replace('kernel', 'weight')
-        if prop.endswith(':0'):
-            prop = prop[:-2]
-        name = f"{layer}.{prop}"
-        var = var.numpy()
-        if not wrap:
-            result[name] = var
-        else:
-            if name.endswith('.weight'):
-                if var.ndim == 2:
-                    phi_tensor = math.wrap(var, math.channel('input,output'))
-                elif var.ndim == 3:
-                    phi_tensor = math.wrap(var, math.channel('x,input,output'))
-                elif var.ndim == 4:
-                    phi_tensor = math.wrap(var, math.channel('x,y,input,output'))
-                elif var.ndim == 5:
-                    phi_tensor = math.wrap(var, math.channel('x,y,z,input,output'))
-            elif name.endswith('.bias'):
-                phi_tensor = math.wrap(var, math.channel('output'))
-            elif var.ndim == 1:
-                phi_tensor = math.wrap(var, math.channel('output'))
-            else:
-                raise NotImplementedError(name, var)
-            result[name] = phi_tensor
-    return result
-
-
-def save_state(obj: Union[keras.models.Model, keras.optimizers.Optimizer], path: str):
-    """
-    Write the state of a module or optimizer to a file.
-
-    See Also:
-        `load_state()`
-
-    Args:
-        obj: `keras.models.Model or keras.optimizers.Optimizer`
-        path: File path as `str`.
-    """
-    if isinstance(obj, keras.models.Model):
-        if not path.endswith('.h5'):
-            path += '.h5'
-        obj.save_weights(path)
-    elif isinstance(obj, keras.optimizers.Optimizer):
-        if not path.endswith('.pkl'):
-            path += '.pkl'
-        weights = obj.get_weights()
-        with open(path, 'wb') as f:
-            pickle.dump(weights, f)
-    else:
-        raise ValueError("obj must be a Keras model or optimizer")
-
-
-def load_state(obj: Union[keras.models.Model, keras.optimizers.Optimizer], path: str):
-    """
-    Read the state of a module or optimizer from a file.
-
-    See Also:
-        `save_state()`
-
-    Args:
-        obj: `keras.models.Model or keras.optimizers.Optimizer`
-        path: File path as `str`.
-    """
-    if isinstance(obj, keras.models.Model):
-        if not path.endswith('.h5'):
-            path += '.h5'
-        obj.load_weights(path)
-    elif isinstance(obj, keras.optimizers.Optimizer):
-        if not path.endswith('.pkl'):
-            path += '.pkl'
-        with open(path, 'rb') as f:
-            weights = pickle.load(f)
-        obj.set_weights(weights)
-    else:
-        raise ValueError("obj must be a Keras model or optimizer")
-
-
-def update_weights(net: keras.Model, optimizer: keras.optimizers.Optimizer, loss_function: Callable, *loss_args, **loss_kwargs):
-    """
-    Computes the gradients of `loss_function` w.r.t. the parameters of `net` and updates its weights using `optimizer`.
-
-    This is the TensorFlow/Keras version. Analogue functions exist for other learning frameworks.
-
-    Args:
-        net: Learning model.
-        optimizer: Optimizer.
-        loss_function: Loss function, called as `loss_function(*loss_args, **loss_kwargs)`.
-        *loss_args: Arguments given to `loss_function`.
-        **loss_kwargs: Keyword arguments given to `loss_function`.
-
-    Returns:
-        Output of `loss_function`.
-    """
-    with tf.GradientTape() as tape:
-        output = loss_function(*loss_args, **loss_kwargs)
-        loss = output[0] if isinstance(output, tuple) else output
-        gradients = tape.gradient(loss.sum, net.trainable_variables)
-    optimizer.apply_gradients(zip(gradients, net.trainable_variables))
-    return output
-
-
-def adam(net: keras.Model, learning_rate: float = 1e-3, betas=(0.9, 0.999), epsilon=1e-07):
-    """
-    Creates an Adam optimizer for `net`, alias for [`keras.optimizers.Adam`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam).
-    Analogous functions exist for other learning frameworks.
-    """
-    return keras.optimizers.Adam(learning_rate, betas[0], betas[1], epsilon)
-
-
-def sgd(net: keras.Model, learning_rate: float = 1e-3, momentum=0, dampening=0, weight_decay=0, nesterov=False):
-    """
-    Creates an SGD optimizer for 'net', alias for ['keras.optimizers.SGD'](https://keras.io/api/optimizers/sgd/)
-    Analogous functions exist for other learning frameworks.
-    """
-    return keras.optimizers.SGD(learning_rate, momentum, nesterov)
-
-
-def adagrad(net: keras.Model, learning_rate: float = 1e-3, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10):
-    """
-    Creates an Adagrad optimizer for 'net', alias for ['keras.optimizers.Adagrad'](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adagrad)
-    Analogous functions exist for other learning frameworks.
-    """
-    return keras.optimizers.Adagrad(learning_rate, initial_accumulator_value, eps)
-
-
-def rmsprop(net: keras.Model, learning_rate: float = 1e-3, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False):
-    """
-    Creates an RMSProp optimizer for 'net', alias for ['keras.optimizers.RMSprop'](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop)
-    Analogous functions exist for other learning frameworks.
-    """
-    return keras.optimizers.RMSprop(learning_rate, alpha, momentum, eps, centered)
-
-
-def dense_net(in_channels: int,
-              out_channels: int,
-              layers: Sequence[int],
-              batch_norm=False,
-              activation='ReLU',
-              softmax=False) -> keras.Model:
-    """
-    Fully-connected neural networks are available in ΦFlow via dense_net().
-    Arguments:
-        in_channels : size of input layer, int
-        out_channels = size of output layer, int
-        layers : tuple of linear layers between input and output neurons, list or tuple
-        activation : activation function used within the layers, string
-        batch_norm : use of batch norm after each linear layer, bool
-
-    Returns:
-        Dense net model as specified by input arguments
-    """
-    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
-    keras_layers = []
-    for neuron_count in layers:
-        keras_layers.append(kl.Dense(neuron_count, activation=activation))
-        if batch_norm:
-            keras_layers.append(kl.BatchNormalization())
-    return keras.models.Sequential([kl.InputLayer(input_shape=(in_channels,)),
-                                    *keras_layers,
-                                    kl.Dense(out_channels, activation='linear'),
-                                    *([kl.Softmax()] if softmax else [])])
-
-
-def u_net(in_channels: int,
-          out_channels: int,
-          levels: int = 4,
-          filters: Union[int, tuple, list] = 16,
-          batch_norm: bool = True,
-          activation: Union[str, Callable] = 'ReLU',
-          in_spatial: Union[tuple, int] = 2,
-          periodic=False,
-          use_res_blocks: bool = False, **kwargs) -> keras.Model:
-    """
-    ΦFlow provides a built-in U-net architecture, classically popular for Semantic Segmentation in Computer Vision, composed of downsampling and upsampling layers.
-
-    Arguments:
-
-        in_channels: input channels of the feature map, dtype : int
-        out_channels : output channels of the feature map, dtype : int
-        levels : number of levels of down-sampling and upsampling, dtype : int
-        filters : filter sizes at each down/up sampling convolutional layer, if the input is integer all conv layers have the same filter size,
-        dtype : int or tuple
-        activation : activation function used within the layers, dtype : string
-        batch_norm : use of batchnorm after each conv layer, dtype : bool
-        in_spatial : spatial dimensions of the input feature map, dtype : int
-        use_res_blocks : use convolutional blocks with skip connections instead of regular convolutional blocks, dtype : bool
-
-    Returns:
-
-        U-net model as specified by input arguments
-    """
-    if isinstance(in_spatial, int):
-        d = in_spatial
-        in_spatial = (None,) * d
-    else:
-        assert isinstance(in_spatial, tuple)
-        d = len(in_spatial)
-    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
-    if isinstance(filters, (tuple, list)):
-        assert len(filters) == levels, f"List of filters has length {len(filters)} but u-net has {levels} levels."
-    else:
-        filters = (filters,) * levels
-    # --- Construct the U-Net ---
-    x = inputs = keras.Input(shape=in_spatial + (in_channels,))
-    x = resnet_block(x.shape[-1], filters[0], periodic, batch_norm, activation, d)(x) if use_res_blocks else double_conv(x, d, filters[0], filters[0], batch_norm, activation, periodic)
-    xs = [x]
-    for i in range(1, levels):
-        x = MAX_POOL[d](2, padding="same")(x)
-        x = resnet_block(x.shape[-1], filters[i], periodic, batch_norm, activation, d)(x) if use_res_blocks else double_conv(x, d, filters[i], filters[i], batch_norm, activation, periodic)
-        xs.insert(0, x)
-    for i in range(1, levels):
-        x = UPSAMPLE[d](2)(x)
-        x = kl.Concatenate()([x, xs[i]])
-        x = resnet_block(x.shape[-1], filters[i - 1], periodic, batch_norm, activation, d)(x) if use_res_blocks else double_conv(x, d, filters[i - 1], filters[i - 1], batch_norm, activation, periodic)
-    x = CONV[d](out_channels, 1)(x)
-    return keras.Model(inputs, x)
-
-
-CONV = [None, kl.Conv1D, kl.Conv2D, kl.Conv3D]
-MAX_POOL = [None, kl.MaxPool1D, kl.MaxPool2D, kl.MaxPool3D]
-UPSAMPLE = [None, kl.UpSampling1D, kl.UpSampling2D, kl.UpSampling3D]
-ACTIVATIONS = {'tanh': keras.activations.tanh, 'ReLU': keras.activations.relu, 'Sigmoid': keras.activations.sigmoid, 'SiLU': keras.activations.selu}
-
-
-def pad_periodic(x: Tensor):
-    d = len(x.shape) - 2
-    if d >= 1:
-        x = tf.concat([tf.expand_dims(x[:, -1, ...], axis=1), x, tf.expand_dims(x[:, 0, ...], axis=1)], axis=1)
-    if d >= 2:
-        x = tf.concat([tf.expand_dims(x[:, :, -1, ...], axis=2), x, tf.expand_dims(x[:, :, 0, ...], axis=2)], axis=2)
-    if d >= 3:
-        x = tf.concat([tf.expand_dims(x[:, :, :, -1, ...], axis=3), x, tf.expand_dims(x[:, :, :, 0, ...], axis=3)],
-                      axis=3)
-    return x
-
-
-def double_conv(x, d: int, out_channels: int, mid_channels: int, batch_norm: bool, activation: Callable, periodic: bool):
-    x = CONV[d](mid_channels, 3, padding='valid')(pad_periodic(x)) if periodic else CONV[d](mid_channels, 3, padding='same')(x)
-    if batch_norm:
-        x = kl.BatchNormalization()(x)
-    x = activation(x)
-    x = CONV[d](out_channels, 3, padding='valid')(pad_periodic(x)) if periodic else CONV[d](out_channels, 3, padding='same')(x)
-    if batch_norm:
-        x = kl.BatchNormalization()(x)
-    x = activation(x)
-    return x
-
-
-def conv_net(in_channels: int,
-             out_channels: int,
-             layers: Sequence[int],
-             batch_norm: bool = False,
-             activation: Union[str, Callable] = 'ReLU',
-             periodic=False,
-             in_spatial: Union[int, tuple] = 2, **kwargs) -> keras.Model:
-    """
-    Built in Conv-Nets are also provided. Contrary to the classical convolutional neural networks, the feature map spatial size remains the same throughout the layers. Each layer of the network is essentially a convolutional block comprising of two conv layers. A filter size of 3 is used in the convolutional layers.
-    Arguments:
-
-        in_channels : input channels of the feature map, dtype : int
-        out_channels : output channels of the feature map, dtype : int
-        layers : list or tuple of output channels for each intermediate layer between the input and final output channels, dtype : list or tuple
-        activation : activation function used within the layers, dtype : string
-        batch_norm : use of batchnorm after each conv layer, dtype : bool
-        in_spatial : spatial dimensions of the input feature map, dtype : int
-
-    Returns:
-
-        Conv-net model as specified by input arguments
-    """
-    if isinstance(in_spatial, int):
-        d = in_spatial
-        in_spatial = (None,) * d
-    else:
-        assert isinstance(in_spatial, tuple)
-        d = len(in_spatial)
-    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
-    x = inputs = keras.Input(shape=in_spatial + (in_channels,))
-    if len(layers) < 1:
-        layers.append(out_channels)
-    for i in range(len(layers)):
-        x = CONV[d](layers[i], 3, padding='valid')(pad_periodic(x)) if periodic else CONV[d](layers[i], 3, padding='same')(x)
-        if batch_norm:
-            x = kl.BatchNormalization()(x)
-        x = activation(x)
-    x = CONV[d](out_channels, 1)(x)
-    return keras.Model(inputs, x)
-
-
-def resnet_block(in_channels: int,
-                 out_channels: int,
-                 periodic: bool,
-                 batch_norm: bool = False,
-                 activation: Union[str, Callable] = 'ReLU',
-                 in_spatial: Union[int, tuple] = 2):
-    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
-    if isinstance(in_spatial, int):
-        d = in_spatial
-    else:
-        assert isinstance(in_spatial, tuple)
-        d = len(in_spatial)
-    x = x_1 = inputs = keras.Input(shape=(None,) * d + (in_channels,))
-    x = CONV[d](out_channels, 3, padding='valid')(pad_periodic(x)) if periodic else CONV[d](out_channels, 3, padding='same')(x)
-    if batch_norm:
-        x = kl.BatchNormalization()(x)
-    x = activation(x)
-    x = CONV[d](out_channels, 3, padding='valid')(pad_periodic(x)) if periodic else CONV[d](out_channels, 3, padding='same')(x)
-    if batch_norm:
-        x = kl.BatchNormalization()(x)
-    x = activation(x)
-    if in_channels != out_channels:
-        x_1 = CONV[d](out_channels, 1)(x_1)
-        if batch_norm:
-            x_1 = kl.BatchNormalization()(x_1)
-    x = kl.Add()([x, x_1])
-    return keras.Model(inputs, x)
-
-
-def res_net(in_channels: int,
-            out_channels: int,
-            layers: Sequence[int],
-            batch_norm: bool = False,
-            activation: Union[str, Callable] = 'ReLU',
-            periodic=False,
-            in_spatial: Union[int, tuple] = 2, **kwargs):
-    """
-    Built in Res-Nets are provided in the ΦFlow framework. Similar to the conv-net, the feature map spatial size remains the same throughout the layers.
-    These networks use residual blocks composed of two conv layers with a skip connection added from the input to the output feature map.
-    A default filter size of 3 is used in the convolutional layers.
-
-    Arguments:
-
-        in_channels : input channels of the feature map, dtype : int
-        out_channels : output channels of the feature map, dtype : int
-        layers : list or tuple of output channels for each intermediate layer between the input and final output channels, dtype : list or tuple
-        activation : activation function used within the layers, dtype : string
-        batch_norm : use of batchnorm after each conv layer, dtype : bool
-        in_spatial : spatial dimensions of the input feature map, dtype : int
-
-    Returns:
-
-        Res-net model as specified by input arguments
-    """
-    if isinstance(in_spatial, int):
-        d = in_spatial
-        in_spatial = (None,) * d
-    else:
-        assert isinstance(in_spatial, tuple)
-        d = len(in_spatial)
-
-    x = inputs = keras.Input(shape=in_spatial + (in_channels,))
-    if len(layers) < 1:
-        layers.append(out_channels)
-    out = resnet_block(in_channels, layers[0], periodic, batch_norm, activation, d)(x)
-    for i in range(1, len(layers)):
-        out = resnet_block(layers[i - 1], layers[i], periodic, batch_norm, activation, d)(out)
-    out = CONV[d](out_channels, 1)(out)
-    return keras.Model(inputs, out)
-
-
-def conv_classifier(in_features: int,
-                    in_spatial: Union[tuple, list],
-                    num_classes: int,
-                    blocks=(64, 128, 256, 256, 512, 512),
-                    dense_layers=(4096, 4096, 100),
-                    batch_norm=True,
-                    activation='ReLU',
-                    softmax=True,
-                    periodic=False):
-    """
-    Based on VGG16.
-    """
-    assert isinstance(in_spatial, (tuple, list))
-    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
-    d = len(in_spatial)
-    x = inputs = keras.Input(shape=in_spatial + (in_features,))
-    for i, next in enumerate(blocks):
-        if i in (0, 1):
-            x = double_conv(x, d, next, next, batch_norm, activation, periodic)
-            x = MAX_POOL[d](2)(x)
-        else:
-            x = double_conv(x, d, next, next, batch_norm, activation, periodic)
-            x = CONV[d](next, 3, padding='valid')(pad_periodic(x)) if periodic else CONV[d](next, 3, padding='same')(x)
-            if batch_norm:
-                x = kl.BatchNormalization()(x)
-            x = activation(x)
-            x = MAX_POOL[d](2)(x)
-    x = kl.Flatten()(x)
-    flat_size = int(np.prod(in_spatial) * blocks[-1] / (2**d) ** len(blocks))
-    x = dense_net(flat_size, num_classes, dense_layers, batch_norm, activation, softmax)(x)
-    return keras.Model(inputs, x)
-
-
-def get_mask(inputs, reverse_mask, data_format='NHWC'):
-    """ Compute mask for slicing input feature map for Invertible Nets """
-    shape = inputs.shape
-    if len(shape) == 2:
-        N = shape[-1]
-        range_n = tf.range(0, N)
-        even_ind = range_n % 2
-        checker = tf.reshape(even_ind, (-1, N))
-    elif len(shape) == 4:
-        H = shape[2] if data_format == 'NCHW' else shape[1]
-        W = shape[3] if data_format == 'NCHW' else shape[2]
-
-        range_h = tf.range(0, H)
-        range_w = tf.range(0, W)
-
-        even_ind_h = tf.cast(range_h % 2, dtype=tf.bool)
-        even_ind_w = tf.cast(range_w % 2, dtype=tf.bool)
-
-        ind_h = tf.tile(tf.expand_dims(even_ind_h, -1), [1, W])
-        ind_w = tf.tile(tf.expand_dims(even_ind_w, 0), [H, 1])
-        # ind_h = even_ind_h.unsqueeze(-1).repeat(1, W)
-        # ind_w = even_ind_w.unsqueeze( 0).repeat(H, 1)
-
-        checker = tf.math.logical_xor(ind_h, ind_w)
-
-        reshape = [-1, 1, H, W] if data_format == 'NCHW' else [-1, H, W, 1]
-        checker = tf.reshape(checker, reshape)
-        checker = tf.cast(checker, dtype=tf.float32)
-
-    else:
-        raise ValueError('Invalid tensor shape. Dimension of the tensor shape must be '
-                         '2 (NxD) or 4 (NxCxHxW or NxHxWxC), got {}.'.format(inputs.get_shape().as_list()))
-
-    if reverse_mask:
-        checker = 1 - checker
-
-    return checker
-
-
-def Dense_resnet_block(in_channels: int,
-                       mid_channels: int,
-                       batch_norm: bool = False,
-                       activation: Union[str, Callable] = 'ReLU'):
-    inputs = keras.Input(shape=(in_channels,))
-    x_1 = inputs
-    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
-
-    x = kl.Dense(mid_channels)(inputs)
-    if batch_norm:
-        x = kl.BatchNormalization()(x)
-    x = activation(x)
-
-    x = kl.Dense(in_channels)(x)
-    if batch_norm:
-        x = kl.BatchNormalization()(x)
-    x = activation(x)
-
-    x = kl.Add()([x, x_1])
-
-    return keras.Model(inputs, x)
-
-
-NET = {'u_net': u_net, 'res_net': res_net, 'conv_net': conv_net}
-
-
-class CouplingLayer(keras.Model):
-
-    def __init__(self, in_channels, activation, batch_norm, in_spatial, net, reverse_mask):
-        super(CouplingLayer, self).__init__()
-
-        self.activation = activation
-        self.batch_norm = batch_norm
-        self.reverse_mask = reverse_mask
-
-        if in_spatial == 0:  # for in_spatial = 0, use dense layers
-            self.s1 = Dense_resnet_block(in_channels, in_channels, batch_norm, activation)
-            self.t1 = Dense_resnet_block(in_channels, in_channels, batch_norm, activation)
-
-            self.s2 = Dense_resnet_block(in_channels, in_channels, batch_norm, activation)
-            self.t2 = Dense_resnet_block(in_channels, in_channels, batch_norm, activation)
-        else:
-            self.s1 = NET[net](in_channels=in_channels, out_channels=in_channels, layers=[],
-                               batch_norm=batch_norm, activation=activation,
-                               in_spatial=in_spatial)
-            self.t1 = NET[net](in_channels=in_channels, out_channels=in_channels, layers=[],
-                               batch_norm=batch_norm, activation=activation,
-                               in_spatial=in_spatial)
-
-            self.s2 = NET[net](in_channels=in_channels, out_channels=in_channels, layers=[],
-                               batch_norm=batch_norm, activation=activation,
-                               in_spatial=in_spatial)
-            self.t2 = NET[net](in_channels=in_channels, out_channels=in_channels, layers=[],
-                               batch_norm=batch_norm, activation=activation,
-                               in_spatial=in_spatial)
-
-    def call(self, x, invert=False):
-        mask = get_mask(x, self.reverse_mask, 'NCHW')
-
-        if invert:
-            v1 = x * mask
-            v2 = x * (1 - mask)
-
-            u2 = (1 - mask) * (v2 - self.t1(v1)) * tf.math.exp(tf.tanh(-self.s1(v1)))
-            u1 = mask * (v1 - self.t2(u2)) * tf.math.exp(tf.tanh(-self.s2(u2)))
-
-            return u1 + u2
-        else:
-            u1 = x * mask
-            u2 = x * (1 - mask)
-
-            v1 = mask * (u1 * tf.math.exp(tf.tanh(self.s2(u2))) + self.t2(u2))
-            v2 = (1 - mask) * (u2 * tf.math.exp(tf.tanh(self.s1(v1))) + self.t1(v1))
-
-            return v1 + v2
-
-
-class InvertibleNet(keras.Model):
-    def __init__(self, in_channels, num_blocks, activation, batch_norm, in_spatial, net):
-        super(InvertibleNet, self).__init__()
-        self.num_blocks = num_blocks
-
-        self.layer_dict = {}
-        for i in range(num_blocks):
-            self.layer_dict[f'coupling_block{i + 1}'] = \
-                CouplingLayer(in_channels,
-                              activation, batch_norm,
-                              in_spatial, net, (i % 2 == 0))
-
-    def call(self, x, backward=False):
-        if backward:
-            for i in range(self.num_blocks, 0, -1):
-                x = self.layer_dict[f'coupling_block{i}'](x, backward)
-        else:
-            for i in range(1, self.num_blocks + 1):
-                x = self.layer_dict[f'coupling_block{i}'](x)
-        return x
-
-
-def invertible_net(in_channels: int,
-                   num_blocks: int,
-                   batch_norm: bool = False,
-                   net: str = 'u_net',
-                   activation: Union[str, type] = 'ReLU',
-                   in_spatial: Union[tuple, int] = 2, **kwargs):
-    """
-    ΦFlow also provides invertible neural networks that are capable of inverting the output tensor back to the input tensor initially passed.\ These networks have far reaching applications in predicting input parameters of a problem given its observations.\ Invertible nets are composed of multiple concatenated coupling blocks wherein each such block consists of arbitrary neural networks.
-
-    Currently, these arbitrary neural networks could be set to u_net(default), conv_net, res_net or dense_net blocks with in_channels = out_channels.
-    The architecture used is popularized by ["Real NVP"](https://arxiv.org/abs/1605.08803).
-
-    Arguments:
-
-        in_channels : input channels of the feature map, dtype : int
-        num_blocks : number of coupling blocks inside the invertible net, dtype : int
-        activation : activation function used within the layers, dtype : string
-        batch_norm : use of batchnorm after each layer, dtype : bool
-        in_spatial : spatial dimensions of the input feature map, dtype : int
-        net : type of neural network blocks used in coupling layers, dtype : str
-        **kwargs : placeholder for arguments not supported by the function
-
-    Returns:
-
-        Invertible Net model as specified by input arguments
-
-    Note: Currently supported values for net are 'u_net'(default), 'conv_net' and 'res_net'.
-    For choosing 'dense_net' as the network block in coupling layers in_spatial must be set to zero.
-    """
-    if isinstance(in_spatial, tuple):
-        in_spatial = len(in_spatial)
-
-    return InvertibleNet(in_channels, num_blocks, activation,
-                         batch_norm, in_spatial, net)
-
-
-##################################################################################################################
-#  Fourier Neural Operators
-#  source: https://github.com/zongyi-li/fourier_neural_operator
-###################################################################################################################
-RFFT = [None, tf.signal.rfft, tf.signal.rfft2d, tf.signal.rfft3d]
-FFT = [None, tf.signal.fft, tf.signal.fft2d, tf.signal.fft3d]
-IRFFT = [None, tf.signal.irfft, tf.signal.irfft2d, tf.signal.irfft3d]
-
-class SpectralConv(keras.Model):
-
-    def __init__(self, in_channels, out_channels, modes, in_spatial):
-
-        super(SpectralConv, self).__init__()
-
-        self.in_channels = in_channels
-        self.out_channels = out_channels
-        self.in_spatial = in_spatial
-        assert in_spatial >= 1 and in_spatial <= 3
-        if isinstance(modes, int):
-            mode = modes
-            modes = [mode for i in range(in_spatial)]
-
-        self.scale = 1 / (in_channels * out_channels)
-
-        self.modes = {i + 1: modes[i] for i in range(len(modes))}
-        self.weights_ = {}
-
-        rand_shape = [in_channels, out_channels]
-        rand_shape += [self.modes[i] for i in range(1, in_spatial + 1)]
-
-        for i in range(2 ** (in_spatial - 1)):
-            self.weights_[f'w{i + 1}'] = tf.complex(tf.Variable(self.scale * tf.random.normal(shape=rand_shape, dtype=tf.dtypes.float32), trainable=True),
-                                                tf.Variable(self.scale * tf.random.normal(shape=rand_shape, dtype=tf.dtypes.float32), trainable=True))
-
-    def complex_mul(self, input, weights):
-
-        if self.in_spatial == 1:
-            return tf.einsum("bix,iox->box", input, weights)
-        elif self.in_spatial == 2:
-            return tf.einsum("bixy,ioxy->boxy", input, weights)
-        elif self.in_spatial == 3:
-            return tf.einsum("bixyz,ioxyz->boxyz", input, weights)
-
-
-    def call(self, x):
-        batch_size = x.shape[0]
-
-        x_ft = RFFT[self.in_spatial](x)
-
-        outft_dims = [batch_size, self.out_channels] + \
-                     [x.shape[-i] for i in range(self.in_spatial, 1, -1)] + [x.shape[-1] // 2 + 1]
-        out_ft0 = tf.complex(tf.Variable(tf.zeros(outft_dims, dtype=tf.dtypes.float32)),
-                            tf.Variable(tf.zeros(outft_dims, dtype=tf.dtypes.float32)))
-
-        if self.in_spatial == 1:
-            out_ft1 = self.complex_mul(x_ft[:, :, :self.modes[1]],
-                                      self.weights_['w1'])
-            out_ft = tf.concat([out_ft1, out_ft0[:, :, self.modes[1]:]], axis=-1)
-        elif self.in_spatial == 2:
-            out_ft1 = self.complex_mul(x_ft[:, :, :self.modes[1], :self.modes[2]],
-                                 self.weights_['w1'])
-            out_ft2 = self.complex_mul(x_ft[:, :, -self.modes[1]:, :self.modes[2]],
-                                 self.weights_['w2'])
-            out_ft3 = tf.concat([out_ft1, out_ft0[:, :, self.modes[1]:-self.modes[1],
-                                         :self.modes[2]], out_ft2], axis=-2)
-            out_ft = tf.concat([out_ft3, out_ft0[:, :, :, self.modes[2]:]], axis=-1)
-        elif self.in_spatial == 3:
-            out_ft1 = self.complex_mul(x_ft[:, :, :self.modes[1], :self.modes[2], :self.modes[3]],
-                                 self.weights_['w1'])
-            out_ft2 = self.complex_mul(x_ft[:, :, -self.modes[1]:, :self.modes[2], :self.modes[3]],
-                                 self.weights_['w2'])
-            out_ft3 = self.complex_mul(x_ft[:, :, :self.modes[1], -self.modes[2]:, :self.modes[3]],
-                                 self.weights_['w3'])
-            out_ft4 = self.complex_mul(x_ft[:, :, -self.modes[1]:, -self.modes[2]:, :self.modes[3]],
-                                 self.weights_['w4'])
-
-            out_ft5 = tf.concat([out_ft1, out_ft0[:, :, self.modes[1]:-self.modes[1], :self.modes[2], :self.modes[3]]
-                                    , out_ft2], axis=-3)
-            out_ft6 = tf.concat([out_ft3, out_ft0[:, :, self.modes[1]:-self.modes[1], -self.modes[2]:, :self.modes[3]]
-                                    , out_ft4], axis=-3)
-            out_ft7 = tf.concat([out_ft5, out_ft0[:, :, :, self.modes[2]:-self.modes[2], :self.modes[3]], out_ft6],
-                                axis=-2)
-            out_ft = tf.concat([out_ft7, out_ft0[:, :, :, :, self.modes[3]:]], axis=-1)
-
-        ##Return to Physical Space
-        x = IRFFT[self.in_spatial](out_ft)
-
-        return x
-
-
-class FNO(keras.Model):
-
-    def __init__(self, in_channels, out_channels, width, modes, activation, batch_norm, in_spatial):
-        super(FNO, self).__init__()
-
-        """
-        The overall network. It contains 4 layers of the Fourier layer.
-        1. Lift the input to the desire channel dimension by self.fc0 .
-        2. 4 layers of the integral operators u' = (W + K)(u).
-            W defined by self.w; K defined by self.conv .
-        3. Project from the channel space to the output space by self.fc1 and self.fc2.
-
-        input shape and output shape: (batchsize b, channels c, *spatial)
-        """
-
-        self.activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
-        self.width = width
-        self.in_spatial = in_spatial
-
-        self.fc0 = kl.Dense(self.width)
-
-        self.model_dict = {}
-        for i in range(4):
-            self.model_dict[f'conv{i}'] = SpectralConv(self.width, self.width, modes, in_spatial)
-            self.model_dict[f'w{i}'] = CONV[self.in_spatial](self.width, kernel_size=1)
-            self.model_dict[f'bn{i}'] = kl.BatchNormalization()
-
-        self.fc1 = kl.Dense(128)
-        self.fc2 = kl.Dense(out_channels)
-
-    # Adding extra spatial channels eg. x, y, z, .... to input x
-    def get_grid(self, shape, device):
-        batch_size = shape[0]
-        grid_channel_sizes = shape[1:-1]  # shape =  (batch_size, *spatial, channels)
-        self.grid_channels = {}
-        for i in range(self.in_spatial):
-            self.grid_channels[f'dim{i}'] = tf.cast(tf.linspace(0, 1,
-                                        grid_channel_sizes[i]), dtype=tf.dtypes.float32)  #tf.tensor(tf.linspace(0, 1, grid_channel_sizes[i]), dtype=tf.dtypes.float32)
-            reshape_dim_tuple = [1,] + [1 if i != j else grid_channel_sizes[j]
-                                        for j in range(self.in_spatial)] + [1,]
-            repeat_dim_tuple = [batch_size,] + [1 if i == j else grid_channel_sizes[j]
-                                                for j in range(self.in_spatial)] + [1,]
-
-            self.grid_channels[f'dim{i}'] = tf.tile(tf.reshape(self.grid_channels[f'dim{i}'], reshape_dim_tuple), repeat_dim_tuple)
-
-        return tf.concat([self.grid_channels[f'dim{i}'] for i in range(self.in_spatial)], axis=-1)
-
-    def call(self, x):
-        grid = self.get_grid(x.shape, x.device)
-        x = tf.concat([x, grid], axis=-1)
-
-        permute_tuple= [0] + [self.in_spatial + 1] + [i + 1 for i in range(self.in_spatial)]
-        permute_tuple_reverse = [0] + [2 + i for i in range(self.in_spatial)] + [1]
-
-        # No need to Transpose x such that channels shape lies
-        # at the end to pass it through linear layers as it's the default in tf
-        #x = tf.transpose(x, permute_tuple)
-
-        x = self.fc0(x)
-
-        for i in range(4):
-            x1 = self.model_dict[f'w{i}'](x)
-            # Spectral conv expects a shape : [batch, channel, *spatial]
-            # hence the transpose:
-            x2 = self.model_dict[f'conv{i}'](tf.transpose(x, permute_tuple))
-            x2 = tf.transpose(x2, permute_tuple_reverse)
-            x = self.model_dict[f'bn{i}'](x1) + self.model_dict[f'bn{i}'](x2)
-            x = self.activation(x)
-
-        x = self.activation(self.fc1(x))
-        x = self.fc2(x)
-
-        return x
-
-
-def fno(in_channels: int,
-        out_channels: int,
-        mid_channels: int,
-        modes: Sequence[int],
-        activation: Union[str, type] = 'ReLU',
-        batch_norm: bool = False,
-        in_spatial: int = 2):
-    """
-    ["Fourier Neural Operator"](https://github.com/zongyi-li/fourier_neural_operator) network contains 4 layers of the Fourier layer.
-    1. Lift the input to the desire channel dimension by self.fc0 .
-    2. 4 layers of the integral operators u' = (W + K)(u). W defined by self.w; K defined by self.conv .
-    3. Project from the channel space to the output space by self.fc1 and self.fc2.
-
-    Arguments:
-
-        in_channels : input channels of the feature map, dtype : int
-        out_channels : output channels of the feature map, dtype : int
-        mid_channels : channels used in Spectral Convolution Layers, dtype : int
-        modes : Fourier modes for each spatial channel, dtype : List[int] or int (in case all number modes are to be the same for each spatial channel)
-        activation : activation function used within the layers, dtype : string
-        batch_norm : use of batchnorm after each conv layer, dtype : bool
-        in_spatial : spatial dimensions of the input feature map, dtype : int
-
-    Returns:
-
-        Fourier Neural Operator model as specified by input arguments.
-    """
-    net = FNO(in_channels, out_channels, mid_channels, modes, activation, batch_norm, in_spatial)
+"""
+Jax implementation of the unified machine learning API.
+Equivalent functions also exist for the other frameworks.
+
+For API documentation, see https://tum-pbs.github.io/PhiFlow/Network_API .
+"""
+import pickle
+from typing import Callable
+from typing import Union, Sequence
+
+import numpy
+import numpy as np
+import tensorflow as tf
+from tensorflow import Tensor
+from tensorflow import keras
+from tensorflow.keras import layers as kl
+
+from .. import math
+
+
+def parameter_count(model: keras.Model):
+    """
+    Counts the number of parameters in a model.
+
+    Args:
+        model: Keras model
+
+    Returns:
+        `int`
+    """
+    total = 0
+    for parameter in model.trainable_weights:
+        total += numpy.prod(parameter.shape)
+    return int(total)
+
+
+def get_parameters(model: keras.Model, wrap=True) -> dict:
+    result = {}
+    for var in model.trainable_weights:
+        name: str = var.name
+        layer = name[:name.index('/')].replace('_', '').replace('dense', 'linear')
+        try:
+            int(layer[-1:])
+        except ValueError:
+            layer += '0'
+        prop = name[name.index('/') + 1:].replace('kernel', 'weight')
+        if prop.endswith(':0'):
+            prop = prop[:-2]
+        name = f"{layer}.{prop}"
+        var = var.numpy()
+        if not wrap:
+            result[name] = var
+        else:
+            if name.endswith('.weight'):
+                if var.ndim == 2:
+                    phi_tensor = math.wrap(var, math.channel('input,output'))
+                elif var.ndim == 3:
+                    phi_tensor = math.wrap(var, math.channel('x,input,output'))
+                elif var.ndim == 4:
+                    phi_tensor = math.wrap(var, math.channel('x,y,input,output'))
+                elif var.ndim == 5:
+                    phi_tensor = math.wrap(var, math.channel('x,y,z,input,output'))
+            elif name.endswith('.bias'):
+                phi_tensor = math.wrap(var, math.channel('output'))
+            elif var.ndim == 1:
+                phi_tensor = math.wrap(var, math.channel('output'))
+            else:
+                raise NotImplementedError(name, var)
+            result[name] = phi_tensor
+    return result
+
+
+def save_state(obj: Union[keras.models.Model, keras.optimizers.Optimizer], path: str):
+    """
+    Write the state of a module or optimizer to a file.
+
+    See Also:
+        `load_state()`
+
+    Args:
+        obj: `keras.models.Model or keras.optimizers.Optimizer`
+        path: File path as `str`.
+    """
+    if isinstance(obj, keras.models.Model):
+        if not path.endswith('.h5'):
+            path += '.h5'
+        obj.save_weights(path)
+    elif isinstance(obj, keras.optimizers.Optimizer):
+        if not path.endswith('.pkl'):
+            path += '.pkl'
+        weights = obj.get_weights()
+        with open(path, 'wb') as f:
+            pickle.dump(weights, f)
+    else:
+        raise ValueError("obj must be a Keras model or optimizer")
+
+
+def load_state(obj: Union[keras.models.Model, keras.optimizers.Optimizer], path: str):
+    """
+    Read the state of a module or optimizer from a file.
+
+    See Also:
+        `save_state()`
+
+    Args:
+        obj: `keras.models.Model or keras.optimizers.Optimizer`
+        path: File path as `str`.
+    """
+    if isinstance(obj, keras.models.Model):
+        if not path.endswith('.h5'):
+            path += '.h5'
+        obj.load_weights(path)
+    elif isinstance(obj, keras.optimizers.Optimizer):
+        if not path.endswith('.pkl'):
+            path += '.pkl'
+        with open(path, 'rb') as f:
+            weights = pickle.load(f)
+        obj.set_weights(weights)
+    else:
+        raise ValueError("obj must be a Keras model or optimizer")
+
+
+def update_weights(net: keras.Model, optimizer: keras.optimizers.Optimizer, loss_function: Callable, *loss_args, **loss_kwargs):
+    """
+    Computes the gradients of `loss_function` w.r.t. the parameters of `net` and updates its weights using `optimizer`.
+
+    This is the TensorFlow/Keras version. Analogue functions exist for other learning frameworks.
+
+    Args:
+        net: Learning model.
+        optimizer: Optimizer.
+        loss_function: Loss function, called as `loss_function(*loss_args, **loss_kwargs)`.
+        *loss_args: Arguments given to `loss_function`.
+        **loss_kwargs: Keyword arguments given to `loss_function`.
+
+    Returns:
+        Output of `loss_function`.
+    """
+    with tf.GradientTape() as tape:
+        output = loss_function(*loss_args, **loss_kwargs)
+        loss = output[0] if isinstance(output, tuple) else output
+        gradients = tape.gradient(loss.sum, net.trainable_variables)
+    optimizer.apply_gradients(zip(gradients, net.trainable_variables))
+    return output
+
+
+def adam(net: keras.Model, learning_rate: float = 1e-3, betas=(0.9, 0.999), epsilon=1e-07):
+    """
+    Creates an Adam optimizer for `net`, alias for [`keras.optimizers.Adam`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam).
+    Analogous functions exist for other learning frameworks.
+    """
+    return keras.optimizers.Adam(learning_rate, betas[0], betas[1], epsilon)
+
+
+def sgd(net: keras.Model, learning_rate: float = 1e-3, momentum=0, dampening=0, weight_decay=0, nesterov=False):
+    """
+    Creates an SGD optimizer for 'net', alias for ['keras.optimizers.SGD'](https://keras.io/api/optimizers/sgd/)
+    Analogous functions exist for other learning frameworks.
+    """
+    return keras.optimizers.SGD(learning_rate, momentum, nesterov)
+
+
+def adagrad(net: keras.Model, learning_rate: float = 1e-3, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10):
+    """
+    Creates an Adagrad optimizer for 'net', alias for ['keras.optimizers.Adagrad'](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adagrad)
+    Analogous functions exist for other learning frameworks.
+    """
+    return keras.optimizers.Adagrad(learning_rate, initial_accumulator_value, eps)
+
+
+def rmsprop(net: keras.Model, learning_rate: float = 1e-3, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False):
+    """
+    Creates an RMSProp optimizer for 'net', alias for ['keras.optimizers.RMSprop'](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop)
+    Analogous functions exist for other learning frameworks.
+    """
+    return keras.optimizers.RMSprop(learning_rate, alpha, momentum, eps, centered)
+
+
+def dense_net(in_channels: int,
+              out_channels: int,
+              layers: Sequence[int],
+              batch_norm=False,
+              activation='ReLU',
+              softmax=False) -> keras.Model:
+    """
+    Fully-connected neural networks are available in ΦFlow via dense_net().
+    Arguments:
+        in_channels : size of input layer, int
+        out_channels = size of output layer, int
+        layers : tuple of linear layers between input and output neurons, list or tuple
+        activation : activation function used within the layers, string
+        batch_norm : use of batch norm after each linear layer, bool
+
+    Returns:
+        Dense net model as specified by input arguments
+    """
+    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
+    keras_layers = []
+    for neuron_count in layers:
+        keras_layers.append(kl.Dense(neuron_count, activation=activation))
+        if batch_norm:
+            keras_layers.append(kl.BatchNormalization())
+    return keras.models.Sequential([kl.InputLayer(input_shape=(in_channels,)),
+                                    *keras_layers,
+                                    kl.Dense(out_channels, activation='linear'),
+                                    *([kl.Softmax()] if softmax else [])])
+
+
+def u_net(in_channels: int,
+          out_channels: int,
+          levels: int = 4,
+          filters: Union[int, tuple, list] = 16,
+          batch_norm: bool = True,
+          activation: Union[str, Callable] = 'ReLU',
+          in_spatial: Union[tuple, int] = 2,
+          periodic=False,
+          use_res_blocks: bool = False, **kwargs) -> keras.Model:
+    """
+    ΦFlow provides a built-in U-net architecture, classically popular for Semantic Segmentation in Computer Vision, composed of downsampling and upsampling layers.
+
+    Arguments:
+
+        in_channels: input channels of the feature map, dtype : int
+        out_channels : output channels of the feature map, dtype : int
+        levels : number of levels of down-sampling and upsampling, dtype : int
+        filters : filter sizes at each down/up sampling convolutional layer, if the input is integer all conv layers have the same filter size,
+        dtype : int or tuple
+        activation : activation function used within the layers, dtype : string
+        batch_norm : use of batchnorm after each conv layer, dtype : bool
+        in_spatial : spatial dimensions of the input feature map, dtype : int
+        use_res_blocks : use convolutional blocks with skip connections instead of regular convolutional blocks, dtype : bool
+
+    Returns:
+
+        U-net model as specified by input arguments
+    """
+    if isinstance(in_spatial, int):
+        d = in_spatial
+        in_spatial = (None,) * d
+    else:
+        assert isinstance(in_spatial, tuple)
+        d = len(in_spatial)
+    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
+    if isinstance(filters, (tuple, list)):
+        assert len(filters) == levels, f"List of filters has length {len(filters)} but u-net has {levels} levels."
+    else:
+        filters = (filters,) * levels
+    # --- Construct the U-Net ---
+    x = inputs = keras.Input(shape=in_spatial + (in_channels,))
+    x = resnet_block(x.shape[-1], filters[0], periodic, batch_norm, activation, d)(x) if use_res_blocks else double_conv(x, d, filters[0], filters[0], batch_norm, activation, periodic)
+    xs = [x]
+    for i in range(1, levels):
+        x = MAX_POOL[d](2, padding="same")(x)
+        x = resnet_block(x.shape[-1], filters[i], periodic, batch_norm, activation, d)(x) if use_res_blocks else double_conv(x, d, filters[i], filters[i], batch_norm, activation, periodic)
+        xs.insert(0, x)
+    for i in range(1, levels):
+        x = UPSAMPLE[d](2)(x)
+        x = kl.Concatenate()([x, xs[i]])
+        x = resnet_block(x.shape[-1], filters[i - 1], periodic, batch_norm, activation, d)(x) if use_res_blocks else double_conv(x, d, filters[i - 1], filters[i - 1], batch_norm, activation, periodic)
+    x = CONV[d](out_channels, 1)(x)
+    return keras.Model(inputs, x)
+
+
+CONV = [None, kl.Conv1D, kl.Conv2D, kl.Conv3D]
+MAX_POOL = [None, kl.MaxPool1D, kl.MaxPool2D, kl.MaxPool3D]
+UPSAMPLE = [None, kl.UpSampling1D, kl.UpSampling2D, kl.UpSampling3D]
+ACTIVATIONS = {'tanh': keras.activations.tanh, 'ReLU': keras.activations.relu, 'Sigmoid': keras.activations.sigmoid, 'SiLU': keras.activations.selu}
+
+
+def pad_periodic(x: Tensor):
+    d = len(x.shape) - 2
+    if d >= 1:
+        x = tf.concat([tf.expand_dims(x[:, -1, ...], axis=1), x, tf.expand_dims(x[:, 0, ...], axis=1)], axis=1)
+    if d >= 2:
+        x = tf.concat([tf.expand_dims(x[:, :, -1, ...], axis=2), x, tf.expand_dims(x[:, :, 0, ...], axis=2)], axis=2)
+    if d >= 3:
+        x = tf.concat([tf.expand_dims(x[:, :, :, -1, ...], axis=3), x, tf.expand_dims(x[:, :, :, 0, ...], axis=3)],
+                      axis=3)
+    return x
+
+
+def double_conv(x, d: int, out_channels: int, mid_channels: int, batch_norm: bool, activation: Callable, periodic: bool):
+    x = CONV[d](mid_channels, 3, padding='valid')(pad_periodic(x)) if periodic else CONV[d](mid_channels, 3, padding='same')(x)
+    if batch_norm:
+        x = kl.BatchNormalization()(x)
+    x = activation(x)
+    x = CONV[d](out_channels, 3, padding='valid')(pad_periodic(x)) if periodic else CONV[d](out_channels, 3, padding='same')(x)
+    if batch_norm:
+        x = kl.BatchNormalization()(x)
+    x = activation(x)
+    return x
+
+
+def conv_net(in_channels: int,
+             out_channels: int,
+             layers: Sequence[int],
+             batch_norm: bool = False,
+             activation: Union[str, Callable] = 'ReLU',
+             periodic=False,
+             in_spatial: Union[int, tuple] = 2, **kwargs) -> keras.Model:
+    """
+    Built in Conv-Nets are also provided. Contrary to the classical convolutional neural networks, the feature map spatial size remains the same throughout the layers. Each layer of the network is essentially a convolutional block comprising of two conv layers. A filter size of 3 is used in the convolutional layers.
+    Arguments:
+
+        in_channels : input channels of the feature map, dtype : int
+        out_channels : output channels of the feature map, dtype : int
+        layers : list or tuple of output channels for each intermediate layer between the input and final output channels, dtype : list or tuple
+        activation : activation function used within the layers, dtype : string
+        batch_norm : use of batchnorm after each conv layer, dtype : bool
+        in_spatial : spatial dimensions of the input feature map, dtype : int
+
+    Returns:
+
+        Conv-net model as specified by input arguments
+    """
+    if isinstance(in_spatial, int):
+        d = in_spatial
+        in_spatial = (None,) * d
+    else:
+        assert isinstance(in_spatial, tuple)
+        d = len(in_spatial)
+    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
+    x = inputs = keras.Input(shape=in_spatial + (in_channels,))
+    if len(layers) < 1:
+        layers.append(out_channels)
+    for i in range(len(layers)):
+        x = CONV[d](layers[i], 3, padding='valid')(pad_periodic(x)) if periodic else CONV[d](layers[i], 3, padding='same')(x)
+        if batch_norm:
+            x = kl.BatchNormalization()(x)
+        x = activation(x)
+    x = CONV[d](out_channels, 1)(x)
+    return keras.Model(inputs, x)
+
+
+def resnet_block(in_channels: int,
+                 out_channels: int,
+                 periodic: bool,
+                 batch_norm: bool = False,
+                 activation: Union[str, Callable] = 'ReLU',
+                 in_spatial: Union[int, tuple] = 2):
+    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
+    if isinstance(in_spatial, int):
+        d = in_spatial
+    else:
+        assert isinstance(in_spatial, tuple)
+        d = len(in_spatial)
+    x = x_1 = inputs = keras.Input(shape=(None,) * d + (in_channels,))
+    x = CONV[d](out_channels, 3, padding='valid')(pad_periodic(x)) if periodic else CONV[d](out_channels, 3, padding='same')(x)
+    if batch_norm:
+        x = kl.BatchNormalization()(x)
+    x = activation(x)
+    x = CONV[d](out_channels, 3, padding='valid')(pad_periodic(x)) if periodic else CONV[d](out_channels, 3, padding='same')(x)
+    if batch_norm:
+        x = kl.BatchNormalization()(x)
+    x = activation(x)
+    if in_channels != out_channels:
+        x_1 = CONV[d](out_channels, 1)(x_1)
+        if batch_norm:
+            x_1 = kl.BatchNormalization()(x_1)
+    x = kl.Add()([x, x_1])
+    return keras.Model(inputs, x)
+
+
+def res_net(in_channels: int,
+            out_channels: int,
+            layers: Sequence[int],
+            batch_norm: bool = False,
+            activation: Union[str, Callable] = 'ReLU',
+            periodic=False,
+            in_spatial: Union[int, tuple] = 2, **kwargs):
+    """
+    Built in Res-Nets are provided in the ΦFlow framework. Similar to the conv-net, the feature map spatial size remains the same throughout the layers.
+    These networks use residual blocks composed of two conv layers with a skip connection added from the input to the output feature map.
+    A default filter size of 3 is used in the convolutional layers.
+
+    Arguments:
+
+        in_channels : input channels of the feature map, dtype : int
+        out_channels : output channels of the feature map, dtype : int
+        layers : list or tuple of output channels for each intermediate layer between the input and final output channels, dtype : list or tuple
+        activation : activation function used within the layers, dtype : string
+        batch_norm : use of batchnorm after each conv layer, dtype : bool
+        in_spatial : spatial dimensions of the input feature map, dtype : int
+
+    Returns:
+
+        Res-net model as specified by input arguments
+    """
+    if isinstance(in_spatial, int):
+        d = in_spatial
+        in_spatial = (None,) * d
+    else:
+        assert isinstance(in_spatial, tuple)
+        d = len(in_spatial)
+
+    x = inputs = keras.Input(shape=in_spatial + (in_channels,))
+    if len(layers) < 1:
+        layers.append(out_channels)
+    out = resnet_block(in_channels, layers[0], periodic, batch_norm, activation, d)(x)
+    for i in range(1, len(layers)):
+        out = resnet_block(layers[i - 1], layers[i], periodic, batch_norm, activation, d)(out)
+    out = CONV[d](out_channels, 1)(out)
+    return keras.Model(inputs, out)
+
+
+def conv_classifier(in_features: int,
+                    in_spatial: Union[tuple, list],
+                    num_classes: int,
+                    blocks=(64, 128, 256, 256, 512, 512),
+                    block_sizes=(2, 2, 3, 3, 3),
+                    dense_layers=(4096, 4096, 100),
+                    batch_norm=True,
+                    activation='ReLU',
+                    softmax=True,
+                    periodic=False):
+    """
+    Based on VGG16.
+    """
+    assert isinstance(in_spatial, (tuple, list))
+    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
+    d = len(in_spatial)
+    x = inputs = keras.Input(shape=in_spatial + (in_features,))
+    for i, (next, block_size) in enumerate(zip(blocks, block_sizes)):
+        for j in range(block_size):
+            x = CONV[d](next, 3, padding='valid')(pad_periodic(x)) if periodic else CONV[d](next, 3, padding='same')(x)
+            if batch_norm:
+                x = kl.BatchNormalization()(x)
+            x = activation(x)
+        x = MAX_POOL[d](2)(x)
+    x = kl.Flatten()(x)
+    flat_size = int(np.prod(in_spatial) * blocks[-1] / (2**d) ** len(blocks))
+    x = dense_net(flat_size, num_classes, dense_layers, batch_norm, activation, softmax)(x)
+    return keras.Model(inputs, x)
+
+
+def get_mask(inputs, reverse_mask, data_format='NHWC'):
+    """ Compute mask for slicing input feature map for Invertible Nets """
+    shape = inputs.shape
+    if len(shape) == 2:
+        N = shape[-1]
+        range_n = tf.range(0, N)
+        even_ind = range_n % 2
+        checker = tf.reshape(even_ind, (-1, N))
+    elif len(shape) == 4:
+        H = shape[2] if data_format == 'NCHW' else shape[1]
+        W = shape[3] if data_format == 'NCHW' else shape[2]
+
+        range_h = tf.range(0, H)
+        range_w = tf.range(0, W)
+
+        even_ind_h = tf.cast(range_h % 2, dtype=tf.bool)
+        even_ind_w = tf.cast(range_w % 2, dtype=tf.bool)
+
+        ind_h = tf.tile(tf.expand_dims(even_ind_h, -1), [1, W])
+        ind_w = tf.tile(tf.expand_dims(even_ind_w, 0), [H, 1])
+        # ind_h = even_ind_h.unsqueeze(-1).repeat(1, W)
+        # ind_w = even_ind_w.unsqueeze( 0).repeat(H, 1)
+
+        checker = tf.math.logical_xor(ind_h, ind_w)
+
+        reshape = [-1, 1, H, W] if data_format == 'NCHW' else [-1, H, W, 1]
+        checker = tf.reshape(checker, reshape)
+        checker = tf.cast(checker, dtype=tf.float32)
+
+    else:
+        raise ValueError('Invalid tensor shape. Dimension of the tensor shape must be '
+                         '2 (NxD) or 4 (NxCxHxW or NxHxWxC), got {}.'.format(inputs.get_shape().as_list()))
+
+    if reverse_mask:
+        checker = 1 - checker
+
+    return checker
+
+
+def Dense_resnet_block(in_channels: int,
+                       mid_channels: int,
+                       batch_norm: bool = False,
+                       activation: Union[str, Callable] = 'ReLU'):
+    inputs = keras.Input(shape=(in_channels,))
+    x_1 = inputs
+    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
+
+    x = kl.Dense(mid_channels)(inputs)
+    if batch_norm:
+        x = kl.BatchNormalization()(x)
+    x = activation(x)
+
+    x = kl.Dense(in_channels)(x)
+    if batch_norm:
+        x = kl.BatchNormalization()(x)
+    x = activation(x)
+
+    x = kl.Add()([x, x_1])
+
+    return keras.Model(inputs, x)
+
+
+NET = {'u_net': u_net, 'res_net': res_net, 'conv_net': conv_net}
+
+
+class CouplingLayer(keras.Model):
+
+    def __init__(self, in_channels, activation, batch_norm, in_spatial, net, reverse_mask):
+        super(CouplingLayer, self).__init__()
+
+        self.activation = activation
+        self.batch_norm = batch_norm
+        self.reverse_mask = reverse_mask
+
+        if in_spatial == 0:  # for in_spatial = 0, use dense layers
+            self.s1 = Dense_resnet_block(in_channels, in_channels, batch_norm, activation)
+            self.t1 = Dense_resnet_block(in_channels, in_channels, batch_norm, activation)
+
+            self.s2 = Dense_resnet_block(in_channels, in_channels, batch_norm, activation)
+            self.t2 = Dense_resnet_block(in_channels, in_channels, batch_norm, activation)
+        else:
+            self.s1 = NET[net](in_channels=in_channels, out_channels=in_channels, layers=[],
+                               batch_norm=batch_norm, activation=activation,
+                               in_spatial=in_spatial)
+            self.t1 = NET[net](in_channels=in_channels, out_channels=in_channels, layers=[],
+                               batch_norm=batch_norm, activation=activation,
+                               in_spatial=in_spatial)
+
+            self.s2 = NET[net](in_channels=in_channels, out_channels=in_channels, layers=[],
+                               batch_norm=batch_norm, activation=activation,
+                               in_spatial=in_spatial)
+            self.t2 = NET[net](in_channels=in_channels, out_channels=in_channels, layers=[],
+                               batch_norm=batch_norm, activation=activation,
+                               in_spatial=in_spatial)
+
+    def call(self, x, invert=False):
+        mask = get_mask(x, self.reverse_mask, 'NCHW')
+
+        if invert:
+            v1 = x * mask
+            v2 = x * (1 - mask)
+
+            u2 = (1 - mask) * (v2 - self.t1(v1)) * tf.math.exp(tf.tanh(-self.s1(v1)))
+            u1 = mask * (v1 - self.t2(u2)) * tf.math.exp(tf.tanh(-self.s2(u2)))
+
+            return u1 + u2
+        else:
+            u1 = x * mask
+            u2 = x * (1 - mask)
+
+            v1 = mask * (u1 * tf.math.exp(tf.tanh(self.s2(u2))) + self.t2(u2))
+            v2 = (1 - mask) * (u2 * tf.math.exp(tf.tanh(self.s1(v1))) + self.t1(v1))
+
+            return v1 + v2
+
+
+class InvertibleNet(keras.Model):
+    def __init__(self, in_channels, num_blocks, activation, batch_norm, in_spatial, net):
+        super(InvertibleNet, self).__init__()
+        self.num_blocks = num_blocks
+
+        self.layer_dict = {}
+        for i in range(num_blocks):
+            self.layer_dict[f'coupling_block{i + 1}'] = \
+                CouplingLayer(in_channels,
+                              activation, batch_norm,
+                              in_spatial, net, (i % 2 == 0))
+
+    def call(self, x, backward=False):
+        if backward:
+            for i in range(self.num_blocks, 0, -1):
+                x = self.layer_dict[f'coupling_block{i}'](x, backward)
+        else:
+            for i in range(1, self.num_blocks + 1):
+                x = self.layer_dict[f'coupling_block{i}'](x)
+        return x
+
+
+def invertible_net(in_channels: int,
+                   num_blocks: int,
+                   batch_norm: bool = False,
+                   net: str = 'u_net',
+                   activation: Union[str, type] = 'ReLU',
+                   in_spatial: Union[tuple, int] = 2, **kwargs):
+    """
+    ΦFlow also provides invertible neural networks that are capable of inverting the output tensor back to the input tensor initially passed.\ These networks have far reaching applications in predicting input parameters of a problem given its observations.\ Invertible nets are composed of multiple concatenated coupling blocks wherein each such block consists of arbitrary neural networks.
+
+    Currently, these arbitrary neural networks could be set to u_net(default), conv_net, res_net or dense_net blocks with in_channels = out_channels.
+    The architecture used is popularized by ["Real NVP"](https://arxiv.org/abs/1605.08803).
+
+    Arguments:
+
+        in_channels : input channels of the feature map, dtype : int
+        num_blocks : number of coupling blocks inside the invertible net, dtype : int
+        activation : activation function used within the layers, dtype : string
+        batch_norm : use of batchnorm after each layer, dtype : bool
+        in_spatial : spatial dimensions of the input feature map, dtype : int
+        net : type of neural network blocks used in coupling layers, dtype : str
+        **kwargs : placeholder for arguments not supported by the function
+
+    Returns:
+
+        Invertible Net model as specified by input arguments
+
+    Note: Currently supported values for net are 'u_net'(default), 'conv_net' and 'res_net'.
+    For choosing 'dense_net' as the network block in coupling layers in_spatial must be set to zero.
+    """
+    if isinstance(in_spatial, tuple):
+        in_spatial = len(in_spatial)
+
+    return InvertibleNet(in_channels, num_blocks, activation,
+                         batch_norm, in_spatial, net)
+
+
+##################################################################################################################
+#  Fourier Neural Operators
+#  source: https://github.com/zongyi-li/fourier_neural_operator
+###################################################################################################################
+RFFT = [None, tf.signal.rfft, tf.signal.rfft2d, tf.signal.rfft3d]
+FFT = [None, tf.signal.fft, tf.signal.fft2d, tf.signal.fft3d]
+IRFFT = [None, tf.signal.irfft, tf.signal.irfft2d, tf.signal.irfft3d]
+
+class SpectralConv(keras.Model):
+
+    def __init__(self, in_channels, out_channels, modes, in_spatial):
+
+        super(SpectralConv, self).__init__()
+
+        self.in_channels = in_channels
+        self.out_channels = out_channels
+        self.in_spatial = in_spatial
+        assert in_spatial >= 1 and in_spatial <= 3
+        if isinstance(modes, int):
+            mode = modes
+            modes = [mode for i in range(in_spatial)]
+
+        self.scale = 1 / (in_channels * out_channels)
+
+        self.modes = {i + 1: modes[i] for i in range(len(modes))}
+        self.weights_ = {}
+
+        rand_shape = [in_channels, out_channels]
+        rand_shape += [self.modes[i] for i in range(1, in_spatial + 1)]
+
+        for i in range(2 ** (in_spatial - 1)):
+            self.weights_[f'w{i + 1}'] = tf.complex(tf.Variable(self.scale * tf.random.normal(shape=rand_shape, dtype=tf.dtypes.float32), trainable=True),
+                                                tf.Variable(self.scale * tf.random.normal(shape=rand_shape, dtype=tf.dtypes.float32), trainable=True))
+
+    def complex_mul(self, input, weights):
+
+        if self.in_spatial == 1:
+            return tf.einsum("bix,iox->box", input, weights)
+        elif self.in_spatial == 2:
+            return tf.einsum("bixy,ioxy->boxy", input, weights)
+        elif self.in_spatial == 3:
+            return tf.einsum("bixyz,ioxyz->boxyz", input, weights)
+
+
+    def call(self, x):
+        batch_size = x.shape[0]
+
+        x_ft = RFFT[self.in_spatial](x)
+
+        outft_dims = [batch_size, self.out_channels] + \
+                     [x.shape[-i] for i in range(self.in_spatial, 1, -1)] + [x.shape[-1] // 2 + 1]
+        out_ft0 = tf.complex(tf.Variable(tf.zeros(outft_dims, dtype=tf.dtypes.float32)),
+                            tf.Variable(tf.zeros(outft_dims, dtype=tf.dtypes.float32)))
+
+        if self.in_spatial == 1:
+            out_ft1 = self.complex_mul(x_ft[:, :, :self.modes[1]],
+                                      self.weights_['w1'])
+            out_ft = tf.concat([out_ft1, out_ft0[:, :, self.modes[1]:]], axis=-1)
+        elif self.in_spatial == 2:
+            out_ft1 = self.complex_mul(x_ft[:, :, :self.modes[1], :self.modes[2]],
+                                 self.weights_['w1'])
+            out_ft2 = self.complex_mul(x_ft[:, :, -self.modes[1]:, :self.modes[2]],
+                                 self.weights_['w2'])
+            out_ft3 = tf.concat([out_ft1, out_ft0[:, :, self.modes[1]:-self.modes[1],
+                                         :self.modes[2]], out_ft2], axis=-2)
+            out_ft = tf.concat([out_ft3, out_ft0[:, :, :, self.modes[2]:]], axis=-1)
+        elif self.in_spatial == 3:
+            out_ft1 = self.complex_mul(x_ft[:, :, :self.modes[1], :self.modes[2], :self.modes[3]],
+                                 self.weights_['w1'])
+            out_ft2 = self.complex_mul(x_ft[:, :, -self.modes[1]:, :self.modes[2], :self.modes[3]],
+                                 self.weights_['w2'])
+            out_ft3 = self.complex_mul(x_ft[:, :, :self.modes[1], -self.modes[2]:, :self.modes[3]],
+                                 self.weights_['w3'])
+            out_ft4 = self.complex_mul(x_ft[:, :, -self.modes[1]:, -self.modes[2]:, :self.modes[3]],
+                                 self.weights_['w4'])
+
+            out_ft5 = tf.concat([out_ft1, out_ft0[:, :, self.modes[1]:-self.modes[1], :self.modes[2], :self.modes[3]]
+                                    , out_ft2], axis=-3)
+            out_ft6 = tf.concat([out_ft3, out_ft0[:, :, self.modes[1]:-self.modes[1], -self.modes[2]:, :self.modes[3]]
+                                    , out_ft4], axis=-3)
+            out_ft7 = tf.concat([out_ft5, out_ft0[:, :, :, self.modes[2]:-self.modes[2], :self.modes[3]], out_ft6],
+                                axis=-2)
+            out_ft = tf.concat([out_ft7, out_ft0[:, :, :, :, self.modes[3]:]], axis=-1)
+
+        ##Return to Physical Space
+        x = IRFFT[self.in_spatial](out_ft)
+
+        return x
+
+
+class FNO(keras.Model):
+
+    def __init__(self, in_channels, out_channels, width, modes, activation, batch_norm, in_spatial):
+        super(FNO, self).__init__()
+
+        """
+        The overall network. It contains 4 layers of the Fourier layer.
+        1. Lift the input to the desire channel dimension by self.fc0 .
+        2. 4 layers of the integral operators u' = (W + K)(u).
+            W defined by self.w; K defined by self.conv .
+        3. Project from the channel space to the output space by self.fc1 and self.fc2.
+
+        input shape and output shape: (batchsize b, channels c, *spatial)
+        """
+
+        self.activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
+        self.width = width
+        self.in_spatial = in_spatial
+
+        self.fc0 = kl.Dense(self.width)
+
+        self.model_dict = {}
+        for i in range(4):
+            self.model_dict[f'conv{i}'] = SpectralConv(self.width, self.width, modes, in_spatial)
+            self.model_dict[f'w{i}'] = CONV[self.in_spatial](self.width, kernel_size=1)
+            self.model_dict[f'bn{i}'] = kl.BatchNormalization()
+
+        self.fc1 = kl.Dense(128)
+        self.fc2 = kl.Dense(out_channels)
+
+    # Adding extra spatial channels eg. x, y, z, .... to input x
+    def get_grid(self, shape, device):
+        batch_size = shape[0]
+        grid_channel_sizes = shape[1:-1]  # shape =  (batch_size, *spatial, channels)
+        self.grid_channels = {}
+        for i in range(self.in_spatial):
+            self.grid_channels[f'dim{i}'] = tf.cast(tf.linspace(0, 1,
+                                        grid_channel_sizes[i]), dtype=tf.dtypes.float32)  #tf.tensor(tf.linspace(0, 1, grid_channel_sizes[i]), dtype=tf.dtypes.float32)
+            reshape_dim_tuple = [1,] + [1 if i != j else grid_channel_sizes[j]
+                                        for j in range(self.in_spatial)] + [1,]
+            repeat_dim_tuple = [batch_size,] + [1 if i == j else grid_channel_sizes[j]
+                                                for j in range(self.in_spatial)] + [1,]
+
+            self.grid_channels[f'dim{i}'] = tf.tile(tf.reshape(self.grid_channels[f'dim{i}'], reshape_dim_tuple), repeat_dim_tuple)
+
+        return tf.concat([self.grid_channels[f'dim{i}'] for i in range(self.in_spatial)], axis=-1)
+
+    def call(self, x):
+        grid = self.get_grid(x.shape, x.device)
+        x = tf.concat([x, grid], axis=-1)
+
+        permute_tuple= [0] + [self.in_spatial + 1] + [i + 1 for i in range(self.in_spatial)]
+        permute_tuple_reverse = [0] + [2 + i for i in range(self.in_spatial)] + [1]
+
+        # No need to Transpose x such that channels shape lies
+        # at the end to pass it through linear layers as it's the default in tf
+        #x = tf.transpose(x, permute_tuple)
+
+        x = self.fc0(x)
+
+        for i in range(4):
+            x1 = self.model_dict[f'w{i}'](x)
+            # Spectral conv expects a shape : [batch, channel, *spatial]
+            # hence the transpose:
+            x2 = self.model_dict[f'conv{i}'](tf.transpose(x, permute_tuple))
+            x2 = tf.transpose(x2, permute_tuple_reverse)
+            x = self.model_dict[f'bn{i}'](x1) + self.model_dict[f'bn{i}'](x2)
+            x = self.activation(x)
+
+        x = self.activation(self.fc1(x))
+        x = self.fc2(x)
+
+        return x
+
+
+def fno(in_channels: int,
+        out_channels: int,
+        mid_channels: int,
+        modes: Sequence[int],
+        activation: Union[str, type] = 'ReLU',
+        batch_norm: bool = False,
+        in_spatial: int = 2):
+    """
+    ["Fourier Neural Operator"](https://github.com/zongyi-li/fourier_neural_operator) network contains 4 layers of the Fourier layer.
+    1. Lift the input to the desire channel dimension by self.fc0 .
+    2. 4 layers of the integral operators u' = (W + K)(u). W defined by self.w; K defined by self.conv .
+    3. Project from the channel space to the output space by self.fc1 and self.fc2.
+
+    Arguments:
+
+        in_channels : input channels of the feature map, dtype : int
+        out_channels : output channels of the feature map, dtype : int
+        mid_channels : channels used in Spectral Convolution Layers, dtype : int
+        modes : Fourier modes for each spatial channel, dtype : List[int] or int (in case all number modes are to be the same for each spatial channel)
+        activation : activation function used within the layers, dtype : string
+        batch_norm : use of batchnorm after each conv layer, dtype : bool
+        in_spatial : spatial dimensions of the input feature map, dtype : int
+
+    Returns:
+
+        Fourier Neural Operator model as specified by input arguments.
+    """
+    net = FNO(in_channels, out_channels, mid_channels, modes, activation, batch_norm, in_spatial)
     return net
```

### Comparing `phiflow-2.3.4/phi/torch/_torch_backend.py` & `phiflow-2.4.0/phi/torch/_torch_backend.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,1163 +1,1234 @@
-import numbers
-import warnings
-from contextlib import contextmanager
-from functools import wraps
-from typing import List, Callable, Optional, Set, Tuple, Any, Union
-
-import numpy as np
-import torch
-import torch.fft
-import torch.nn.functional as torchf
-from packaging import version
-
-from phi.math import DType
-from phi.math.backend import Backend, NUMPY, ComputeDevice, PHI_LOGGER
-from phi.math.backend._backend import combined_dim, SolveResult, get_functional_derivative_order, TensorType
-
-
-class TorchBackend(Backend):
-
-    def __init__(self):
-        cpu = NUMPY.get_default_device()
-        devices = [ComputeDevice(self, "CPU", 'CPU', cpu.memory, cpu.processor_count, cpu.description, ref='cpu')]
-        for index in range(torch.cuda.device_count()):
-            properties = torch.cuda.get_device_properties(index)
-            devices.append(ComputeDevice(self, properties.name, 'GPU', properties.total_memory, properties.multi_processor_count, f"compute capability {properties.major}.{properties.minor}", f'cuda:{index}'))
-        Backend.__init__(self, 'PyTorch', devices, devices[1 if len(devices) > 1 else 0])
-
-    def prefers_channels_last(self) -> bool:
-        return False
-
-    def is_module(self, obj):
-        return isinstance(obj, (JITFunction, torch.nn.Module))
-
-    def is_tensor(self, x, only_native=False):
-        if isinstance(x, torch.Tensor):
-            return True
-        if only_native:
-            return False
-        if isinstance(x, numbers.Number):
-            return True
-        if isinstance(x, (tuple, list)) and all(isinstance(c, numbers.Number) for c in x):
-            return True
-        if isinstance(x, np.ndarray) and x.dtype != object:
-            return True  # this is pretty much required, else we couldn't perform NP+PyTorch operations
-        return False
-
-    def as_tensor(self, x, convert_external=True):
-        if isinstance(x, torch.nn.Module):
-            return x
-        if self.is_tensor(x, only_native=convert_external):
-            tensor = x
-        elif isinstance(x, np.ndarray):
-            try:
-                tensor = torch.from_numpy(x)
-            except ValueError:  # or TypeError?
-                tensor = torch.from_numpy(x.copy())
-            tensor = tensor.to(self.get_default_device().ref)
-        elif isinstance(x, (tuple, list)):
-            try:
-                x = np.stack(x)
-                tensor = torch.tensor(x, device=self.get_default_device().ref)
-            except ValueError:  # there may be Tensors inside the list
-                components = [self.as_tensor(c) for c in x]
-                tensor = torch.stack(components, dim=0)
-        else:
-            tensor = torch.tensor(x, device=self.get_default_device().ref)
-        # --- Enforce Precision ---
-        if self.is_tensor(tensor, only_native=True):
-            dtype = self.dtype(tensor)
-            if dtype.kind == float:
-                tensor = self.to_float(tensor)
-            elif dtype.kind == complex:
-                tensor = self.to_complex(tensor)
-        # --- Move to default device ---
-        if isinstance(tensor, torch.Tensor) and tensor.device != self.get_default_device().ref:
-            tensor = tensor.to(self.get_default_device().ref)
-        return tensor
-
-    def recursive_as_tensor(self, obj):
-        if isinstance(obj, (tuple, list)):
-            return type(obj)([self.recursive_as_tensor(item) for item in obj])
-        elif isinstance(obj, dict):
-            raise NotImplementedError()
-        else:
-            return self.as_tensor(obj)
-
-    def auto_cast(self, *tensors, **kwargs) -> list:
-        tensors = [t if isinstance(t, (numbers.Number, bool)) else self.as_tensor(t, True) for t in tensors]
-        return Backend.auto_cast(self, *tensors, **kwargs)
-
-    def is_available(self, tensor) -> bool:
-        # return True
-        return torch._C._get_tracing_state() is None  # TODO can we find out whether this tensor specifically is being traced?
-
-    def numpy(self, tensor):
-        if tensor.requires_grad:
-            tensor = tensor.detach()
-        if hasattr(tensor, 'resolve_conj'):
-            tensor = tensor.resolve_conj()
-        return tensor.cpu().numpy()
-
-    def to_dlpack(self, tensor):
-        from torch.utils import dlpack
-        return dlpack.to_dlpack(tensor)
-
-    def from_dlpack(self, capsule):
-        from torch.utils import dlpack
-        tensor = dlpack.from_dlpack(capsule)
-        tensor = tensor.to(self.get_default_device().ref)
-        return tensor
-
-    def copy(self, tensor, only_mutable=False):
-        return torch.clone(tensor)
-
-    def get_device(self, tensor: TensorType) -> ComputeDevice:
-        return self.get_device_by_ref(str(tensor.device))
-
-    def allocate_on_device(self, tensor: TensorType, device: ComputeDevice) -> TensorType:
-        return self.as_tensor(tensor).to(device.ref)
-
-    def multi_slice(self, tensor, slices: tuple):
-        neg_slices = [i for i, s in enumerate(slices) if isinstance(s, slice) and s.step is not None and s.step < 0]
-        if neg_slices:
-            tensor = torch.flip(tensor, neg_slices)
-        pos_slices = [slice(s.start, s.stop, -s.step) if i in neg_slices else s for i, s in enumerate(slices)]
-        return tensor[tuple(pos_slices)]
-
-    sqrt = torch.sqrt
-    exp = torch.exp
-    sin = torch.sin
-    arcsin = torch.arcsin
-    cos = torch.cos
-    arccos = torch.arccos
-    tan = torch.tan
-    arctan = torch.arctan
-    sinh = torch.sinh
-    arcsinh = torch.arcsinh
-    cosh = torch.cosh
-    arccosh = torch.arccosh
-    tanh = torch.tanh
-    arctanh = torch.arctanh
-    log = torch.log
-    log2 = torch.log2
-    log10 = torch.log10
-    sigmoid = torch.sigmoid
-    isfinite = torch.isfinite
-    abs = torch.abs
-    sign = torch.sign
-    round = torch.round
-    ceil = torch.ceil
-    floor = torch.floor
-    nonzero = torch.nonzero
-    flip = torch.flip
-    seed = staticmethod(torch.manual_seed)
-
-    def einsum(self, equation, *tensors):
-        tensors = self.auto_cast(*tensors, bool_to_int=True, int_to_float=True)
-        return torch.einsum(equation, *tensors)
-
-    def jit_compile(self, f: Callable) -> Callable:
-        return JITFunction(self, f)
-
-    def custom_gradient(self, f: Callable, gradient: Callable = None, get_external_cache: Callable = None, on_call_skipped: Callable = None) -> Callable:
-        """ See PyTorch_Jit.md """
-        def select_jit(*args):
-            args = [self.as_tensor(arg) for arg in args]
-            if not CURRENT_JIT_CALLS:
-                return torch_function.apply(*args)
-            jit = CURRENT_JIT_CALLS[-1]
-            if torch._C._get_tracing_state() is not None:  # second call: we are tracing
-                compiled_function, ext_cache = jit.get_compiled_function(torch_function, args)  # increases counter
-                if on_call_skipped:
-                    on_call_skipped(ext_cache)
-                return compiled_function.apply(*args)  # this adds the compiled function to TorchScript. The function must not call any torch functions while being traced lest they be double-executed later.
-            else:  # first call: record this function
-                output = torch_function.apply(*args)
-                ext_cache = get_external_cache() if get_external_cache else None
-                jit.record_autograd_function_call(torch_function, args, output, ext_cache)
-                return output
-
-        torch_function = construct_torch_custom_function(f, None, None, gradient, is_f_traced=False, backend=self)
-        return select_jit
-
-    def transpose(self, tensor, axes):
-        return tensor.permute(axes)
-
-    def equal(self, x, y):
-        x, y = self.auto_cast(x, y)
-        return x == y
-
-    def random_uniform(self, shape, low, high, dtype: Union[DType, None]):
-        dtype = dtype or self.float_type
-        if dtype.kind == float:
-            return low + (high - low) * torch.rand(size=shape, dtype=to_torch_dtype(dtype), device=self.get_default_device().ref)
-        elif dtype.kind == complex:
-            real = low.real + (high.real - low.real) * torch.rand(size=shape, dtype=to_torch_dtype(DType(float, dtype.precision)), device=self.get_default_device().ref)
-            imag = low.imag + (high.imag - low.imag) * torch.rand(size=shape, dtype=to_torch_dtype(DType(float, dtype.precision)), device=self.get_default_device().ref)
-            return real + 1j * imag
-        elif dtype.kind == int:
-            return torch.randint(low, high, shape, dtype=to_torch_dtype(dtype))
-        else:
-            raise ValueError(dtype)
-
-    def random_normal(self, shape, dtype: DType):
-        return torch.randn(size=shape, dtype=to_torch_dtype(dtype or self.float_type), device=self.get_default_device().ref)
-
-    def stack(self, values, axis=0):
-        values = [self.as_tensor(v) for v in values]
-        return torch.stack(values, dim=axis)
-
-    def concat(self, values, axis):
-        values = [self.as_tensor(v) for v in values]
-        return torch.cat(values, dim=axis)
-
-    def pad(self, value, pad_width, mode='constant', constant_values=0):
-        """
-        pad tensor using mode
-
-        Args:
-          value(torch.Tensor): values
-          pad_width(iterable): left, right, upper, lower
-          mode(str, optional, optional): type of padding to be applied, defaults to 'constant'
-          constant_values(int, optional, optional): value to pad, defaults to 0
-
-        Returns:
-          torch.Tensor: padded tensor
-        """
-        mode = {'constant': 'constant', 'reflect': 'reflect', 'boundary': 'replicate', 'periodic': 'circular'}.get(mode, None)
-        if not mode:
-            return NotImplemented
-        # for PyTorch, we have to reshape value such that the outer 2 dimensions are not padded.
-        ndims = self.ndims(value)
-        no_pad_dims = [i for i in range(ndims) if pad_width[i] == (0, 0)]
-        pad_dims = [i for i in range(ndims) if pad_width[i] != (0, 0)]
-        if not pad_dims:
-            return value
-        if len(pad_dims) > 3:
-            return NotImplemented
-        value = torch.permute(value, no_pad_dims + pad_dims)
-        if len(no_pad_dims) == 0:
-            value = torch.unsqueeze(torch.unsqueeze(value, 0), 0)
-            undo_transform = lambda x: torch.squeeze(torch.squeeze(x, 0), 0)
-        elif len(no_pad_dims) == 1:
-            value = torch.unsqueeze(value, 0)
-            undo_transform = lambda x: torch.squeeze(x, 0)
-        elif len(no_pad_dims) == 2:
-            undo_transform = lambda x: x
-        else:
-            old_shape = value.shape
-            value = self.reshape(value, (1, np.prod([value.shape[i] for i in range(len(no_pad_dims))]), *value.shape[len(no_pad_dims):]))
-            undo_transform = lambda x: x.view(*[old_shape[i] for i in range(len(no_pad_dims))], *x.shape[2:])
-        pad_width_reordered = [pad_width[i] for i in pad_dims]
-        pad_width_spatial = [item for sublist in reversed(pad_width_reordered) for item in sublist]  # flatten
-        try:
-            constant_values = self.dtype(value).kind(constant_values)
-            result = torchf.pad(value, pad_width_spatial, mode, value=constant_values)  # supports 3D to 5D (batch, channel, 1D to 3D)
-        except RuntimeError as err:
-            warnings.warn(f"PyTorch error {err}", RuntimeWarning)
-            return NotImplemented
-        result = undo_transform(result)
-        inv_perm = tuple(np.argsort(no_pad_dims + pad_dims))
-        result = torch.permute(result, inv_perm)
-        return result
-
-    def grid_sample(self, grid, coordinates, extrapolation: str):
-        assert extrapolation in ('undefined', 'zeros', 'boundary', 'periodic', 'symmetric', 'reflect'), extrapolation
-        if get_functional_derivative_order() > 1:
-            return NotImplemented  # PyTorch's grid_sample operator does not define higher-order derivatives
-        extrapolation = {'undefined': 'zeros', 'zeros': 'zeros', 'boundary': 'border', 'reflect': 'reflection'}.get(extrapolation, None)
-        if extrapolation is None:
-            return NotImplemented
-        grid = channels_first(self.as_tensor(grid))
-        coordinates = self.as_tensor(coordinates)
-        if coordinates.shape[0] != grid.shape[0]:  # repeating yields wrong result
-            return NotImplemented
-        if coordinates.ndim != grid.ndim or coordinates.ndim not in (4, 5):
-            return NotImplemented  # torchf.grid_sample cannot handle this case
-        if coordinates.dtype.is_floating_point and not grid.dtype.is_complex and not grid.dtype.is_floating_point:
-            grid = self.to_float(grid)
-        resolution = torch.tensor(self.staticshape(grid)[2:], dtype=coordinates.dtype, device=coordinates.device)
-        coordinates = 2 * coordinates / (resolution - 1) - 1
-        coordinates = torch.flip(coordinates, dims=[-1])
-        batch_size = combined_dim(coordinates.shape[0], grid.shape[0])
-        coordinates = coordinates.repeat(batch_size, *[1] * (len(coordinates.shape-1))) if coordinates.shape[0] < batch_size else coordinates
-        grid = grid.repeat(batch_size, *[1] * (len(grid.shape)-1)) if grid.shape[0] < batch_size else grid
-        result = torchf.grid_sample(grid, coordinates, mode='bilinear', padding_mode=extrapolation, align_corners=True)  # can cause segmentation violation if NaN or inf are present
-        result = channels_last(result)
-        return result
-
-    def reshape(self, value, shape):
-        value = self.as_tensor(value)
-        if value.is_contiguous():
-            return value.view(*shape)
-        else:
-            return torch.reshape(value, shape)
-
-    def sum(self, value, axis=None, keepdims=False):
-        if axis is None:
-            axis = tuple(range(len(value.shape)))
-        if axis == () or axis == []:
-            return value
-        return torch.sum(value, dim=axis, keepdim=keepdims)
-
-    def prod(self, value, axis=None):
-        if isinstance(axis, (tuple, list)):
-            for dim in reversed(sorted(axis)):
-                value = torch.prod(value, dim=dim)
-            return value
-        return torch.prod(value, dim=axis)
-
-    def any(self, boolean_tensor, axis=None, keepdims=False):
-        boolean_tensor = self.as_tensor(boolean_tensor, convert_external=True)
-        if self.dtype(boolean_tensor).kind != bool:
-            boolean_tensor = boolean_tensor != 0
-        if axis is None:
-            return torch.any(boolean_tensor)
-        else:
-            axes = axis if isinstance(axis, (tuple, list)) else [axis]
-            for axis in reversed(sorted(axes)):
-                boolean_tensor = torch.any(boolean_tensor, dim=axis, keepdim=keepdims)
-            return boolean_tensor
-
-    def all(self, boolean_tensor, axis=None, keepdims=False):
-        boolean_tensor = self.as_tensor(boolean_tensor, convert_external=True)
-        if self.dtype(boolean_tensor).kind != bool:
-            boolean_tensor = boolean_tensor != 0
-        if axis is None:
-            return torch.all(boolean_tensor)
-        else:
-            axes = axis if isinstance(axis, (tuple, list)) else [axis]
-            for axis in reversed(sorted(axes)):
-                boolean_tensor = torch.all(boolean_tensor, dim=axis, keepdim=keepdims)
-            return boolean_tensor
-
-    def quantile(self, x, quantiles):
-        x = self.to_float(x)
-        result = torch.quantile(x, quantiles, dim=-1)
-        return result
-
-    def divide_no_nan(self, x, y):
-        x, y = self.auto_cast(x, y)
-        return divide_no_nan(x, y)
-
-    def where(self, condition, x=None, y=None):
-        condition = self.as_tensor(condition).bool()
-        x, y = self.auto_cast(x, y)
-        x = self.as_tensor(x)
-        y = self.as_tensor(y)
-        return torch.where(condition, x, y)
-
-    def mean(self, value, axis=None, keepdims=False):
-        if self.dtype(value).kind not in (float, complex):
-            value = self.to_float(value)
-        return torch.mean(value, dim=axis, keepdim=keepdims)
-
-    def range(self, start, limit=None, delta=1, dtype: DType = DType(int, 32)):
-        if limit is None:
-            start, limit = 0, start
-        return torch.arange(start, limit, delta, dtype=to_torch_dtype(dtype))
-
-    def zeros(self, shape, dtype=None):
-        return torch.zeros(shape, dtype=to_torch_dtype(dtype or self.float_type), device=self.get_default_device().ref)
-
-    def zeros_like(self, tensor):
-        return torch.zeros_like(self.as_tensor(tensor), device=self.get_default_device().ref)
-
-    def ones(self, shape, dtype: DType = None):
-        return torch.ones(shape, dtype=to_torch_dtype(dtype or self.float_type), device=self.get_default_device().ref)
-
-    def ones_like(self, tensor):
-        return torch.ones_like(self.as_tensor(tensor), device=self.get_default_device().ref)
-
-    def meshgrid(self, *coordinates):
-        coordinates = [self.as_tensor(c) for c in coordinates]
-        from packaging import version
-        if version.parse(torch.__version__) >= version.parse('1.10'):
-            return torch.meshgrid(*coordinates, indexing='ij')
-        else:
-            return torch.meshgrid(*coordinates)
-
-    def linspace(self, start, stop, number):
-        if self.is_tensor(stop, only_native=True) or self.is_tensor(start, only_native=True):
-            unit = torch.linspace(0, 1, number, dtype=to_torch_dtype(self.float_type), device=self.get_default_device().ref)
-            return unit * (stop - start) + start
-        else:
-            return torch.linspace(start, stop, number, dtype=to_torch_dtype(self.float_type), device=self.get_default_device().ref)
-
-    def tensordot(self, a, a_axes: Union[tuple, list], b, b_axes: Union[tuple, list]):
-        a, b = self.auto_cast(a, b)
-        return torch.tensordot(a, b, (a_axes, b_axes))
-
-    def mul_matrix_batched_vector(self, A, b):
-        A, b = self.auto_cast(A, b)
-        if isinstance(A, torch.Tensor) and (A.is_sparse or A.is_sparse_csr):
-            result = torch.sparse.mm(A, torch.transpose(b, 0, 1))
-            return torch.transpose(result, 0, 1)
-        else:
-            return torch.transpose(torch.matmul(A, torch.transpose(b, -1, -2)), -1, -2)
-
-    def get_diagonal(self, matrices, offset=0):
-        return torch.transpose(torch.diagonal(matrices, offset=offset, dim1=1, dim2=2), 1, 2)
-
-    def cumsum(self, x, axis: int):
-        return torch.cumsum(x, dim=axis)
-
-    def while_loop(self, loop: Callable, values: tuple, max_iter: Union[int, Tuple[int, ...], List[int]]):
-        tracing = torch._C._get_tracing_state() is not None
-        if not tracing:
-            return Backend.while_loop(self, loop, values, max_iter)
-        # --- We are tracing ---
-        warnings.warn("PyTorch while_loop always iterates until max_iter. Please put a while loop into a torch.ScriptFunction instead.", RuntimeWarning)
-        values = self.stop_gradient_tree(values)
-        if isinstance(max_iter, (tuple, list)):
-            trj = [values] if 0 in max_iter else []
-            for i in range(1, max(max_iter) + 1):
-                values = loop(*values)
-                if i in max_iter:
-                    trj.append(self.copy_leaves(values, only_mutable=True))
-            trj.extend([trj[-1]] * (len(max_iter) - len(trj)))  # fill trj with final values
-            return self.stop_gradient_tree(self.stack_leaves(trj))
-        else:
-            for i in range(1, max_iter + 1):
-                values = loop(*values)
-            return self.stop_gradient_tree(values)
-        # if isinstance(loop, torch.ScriptFunction):
-        #     jit_loop = loop
-        #     i = 0
-        #     while torch.any(values[0]):
-        #         values = jit_loop(*values)
-        #         i += 1
-        #         if max_iter is not None and i >= max_iter:
-        #             break
-        #     return values
-            # def trace_later():
-            #     jit_loop = torch.jit.trace(loop, check_trace=False)
-            #     @torch.jit.script
-            #     def loop_script(values: Tuple[torch.Tensor], loop_script: Callable):
-            #         while torch.any(values[0]):
-            #             values = loop_script(*values)
-            #         return values
-            # CURRENT_JIT_CALLS[-1].post_trace.append(trace_later)
-
-    def max(self, x, axis=None, keepdims=False):
-        if axis is None:
-            result = torch.max(x)
-            if keepdims:
-                result = self.expand_dims(result, axis=0, number=self.ndims(x))
-            return result
-        elif isinstance(axis, (tuple, list)):
-            for dim in reversed(sorted(axis)):
-                x, _ = torch.max(x, dim=dim, keepdim=keepdims)
-            return x
-        else:
-            return torch.max(x, dim=axis, keepdim=keepdims)[0]
-
-    def min(self, x, axis=None, keepdims=False):
-        if axis is None:
-            result = torch.min(x)
-            if keepdims:
-                result = self.expand_dims(result, axis=0, number=self.ndims(x))
-            return result
-        elif isinstance(axis, (tuple, list)):
-            for dim in reversed(sorted(axis)):
-                x, _ = torch.min(x, dim=dim, keepdim=keepdims)
-            return x
-        else:
-            return torch.min(x, dim=axis, keepdim=keepdims)[0]
-
-    def maximum(self, a, b):
-        a_ = self.as_tensor(a)
-        b_ = self.as_tensor(b)
-        return torch.max(a_, other=b_)
-
-    def minimum(self, a, b):
-        a_ = self.as_tensor(a)
-        b_ = self.as_tensor(b)
-        return torch.min(a_, other=b_)
-
-    def clip(self, x, minimum, maximum):
-        if isinstance(minimum, numbers.Number) and isinstance(maximum, numbers.Number):
-            return torch.clamp(self.as_tensor(x), minimum, maximum)
-        else:
-            return self.maximum(minimum, self.minimum(x, maximum))
-
-    def conv(self, value, kernel, zero_padding=True):
-        value = self.as_tensor(value)
-        kernel = self.as_tensor(kernel)
-        value, kernel = self.auto_cast(value, kernel)
-        if self.dtype(value).kind in (bool, int):
-            value = self.to_float(value)
-            kernel = self.to_float(kernel)
-        if zero_padding:
-            if all(s % 2 == 1 for s in kernel.shape[3:]):
-                padding = [s // 2 for s in kernel.shape[3:]]
-            else:
-                padding = 0
-                value_padding = sum([[s // 2, (s - 1) // 2] for s in kernel.shape[3:]], [])
-                value = torchf.pad(value, value_padding)
-        else:
-            padding = 0
-        convf = {3: torchf.conv1d, 4: torchf.conv2d, 5: torchf.conv3d}[len(value.shape)]
-        if kernel.shape[0] == 1:
-            result = convf(value, kernel[0, ...], padding=padding)
-        else:
-            result = []
-            for b in range(kernel.shape[0]):
-                result.append(convf(value[b:b+1, ...], kernel[b, ...], padding=padding))
-            result = torch.cat(result, 0)
-        return result
-
-    def expand_dims(self, a, axis=0, number=1):
-        for _ in range(number):
-            a = torch.unsqueeze(a, dim=axis)
-        return a
-
-    def shape(self, tensor):
-        if self.is_tensor(tensor, only_native=True):
-            return tensor.shape
-        else:
-            return NUMPY.shape(tensor)
-
-    def staticshape(self, tensor):
-        if isinstance(tensor, torch.nn.Module):
-            return ()
-        if self.is_tensor(tensor, only_native=True):
-            return tuple([int(s) for s in tensor.shape])
-        else:
-            return NUMPY.staticshape(tensor)
-
-    def gather(self, values, indices, axis: int):
-        slices = [indices if i == axis else slice(None) for i in range(self.ndims(values))]
-        return values[tuple(slices)]
-
-    def batched_gather_nd(self, values, indices):
-        values = self.as_tensor(values)
-        indices = self.as_tensor(indices).long()
-        batch_size = combined_dim(values.shape[0], indices.shape[0])
-        result = []
-        for b in range(batch_size):
-            b_indices = self.unstack(indices[min(b, indices.shape[0] - 1)], -1)
-            result.append(values[(min(b, values.shape[0] - 1),) + b_indices])
-        return self.stack(result, axis=0)
-
-    def unstack(self, tensor, axis=0, keepdims=False):
-        unstacked = torch.unbind(tensor, dim=axis)
-        if keepdims:
-            unstacked = [self.expand_dims(c, axis=axis) for c in unstacked]
-        return unstacked
-
-    def std(self, x, axis=None, keepdims=False):
-        if self.dtype(x).kind not in (float, complex):
-            x = self.to_float(x)
-        return torch.std(x, dim=axis, keepdim=keepdims, unbiased=False)
-
-    def boolean_mask(self, x, mask, axis=0):
-        x = self.as_tensor(x)
-        mask = self.as_tensor(mask)
-        result = []
-        for selected, data in zip(mask, self.unstack(x, axis)):
-            if selected:
-                result.append(data)
-        return self.stack(result, axis)
-        # return torch.masked_select(x_, mask_)
-
-    def scatter(self, base_grid, indices, values, mode: str):
-        base_grid, values = self.auto_cast(base_grid, values)
-        indices = self.as_tensor(indices)
-        batch_size = combined_dim(combined_dim(indices.shape[0], values.shape[0]), base_grid.shape[0])
-        scatter = torch.scatter_add if mode == 'add' else torch.scatter
-        if indices.shape[0] < batch_size:
-            indices = indices.repeat([batch_size] + [1] * (len(indices.shape)-1))
-        if values.shape[0] < batch_size or values.shape[1] == 1:
-            values = values.repeat([batch_size // values.shape[0], indices.shape[1] // indices.shape[1]] + [1] * (len(values.shape)-2))
-        if len(base_grid.shape) > 3:
-            resolution = base_grid.shape[1:-1]
-            ravel = [1]
-            for i in range(1, len(resolution)):
-                ravel.insert(0, ravel[0] * resolution[-i])
-            ravel = self.to_int64(self.as_tensor(ravel, True))
-            indices = torch.sum(indices * ravel, dim=-1, keepdim=True)
-        base_grid_flat = torch.reshape(base_grid, [base_grid.shape[0], -1, base_grid.shape[-1]])
-        indices = indices.long().repeat([1, 1, values.shape[-1]])
-        result = scatter(base_grid_flat, dim=1, index=indices, src=values)
-        return torch.reshape(result, base_grid.shape)
-
-    def arctan2(self, y, x):
-        y, x = self.auto_cast(y, x)
-        return torch.arctan2(y, x)
-
-    def fft(self, x, axes: Union[tuple, list]):
-        if not x.is_complex():
-            x = self.to_complex(x)
-        for i in axes:
-            x = torch.fft.fft(x, dim=i)
-        return x
-
-    def ifft(self, k, axes: Union[tuple, list]):
-        if not k.is_complex():
-            k = self.to_complex(k)
-        for i in axes:
-            k = torch.fft.ifft(k, dim=i)
-        return k
-
-    def imag(self, x):
-        dtype = self.dtype(x)
-        if dtype.kind == complex:
-            return torch.imag(x)
-        else:
-            return self.zeros(x.shape, DType(float, dtype.precision))
-
-    def real(self, x):
-        if self.dtype(x).kind == complex:
-            return torch.real(x)
-        else:
-            return x
-
-    def conj(self, x):
-        if self.dtype(x).kind == complex:
-            return torch.conj(x)
-        else:
-            return x
-
-    def cast(self, x, dtype: DType):
-        if isinstance(x, (numbers.Number, bool)):
-            return dtype.kind(x)  # Creating a Tensor here would raise warnings during tracing.
-        if not self.is_tensor(x, only_native=True):
-            x = self.as_tensor(x)
-        if self.dtype(x) == dtype:
-            return x
-        else:
-            return x.to(to_torch_dtype(dtype))
-
-    def dtype(self, array) -> DType:
-        if self.is_tensor(array, only_native=True):
-            return from_torch_dtype(array.dtype)
-        else:
-            return NUMPY.dtype(array)
-
-    def tile(self, value, multiples):
-        if isinstance(multiples, np.ndarray):
-            multiples = multiples.tolist()
-        return self.as_tensor(value).repeat(multiples)
-
-    def repeat(self, x, repeats, axis: int):
-        if isinstance(repeats, (np.ndarray, tuple, list)):
-            repeats = self.as_tensor(repeats)
-        return torch.repeat_interleave(self.as_tensor(x), repeats, axis)
-
-    def sparse_coo_tensor(self, indices, values, shape):
-        indices = self.to_int64(indices)
-        indices = self.transpose(indices, [1, 0])
-        values = self.to_float(values)
-
-        @torch.jit.script  # the output of torch.sparse_coo_tensor is considered constant
-        def sparse_coo_tensor(values, indices, cols: int, rows: int, dtype: torch.dtype) -> torch.sparse.Tensor:
-            size = torch.Size([cols, rows])
-            return torch.sparse_coo_tensor(indices, values, size=size, dtype=dtype)
-
-        return sparse_coo_tensor(values, indices, shape[0], shape[1], to_torch_dtype(self.float_type))
-
-    def csr_matrix(self, column_indices, row_pointers, values, shape: Tuple[int, int]):
-        row_pointers = self.as_tensor(row_pointers)
-        column_indices = self.as_tensor(column_indices)
-        return torch.sparse_csr_tensor(row_pointers, column_indices, values, shape, device=values.device)
-
-    # def csr_matrix_batched(self, column_indices, row_pointers, values, shape: Tuple[int, int]):
-    #     batch_size, nnz, channels = values.shape
-    #     if version.parse(torch.__version__) >= version.parse('1.13.0'):
-    #         return torch.sparse_csr_tensor(row_pointers, column_indices, values, (batch_size, *shape, channels), device=values.device)
-    #     else:
-    #         warnings.warn("PyTorch >= 1.13 is required for batched CSR matrices. Visit https://pytorch.org/ to download the latest version.", RuntimeWarning)
-    #         raise NotImplementedError
-    #         # matrices = []
-    #         # for b in range(batch_size):
-    #         #     if values.shape[-1] == 1:
-    #         #         b_matrix = torch.sparse_csr_tensor(row_pointers[b], column_indices[b], values[b, :, 0], shape, device=values.device)
-    #         #     else:
-    #         #         raise NotImplementedError
-    #         #     matrices.append(b_matrix)
-    #         # return matrices
-
-    def csc_matrix(self, column_pointers, row_indices, values, shape: tuple):
-        batch_size, nnz, channels = values.shape
-        if version.parse(torch.__version__) >= version.parse('1.13.0'):
-            return torch.sparse_csc_tensor(column_pointers, row_indices, values, (batch_size, *shape, channels), device=values.device)
-        else:
-            warnings.warn("PyTorch >= 1.13 is required for batched CSC matrices. Visit https://pytorch.org/ to download the latest version.", RuntimeWarning)
-            raise NotImplementedError
-            # batch_size, nnz, channels = values.shape
-            # if batch_size == channels == 1:
-            #     return scipy.sparse.csc_matrix((values[0, :, 0], row_indices[0], column_pointers[0]), shape=shape)
-            # matrices = []
-            # for b in range(batch_size):
-            #     if values.shape[-1] == 1:
-            #         b_matrix = scipy.sparse.csc_matrix((values[b, :, 0], row_indices[b], column_pointers[b]), shape=shape)
-            #     else:
-            #         raise NotImplementedError
-            #     matrices.append(b_matrix)
-            # return matrices
-
-    def mul_csr_dense(self, column_indices, row_pointers, values, shape: tuple, dense):
-        values, dense = self.auto_cast(values, dense, bool_to_int=True, int_to_float=True)
-        batch_size, nnz, channels = values.shape
-        result = []
-        for b in range(batch_size):
-            b_result = []
-            for c in range(channels):
-                matrix = torch.sparse_csr_tensor(row_pointers[b], column_indices[b], values[b, :, c], shape, device=values.device)
-                b_result.append(torch.sparse.mm(matrix, self.as_tensor(dense[b, :, c, :])))
-            result.append(torch.stack(b_result))
-        return torch.stack(result)
-        # if channel_count == 1:
-        #     matrix = torch.sparse_csr_tensor(row_pointers, column_indices, values[:, :, 0], (batch_size, *shape), device=values.device)
-        #     matrix.matmul(self.as_tensor(dense[:, 0, :, :]))
-        #     # torch.sparse.mm(matrix, self.as_tensor(dense[:, 0, :, :]))
-        #     raise NotImplementedError
-        # else:
-        #     # tile
-        #     raise NotImplementedError
-
-    def conjugate_gradient(self, lin, y, x0, tol_sq, max_iter) -> SolveResult:
-        if callable(lin) or len(max_iter) > 1:
-            assert self.is_available(y), "Tracing conjugate_gradient with linear operator is not yet supported."
-            return Backend.conjugate_gradient(self, lin, y, x0, tol_sq, max_iter)
-        assert isinstance(lin, torch.Tensor), "Batched matrices are not yet supported"
-        batch_size = self.staticshape(y)[0]
-        y = self.to_float(y)
-        x0 = self.copy(self.to_float(x0))
-        tol_sq = self.as_tensor(tol_sq)
-        max_iter = self.as_tensor(max_iter[0])
-        x, residual, iterations, function_evaluations, converged, diverged = torch_sparse_cg(lin, y, x0, tol_sq, max_iter)
-        return SolveResult(f"Φ-Flow CG ({'PyTorch*' if self.is_available(y) else 'TorchScript'})", x, residual, iterations, function_evaluations, converged, diverged, [""] * batch_size)
-
-    def conjugate_gradient_adaptive(self, lin, y, x0, tol_sq, max_iter) -> SolveResult:
-        if callable(lin) or len(max_iter) > 1:
-            assert self.is_available(y), "Tracing conjugate_gradient with linear operator is not yet supported."
-            return Backend.conjugate_gradient_adaptive(self, lin, y, x0, tol_sq, max_iter)
-        assert isinstance(lin, torch.Tensor), "Batched matrices are not yet supported"
-        batch_size = self.staticshape(y)[0]
-        y = self.to_float(y)
-        x0 = self.copy(self.to_float(x0))
-        tol_sq = self.as_tensor(tol_sq)
-        max_iter = self.as_tensor(max_iter[0])
-        x, residual, iterations, function_evaluations, converged, diverged = torch_sparse_cg_adaptive(lin, y, x0, tol_sq, max_iter)
-        return SolveResult(f"Φ-Flow CG ({'PyTorch*' if self.is_available(y) else 'TorchScript'})", x, residual, iterations, function_evaluations, converged, diverged, [""] * batch_size)
-
-    def bi_conjugate_gradient(self, lin, y, x0, tol_sq, max_iter, poly_order=2) -> SolveResult:
-        if not self.is_available(y):
-            warnings.warn("Bi-CG is not optimized for PyTorch and will always run the maximum number of iterations.", RuntimeWarning)
-        return Backend.bi_conjugate_gradient(self, lin, y, x0, tol_sq, max_iter, poly_order)
-
-    def matrix_solve_least_squares(self, matrix: TensorType, rhs: TensorType) -> Tuple[TensorType, TensorType, TensorType, TensorType]:
-        assert version.parse(torch.__version__) >= version.parse('1.9.0'), "least squares requires PyTorch >= 1.9.0"
-        matrix, rhs = self.auto_cast(matrix, rhs)
-        solution, residuals, rank, singular_values = torch.linalg.lstsq(matrix, rhs)
-        return solution, residuals, rank, singular_values
-
-    def solve_triangular_dense(self, matrix, rhs, lower: bool, unit_diagonal: bool):
-        matrix, rhs = self.auto_cast(matrix, rhs, int_to_float=True, bool_to_int=True)
-        rhs = self.expand_dims(rhs, -1)
-        x = torch.linalg.solve_triangular(matrix, rhs, upper=not lower, unitriangular=unit_diagonal)
-        return x[..., 0]
-
-    def _prepare_graph_inputs(self, args: tuple, wrt: Union[tuple, list]):
-        args = [self.as_tensor(arg, True) if i in wrt else arg for i, arg in enumerate(args)]
-        args = [self.to_float(arg) if self.dtype(arg).kind == int else arg for arg in args]
-        for i, arg in enumerate(args):
-            if self.is_tensor(arg, True) and arg.requires_grad and not arg.is_leaf:
-                arg = torch.clone(arg).detach()
-                arg.requires_grad = i in wrt
-                args[i] = arg
-            elif i in wrt:
-                arg = self.as_tensor(arg, True)
-                arg = arg.detach()  # returns a new tensor in any case
-                arg.requires_grad = True
-                args[i] = arg
-        wrt_args = [arg for i, arg in enumerate(args) if i in wrt]
-        for t in wrt_args:
-            assert t.requires_grad
-        return args, wrt_args
-
-    def jacobian(self, f, wrt: Union[tuple, list], get_output: bool, is_f_scalar: bool):
-        @wraps(f)
-        def eval_grad(*args):
-            args, wrt_args = self._prepare_graph_inputs(args, wrt)
-            loss, output = f(*args)
-            if np.prod(self.staticshape(loss)) == 1:
-                grads = torch.autograd.grad(loss, wrt_args)  # grad() cannot be called during jit trace
-            else:
-                raise NotImplementedError()
-                grads = torch.autograd.grad(loss, wrt_args, retain_graph=True)
-            return (*output, *grads) if get_output else grads
-        return eval_grad
-
-    def hessian(self, f: Callable, wrt: Union[tuple, list], get_output: bool, get_gradient: bool):
-        # if not get_output and not get_gradient:
-        # @wraps(f)
-        # def eval_hessian(*args):
-        #     batch_size = args[0].shape[0]
-        #     for arg in args:
-        #         assert arg.shape[0] == batch_size, f"All arguments must have a matching batch dimension as their first dimension. Got shapes {[arg.shape for arg in args]}"
-        #
-        #     def f_only_wrt_inputs(*wrt_args_only, reduce_batch=False):
-        #         all_args = list(args)
-        #         for i, arg in zip(wrt, wrt_args_only):
-        #             all_args[i] = arg
-        #         output = f(*all_args)
-        #         loss, aux = (output[0], output[1:]) if isinstance(output, (tuple, list)) else (output, None)
-        #         if reduce_batch:
-        #             if loss.ndim > 0:
-        #                 loss = loss.sum()
-        #         else:
-        #             assert np.prod(loss.shape) == 1, f"Loss (first output of f) must be scalar but has shape {loss.shape}"
-        #             loss = loss.sum()
-        #         return loss
-        #
-        #     wrt_args = tuple([self.as_tensor(arg, True) for i, arg in enumerate(args) if i in wrt])
-        #     result = ()
-        #     if get_output:
-        #         result += f(*args),
-        #     if get_gradient:
-        #         result += torch.autograd.functional.jacobian(lambda *a: f_only_wrt_inputs(*a, reduce_batch=True), wrt_args),
-        #     if hasattr(torch, 'vmap'):
-        #         # single_hessian_f = lambda *args: torch.autograd.functional.hessian(f_only_wrt_inputs, args)
-        #         # multi_hessian_f = torch.vmap
-        #         raise NotImplementedError()
-        #     else:
-        #         hessian = tuple([tuple([[] for _1 in range(len(wrt))]) for _2 in range(len(wrt))])  # n x n matrix of lists
-        #         for b in range(batch_size):
-        #             h = torch.autograd.functional.hessian(f_only_wrt_inputs, tuple([arg[b:b + 1] for arg in wrt_args]))
-        #             for i in range(len(wrt)):
-        #                 for j in range(len(wrt)):
-        #                     fake_batch_dim = args[i].ndim
-        #                     hessian[i][j].append(torch.squeeze(torch.squeeze(h[i][j], fake_batch_dim), 0))
-        #         hessian = [[torch.stack(hessian[i][j]) for j in range(len(wrt))] for i in range(len(wrt))]
-        #         # hessian = torch.stack([torch.autograd.functional.hessian(f_only_wrt_inputs, tuple([arg[b:b+1] for arg in wrt_args])) for b in range(batch_size)])  # manual batch loop
-        #     result += hessian,
-        #     return result
-        # else:
-        @wraps(f)
-        def eval_hessian(*args):
-            args, wrt_args = self._prepare_graph_inputs(args, wrt)
-            output = f(*args)
-            loss, aux = (output[0], output[1:]) if isinstance(output, (tuple, list)) else (output, None)
-            scalar_loss = loss.sum() if loss.ndim > 0 else loss
-            grads = torch.autograd.grad(scalar_loss, wrt_args, create_graph=True, retain_graph=True)  # grad() cannot be called during jit trace
-            hessian = []
-            for grad in grads:
-                if not grad.requires_grad:
-                    raise NotImplementedError("Linear dependency detected. Hessian = 0.")
-                hessian.append([[] for _ in grads])
-                for lin_index in range(int(np.prod(grad.shape[1:]))):
-                    multi_index = np.unravel_index(lin_index, grad.shape[1:])
-                    h = torch.autograd.grad(grad[(slice(None),) + multi_index].sum(), wrt_args, allow_unused=True, retain_graph=True)  # grad of every entry in grad
-                    # Warning: This returns incorrect values for certain inputs. Hessian of x^2 returns 0 at x=0 but is correct everywhere else.
-                    # ToDo torch.autograd.functional.hessian does not seem to have this issue. Wait for torch.vmap(), then conditionally switch.
-                    for i, h_ in enumerate(h):
-                        hessian[-1][i].append(h_)
-            for col in hessian:
-                for i, row in enumerate(col):
-                    if len(row) > 1:
-                        col[i] = torch.stack(row, dim=1)
-                    else:
-                        col[i] = row[0]
-                    h_shape = tuple(grads[i].shape) + tuple(grads[i].shape[1:])
-                    col[i] = torch.reshape(col[i], h_shape)
-
-            result = ()
-            if get_output:
-                loss = loss.detach()
-                if aux is not None:
-                    aux = [aux_.detach() if isinstance(aux_, torch.Tensor) else aux_ for aux_ in aux]
-                    result += (loss, *aux),
-                else:
-                    result += loss,
-            if get_gradient:
-                result += tuple([g.detach() for g in grads]),
-            result += hessian,
-            return result
-
-        return eval_hessian
-
-    def jit_compile_grad(self, f, wrt: Union[tuple, list], get_output: bool, is_f_scalar: bool):
-        jit = self.jit_compile(f)
-        return self.jacobian(jit, wrt, get_output, is_f_scalar)
-
-    def jit_compile_hessian(self, f, wrt: Union[tuple, list], get_output: bool, get_gradient: bool):
-        jit = self.jit_compile(f)
-        return self.hessian(jit, wrt, get_output, get_gradient)
-
-    def stop_gradient(self, value):
-        return value.detach()
-
-
-def channels_first(x):
-    return x.permute(*((0, -1) + tuple(range(1, len(x.shape) - 1))))
-
-
-def channels_last(x):
-    return x.permute((0,) + tuple(range(2, len(x.shape))) + (1,))
-
-
-class JITFunction:
-    """
-    PyTorch Tracing Procedure:
-    1. Call function non-tracing, record all called nn.Modules and autograd.Function forward calls with their args
-    2. Compile autograd.Functions forward and backward passes
-    3. Add nn.Modules to JIT Module
-    4. Trace JIT Module
-
-    Nested jit calls are ignored.
-
-    See PyTorch_Jit.md
-    """
-
-    def __init__(self, backend: TorchBackend, f):
-        self.backend = backend
-        self.f = f
-        self.traced = None
-        self.autograd_function_calls = []  # (TorchCustomFunction, args, output, ext_cache)
-        self.compiled_functions = []  # (TorchCustomFunction, TorchCustomFunction)
-        self.autograd_function_call_counts = 0
-        self.called_modules: Set[torch.nn.Module] = set()
-
-    def __call__(self, *args, **kwargs):
-        if kwargs:
-            raise NotImplementedError("kwargs not supported for traced function")
-        if CURRENT_JIT_CALLS:
-            warnings.warn(f"PyTorch does not support nested tracing. The inner JIT of {self.f.__name__} will be ignored.", RuntimeWarning)
-            return self.f(*args)
-        args = self.backend.recursive_as_tensor(args)
-        if self.traced is None:
-            self_jit = self
-            CURRENT_JIT_CALLS.append(self)
-            self.f(*args)  # records all autograd.Function / nn.Module calls with their args -> self.autograd_function_calls, self.called_modules
-            for i, (rec_function, rec_args, rec_output, _ext_cache) in enumerate(self.autograd_function_calls):
-                self.compiled_functions.append((rec_function, rec_function.compile(rec_args, rec_output)))
-            assert self.autograd_function_call_counts == 0
-
-            class JitModule(torch.nn.Module):
-
-                def __init__(self):
-                    super().__init__()
-                    for submodule in self_jit.called_modules:
-                        self.add_module(str(f"{type(submodule).__name__}_{id(submodule)}"), submodule)
-
-                def forward(self, *args):
-                    PHI_LOGGER.debug(f"Tracing Pytorch jit module for {self_jit.f.__name__}")
-                    return self_jit.f(*args)
-
-            module = JitModule()
-            self.traced = torch.jit.trace(module, tuple(args), check_trace=False, strict=False)
-            assert self.autograd_function_call_counts == len(self.autograd_function_calls), "Not all custom-gradient functions were called during tracing! Nested custom gradients are not supported."
-            assert CURRENT_JIT_CALLS.pop(-1) == self
-        from phi.math.backend import choose_backend
-        return choose_backend(self).call(self.traced, *args, name=f"run jit-compiled '{self.f.__name__}'")
-
-    def record_autograd_function_call(self, function: torch.autograd.Function, args, output, ext_cache):
-        self.autograd_function_calls.append((function, args, output, ext_cache))
-
-    def get_compiled_function(self, function: torch.autograd.Function, args) -> Tuple[torch.autograd.Function, Any]:
-        assert torch._C._get_tracing_state() is not None
-        assert self.autograd_function_call_counts < len(self.autograd_function_calls), f"More custom-gradient functions were called during tracing!\nLast encountered: {function}"
-        assert len(self.autograd_function_calls) == len(self.compiled_functions)
-        original_function, compiled_function = self.compiled_functions[self.autograd_function_call_counts]
-        assert isinstance(compiled_function, torch.autograd.Function)
-        function, args, output, ext_cache = self.autograd_function_calls[self.autograd_function_call_counts]
-        self.autograd_function_call_counts += 1
-        return compiled_function, ext_cache
-
-    def __repr__(self):
-        return f"TorchScript[{self.f.__name__}]"
-
-
-CURRENT_JIT_CALLS: List[JITFunction] = []  # should contain no more than 1 element; PyTorch doesn't support nested tracing
-
-
-def register_module_call(module: torch.nn.Module):
-    if CURRENT_JIT_CALLS:
-        CURRENT_JIT_CALLS[-1].called_modules.add(module)
-
-
-def construct_torch_custom_function(f: Callable, jit_f: Optional[Callable], f_example_output, g: Callable, is_f_traced: bool, backend: TorchBackend):
-    jit_g = []
-
-    class TorchCustomFunction(torch.autograd.Function):
-        """ See PyTorch_Jit.md """
-
-        @staticmethod
-        def forward(ctx, *args, **kwargs):  # The result of this is used in the graph.
-            if torch._C._get_tracing_state():
-                PHI_LOGGER.debug(f"torch.jit.trace encountered forward pass of {f.__name__}. Returning cached output to avoid double execution.")
-                # jit_context = CURRENT_JIT_CALLS[-1]; jit_context.cached_output[torch_custom_function]
-                return f_example_output
-            y = (jit_f or f)(*args, **kwargs)
-            ctx.save_for_backward(*args, *y)
-            ctx.input_count = len(args)
-            return y
-
-        # @torch.jit.unused, @torch.jit.ignore(drop=True)  do not work here
-
-        @staticmethod
-        def backward(ctx, *grad_args):  # Breakpoints not supported here
-            x = ctx.saved_tensors[:ctx.input_count]
-            y = ctx.saved_tensors[ctx.input_count:]
-            if is_f_traced:
-                if not jit_g:
-                    # backward pass can return None but that's not allowed in JIT functions
-                    needs_input_grad = ctx.needs_input_grad
-                    none_indices = []  # jit function cannot return None but gradient returns None to indicate there is no gradient
-
-                    def filter_required_grads(*args):  # traced once
-                        grads = g(*args)
-                        filtered = [gv for gv, need in zip(grads, needs_input_grad) if need]
-                        none_indices.clear()
-                        none_indices.extend([i for i, g in enumerate(filtered) if g is None])
-                        filtered = [gv for gv in filtered if gv is not None]
-                        assert len(filtered) > 0, "Custom backward function must return at least one valid gradient."
-                        assert all([isinstance(gv, torch.Tensor) for gv in filtered]), [type(gv) for gv in grads]
-                        return filtered
-
-                    PHI_LOGGER.debug(f"Tracing backward pass of '{f.__name__}' which uses a custom gradient")
-                    needed_g = backend.jit_compile(filter_required_grads)
-                    # needed_g = torch.jit.trace(filter_required_grads, tuple([x, y, grad_args]), check_trace=False, strict=False)
-
-                    def g_(*args):  # called each time, not jitted
-                        needed = backend.as_registered.call(needed_g, *args, name=f"run jit-compiled custom backward '{g.__name__}'")
-                        assert isinstance(needed, (tuple, list))
-                        needed = list(needed)
-                        for i in none_indices:
-                            needed.insert(i, None)
-                        result = [(needed.pop(0) if need else None) for need in needs_input_grad]
-                        return result
-
-                    jit_g.append(g_)
-                output = jit_g[0](x, y, grad_args)
-            else:
-                output = g(x, y, grad_args)
-            result = output[0] if len(output) == 1 else (*output,)
-            return result
-
-        @staticmethod
-        def compile(args: tuple, output):
-            assert jit_f is None
-            PHI_LOGGER.debug(f"Tracing forward pass of '{f.__name__}' which uses a custom gradient")
-            jit_f_ = torch.jit.trace(f, args, strict=False, check_trace=False)
-            return construct_torch_custom_function(f, jit_f_, output, g, is_f_traced=True, backend=backend)
-
-        @property
-        def __name__(self):
-            return f"TorchCustomFunction-{'jit' if is_f_traced else 'non-jit'}[{f.__name__}]"
-
-        def __repr__(self):
-            return self.__name__
-
-    torch_custom_function = TorchCustomFunction()
-    return torch_custom_function
-
-
-def to_torch_dtype(dtype: DType):
-    return _TO_TORCH[dtype]
-
-
-def from_torch_dtype(torch_dtype):
-    if torch_dtype in _FROM_TORCH:
-        return _FROM_TORCH[torch_dtype]
-    else:
-        kind = {'i': int, 'b': bool, 'f': float, 'c': complex}[torch_dtype.kind]
-        return DType(kind, torch_dtype.itemsize * 8)
-
-
-_TO_TORCH = {
-    DType(float, 16): torch.float16,
-    DType(float, 32): torch.float32,
-    DType(float, 64): torch.float64,
-    DType(complex, 64): torch.complex64,
-    DType(complex, 128): torch.complex128,
-    DType(int, 8): torch.int8,
-    DType(int, 16): torch.int16,
-    DType(int, 32): torch.int32,
-    DType(int, 64): torch.int64,
-    DType(bool): torch.bool,
-}
-_FROM_TORCH = {np: dtype for dtype, np in _TO_TORCH.items()}
-
-
-@torch.jit._script_if_tracing
-def torch_sparse_cg(lin, y, x0, tolerance_sq, max_iter):
-    batch_size = y.shape[0]
-    x = x0
-    dx = residual = y - sparse_matmul(lin, x)
-    it_counter = torch.tensor(0, dtype=torch.int32, device=x.device)
-    iterations = torch.zeros([batch_size], dtype=torch.int32, device=x.device)
-    function_evaluations = torch.ones([batch_size], dtype=torch.int32, device=x.device)
-    residual_squared = rsq0 = torch.sum(residual ** 2, -1, keepdim=True)
-    diverged = torch.any(~torch.isfinite(x), dim=1)
-    converged = torch.all(residual_squared <= tolerance_sq, dim=1)
-    finished = converged | diverged | (iterations >= max_iter); not_finished_1 = (~finished).to(torch.int32)
-    while ~torch.all(finished):
-        it_counter += 1; iterations += not_finished_1
-        dy = sparse_matmul(lin, dx); function_evaluations += not_finished_1
-        dx_dy = torch.sum(dx * dy, dim=-1, keepdim=True)
-        step_size = divide_no_nan(residual_squared, dx_dy)
-        step_size *= torch.unsqueeze(not_finished_1.to(y.dtype), -1)  # this is not really necessary but ensures batch-independence
-        x += step_size * dx
-        if it_counter % 20 == 0:
-            residual = y - sparse_matmul(lin, x); function_evaluations += 1
-        else:
-            residual = residual - step_size * dy  # in-place subtraction affects convergence
-        residual_squared_old = residual_squared
-        residual_squared = torch.sum(residual ** 2, -1, keepdim=True)
-        dx = residual + divide_no_nan(residual_squared, residual_squared_old) * dx
-        diverged = torch.any(residual_squared / rsq0 > 100, dim=1) & (iterations >= 8)
-        converged = torch.all(residual_squared <= tolerance_sq, dim=1)
-        finished = converged | diverged | (iterations >= max_iter); not_finished_1 = (~finished).to(torch.int32)
-    return x, residual, iterations, function_evaluations, converged, diverged
-
-
-@torch.jit._script_if_tracing
-def torch_sparse_cg_adaptive(lin, y, x0, tolerance_sq, max_iter):
-    batch_size = y.shape[0]
-    x = x0
-    dx = residual = y - sparse_matmul(lin, x)
-    it_counter = torch.tensor(0, dtype=torch.int32, device=x.device)
-    iterations = torch.zeros([batch_size], dtype=torch.int32, device=x.device)
-    function_evaluations = torch.ones([batch_size], dtype=torch.int32, device=x.device)
-    residual_squared = rsq0 = torch.sum(residual ** 2, -1, keepdim=True)
-    diverged = torch.any(~torch.isfinite(x), dim=1)
-    converged = torch.all(residual_squared <= tolerance_sq, dim=1)
-    finished = converged | diverged | (iterations >= max_iter); not_finished_1 = (~finished).to(torch.int32)
-    while ~torch.all(finished):
-        it_counter += 1; iterations += not_finished_1
-        dy = sparse_matmul(lin, dx); function_evaluations += not_finished_1
-        dx_dy = torch.sum(dx * dy, dim=-1, keepdim=True)
-        step_size = divide_no_nan(torch.sum(dx * residual, dim=1, keepdim=True), dx_dy)
-        step_size *= torch.unsqueeze(not_finished_1.to(y.dtype), -1)  # this is not really necessary but ensures batch-independence
-        x += step_size * dx
-        if it_counter % 20 == 0:
-            residual = y - sparse_matmul(lin, x); function_evaluations += 1
-        else:
-            residual = residual - step_size * dy  # in-place subtraction affects convergence
-        residual_squared = torch.sum(residual ** 2, -1, keepdim=True)
-        dx = residual - divide_no_nan(torch.sum(residual * dy, dim=1, keepdim=True) * dx, dx_dy)
-        diverged = torch.any(residual_squared / rsq0 > 100, dim=1) & (iterations >= 8)
-        converged = torch.all(residual_squared <= tolerance_sq, dim=1)
-        finished = converged | diverged | (iterations >= max_iter); not_finished_1 = (~finished).to(torch.int32)
-    return x, residual, iterations, function_evaluations, converged, diverged
-
-
-def sparse_matmul(matrix: torch.Tensor, b: torch.Tensor):
-    if matrix.is_sparse or matrix.is_sparse_csr:
-        return torch.transpose(torch.sparse.mm(matrix, torch.transpose(b, 0, 1)), 0, 1)
-    else:
-        return torch.transpose(torch.matmul(matrix, torch.transpose(b, 0, 1)), 0, 1)
-
-
-def divide_no_nan(x: torch.Tensor, y: torch.Tensor):
-    # --- PyTorch backward pass of where produces nan gradients when inf values are present.
-    # Workaround is to avoid zero division by replacing zeros with ones (which then get filtered
-    # in the return where). ---
-    result = x / torch.where(y == 0, torch.ones_like(y), y)
-    result = torch.where(y == 0, torch.zeros_like(result), result)
-    return result
+import numbers
+import warnings
+from contextlib import contextmanager
+from functools import wraps
+from typing import List, Callable, Optional, Set, Tuple, Any, Union
+
+import numpy as np
+import torch
+import torch.fft
+import torch.nn.functional as torchf
+from packaging import version
+
+from phi.math import DType
+from phi.math.backend import Backend, NUMPY, ComputeDevice, PHI_LOGGER
+from phi.math.backend._backend import combined_dim, SolveResult, get_functional_derivative_order, TensorType
+
+
+class TorchBackend(Backend):
+
+    def __init__(self):
+        cpu = NUMPY.get_default_device()
+        devices = [ComputeDevice(self, "CPU", 'CPU', cpu.memory, cpu.processor_count, cpu.description, ref='cpu')]
+        for index in range(torch.cuda.device_count()):
+            properties = torch.cuda.get_device_properties(index)
+            devices.append(ComputeDevice(self, properties.name, 'GPU', properties.total_memory, properties.multi_processor_count, f"compute capability {properties.major}.{properties.minor}", f'cuda:{index}'))
+        Backend.__init__(self, 'PyTorch', devices, devices[1 if len(devices) > 1 else 0])
+
+    def prefers_channels_last(self) -> bool:
+        return False
+
+    def is_module(self, obj):
+        return isinstance(obj, (JITFunction, torch.nn.Module))
+
+    def is_tensor(self, x, only_native=False):
+        if isinstance(x, torch.Tensor):
+            return True
+        if only_native:
+            return False
+        if isinstance(x, numbers.Number):
+            return True
+        if isinstance(x, (tuple, list)) and all(isinstance(c, numbers.Number) for c in x):
+            return True
+        if isinstance(x, np.ndarray) and x.dtype != object:
+            return True  # this is pretty much required, else we couldn't perform NP+PyTorch operations
+        return False
+
+    def is_sparse(self, x) -> bool:
+        return x.is_sparse
+
+    def as_tensor(self, x, convert_external=True):
+        if isinstance(x, torch.nn.Module):
+            return x
+        if self.is_tensor(x, only_native=convert_external):
+            tensor = x
+        elif isinstance(x, np.ndarray):
+            try:
+                tensor = torch.from_numpy(x)
+            except ValueError:  # or TypeError?
+                tensor = torch.from_numpy(x.copy())
+            tensor = tensor.to(self.get_default_device().ref)
+        elif isinstance(x, (tuple, list)):
+            try:
+                x = np.stack(x)
+                tensor = torch.tensor(x, device=self.get_default_device().ref)
+            except ValueError:  # there may be Tensors inside the list
+                components = [self.as_tensor(c) for c in x]
+                tensor = torch.stack(components, dim=0)
+        else:
+            tensor = torch.tensor(x, device=self.get_default_device().ref)
+        # --- Enforce Precision ---
+        if self.is_tensor(tensor, only_native=True):
+            dtype = self.dtype(tensor)
+            if dtype.kind == float:
+                tensor = self.to_float(tensor)
+            elif dtype.kind == complex:
+                tensor = self.to_complex(tensor)
+        # --- Move to default device ---
+        if isinstance(tensor, torch.Tensor) and tensor.device != self.get_default_device().ref:
+            tensor = tensor.to(self.get_default_device().ref)
+        return tensor
+
+    def recursive_as_tensor(self, obj):
+        if isinstance(obj, (tuple, list)):
+            return type(obj)([self.recursive_as_tensor(item) for item in obj])
+        elif isinstance(obj, dict):
+            raise NotImplementedError()
+        else:
+            return self.as_tensor(obj)
+
+    def auto_cast(self, *tensors, **kwargs) -> list:
+        tensors = [t if isinstance(t, (numbers.Number, bool)) else self.as_tensor(t, True) for t in tensors]
+        return Backend.auto_cast(self, *tensors, **kwargs)
+
+    def is_available(self, tensor) -> bool:
+        # return True
+        return torch._C._get_tracing_state() is None  # TODO can we find out whether this tensor specifically is being traced?
+
+    def numpy(self, tensor, coalesce=False):
+        if tensor.requires_grad:
+            tensor = tensor.detach()
+        if hasattr(tensor, 'resolve_conj'):
+            tensor = tensor.resolve_conj()
+        if tensor.is_sparse:
+            if coalesce:
+                tensor = tensor.coalesce()
+                indices = tensor.indices()
+                values = tensor.values()
+            else:
+                indices = tensor._indices()
+                values = tensor._values()
+            indices = self.numpy(indices)
+            values = self.numpy(values)
+            from scipy.sparse import coo_matrix
+            return coo_matrix((values, indices), shape=self.staticshape(tensor))
+        elif tensor.is_sparse_csr:
+            values = self.numpy(tensor.values())
+            indices = self.numpy(tensor.col_indices())
+            pointers = self.numpy(tensor.crow_indices())
+            from scipy.sparse import csr_matrix
+            return csr_matrix((values, indices, pointers), shape=self.staticshape(tensor))
+        else:
+            return tensor.cpu().numpy()
+
+    def to_dlpack(self, tensor):
+        from torch.utils import dlpack
+        return dlpack.to_dlpack(tensor)
+
+    def from_dlpack(self, capsule):
+        from torch.utils import dlpack
+        tensor = dlpack.from_dlpack(capsule)
+        tensor = tensor.to(self.get_default_device().ref)
+        return tensor
+
+    def copy(self, tensor, only_mutable=False):
+        return torch.clone(tensor)
+
+    def get_device(self, tensor: TensorType) -> ComputeDevice:
+        return self.get_device_by_ref(str(tensor.device))
+
+    def allocate_on_device(self, tensor: TensorType, device: ComputeDevice) -> TensorType:
+        return self.as_tensor(tensor).to(device.ref)
+
+    def multi_slice(self, tensor, slices: tuple):
+        neg_slices = [i for i, s in enumerate(slices) if isinstance(s, slice) and s.step is not None and s.step < 0]
+        if neg_slices:
+            tensor = torch.flip(tensor, neg_slices)
+        pos_slices = [slice(s.start, s.stop, -s.step) if i in neg_slices else s for i, s in enumerate(slices)]
+        return tensor[tuple(pos_slices)]
+
+    sqrt = torch.sqrt
+    exp = torch.exp
+    sin = torch.sin
+    arcsin = torch.arcsin
+    cos = torch.cos
+    arccos = torch.arccos
+    tan = torch.tan
+    arctan = torch.arctan
+    sinh = torch.sinh
+    arcsinh = torch.arcsinh
+    cosh = torch.cosh
+    arccosh = torch.arccosh
+    tanh = torch.tanh
+    arctanh = torch.arctanh
+    log = torch.log
+    log2 = torch.log2
+    log10 = torch.log10
+    sigmoid = torch.sigmoid
+    isfinite = torch.isfinite
+    isnan = torch.isnan
+    isinf = torch.isinf
+    abs = torch.abs
+    sign = torch.sign
+    round = torch.round
+    ceil = torch.ceil
+    floor = torch.floor
+    nonzero = torch.nonzero
+    flip = torch.flip
+    seed = staticmethod(torch.manual_seed)
+    log_gamma = torch.lgamma
+
+    def softplus(self, x):
+        return torch.nn.Softplus()(x)
+
+    def einsum(self, equation, *tensors):
+        tensors = self.auto_cast(*tensors, bool_to_int=True, int_to_float=True)
+        return torch.einsum(equation, *tensors)
+
+    def vectorized_call(self, f, *args, output_dtypes=None, **aux_args):
+        if not hasattr(torch, 'vmap'):
+            return Backend.vectorized_call(self, f, *args, output_dtypes=output_dtypes, **aux_args)
+        batch_size = self.determine_size(args, 0)
+        args = [self.tile_to(t, 0, batch_size) for t in args]
+        f_vec = torch.vmap(f, 0, 0)
+        return f_vec(*args, **aux_args)
+
+    def jit_compile(self, f: Callable) -> Callable:
+        return JITFunction(self, f)
+
+    def custom_gradient(self, f: Callable, gradient: Callable = None, get_external_cache: Callable = None, on_call_skipped: Callable = None) -> Callable:
+        """ See PyTorch_Jit.md """
+        def select_jit(*args):
+            args = [self.as_tensor(arg) for arg in args]
+            if not CURRENT_JIT_CALLS:
+                return torch_function.apply(*args)
+            jit = CURRENT_JIT_CALLS[-1]
+            if torch._C._get_tracing_state() is not None:  # second call: we are tracing
+                compiled_function, ext_cache = jit.get_compiled_function(torch_function, args)  # increases counter
+                if on_call_skipped:
+                    on_call_skipped(ext_cache)
+                return compiled_function.apply(*args)  # this adds the compiled function to TorchScript. The function must not call any torch functions while being traced lest they be double-executed later.
+            else:  # first call: record this function
+                output = torch_function.apply(*args)
+                ext_cache = get_external_cache() if get_external_cache else None
+                jit.record_autograd_function_call(torch_function, args, output, ext_cache)
+                return output
+
+        torch_function = construct_torch_custom_function(f, None, None, gradient, is_f_traced=False, backend=self)
+        return select_jit
+
+    def transpose(self, tensor, axes):
+        return tensor.permute(axes)
+
+    def equal(self, x, y):
+        x, y = self.auto_cast(x, y)
+        return x == y
+
+    def random_uniform(self, shape, low, high, dtype: Union[DType, None]):
+        dtype = dtype or self.float_type
+        if dtype.kind == float:
+            return low + (high - low) * torch.rand(size=shape, dtype=to_torch_dtype(dtype), device=self.get_default_device().ref)
+        elif dtype.kind == complex:
+            real = low.real + (high.real - low.real) * torch.rand(size=shape, dtype=to_torch_dtype(DType(float, dtype.precision)), device=self.get_default_device().ref)
+            imag = low.imag + (high.imag - low.imag) * torch.rand(size=shape, dtype=to_torch_dtype(DType(float, dtype.precision)), device=self.get_default_device().ref)
+            return real + 1j * imag
+        elif dtype.kind == int:
+            return torch.randint(low, high, shape, dtype=to_torch_dtype(dtype))
+        else:
+            raise ValueError(dtype)
+
+    def random_normal(self, shape, dtype: DType):
+        return torch.randn(size=shape, dtype=to_torch_dtype(dtype or self.float_type), device=self.get_default_device().ref)
+
+    def stack(self, values, axis=0):
+        values = [self.as_tensor(v) for v in values]
+        return torch.stack(values, dim=axis)
+
+    def concat(self, values, axis):
+        values = [self.as_tensor(v) for v in values]
+        return torch.cat(values, dim=axis)
+
+    def pad(self, value, pad_width, mode='constant', constant_values=0):
+        """
+        pad tensor using mode
+
+        Args:
+          value(torch.Tensor): values
+          pad_width(iterable): left, right, upper, lower
+          mode(str, optional, optional): type of padding to be applied, defaults to 'constant'
+          constant_values(int, optional, optional): value to pad, defaults to 0
+
+        Returns:
+          torch.Tensor: padded tensor
+        """
+        mode = {'constant': 'constant', 'reflect': 'reflect', 'boundary': 'replicate', 'periodic': 'circular'}.get(mode, None)
+        if not mode:
+            return NotImplemented
+        # for PyTorch, we have to reshape value such that the outer 2 dimensions are not padded.
+        ndims = self.ndims(value)
+        no_pad_dims = [i for i in range(ndims) if pad_width[i] == (0, 0)]
+        pad_dims = [i for i in range(ndims) if pad_width[i] != (0, 0)]
+        if not pad_dims:
+            return value
+        if len(pad_dims) > 3:
+            return NotImplemented
+        value = torch.permute(value, no_pad_dims + pad_dims)
+        if len(no_pad_dims) == 0:
+            value = torch.unsqueeze(torch.unsqueeze(value, 0), 0)
+            undo_transform = lambda x: torch.squeeze(torch.squeeze(x, 0), 0)
+        elif len(no_pad_dims) == 1:
+            value = torch.unsqueeze(value, 0)
+            undo_transform = lambda x: torch.squeeze(x, 0)
+        elif len(no_pad_dims) == 2:
+            undo_transform = lambda x: x
+        else:
+            old_shape = value.shape
+            value = self.reshape(value, (1, np.prod([value.shape[i] for i in range(len(no_pad_dims))]), *value.shape[len(no_pad_dims):]))
+            undo_transform = lambda x: x.view(*[old_shape[i] for i in range(len(no_pad_dims))], *x.shape[2:])
+        pad_width_reordered = [pad_width[i] for i in pad_dims]
+        pad_width_spatial = [item for sublist in reversed(pad_width_reordered) for item in sublist]  # flatten
+        try:
+            constant_values = self.dtype(value).kind(constant_values)
+            result = torchf.pad(value, pad_width_spatial, mode, value=constant_values)  # supports 3D to 5D (batch, channel, 1D to 3D)
+        except RuntimeError as err:
+            warnings.warn(f"PyTorch error {err}", RuntimeWarning)
+            return NotImplemented
+        result = undo_transform(result)
+        inv_perm = tuple(np.argsort(no_pad_dims + pad_dims))
+        result = torch.permute(result, inv_perm)
+        return result
+
+    def grid_sample(self, grid, coordinates, extrapolation: str):
+        assert extrapolation in ('undefined', 'zeros', 'boundary', 'periodic', 'symmetric', 'reflect'), extrapolation
+        if get_functional_derivative_order() > 1:
+            return NotImplemented  # PyTorch's grid_sample operator does not define higher-order derivatives
+        extrapolation = {'undefined': 'zeros', 'zeros': 'zeros', 'boundary': 'border', 'reflect': 'reflection'}.get(extrapolation, None)
+        if extrapolation is None:
+            return NotImplemented
+        grid = channels_first(self.as_tensor(grid))
+        coordinates = self.as_tensor(coordinates)
+        if coordinates.shape[0] != grid.shape[0]:  # repeating yields wrong result
+            return NotImplemented
+        if coordinates.ndim != grid.ndim or coordinates.ndim not in (4, 5):
+            return NotImplemented  # torchf.grid_sample cannot handle this case
+        if coordinates.dtype.is_floating_point and not grid.dtype.is_complex and not grid.dtype.is_floating_point:
+            grid = self.to_float(grid)
+        resolution = torch.tensor(self.staticshape(grid)[2:], dtype=coordinates.dtype, device=coordinates.device)
+        coordinates = 2 * coordinates / (resolution - 1) - 1
+        coordinates = torch.flip(coordinates, dims=[-1])
+        batch_size = combined_dim(coordinates.shape[0], grid.shape[0])
+        coordinates = coordinates.repeat(batch_size, *[1] * (len(coordinates.shape-1))) if coordinates.shape[0] < batch_size else coordinates
+        grid = grid.repeat(batch_size, *[1] * (len(grid.shape)-1)) if grid.shape[0] < batch_size else grid
+        result = torchf.grid_sample(grid, coordinates, mode='bilinear', padding_mode=extrapolation, align_corners=True)  # can cause segmentation violation if NaN or inf are present
+        result = channels_last(result)
+        return result
+
+    def reshape(self, value, shape):
+        value = self.as_tensor(value)
+        if value.is_contiguous():
+            return value.view(*shape)
+        else:
+            return torch.reshape(value, shape)
+
+    def sum(self, value, axis=None, keepdims=False):
+        if axis is None:
+            axis = tuple(range(len(value.shape)))
+        if axis == () or axis == []:
+            return value
+        return torch.sum(value, dim=axis, keepdim=keepdims)
+
+    def prod(self, value, axis=None):
+        if not self.is_tensor(value, only_native=True):
+            return NUMPY.prod(value, axis)
+        if axis is None:
+            axis = tuple(range(len(value.shape)))
+        if isinstance(axis, (tuple, list)):
+            for dim in reversed(sorted(axis)):
+                value = torch.prod(value, dim=dim)
+            return value
+        return torch.prod(value, dim=axis)
+
+    def any(self, boolean_tensor, axis=None, keepdims=False):
+        boolean_tensor = self.as_tensor(boolean_tensor, convert_external=True)
+        if self.dtype(boolean_tensor).kind != bool:
+            boolean_tensor = boolean_tensor != 0
+        if axis is None:
+            return torch.any(boolean_tensor)
+        else:
+            axes = axis if isinstance(axis, (tuple, list)) else [axis]
+            for axis in reversed(sorted(axes)):
+                boolean_tensor = torch.any(boolean_tensor, dim=axis, keepdim=keepdims)
+            return boolean_tensor
+
+    def all(self, boolean_tensor, axis=None, keepdims=False):
+        boolean_tensor = self.as_tensor(boolean_tensor, convert_external=True)
+        if self.dtype(boolean_tensor).kind != bool:
+            boolean_tensor = boolean_tensor != 0
+        if axis is None:
+            return torch.all(boolean_tensor)
+        else:
+            axes = axis if isinstance(axis, (tuple, list)) else [axis]
+            for axis in reversed(sorted(axes)):
+                boolean_tensor = torch.all(boolean_tensor, dim=axis, keepdim=keepdims)
+            return boolean_tensor
+
+    def quantile(self, x, quantiles):
+        x = self.to_float(x)
+        result = torch.quantile(x, quantiles, dim=-1)
+        return result
+    
+    def argsort(self, x, axis=-1):
+        return torch.argsort(x, axis)
+
+    def searchsorted(self, sorted_sequence, search_values, side: str, dtype=DType(int, 32)):
+        int32 = {32: True, 64: False}[dtype.bits]
+        return torch.searchsorted(sorted_sequence, search_values, right=side == 'right', side=side, out_int32=int32)
+
+    def divide_no_nan(self, x, y):
+        x, y = self.auto_cast(x, y)
+        return divide_no_nan(x, y)
+
+    def where(self, condition, x=None, y=None):
+        condition = self.as_tensor(condition).bool()
+        x, y = self.auto_cast(x, y)
+        x = self.as_tensor(x)
+        y = self.as_tensor(y)
+        return torch.where(condition, x, y)
+
+    def mean(self, value, axis=None, keepdims=False):
+        if self.dtype(value).kind not in (float, complex):
+            value = self.to_float(value)
+        return torch.mean(value, dim=axis, keepdim=keepdims)
+
+    def range(self, start, limit=None, delta=1, dtype: DType = DType(int, 32)):
+        if limit is None:
+            start, limit = 0, start
+        return torch.arange(start, limit, delta, dtype=to_torch_dtype(dtype), device=self.get_default_device().ref)
+
+    def zeros(self, shape, dtype=None):
+        return torch.zeros(shape, dtype=to_torch_dtype(dtype or self.float_type), device=self.get_default_device().ref)
+
+    def zeros_like(self, tensor):
+        return torch.zeros_like(self.as_tensor(tensor), device=self.get_default_device().ref)
+
+    def ones(self, shape, dtype: DType = None):
+        return torch.ones(shape, dtype=to_torch_dtype(dtype or self.float_type), device=self.get_default_device().ref)
+
+    def ones_like(self, tensor):
+        return torch.ones_like(self.as_tensor(tensor), device=self.get_default_device().ref)
+
+    def meshgrid(self, *coordinates):
+        coordinates = [self.as_tensor(c) for c in coordinates]
+        from packaging import version
+        if version.parse(torch.__version__) >= version.parse('1.10'):
+            return torch.meshgrid(*coordinates, indexing='ij')
+        else:
+            return torch.meshgrid(*coordinates)
+
+    def linspace(self, start, stop, number):
+        if self.is_tensor(stop, only_native=True) or self.is_tensor(start, only_native=True):
+            unit = torch.linspace(0, 1, number, dtype=to_torch_dtype(self.float_type), device=self.get_default_device().ref)
+            return unit * (stop - start) + start
+        else:
+            return torch.linspace(float(start), float(stop), number, dtype=to_torch_dtype(self.float_type), device=self.get_default_device().ref)
+
+    def tensordot(self, a, a_axes: Union[tuple, list], b, b_axes: Union[tuple, list]):
+        a, b = self.auto_cast(a, b)
+        return torch.tensordot(a, b, (a_axes, b_axes))
+
+    def mul_matrix_batched_vector(self, A, b):
+        A, b = self.auto_cast(A, b)
+        if isinstance(A, torch.Tensor) and (A.is_sparse or A.is_sparse_csr):
+            result = torch.sparse.mm(A, torch.transpose(b, 0, 1))
+            return torch.transpose(result, 0, 1)
+        else:
+            return torch.transpose(torch.matmul(A, torch.transpose(b, -1, -2)), -1, -2)
+
+    def get_diagonal(self, matrices, offset=0):
+        return torch.transpose(torch.diagonal(matrices, offset=offset, dim1=1, dim2=2), 1, 2)
+
+    def cumsum(self, x, axis: int):
+        return torch.cumsum(x, dim=axis)
+
+    def while_loop(self, loop: Callable, values: tuple, max_iter: Union[int, Tuple[int, ...], List[int]]):
+        tracing = torch._C._get_tracing_state() is not None
+        if not tracing:
+            return Backend.while_loop(self, loop, values, max_iter)
+        # --- We are tracing ---
+        warnings.warn("PyTorch while_loop always iterates until max_iter. Please put a while loop into a torch.ScriptFunction instead.", RuntimeWarning)
+        values = self.stop_gradient_tree(values)
+        if isinstance(max_iter, (tuple, list)):
+            trj = [values] if 0 in max_iter else []
+            for i in range(1, max(max_iter) + 1):
+                values = loop(*values)
+                if i in max_iter:
+                    trj.append(self.copy_leaves(values, only_mutable=True))
+            trj.extend([trj[-1]] * (len(max_iter) - len(trj)))  # fill trj with final values
+            return self.stop_gradient_tree(self.stack_leaves(trj))
+        else:
+            for i in range(1, max_iter + 1):
+                values = loop(*values)
+            return self.stop_gradient_tree(values)
+        # if isinstance(loop, torch.ScriptFunction):
+        #     jit_loop = loop
+        #     i = 0
+        #     while torch.any(values[0]):
+        #         values = jit_loop(*values)
+        #         i += 1
+        #         if max_iter is not None and i >= max_iter:
+        #             break
+        #     return values
+            # def trace_later():
+            #     jit_loop = torch.jit.trace(loop, check_trace=False)
+            #     @torch.jit.script
+            #     def loop_script(values: Tuple[torch.Tensor], loop_script: Callable):
+            #         while torch.any(values[0]):
+            #             values = loop_script(*values)
+            #         return values
+            # CURRENT_JIT_CALLS[-1].post_trace.append(trace_later)
+
+    def max(self, x, axis=None, keepdims=False):
+        if axis is None:
+            result = torch.max(x)
+            if keepdims:
+                result = self.expand_dims(result, axis=0, number=self.ndims(x))
+            return result
+        elif isinstance(axis, (tuple, list)):
+            for dim in reversed(sorted(axis)):
+                x, _ = torch.max(x, dim=dim, keepdim=keepdims)
+            return x
+        else:
+            return torch.max(x, dim=axis, keepdim=keepdims)[0]
+
+    def min(self, x, axis=None, keepdims=False):
+        if axis is None:
+            result = torch.min(x)
+            if keepdims:
+                result = self.expand_dims(result, axis=0, number=self.ndims(x))
+            return result
+        elif isinstance(axis, (tuple, list)):
+            for dim in reversed(sorted(axis)):
+                x, _ = torch.min(x, dim=dim, keepdim=keepdims)
+            return x
+        else:
+            return torch.min(x, dim=axis, keepdim=keepdims)[0]
+
+    def maximum(self, a, b):
+        a_ = self.as_tensor(a)
+        b_ = self.as_tensor(b)
+        return torch.max(a_, other=b_)
+
+    def minimum(self, a, b):
+        a_ = self.as_tensor(a)
+        b_ = self.as_tensor(b)
+        return torch.min(a_, other=b_)
+
+    def clip(self, x, minimum, maximum):
+        if isinstance(minimum, numbers.Number) and isinstance(maximum, numbers.Number):
+            return torch.clamp(self.as_tensor(x), minimum, maximum)
+        else:
+            return self.maximum(minimum, self.minimum(x, maximum))
+
+    def conv(self, value, kernel, zero_padding=True):
+        value = self.as_tensor(value)
+        kernel = self.as_tensor(kernel)
+        value, kernel = self.auto_cast(value, kernel)
+        if self.dtype(value).kind in (bool, int):
+            value = self.to_float(value)
+            kernel = self.to_float(kernel)
+        if zero_padding:
+            if all(s % 2 == 1 for s in kernel.shape[3:]):
+                padding = [s // 2 for s in kernel.shape[3:]]
+            else:
+                padding = 0
+                value_padding = sum([[s // 2, (s - 1) // 2] for s in kernel.shape[3:]], [])
+                value = torchf.pad(value, value_padding)
+        else:
+            padding = 0
+        convf = {3: torchf.conv1d, 4: torchf.conv2d, 5: torchf.conv3d}[len(value.shape)]
+        if kernel.shape[0] == 1:
+            result = convf(value, kernel[0, ...], padding=padding)
+        else:
+            result = []
+            for b in range(kernel.shape[0]):
+                result.append(convf(value[b:b+1, ...], kernel[b, ...], padding=padding))
+            result = torch.cat(result, 0)
+        return result
+
+    def expand_dims(self, a, axis=0, number=1):
+        for _ in range(number):
+            a = torch.unsqueeze(a, dim=axis)
+        return a
+
+    def shape(self, tensor):
+        if self.is_tensor(tensor, only_native=True):
+            return tensor.shape
+        else:
+            return NUMPY.shape(tensor)
+
+    def staticshape(self, tensor):
+        if isinstance(tensor, torch.nn.Module):
+            return ()
+        if self.is_tensor(tensor, only_native=True):
+            return tuple([int(s) for s in tensor.shape])
+        else:
+            return NUMPY.staticshape(tensor)
+
+    def gather(self, values, indices, axis: int):
+        indices = self.to_int64(indices)
+        slices = [indices if i == axis else slice(None) for i in range(self.ndims(values))]
+        return values[tuple(slices)]
+
+    def gather_by_component_indices(self, values, *component_indices):
+        component_indices = [self.to_int64(c) for c in component_indices]
+        return values[tuple(component_indices)]
+
+    def batched_gather_nd(self, values, indices):
+        values = self.as_tensor(values)
+        indices = self.as_tensor(indices).long()
+        batch_size = combined_dim(values.shape[0], indices.shape[0])
+        result = []
+        for b in range(batch_size):
+            b_indices = self.unstack(indices[min(b, indices.shape[0] - 1)], -1)
+            result.append(values[(min(b, values.shape[0] - 1),) + b_indices])
+        return self.stack(result, axis=0)
+
+    def unstack(self, tensor, axis=0, keepdims=False):
+        unstacked = torch.unbind(tensor, dim=axis)
+        if keepdims:
+            unstacked = [self.expand_dims(c, axis=axis) for c in unstacked]
+        return unstacked
+
+    def std(self, x, axis=None, keepdims=False):
+        if self.dtype(x).kind not in (float, complex):
+            x = self.to_float(x)
+        return torch.std(x, dim=axis, keepdim=keepdims, unbiased=False)
+
+    def boolean_mask(self, x, mask, axis=0, new_length=None, fill_value=0):
+        x = self.as_tensor(x)
+        mask = self.as_tensor(mask)
+        result = []
+        for selected, data in zip(mask, self.unstack(x, axis)):
+            if selected:
+                result.append(data)
+        return self.stack(result, axis)
+        # return torch.masked_select(x_, mask_)
+
+    def scatter(self, base_grid, indices, values, mode: str):
+        base_grid, values = self.auto_cast(base_grid, values)
+        indices = self.as_tensor(indices)
+        batch_size = combined_dim(combined_dim(indices.shape[0], values.shape[0]), base_grid.shape[0])
+        scatter = torch.scatter_add if mode == 'add' else torch.scatter
+        if indices.shape[0] < batch_size:
+            indices = indices.repeat([batch_size] + [1] * (len(indices.shape)-1))
+        if values.shape[0] < batch_size or values.shape[1] == 1:
+            values = values.repeat([batch_size // values.shape[0], indices.shape[1] // indices.shape[1]] + [1] * (len(values.shape)-2))
+        if len(base_grid.shape) > 3:
+            resolution = base_grid.shape[1:-1]
+            ravel = [1]
+            for i in range(1, len(resolution)):
+                ravel.insert(0, ravel[0] * resolution[-i])
+            ravel = self.to_int64(self.as_tensor(ravel, True))
+            indices = torch.sum(indices * ravel, dim=-1, keepdim=True)
+        base_grid_flat = torch.reshape(base_grid, [base_grid.shape[0], -1, base_grid.shape[-1]])
+        indices = indices.long().repeat([1, 1, values.shape[-1]])
+        result = scatter(base_grid_flat, dim=1, index=indices, src=values)
+        return torch.reshape(result, base_grid.shape)
+
+    def histogram1d(self, values, weights, bin_edges):
+        values = self.as_tensor(values)
+        weights = self.as_tensor(weights)
+        bin_edges = self.as_tensor(bin_edges)
+        bin_count = self.staticshape(bin_edges)[-1] - 1
+        batch_size, _ = self.staticshape(values)
+        bin_indices = torch.minimum(torch.searchsorted(bin_edges, values, side='right') - 1, self.as_tensor(bin_count - 1))  # ToDo this includes values outside
+        result = torch.zeros(batch_size, bin_count, dtype=weights.dtype, device=values.device)
+        hist = torch.scatter_add(result, -1, bin_indices, weights)
+        return hist
+
+    def bincount(self, x, weights, bins: int):
+        return torch.bincount(x, weights, minlength=bins)
+
+    def arctan2(self, y, x):
+        y, x = self.auto_cast(y, x)
+        return torch.arctan2(y, x)
+
+    def fft(self, x, axes: Union[tuple, list]):
+        if not x.is_complex():
+            x = self.to_complex(x)
+        for i in axes:
+            x = torch.fft.fft(x, dim=i)
+        return x
+
+    def ifft(self, k, axes: Union[tuple, list]):
+        if not k.is_complex():
+            k = self.to_complex(k)
+        for i in axes:
+            k = torch.fft.ifft(k, dim=i)
+        return k
+
+    def imag(self, x):
+        dtype = self.dtype(x)
+        if dtype.kind == complex:
+            return torch.imag(x)
+        else:
+            return self.zeros(x.shape, DType(float, dtype.precision))
+
+    def real(self, x):
+        if self.dtype(x).kind == complex:
+            return torch.real(x)
+        else:
+            return x
+
+    def conj(self, x):
+        if self.dtype(x).kind == complex:
+            return torch.conj(x)
+        else:
+            return x
+
+    def cast(self, x, dtype: DType):
+        if isinstance(x, (numbers.Number, bool)):
+            return dtype.kind(x)  # Creating a Tensor here would raise warnings during tracing.
+        if not self.is_tensor(x, only_native=True):
+            x = self.as_tensor(x)
+        if self.dtype(x) == dtype:
+            return x
+        else:
+            return x.to(to_torch_dtype(dtype))
+
+    def dtype(self, array) -> DType:
+        if self.is_tensor(array, only_native=True):
+            return from_torch_dtype(array.dtype)
+        else:
+            return NUMPY.dtype(array)
+
+    def tile(self, value, multiples):
+        if isinstance(multiples, np.ndarray):
+            multiples = multiples.tolist()
+        return self.as_tensor(value).repeat(multiples)
+
+    def repeat(self, x, repeats, axis: int, new_length=None):
+        if isinstance(repeats, (np.ndarray, tuple, list)):
+            repeats = self.as_tensor(repeats)
+        return torch.repeat_interleave(self.as_tensor(x), repeats, axis)
+
+    def sparse_coo_tensor(self, indices, values, shape):
+        indices = self.to_int64(indices)
+        indices = self.transpose(indices, [1, 0])
+        values = self.to_float(values)
+
+        @torch.jit.script  # the output of torch.sparse_coo_tensor is considered constant
+        def sparse_coo_tensor(values, indices, cols: int, rows: int, dtype: torch.dtype) -> torch.sparse.Tensor:
+            size = torch.Size([cols, rows])
+            return torch.sparse_coo_tensor(indices, values, size=size, dtype=dtype)
+
+        return sparse_coo_tensor(values, indices, shape[0], shape[1], to_torch_dtype(self.float_type))
+
+    def csr_matrix(self, column_indices, row_pointers, values, shape: Tuple[int, int]):
+        row_pointers = self.as_tensor(row_pointers)
+        column_indices = self.as_tensor(column_indices)
+        values = self.as_tensor(values)
+        return torch.sparse_csr_tensor(row_pointers, column_indices, values, shape, device=values.device)
+
+    # def csr_matrix_batched(self, column_indices, row_pointers, values, shape: Tuple[int, int]):
+    #     batch_size, nnz, channels = values.shape
+    #     if version.parse(torch.__version__) >= version.parse('1.13.0'):
+    #         return torch.sparse_csr_tensor(row_pointers, column_indices, values, (batch_size, *shape, channels), device=values.device)
+    #     else:
+    #         warnings.warn("PyTorch >= 1.13 is required for batched CSR matrices. Visit https://pytorch.org/ to download the latest version.", RuntimeWarning)
+    #         raise NotImplementedError
+    #         # matrices = []
+    #         # for b in range(batch_size):
+    #         #     if values.shape[-1] == 1:
+    #         #         b_matrix = torch.sparse_csr_tensor(row_pointers[b], column_indices[b], values[b, :, 0], shape, device=values.device)
+    #         #     else:
+    #         #         raise NotImplementedError
+    #         #     matrices.append(b_matrix)
+    #         # return matrices
+
+    def csc_matrix(self, column_pointers, row_indices, values, shape: tuple):
+        batch_size, nnz, channels = values.shape
+        if version.parse(torch.__version__) >= version.parse('1.13.0'):
+            return torch.sparse_csc_tensor(column_pointers, row_indices, values, (batch_size, *shape, channels), device=values.device)
+        else:
+            warnings.warn("PyTorch >= 1.13 is required for batched CSC matrices. Visit https://pytorch.org/ to download the latest version.", RuntimeWarning)
+            raise NotImplementedError
+            # batch_size, nnz, channels = values.shape
+            # if batch_size == channels == 1:
+            #     return scipy.sparse.csc_matrix((values[0, :, 0], row_indices[0], column_pointers[0]), shape=shape)
+            # matrices = []
+            # for b in range(batch_size):
+            #     if values.shape[-1] == 1:
+            #         b_matrix = scipy.sparse.csc_matrix((values[b, :, 0], row_indices[b], column_pointers[b]), shape=shape)
+            #     else:
+            #         raise NotImplementedError
+            #     matrices.append(b_matrix)
+            # return matrices
+
+    def mul_csr_dense(self, column_indices, row_pointers, values, shape: tuple, dense):
+        values, dense = self.auto_cast(values, dense, bool_to_int=True, int_to_float=True)
+        batch_size, nnz, channels = values.shape
+        result = []
+        for b in range(batch_size):
+            b_result = []
+            for c in range(channels):
+                matrix = torch.sparse_csr_tensor(row_pointers[b], column_indices[b], values[b, :, c], shape, device=values.device)
+                b_result.append(torch.sparse.mm(matrix, self.as_tensor(dense[b, :, c, :])))
+            result.append(torch.stack(b_result))
+        return torch.stack(result)
+        # if channel_count == 1:
+        #     matrix = torch.sparse_csr_tensor(row_pointers, column_indices, values[:, :, 0], (batch_size, *shape), device=values.device)
+        #     matrix.matmul(self.as_tensor(dense[:, 0, :, :]))
+        #     # torch.sparse.mm(matrix, self.as_tensor(dense[:, 0, :, :]))
+        #     raise NotImplementedError
+        # else:
+        #     # tile
+        #     raise NotImplementedError
+
+    def conjugate_gradient(self, lin, y, x0, rtol, atol, max_iter, pre) -> SolveResult:
+        if callable(lin) or len(max_iter) > 1 or pre:
+            assert self.is_available(y), "Tracing conjugate_gradient with linear operator is not yet supported."
+            return Backend.conjugate_gradient(self, lin, y, x0, rtol, atol, max_iter, pre)
+        assert isinstance(lin, torch.Tensor), "Batched matrices are not yet supported"
+        batch_size = self.staticshape(y)[0]
+        y = self.to_float(y)
+        x0 = self.copy(self.to_float(x0))
+        rtol = self.as_tensor(rtol)
+        atol = self.as_tensor(atol)
+        tol_sq = self.maximum(rtol ** 2 * self.sum(y ** 2, -1), atol ** 2)
+        max_iter = self.as_tensor(max_iter[0])
+        x, residual, iterations, function_evaluations, converged, diverged = torch_sparse_cg(lin, y, x0, tol_sq, max_iter)
+        return SolveResult(f"Φ-Flow CG ({'PyTorch*' if self.is_available(y) else 'TorchScript'})", x, residual, iterations, function_evaluations, converged, diverged, [""] * batch_size)
+
+    def conjugate_gradient_adaptive(self, lin, y, x0, rtol, atol, max_iter, pre) -> SolveResult:
+        if callable(lin) or len(max_iter) > 1 or pre:
+            assert self.is_available(y), "Tracing conjugate_gradient with linear operator is not yet supported."
+            return Backend.conjugate_gradient_adaptive(self, lin, y, x0, rtol, atol, max_iter, pre)
+        assert isinstance(lin, torch.Tensor), "Batched matrices are not yet supported"
+        batch_size = self.staticshape(y)[0]
+        y = self.to_float(y)
+        x0 = self.copy(self.to_float(x0))
+        rtol = self.as_tensor(rtol)
+        atol = self.as_tensor(atol)
+        tol_sq = self.maximum(rtol ** 2 * self.sum(y ** 2, -1), atol ** 2)
+        max_iter = self.as_tensor(max_iter[0])
+        x, residual, iterations, function_evaluations, converged, diverged = torch_sparse_cg_adaptive(lin, y, x0, tol_sq, max_iter)
+        return SolveResult(f"Φ-Flow CG ({'PyTorch*' if self.is_available(y) else 'TorchScript'})", x, residual, iterations, function_evaluations, converged, diverged, [""] * batch_size)
+
+    def bi_conjugate_gradient(self, lin, y, x0, rtol, atol, max_iter, pre, poly_order=2) -> SolveResult:
+        if not self.is_available(y):
+            warnings.warn("Bi-CG is not optimized for PyTorch and will always run the maximum number of iterations.", RuntimeWarning)
+        return Backend.bi_conjugate_gradient(self, lin, y, x0, rtol, atol, max_iter, pre, poly_order)
+
+    def matrix_solve_least_squares(self, matrix: TensorType, rhs: TensorType) -> Tuple[TensorType, TensorType, TensorType, TensorType]:
+        assert version.parse(torch.__version__) >= version.parse('1.9.0'), "least squares requires PyTorch >= 1.9.0"
+        matrix, rhs = self.auto_cast(matrix, rhs)
+        solution, residuals, rank, singular_values = torch.linalg.lstsq(matrix, rhs)
+        return solution, residuals, rank, singular_values
+
+    def solve_triangular_dense(self, matrix, rhs, lower: bool, unit_diagonal: bool):
+        matrix, rhs = self.auto_cast(matrix, rhs, int_to_float=True, bool_to_int=True)
+        rhs = self.expand_dims(rhs, -1)
+        x = torch.linalg.solve_triangular(matrix, rhs, upper=not lower, unitriangular=unit_diagonal)
+        return x[..., 0]
+
+    def _prepare_graph_inputs(self, args: tuple, wrt: Union[tuple, list]):
+        args = [self.as_tensor(arg, True) if i in wrt else arg for i, arg in enumerate(args)]
+        args = [self.to_float(arg) if self.dtype(arg).kind == int else arg for arg in args]
+        for i, arg in enumerate(args):
+            if self.is_tensor(arg, True) and arg.requires_grad and not arg.is_leaf:
+                arg = torch.clone(arg).detach()
+                arg.requires_grad = i in wrt
+                args[i] = arg
+            elif i in wrt:
+                arg = self.as_tensor(arg, True)
+                arg = arg.detach()  # returns a new tensor in any case
+                arg.requires_grad = True
+                args[i] = arg
+        wrt_args = [arg for i, arg in enumerate(args) if i in wrt]
+        for t in wrt_args:
+            assert t.requires_grad
+        return args, wrt_args
+
+    def jacobian(self, f, wrt: Union[tuple, list], get_output: bool, is_f_scalar: bool):
+        @wraps(f)
+        def eval_grad(*args):
+            args, wrt_args = self._prepare_graph_inputs(args, wrt)
+            loss, output = f(*args)
+            if np.prod(self.staticshape(loss)) == 1:
+                grads = torch.autograd.grad(loss, wrt_args)  # grad() cannot be called during jit trace
+            else:
+                raise NotImplementedError()
+                grads = torch.autograd.grad(loss, wrt_args, retain_graph=True)
+            return (*output, *grads) if get_output else grads
+        return eval_grad
+
+    def hessian(self, f: Callable, wrt: Union[tuple, list], get_output: bool, get_gradient: bool):
+        # if not get_output and not get_gradient:
+        # @wraps(f)
+        # def eval_hessian(*args):
+        #     batch_size = args[0].shape[0]
+        #     for arg in args:
+        #         assert arg.shape[0] == batch_size, f"All arguments must have a matching batch dimension as their first dimension. Got shapes {[arg.shape for arg in args]}"
+        #
+        #     def f_only_wrt_inputs(*wrt_args_only, reduce_batch=False):
+        #         all_args = list(args)
+        #         for i, arg in zip(wrt, wrt_args_only):
+        #             all_args[i] = arg
+        #         output = f(*all_args)
+        #         loss, aux = (output[0], output[1:]) if isinstance(output, (tuple, list)) else (output, None)
+        #         if reduce_batch:
+        #             if loss.ndim > 0:
+        #                 loss = loss.sum()
+        #         else:
+        #             assert np.prod(loss.shape) == 1, f"Loss (first output of f) must be scalar but has shape {loss.shape}"
+        #             loss = loss.sum()
+        #         return loss
+        #
+        #     wrt_args = tuple([self.as_tensor(arg, True) for i, arg in enumerate(args) if i in wrt])
+        #     result = ()
+        #     if get_output:
+        #         result += f(*args),
+        #     if get_gradient:
+        #         result += torch.autograd.functional.jacobian(lambda *a: f_only_wrt_inputs(*a, reduce_batch=True), wrt_args),
+        #     if hasattr(torch, 'vmap'):
+        #         # single_hessian_f = lambda *args: torch.autograd.functional.hessian(f_only_wrt_inputs, args)
+        #         # multi_hessian_f = torch.vmap
+        #         raise NotImplementedError()
+        #     else:
+        #         hessian = tuple([tuple([[] for _1 in range(len(wrt))]) for _2 in range(len(wrt))])  # n x n matrix of lists
+        #         for b in range(batch_size):
+        #             h = torch.autograd.functional.hessian(f_only_wrt_inputs, tuple([arg[b:b + 1] for arg in wrt_args]))
+        #             for i in range(len(wrt)):
+        #                 for j in range(len(wrt)):
+        #                     fake_batch_dim = args[i].ndim
+        #                     hessian[i][j].append(torch.squeeze(torch.squeeze(h[i][j], fake_batch_dim), 0))
+        #         hessian = [[torch.stack(hessian[i][j]) for j in range(len(wrt))] for i in range(len(wrt))]
+        #         # hessian = torch.stack([torch.autograd.functional.hessian(f_only_wrt_inputs, tuple([arg[b:b+1] for arg in wrt_args])) for b in range(batch_size)])  # manual batch loop
+        #     result += hessian,
+        #     return result
+        # else:
+        @wraps(f)
+        def eval_hessian(*args):
+            args, wrt_args = self._prepare_graph_inputs(args, wrt)
+            output = f(*args)
+            loss, aux = (output[0], output[1:]) if isinstance(output, (tuple, list)) else (output, None)
+            scalar_loss = loss.sum() if loss.ndim > 0 else loss
+            grads = torch.autograd.grad(scalar_loss, wrt_args, create_graph=True, retain_graph=True)  # grad() cannot be called during jit trace
+            hessian = []
+            for grad in grads:
+                if not grad.requires_grad:
+                    raise NotImplementedError("Linear dependency detected. Hessian = 0.")
+                hessian.append([[] for _ in grads])
+                for lin_index in range(int(np.prod(grad.shape[1:]))):
+                    multi_index = np.unravel_index(lin_index, grad.shape[1:])
+                    h = torch.autograd.grad(grad[(slice(None),) + multi_index].sum(), wrt_args, allow_unused=True, retain_graph=True)  # grad of every entry in grad
+                    # Warning: This returns incorrect values for certain inputs. Hessian of x^2 returns 0 at x=0 but is correct everywhere else.
+                    # ToDo torch.autograd.functional.hessian does not seem to have this issue. Wait for torch.vmap(), then conditionally switch.
+                    for i, h_ in enumerate(h):
+                        hessian[-1][i].append(h_)
+            for col in hessian:
+                for i, row in enumerate(col):
+                    if len(row) > 1:
+                        col[i] = torch.stack(row, dim=1)
+                    else:
+                        col[i] = row[0]
+                    h_shape = tuple(grads[i].shape) + tuple(grads[i].shape[1:])
+                    col[i] = torch.reshape(col[i], h_shape)
+
+            result = ()
+            if get_output:
+                loss = loss.detach()
+                if aux is not None:
+                    aux = [aux_.detach() if isinstance(aux_, torch.Tensor) else aux_ for aux_ in aux]
+                    result += (loss, *aux),
+                else:
+                    result += loss,
+            if get_gradient:
+                result += tuple([g.detach() for g in grads]),
+            result += hessian,
+            return result
+
+        return eval_hessian
+
+    def jit_compile_grad(self, f, wrt: Union[tuple, list], get_output: bool, is_f_scalar: bool):
+        jit = self.jit_compile(f)
+        return self.jacobian(jit, wrt, get_output, is_f_scalar)
+
+    def jit_compile_hessian(self, f, wrt: Union[tuple, list], get_output: bool, get_gradient: bool):
+        jit = self.jit_compile(f)
+        return self.hessian(jit, wrt, get_output, get_gradient)
+
+    def stop_gradient(self, value):
+        return value.detach()
+
+
+def channels_first(x):
+    return x.permute(*((0, -1) + tuple(range(1, len(x.shape) - 1))))
+
+
+def channels_last(x):
+    return x.permute((0,) + tuple(range(2, len(x.shape))) + (1,))
+
+
+class JITFunction:
+    """
+    PyTorch Tracing Procedure:
+    1. Call function non-tracing, record all called nn.Modules and autograd.Function forward calls with their args
+    2. Compile autograd.Functions forward and backward passes
+    3. Add nn.Modules to JIT Module
+    4. Trace JIT Module
+
+    Nested jit calls are ignored.
+
+    See PyTorch_Jit.md
+    """
+
+    def __init__(self, backend: TorchBackend, f):
+        self.backend = backend
+        self.f = f
+        self.traced = None
+        self.autograd_function_calls = []  # (TorchCustomFunction, args, output, ext_cache)
+        self.compiled_functions = []  # (TorchCustomFunction, TorchCustomFunction)
+        self.autograd_function_call_counts = 0
+        self.called_modules: Set[torch.nn.Module] = set()
+
+    def __call__(self, *args, **kwargs):
+        if kwargs:
+            raise NotImplementedError("kwargs not supported for traced function")
+        if CURRENT_JIT_CALLS:
+            warnings.warn(f"PyTorch does not support nested tracing. The inner JIT of {self.f.__name__} will be ignored.", RuntimeWarning)
+            return self.f(*args)
+        args = self.backend.recursive_as_tensor(args)
+        if self.traced is None:
+            self_jit = self
+            CURRENT_JIT_CALLS.append(self)
+            self.f(*args)  # records all autograd.Function / nn.Module calls with their args -> self.autograd_function_calls, self.called_modules
+            for i, (rec_function, rec_args, rec_output, _ext_cache) in enumerate(self.autograd_function_calls):
+                self.compiled_functions.append((rec_function, rec_function.compile(rec_args, rec_output)))
+            assert self.autograd_function_call_counts == 0
+
+            class JitModule(torch.nn.Module):
+
+                def __init__(self):
+                    super().__init__()
+                    for submodule in self_jit.called_modules:
+                        self.add_module(str(f"{type(submodule).__name__}_{id(submodule)}"), submodule)
+
+                def forward(self, *args):
+                    PHI_LOGGER.debug(f"Tracing Pytorch jit module for {self_jit.f.__name__}")
+                    return self_jit.f(*args)
+
+            module = JitModule()
+            self.traced = torch.jit.trace(module, tuple(args), check_trace=False, strict=False)
+            assert self.autograd_function_call_counts == len(self.autograd_function_calls), "Not all custom-gradient functions were called during tracing! Nested custom gradients are not supported."
+            assert CURRENT_JIT_CALLS.pop(-1) == self
+        from phi.math.backend import choose_backend
+        return choose_backend(self).call(self.traced, *args, name=f"run jit-compiled '{self.f.__name__}'")
+
+    def record_autograd_function_call(self, function: torch.autograd.Function, args, output, ext_cache):
+        self.autograd_function_calls.append((function, args, output, ext_cache))
+
+    def get_compiled_function(self, function: torch.autograd.Function, args) -> Tuple[torch.autograd.Function, Any]:
+        assert torch._C._get_tracing_state() is not None
+        assert self.autograd_function_call_counts < len(self.autograd_function_calls), f"More custom-gradient functions were called during tracing!\nLast encountered: {function}"
+        assert len(self.autograd_function_calls) == len(self.compiled_functions)
+        original_function, compiled_function = self.compiled_functions[self.autograd_function_call_counts]
+        assert isinstance(compiled_function, torch.autograd.Function)
+        function, args, output, ext_cache = self.autograd_function_calls[self.autograd_function_call_counts]
+        self.autograd_function_call_counts += 1
+        return compiled_function, ext_cache
+
+    def __repr__(self):
+        return f"TorchScript[{self.f.__name__}]"
+
+
+CURRENT_JIT_CALLS: List[JITFunction] = []  # should contain no more than 1 element; PyTorch doesn't support nested tracing
+
+
+def register_module_call(module: torch.nn.Module):
+    if CURRENT_JIT_CALLS:
+        CURRENT_JIT_CALLS[-1].called_modules.add(module)
+
+
+def construct_torch_custom_function(f: Callable, jit_f: Optional[Callable], f_example_output, g: Callable, is_f_traced: bool, backend: TorchBackend):
+    jit_g = []
+
+    class TorchCustomFunction(torch.autograd.Function):
+        """ See PyTorch_Jit.md """
+
+        @staticmethod
+        def forward(ctx, *args, **kwargs):  # The result of this is used in the graph.
+            if torch._C._get_tracing_state():
+                PHI_LOGGER.debug(f"torch.jit.trace encountered forward pass of {f.__name__}. Returning cached output to avoid double execution.")
+                # jit_context = CURRENT_JIT_CALLS[-1]; jit_context.cached_output[torch_custom_function]
+                return f_example_output
+            y = (jit_f or f)(*args, **kwargs)
+            ctx.save_for_backward(*args, *y)
+            ctx.input_count = len(args)
+            return y
+
+        # @torch.jit.unused, @torch.jit.ignore(drop=True)  do not work here
+
+        @staticmethod
+        def backward(ctx, *grad_args):  # Breakpoints not supported here
+            x = ctx.saved_tensors[:ctx.input_count]
+            y = ctx.saved_tensors[ctx.input_count:]
+            if is_f_traced:
+                if not jit_g:
+                    # backward pass can return None but that's not allowed in JIT functions
+                    needs_input_grad = ctx.needs_input_grad
+                    none_indices = []  # jit function cannot return None but gradient returns None to indicate there is no gradient
+
+                    def filter_required_grads(*args):  # traced once
+                        grads = g(*args)
+                        filtered = [gv for gv, need in zip(grads, needs_input_grad) if need]
+                        none_indices.clear()
+                        none_indices.extend([i for i, g in enumerate(filtered) if g is None])
+                        filtered = [gv for gv in filtered if gv is not None]
+                        assert len(filtered) > 0, "Custom backward function must return at least one valid gradient."
+                        assert all([isinstance(gv, torch.Tensor) for gv in filtered]), [type(gv) for gv in grads]
+                        return filtered
+
+                    PHI_LOGGER.debug(f"Tracing backward pass of '{f.__name__}' which uses a custom gradient")
+                    needed_g = backend.jit_compile(filter_required_grads)
+                    # needed_g = torch.jit.trace(filter_required_grads, tuple([x, y, grad_args]), check_trace=False, strict=False)
+
+                    def g_(*args):  # called each time, not jitted
+                        needed = backend.as_registered.call(needed_g, *args, name=f"run jit-compiled custom backward '{g.__name__}'")
+                        assert isinstance(needed, (tuple, list))
+                        needed = list(needed)
+                        for i in none_indices:
+                            needed.insert(i, None)
+                        result = [(needed.pop(0) if need else None) for need in needs_input_grad]
+                        return result
+
+                    jit_g.append(g_)
+                output = jit_g[0](x, y, grad_args)
+            else:
+                output = g(x, y, grad_args)
+            result = output[0] if len(output) == 1 else (*output,)
+            return result
+
+        @staticmethod
+        def compile(args: tuple, output):
+            assert jit_f is None
+            PHI_LOGGER.debug(f"Tracing forward pass of '{f.__name__}' which uses a custom gradient")
+            jit_f_ = torch.jit.trace(f, args, strict=False, check_trace=False)
+            return construct_torch_custom_function(f, jit_f_, output, g, is_f_traced=True, backend=backend)
+
+        @property
+        def __name__(self):
+            return f"TorchCustomFunction-{'jit' if is_f_traced else 'non-jit'}[{f.__name__}]"
+
+        def __repr__(self):
+            return self.__name__
+
+    torch_custom_function = TorchCustomFunction()
+    return torch_custom_function
+
+
+def to_torch_dtype(dtype: DType):
+    return _TO_TORCH[dtype]
+
+
+def from_torch_dtype(torch_dtype):
+    if torch_dtype in _FROM_TORCH:
+        return _FROM_TORCH[torch_dtype]
+    else:
+        kind = {'i': int, 'b': bool, 'f': float, 'c': complex}[torch_dtype.kind]
+        return DType(kind, torch_dtype.itemsize * 8)
+
+
+_TO_TORCH = {
+    DType(float, 16): torch.float16,
+    DType(float, 32): torch.float32,
+    DType(float, 64): torch.float64,
+    DType(complex, 64): torch.complex64,
+    DType(complex, 128): torch.complex128,
+    DType(int, 8): torch.int8,
+    DType(int, 16): torch.int16,
+    DType(int, 32): torch.int32,
+    DType(int, 64): torch.int64,
+    DType(bool): torch.bool,
+}
+_FROM_TORCH = {np: dtype for dtype, np in _TO_TORCH.items()}
+
+
+@torch.jit._script_if_tracing
+def torch_sparse_cg(lin, y, x0, tolerance_sq, max_iter):
+    batch_size = y.shape[0]
+    x = x0
+    dx = residual = y - sparse_matmul(lin, x)
+    it_counter = torch.tensor(0, dtype=torch.int32, device=x.device)
+    iterations = torch.zeros([batch_size], dtype=torch.int32, device=x.device)
+    function_evaluations = torch.ones([batch_size], dtype=torch.int32, device=x.device)
+    residual_squared = rsq0 = torch.sum(residual ** 2, -1, keepdim=True)
+    diverged = torch.any(~torch.isfinite(x), dim=1)
+    converged = torch.all(residual_squared <= tolerance_sq, dim=1)
+    finished = converged | diverged | (iterations >= max_iter); not_finished_1 = (~finished).to(torch.int32)
+    while ~torch.all(finished):
+        it_counter += 1; iterations += not_finished_1
+        dy = sparse_matmul(lin, dx); function_evaluations += not_finished_1
+        dx_dy = torch.sum(dx * dy, dim=-1, keepdim=True)
+        step_size = divide_no_nan(residual_squared, dx_dy)
+        step_size *= torch.unsqueeze(not_finished_1.to(y.dtype), -1)  # this is not really necessary but ensures batch-independence
+        x += step_size * dx
+        if it_counter % 20 == 0:
+            residual = y - sparse_matmul(lin, x); function_evaluations += 1
+        else:
+            residual = residual - step_size * dy  # in-place subtraction affects convergence
+        residual_squared_old = residual_squared
+        residual_squared = torch.sum(residual ** 2, -1, keepdim=True)
+        dx = residual + divide_no_nan(residual_squared, residual_squared_old) * dx
+        diverged = torch.any(residual_squared / rsq0 > 100, dim=1) & (iterations >= 8)
+        converged = torch.all(residual_squared <= tolerance_sq, dim=1)
+        finished = converged | diverged | (iterations >= max_iter); not_finished_1 = (~finished).to(torch.int32)
+    return x, residual, iterations, function_evaluations, converged, diverged
+
+
+@torch.jit._script_if_tracing
+def torch_sparse_cg_adaptive(lin, y, x0, tolerance_sq, max_iter):
+    batch_size = y.shape[0]
+    x = x0
+    dx = residual = y - sparse_matmul(lin, x)
+    it_counter = torch.tensor(0, dtype=torch.int32, device=x.device)
+    iterations = torch.zeros([batch_size], dtype=torch.int32, device=x.device)
+    function_evaluations = torch.ones([batch_size], dtype=torch.int32, device=x.device)
+    residual_squared = rsq0 = torch.sum(residual ** 2, -1, keepdim=True)
+    diverged = torch.any(~torch.isfinite(x), dim=1)
+    converged = torch.all(residual_squared <= tolerance_sq, dim=1)
+    finished = converged | diverged | (iterations >= max_iter); not_finished_1 = (~finished).to(torch.int32)
+    while ~torch.all(finished):
+        it_counter += 1; iterations += not_finished_1
+        dy = sparse_matmul(lin, dx); function_evaluations += not_finished_1
+        dx_dy = torch.sum(dx * dy, dim=-1, keepdim=True)
+        step_size = divide_no_nan(torch.sum(dx * residual, dim=1, keepdim=True), dx_dy)
+        step_size *= torch.unsqueeze(not_finished_1.to(y.dtype), -1)  # this is not really necessary but ensures batch-independence
+        x += step_size * dx
+        if it_counter % 20 == 0:
+            residual = y - sparse_matmul(lin, x); function_evaluations += 1
+        else:
+            residual = residual - step_size * dy  # in-place subtraction affects convergence
+        residual_squared = torch.sum(residual ** 2, -1, keepdim=True)
+        dx = residual - divide_no_nan(torch.sum(residual * dy, dim=1, keepdim=True) * dx, dx_dy)
+        diverged = torch.any(residual_squared / rsq0 > 100, dim=1) & (iterations >= 8)
+        converged = torch.all(residual_squared <= tolerance_sq, dim=1)
+        finished = converged | diverged | (iterations >= max_iter); not_finished_1 = (~finished).to(torch.int32)
+    return x, residual, iterations, function_evaluations, converged, diverged
+
+
+def sparse_matmul(matrix: torch.Tensor, b: torch.Tensor):
+    if matrix.is_sparse or matrix.is_sparse_csr:
+        return torch.transpose(torch.sparse.mm(matrix, torch.transpose(b, 0, 1)), 0, 1)
+    else:
+        return torch.transpose(torch.matmul(matrix, torch.transpose(b, 0, 1)), 0, 1)
+
+
+def divide_no_nan(x: torch.Tensor, y: torch.Tensor):
+    # --- PyTorch backward pass of where produces nan gradients when inf values are present.
+    # Workaround is to avoid zero division by replacing zeros with ones (which then get filtered
+    # in the return where). ---
+    result = x / torch.where(y == 0, torch.ones_like(y), y)
+    result = torch.where(y == 0, torch.zeros_like(result), result)
+    return result
```

### Comparing `phiflow-2.3.4/phi/torch/flow.py` & `phiflow-2.4.0/phi/torch/flow.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,29 +1,29 @@
-# pylint: disable-msg = wildcard-import, unused-wildcard-import, unused-import
-"""
-Standard import for PyTorch mode.
-
-Extends the import `from phi.flow import *` by PyTorch-related functions and modules.
-
-The following PyTorch modules are included: `torch`, *torch.nn.functional* as `torchf`, `optim`.
-
-Importing this module registers the PyTorch backend as the default backend unless called within a backend context.
-New tensors created via `phi.math` functions will be backed by PyTorch tensors.
-
-See `phi.flow`, `phi.tf.flow`, `phi.jax.flow`.
-"""
-
-from phi.flow import *
-from . import TORCH
-
-from . import nets
-from .nets import parameter_count, get_parameters, save_state, load_state, dense_net, u_net, update_weights, adam, conv_net, res_net, sgd, sgd as SGD, rmsprop, adagrad, conv_classifier, invertible_net, fno
-
-import torch
-import torch.nn.functional as torchf
-import torch.optim as optim
-
-if not backend.context_backend():
-    backend.set_global_default_backend(TORCH)
-else:
-    from ..math.backend import PHI_LOGGER as _LOGGER
-    _LOGGER.warning(f"Importing '{__name__}' within a backend context will not set the default backend.")
+# pylint: disable-msg = wildcard-import, unused-wildcard-import, unused-import
+"""
+Standard import for PyTorch mode.
+
+Extends the import `from phi.flow import *` by PyTorch-related functions and modules.
+
+The following PyTorch modules are included: `torch`, *torch.nn.functional* as `torchf`, `optim`.
+
+Importing this module registers the PyTorch backend as the default backend unless called within a backend context.
+New tensors created via `phi.math` functions will be backed by PyTorch tensors.
+
+See `phi.flow`, `phi.tf.flow`, `phi.jax.flow`.
+"""
+
+from phi.flow import *
+from . import TORCH
+
+from . import nets
+from .nets import parameter_count, get_parameters, save_state, load_state, dense_net, u_net, update_weights, adam, conv_net, res_net, sgd, sgd as SGD, rmsprop, adagrad, conv_classifier, invertible_net, fno
+
+import torch
+import torch.nn.functional as torchf
+import torch.optim as optim
+
+if not backend.context_backend():
+    backend.set_global_default_backend(TORCH)
+else:
+    from ..math.backend import PHI_LOGGER as _LOGGER
+    _LOGGER.warning(f"Importing '{__name__}' within a backend context will not set the default backend.")
```

### Comparing `phiflow-2.3.4/phi/torch/nets.py` & `phiflow-2.4.0/phi/torch/nets.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,910 +1,921 @@
-"""
-PyTorch implementation of the unified machine learning API.
-Equivalent functions also exist for the other frameworks.
-
-For API documentation, see https://tum-pbs.github.io/PhiFlow/Network_API .
-"""
-from typing import Callable, Union, Sequence
-
-import numpy
-import numpy as np
-import torch
-import torch.nn as nn
-from torch import optim
-
-from . import TORCH
-from ._torch_backend import register_module_call
-from .. import math
-from ..math import channel
-
-
-def parameter_count(model: nn.Module) -> int:
-    """
-    Counts the number of parameters in a model.
-
-    Args:
-        model: PyTorch model
-
-    Returns:
-        `int`
-    """
-    total = 0
-    for parameter in model.parameters():
-        total += numpy.prod(parameter.shape)
-    return int(total)
-
-
-def get_parameters(net: nn.Module, wrap=True) -> dict:
-    if not wrap:
-        return {name: param for name, param in net.named_parameters()}
-    result = {}
-    for name, param in net.named_parameters():
-        if name.endswith('.weight'):
-            if param.ndim == 2:
-                phi_tensor = math.wrap(param, channel('input,output'))
-            elif param.ndim == 3:
-                phi_tensor = math.wrap(param, channel('x,input,output'))
-            elif param.ndim == 4:
-                phi_tensor = math.wrap(param, channel('x,y,input,output'))
-            elif param.ndim == 5:
-                phi_tensor = math.wrap(param, channel('x,y,z,input,output'))
-        elif name.endswith('.bias'):
-            phi_tensor = math.wrap(param, channel('output'))
-        else:
-            raise NotImplementedError
-        result[name] = phi_tensor
-    return result
-
-
-def save_state(obj: Union[nn.Module, optim.Optimizer], path: str):
-    """
-    Write the state of a module or optimizer to a file.
-
-    See Also:
-        `load_state()`
-
-    Args:
-        obj: `torch.nn.Module or torch.optim.Optimizer`
-        path: File path as `str`.
-    """
-    if not path.endswith('.pth'):
-        path += '.pth'
-    torch.save(obj.state_dict(), path)
-
-
-def load_state(obj: Union[nn.Module, optim.Optimizer], path: str):
-    """
-    Read the state of a module or optimizer from a file.
-
-    See Also:
-        `save_state()`
-
-    Args:
-        obj: `torch.nn.Module or torch.optim.Optimizer`
-        path: File path as `str`.
-    """
-    if not path.endswith('.pth'):
-        path += '.pth'
-    obj.load_state_dict(torch.load(path))
-
-
-def update_weights(net: nn.Module, optimizer: optim.Optimizer, loss_function: Callable, *loss_args, **loss_kwargs):
-    """
-    Computes the gradients of `loss_function` w.r.t. the parameters of `net` and updates its weights using `optimizer`.
-
-    This is the PyTorch version. Analogue functions exist for other learning frameworks.
-
-    Args:
-        net: Learning model.
-        optimizer: Optimizer.
-        loss_function: Loss function, called as `loss_function(*loss_args, **loss_kwargs)`.
-        *loss_args: Arguments given to `loss_function`.
-        **loss_kwargs: Keyword arguments given to `loss_function`.
-
-    Returns:
-        Output of `loss_function`.
-    """
-    optimizer.zero_grad()
-    output = loss_function(*loss_args, **loss_kwargs)
-    loss = output[0] if isinstance(output, tuple) else output
-    loss.sum.backward()
-    optimizer.step()
-    return output
-
-
-def adam(net: nn.Module, learning_rate: float = 1e-3, betas=(0.9, 0.999), epsilon=1e-07):
-    """
-    Creates an Adam optimizer for `net`, alias for [`torch.optim.Adam`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html).
-    Analogue functions exist for other learning frameworks.
-    """
-    return optim.Adam(net.parameters(), learning_rate, betas, epsilon)
-
-
-def sgd(net: nn.Module, learning_rate: float = 1e-3, momentum=0, dampening=0, weight_decay=0, nesterov=False):
-    """
-    Creates an SGD optimizer for 'net', alias for ['torch.optim.SGD'](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)
-    Analogue functions exist for other learning frameworks.
-    """
-    return optim.SGD(net.parameters(), learning_rate, momentum, dampening, weight_decay, nesterov)
-
-
-def adagrad(net: nn.Module, learning_rate: float = 1e-3, lr_decay=0, weight_decay=0, initial_accumulator_value=0,
-            eps=1e-10):
-    """
-    Creates an Adagrad optimizer for 'net', alias for ['torch.optim.Adagrad'](https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html)
-    Analogue functions exist for other learning frameworks.
-    """
-    return optim.Adagrad(net.parameters(), learning_rate, lr_decay, weight_decay, initial_accumulator_value, eps)
-
-
-def rmsprop(net: nn.Module, learning_rate: float = 1e-3, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0,
-            centered=False):
-    """
-    Creates an RMSProp optimizer for 'net', alias for ['torch.optim.RMSprop'](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html)
-    Analogue functions exist for other learning frameworks.
-    """
-    return optim.RMSprop(net.parameters(), learning_rate, alpha, eps, weight_decay, momentum, centered)
-
-
-def _bias0(conv):
-    def initialize(*args, **kwargs):
-        module = conv(*args, **kwargs)
-        module.bias.data.fill_(0)
-        return module
-    return initialize
-
-
-CONV = [None, _bias0(nn.Conv1d), _bias0(nn.Conv2d), _bias0(nn.Conv3d)]
-NORM = [None, nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d]
-ACTIVATIONS = {'ReLU': nn.ReLU, 'Sigmoid': nn.Sigmoid, 'tanh': nn.Tanh, 'SiLU': nn.SiLU, 'GeLU': nn.GELU}
-
-
-def dense_net(in_channels: int,
-              out_channels: int,
-              layers: Sequence[int],
-              batch_norm=False,
-              activation: Union[str, Callable] = 'ReLU',
-              softmax=False) -> nn.Module:
-    """
-    Fully-connected neural networks are available in Φ-Flow via dense_net().
-    Arguments:
-        in_channels : size of input layer, int
-        out_channels = size of output layer, int
-        layers : tuple of linear layers between input and output neurons, list or tuple
-        activation : activation function used within the layers, string
-        batch_norm : use of batch norm after each linear layer, bool
-
-    Returns:
-        Dense net model as specified by input arguments
-    """
-    layers = [in_channels, *layers, out_channels]
-    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
-    net = DenseNet(layers, activation, batch_norm, softmax)
-    return net.to(TORCH.get_default_device().ref)
-
-
-class DenseNet(nn.Module):
-
-    def __init__(self,
-                 layers: list,
-                 activation: type,
-                 batch_norm: bool,
-                 use_softmax: bool):
-        super(DenseNet, self).__init__()
-        self._layers = layers
-        self._activation = activation
-        self._batch_norm = batch_norm
-        for i, (s1, s2) in enumerate(zip(layers[:-2], layers[1:-1])):
-            self.add_module(f'linear{i}', _bias0(nn.Linear)(s1, s2, bias=True))
-            if batch_norm:
-                self.add_module(f'norm{i}', nn.BatchNorm1d(s2))
-        self.add_module(f'linear_out', _bias0(nn.Linear)(layers[-2], layers[-1], bias=True))
-        self.softmax = nn.Softmax() if use_softmax else None
-
-    def forward(self, x):
-        register_module_call(self)
-        x = TORCH.as_tensor(x)
-        for i in range(len(self._layers) - 2):
-            x = self._activation()(getattr(self, f'linear{i}')(x))
-            if self._batch_norm:
-                x = getattr(self, f'norm{i}')(x)
-        x = getattr(self, f'linear_out')(x)
-        if self.softmax:
-            x = self.softmax(x)
-        return x
-
-
-def u_net(in_channels: int,
-          out_channels: int,
-          levels: int = 4,
-          filters: Union[int, tuple, list] = 16,
-          batch_norm: bool = True,
-          activation: Union[str, type] = 'ReLU',
-          in_spatial: Union[tuple, int] = 2,
-          periodic=False,
-          use_res_blocks: bool = False,
-          **kwargs) -> nn.Module:
-    """
-    ΦFlow provides a built-in U-net architecture, classically popular for Semantic Segmentation in Computer Vision, composed of downsampling and upsampling layers.
-
-    Arguments:
-
-        in_channels: input channels of the feature map, dtype : int
-        out_channels : output channels of the feature map, dtype : int
-        levels : number of levels of down-sampling and upsampling, dtype : int
-        filters : filter sizes at each down/up sampling convolutional layer, if the input is integer all conv layers have the same filter size,
-        activation : activation function used within the layers, dtype : string
-        batch_norm : use of batchnorm after each conv layer, dtype : bool
-        in_spatial : spatial dimensions of the input feature map, dtype : int
-        use_res_blocks : use convolutional blocks with skip connections instead of regular convolutional blocks, dtype : bool
-
-    Returns:
-
-        U-net model as specified by input arguments
-
-    """
-    if isinstance(filters, (tuple, list)):
-        assert len(filters) == levels, f"List of filters has length {len(filters)} but u-net has {levels} levels."
-    else:
-        filters = (filters,) * levels
-    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
-    if isinstance(in_spatial, int):
-        d = in_spatial
-    else:
-        assert isinstance(in_spatial, tuple)
-        d = len(in_spatial)
-    net = UNet(d, in_channels, out_channels, filters, batch_norm, activation, periodic, use_res_blocks)
-    return net.to(TORCH.get_default_device().ref)
-
-
-class UNet(nn.Module):
-
-    def __init__(self, d: int, in_channels: int, out_channels: int, filters: tuple, batch_norm: bool, activation: type, periodic: bool, use_res_blocks: bool):
-        super(UNet, self).__init__()
-        self._levels = len(filters)
-        self._spatial_rank = d
-        if use_res_blocks:
-            self.add_module('inc', resnet_block(d, in_channels, filters[0], batch_norm, activation, periodic))
-        else:
-            self.add_module('inc', DoubleConv(d, in_channels, filters[0], filters[0], batch_norm, activation, periodic))
-        for i in range(1, self._levels):
-            self.add_module(f'down{i}', Down(d, filters[i - 1], filters[i], batch_norm, activation, periodic, use_res_blocks))
-            self.add_module(f'up{i}', Up(d, filters[i] + filters[i - 1], filters[i - 1], batch_norm, activation, periodic, use_res_blocks))
-        self.add_module('outc', CONV[d](filters[0], out_channels, kernel_size=1))
-
-    def forward(self, x):
-        register_module_call(self)
-        x = TORCH.as_tensor(x)
-        x = self.inc(x)
-        xs = [x]
-        for i in range(1, self._levels):
-            x = getattr(self, f'down{i}')(x)
-            xs.insert(0, x)
-        for i in range(1, self._levels):
-            x = getattr(self, f'up{i}')(x, xs[i])
-        x = self.outc(x)
-        return x
-
-
-class DoubleConv(nn.Module):
-    """(convolution => [BN] => ReLU) * 2"""
-
-    def __init__(self, d: int, in_channels: int, out_channels: int, mid_channels: int, batch_norm: bool, activation: type, periodic: bool):
-        super().__init__()
-        self.add_module('double_conv', nn.Sequential(
-            CONV[d](in_channels, mid_channels, kernel_size=3, padding=1, padding_mode='circular' if periodic else 'zeros'),
-            NORM[d](mid_channels) if batch_norm else nn.Identity(),
-            activation(),
-            CONV[d](mid_channels, out_channels, kernel_size=3, padding=1, padding_mode='circular' if periodic else 'zeros'),
-            NORM[d](out_channels) if batch_norm else nn.Identity(),
-            nn.ReLU(inplace=True)
-        ))
-
-    def forward(self, x):
-        return self.double_conv(x)
-
-
-MAX_POOL = [None, nn.MaxPool1d, nn.MaxPool2d, nn.MaxPool3d]
-
-
-class Down(nn.Module):
-    """Downscaling with maxpool then double conv or resnet_block"""
-
-    def __init__(self, d: int, in_channels: int, out_channels: int, batch_norm: bool, activation: Union[str, type], use_res_blocks: bool, periodic):
-        super().__init__()
-        self.add_module('maxpool', MAX_POOL[d](2))
-        if use_res_blocks:
-            self.add_module('conv', resnet_block(d, in_channels, out_channels, batch_norm, activation, periodic))
-        else:
-            self.add_module('conv', DoubleConv(d, in_channels, out_channels, out_channels, batch_norm, activation, periodic))
-
-    def forward(self, x):
-        x = self.maxpool(x)
-        return self.conv(x)
-
-
-class Up(nn.Module):
-    """Upscaling then double conv"""
-
-    _MODES = [None, 'linear', 'bilinear', 'trilinear']
-
-    def __init__(self, d: int, in_channels: int, out_channels: int, batch_norm: bool, activation: type, periodic: bool, use_res_blocks: bool):
-        super().__init__()
-        up = nn.Upsample(scale_factor=2, mode=Up._MODES[d])
-        if use_res_blocks:
-            conv = resnet_block(d, in_channels, out_channels, batch_norm, activation, periodic)
-        else:
-            conv = DoubleConv(d, in_channels, out_channels, in_channels // 2, batch_norm, activation, periodic)
-        self.add_module('up', up)
-        self.add_module('conv', conv)
-
-    def forward(self, x1, x2):
-        x1 = self.up(x1)
-        # input is CHW
-        # diff = [x2.size()[i] - x1.size()[i] for i in range(2, len(x1.shape))]
-        # x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,
-        #                 diffY // 2, diffY - diffY // 2])
-        # if you have padding issues, see
-        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a
-        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd
-        x = torch.cat([x2, x1], dim=1)
-        return self.conv(x)
-
-
-class ConvNet(nn.Module):
-
-    def __init__(self, in_spatial, in_channels, out_channels, layers, batch_norm, activation, periodic: bool):
-        super(ConvNet, self).__init__()
-        activation = ACTIVATIONS[activation]
-        if len(layers) < 1:
-            layers.append(out_channels)
-        self.layers = layers
-        self.add_module(f'Conv_in', nn.Sequential(
-            CONV[in_spatial](in_channels, layers[0], kernel_size=3, padding=1, padding_mode='circular' if periodic else 'zeros'),
-            NORM[in_spatial](layers[0]) if batch_norm else nn.Identity(),
-            activation()))
-        for i in range(1, len(layers)):
-            self.add_module(f'Conv{i}', nn.Sequential(
-                CONV[in_spatial](layers[i - 1], layers[i], kernel_size=3, padding=1, padding_mode='circular' if periodic else 'zeros'),
-                NORM[in_spatial](layers[i]) if batch_norm else nn.Identity(),
-                activation()))
-        self.add_module(f'Conv_out', CONV[in_spatial](layers[len(layers) - 1], out_channels, kernel_size=1))
-
-    def forward(self, x):
-        x = getattr(self, f'Conv_in')(x)
-        for i in range(1, len(self.layers)):
-            x = getattr(self, f'Conv{i}')(x)
-        x = getattr(self, f'Conv_out')(x)
-        return x
-
-
-def conv_net(in_channels: int,
-             out_channels: int,
-             layers: Sequence[int],
-             batch_norm: bool = False,
-             activation: Union[str, type] = 'ReLU',
-             in_spatial: Union[int, tuple] = 2,
-             periodic=False,
-             **kwargs) -> nn.Module:
-    """
-    Built in Conv-Nets are also provided. Contrary to the classical convolutional neural networks, the feature map spatial size remains the same throughout the layers. Each layer of the network is essentially a convolutional block comprising of two conv layers. A filter size of 3 is used in the convolutional layers.
-    Arguments:
-
-        in_channels : input channels of the feature map, dtype : int
-        out_channels : output channels of the feature map, dtype : int
-        layers : list or tuple of output channels for each intermediate layer between the input and final output channels, dtype : list or tuple
-        activation : activation function used within the layers, dtype : string
-        batch_norm : use of batchnorm after each conv layer, dtype : bool
-        in_spatial : spatial dimensions of the input feature map, dtype : int
-
-    Returns:
-
-        Conv-net model as specified by input arguments
-    """
-    if isinstance(in_spatial, int):
-        d = in_spatial
-    else:
-        assert isinstance(in_spatial, tuple)
-        d = len(in_spatial)
-    net = ConvNet(d, in_channels, out_channels, layers, batch_norm, activation, periodic)
-    net = net.to(TORCH.get_default_device().ref)
-    return net
-
-
-class resnet_block(nn.Module):
-
-    def __init__(self, in_spatial, in_channels, out_channels, batch_norm, activation, periodic: bool):
-        # Since in_channels and out_channels might be different
-        # we need a sampling layer for up/down sampling input
-        # in order to add it as a skip connection
-        super(resnet_block, self).__init__()
-        if in_channels != out_channels:
-            self.sample_input = CONV[in_spatial](in_channels, out_channels, kernel_size=1, padding=0)
-            self.bn_sample = NORM[in_spatial](out_channels) if batch_norm else nn.Identity()
-        else:
-            self.sample_input = nn.Identity()
-            self.bn_sample = nn.Identity()
-        self.activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
-        self.bn1 = NORM[in_spatial](out_channels) if batch_norm else nn.Identity()
-        self.conv1 = CONV[in_spatial](in_channels, out_channels, kernel_size=3, padding=1, padding_mode='circular' if periodic else 'zeros')
-        self.bn2 = NORM[in_spatial](out_channels) if batch_norm else nn.Identity()
-        self.conv2 = CONV[in_spatial](out_channels, out_channels, kernel_size=3, padding=1, padding_mode='circular' if periodic else 'zeros')
-
-    def forward(self, x):
-        x = TORCH.as_tensor(x)
-        out = self.activation()(self.bn1(self.conv1(x)))
-        out = self.activation()(self.bn2(self.conv2(out)))
-        out = (out + self.bn_sample(self.sample_input(x)))
-        return out
-
-
-class Dense_resnet_block(nn.Module):
-
-    def __init__(self, in_channels, mid_channels, batch_norm, activation):
-        super(Dense_resnet_block, self).__init__()
-        self.activation = activation
-        self.bn1 = NORM[1](in_channels) if batch_norm else nn.Identity()
-        self.linear1 = _bias0(nn.Linear)(in_channels, mid_channels)
-        self.bn2 = NORM[1](mid_channels) if batch_norm else nn.Identity()
-        self.linear2 = _bias0(nn.Linear)(mid_channels, in_channels)
-
-    def forward(self, x):
-        x = TORCH.as_tensor(x)
-        out = self.activation()(self.bn1(self.linear1(x)))
-        out = self.activation()(self.bn2(self.linear2(out)))
-        out = out + x
-        return out
-
-
-def get_mask(inputs, reverse_mask, data_format='NHWC'):
-    """ Compute mask for slicing input feature map for Invertible Nets """
-    shape = inputs.shape
-    if len(shape) == 2:
-        N = shape[-1]
-        range_n = torch.arange(0, N)
-        even_ind = range_n % 2
-        checker = torch.reshape(even_ind, (-1, N))
-    elif len(shape) == 4:
-        H = shape[2] if data_format == 'NCHW' else shape[1]
-        W = shape[3] if data_format == 'NCHW' else shape[2]
-
-        range_h = torch.arange(0, H)
-        range_w = torch.arange(0, W)
-
-        even_ind_h = range_h % 2
-        even_ind_w = range_w % 2
-
-        ind_h = even_ind_h.unsqueeze(-1).repeat(1, W)
-        ind_w = even_ind_w.unsqueeze(0).repeat(H, 1)
-
-        checker = torch.logical_xor(ind_h, ind_w)
-
-        checker = checker.reshape(1, 1, H, W) if data_format == 'NCHW' else checker.reshape(1, H, W, 1)
-        checker = checker.long()
-
-    else:
-        raise ValueError('Invalid tensor shape. Dimension of the tensor shape must be '
-                         '2 (NxD) or 4 (NxCxHxW or NxHxWxC), got {}.'.format(inputs.get_shape().as_list()))
-
-    if reverse_mask:
-        checker = 1 - checker
-
-    return checker.to(TORCH.get_default_device().ref)
-
-
-class ResNet(nn.Module):
-
-    def __init__(self, in_spatial, in_channels, out_channels, layers, batch_norm, activation, periodic: bool):
-        super(ResNet, self).__init__()
-        self.layers = layers
-        if len(self.layers) < 1:
-            layers.append(out_channels)
-        self.add_module('Res_in', resnet_block(in_spatial, in_channels, layers[0], batch_norm, activation, periodic))
-        for i in range(1, len(layers)):
-            self.add_module(f'Res{i}', resnet_block(in_spatial, layers[i - 1], layers[i], batch_norm, activation, periodic))
-        self.add_module('Res_out', CONV[in_spatial](layers[len(layers) - 1], out_channels, kernel_size=1))
-
-    def forward(self, x):
-        x = TORCH.as_tensor(x)
-        x = getattr(self, 'Res_in')(x)
-        for i in range(1, len(self.layers)):
-            x = getattr(self, f'Res{i}')(x)
-        x = getattr(self, 'Res_out')(x)
-        return x
-
-
-def res_net(in_channels: int,
-            out_channels: int,
-            layers: Sequence[int],
-            batch_norm: bool = False,
-            activation: Union[str, type] = 'ReLU',
-            in_spatial: Union[int, tuple] = 2,
-            periodic=False,
-            **kwargs) -> nn.Module:
-    """
-    Built in Res-Nets are provided in the ΦFlow framework. Similar to the conv-net, the feature map spatial size remains the same throughout the layers.
-    These networks use residual blocks composed of two conv layers with a skip connection added from the input to the output feature map.
-    A default filter size of 3 is used in the convolutional layers.
-
-    Arguments:
-
-        in_channels : input channels of the feature map, dtype : int
-        out_channels : output channels of the feature map, dtype : int
-        layers : list or tuple of output channels for each intermediate layer between the input and final output channels, dtype : list or tuple
-        activation : activation function used within the layers, dtype : string
-        batch_norm : use of batchnorm after each conv layer, dtype : bool
-        in_spatial : spatial dimensions of the input feature map, dtype : int
-
-    Returns:
-
-        Res-net model as specified by input arguments
-
-    """
-    if (isinstance(in_spatial, int)):
-        d = in_spatial
-    else:
-        assert isinstance(in_spatial, tuple)
-        d = len(in_spatial)
-    net = ResNet(d, in_channels, out_channels, layers, batch_norm, activation, periodic)
-    net = net.to(TORCH.get_default_device().ref)
-    return net
-
-
-def conv_classifier(in_features: int,
-                    in_spatial: Union[tuple, list],
-                    num_classes: int,
-                    blocks=(64, 128, 256, 256, 512, 512),
-                    dense_layers=(4096, 4096, 100),
-                    batch_norm=True,
-                    activation='ReLU',
-                    softmax=True,
-                    periodic=False):
-    """
-    Based on VGG16.
-    """
-    assert isinstance(in_spatial, (tuple, list))
-    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
-    net = ConvClassifier(in_features, in_spatial, num_classes, batch_norm, softmax, blocks, dense_layers, periodic, activation)
-    return net.to(TORCH.get_default_device().ref)
-
-
-class ConvClassifier(nn.Module):
-
-    def __init__(self, in_features, in_spatial: list, num_classes: int, batch_norm: bool, use_softmax: bool, blocks: tuple, dense_layers: tuple, periodic: bool, activation):
-        super(ConvClassifier, self).__init__()
-        d = len(in_spatial)
-        self.in_spatial = in_spatial
-        self._blocks = blocks
-        self.add_module('maxpool', MAX_POOL[d](2))
-        for i, (prev, next) in enumerate(zip((in_features,) + tuple(blocks[:-1]), blocks)):
-            if i in (0, 1):
-                conv = DoubleConv(d, prev, next, next, batch_norm, activation, periodic)
-            else:
-                conv = nn.Sequential(DoubleConv(d, prev, next, next, batch_norm, activation, periodic),
-                                     CONV[d](next, next, 3, padding=1, padding_mode='circular' if periodic else 'zeros'),
-                                     NORM[d](next) if batch_norm else nn.Identity(),
-                                     activation())
-            self.add_module(f'conv{i+1}', conv)
-        flat_size = int(np.prod(in_spatial) * blocks[-1] / (2**d) ** len(blocks))
-        self.dense_net = dense_net(flat_size, num_classes, dense_layers, batch_norm, activation, use_softmax)
-        self.flatten = nn.Flatten()
-
-    def forward(self, x):
-        for i in range(len(self._blocks)):
-            x = getattr(self, f'conv{i+1}')(x)
-            x = self.maxpool(x)
-        x = self.flatten(x)
-        x = self.dense_net(x)
-        return x
-
-
-NET = {'u_net': u_net, 'res_net': res_net, 'conv_net': conv_net}
-
-
-class CouplingLayer(nn.Module):
-
-    def __init__(self, in_channels, activation, batch_norm, in_spatial, net, reverse_mask):
-        super(CouplingLayer, self).__init__()
-
-        self.activation = activation
-        self.batch_norm = batch_norm
-        self.reverse_mask = reverse_mask
-
-        if in_spatial == 0:  # for in_spatial = 0, use dense layers
-            self.s1 = nn.Sequential(Dense_resnet_block(in_channels, in_channels, batch_norm, activation),
-                                    torch.nn.Tanh())
-            self.t1 = Dense_resnet_block(in_channels, in_channels, batch_norm, activation)
-
-            self.s2 = nn.Sequential(Dense_resnet_block(in_channels, in_channels, batch_norm, activation),
-                                    torch.nn.Tanh())
-            self.t2 = Dense_resnet_block(in_channels, in_channels, batch_norm, activation)
-        else:
-            self.s1 = nn.Sequential(NET[net](in_channels=in_channels, out_channels=in_channels,
-                                             layers=[], batch_norm=batch_norm, activation=activation,
-                                             in_spatial=in_spatial), torch.nn.Tanh())
-            self.t1 = NET[net](in_channels=in_channels, out_channels=in_channels,
-                               layers=[], batch_norm=batch_norm, activation=activation,
-                               in_spatial=in_spatial)
-            self.s2 = nn.Sequential(NET[net](in_channels=in_channels, out_channels=in_channels,
-                                             layers=[], batch_norm=batch_norm, activation=activation,
-                                             in_spatial=in_spatial), torch.nn.Tanh())
-            self.t2 = NET[net](in_channels=in_channels, out_channels=in_channels,
-                               layers=[], batch_norm=batch_norm, activation=activation,
-                               in_spatial=in_spatial)
-
-    def forward(self, x, invert=False):
-        x = TORCH.as_tensor(x)
-        mask = get_mask(x, self.reverse_mask, 'NCHW')
-        if invert:
-            v1 = x * mask
-            v2 = x * (1 - mask)
-            u2 = (1 - mask) * (v2 - self.t1(v1)) * torch.exp(-self.s1(v1))
-            u1 = mask * (v1 - self.t2(u2)) * torch.exp(-self.s2(u2))
-            return u1 + u2
-        else:
-            u1 = x * mask
-            u2 = x * (1 - mask)
-            v1 = mask * (u1 * torch.exp(self.s2(u2)) + self.t2(u2))
-            v2 = (1 - mask) * (u2 * torch.exp(self.s1(v1)) + self.t1(v1))
-            return v1 + v2
-
-
-class InvertibleNet(nn.Module):
-    def __init__(self, in_channels, num_blocks, activation, batch_norm, in_spatial, net):
-        super(InvertibleNet, self).__init__()
-        self.num_blocks = num_blocks
-        for i in range(num_blocks):
-            self.add_module(f'coupling_block{i + 1}',
-                            CouplingLayer(in_channels, activation,
-                                          batch_norm, in_spatial, net, (i % 2 == 0)))
-
-    def forward(self, x, backward=False):
-        if backward:
-            for i in range(self.num_blocks, 0, -1):
-                x = getattr(self, f'coupling_block{i}')(x, backward)
-        else:
-            for i in range(1, self.num_blocks + 1):
-                x = getattr(self, f'coupling_block{i}')(x, backward)
-        return x
-
-
-def invertible_net(in_channels: int,
-                   num_blocks: int,
-                   batch_norm: bool = False,
-                   net: str = 'u_net',
-                   activation: Union[str, type] = 'ReLU',
-                   in_spatial: Union[tuple, int] = 2, **kwargs):
-    """
-    Phiflow also provides invertible neural networks that are capable of inverting the output tensor back to the input tensor initially passed.\ These networks have far reaching applications in predicting input parameters of a problem given its observations.\ Invertible nets are composed of multiple concatenated coupling blocks wherein each such block consists of arbitrary neural networks.
-
-    Currently, these arbitrary neural networks could be set to u_net(default), conv_net, res_net or dense_net blocks with in_channels = out_channels.
-    The architecture used is popularized by ["Real NVP"](https://arxiv.org/abs/1605.08803).
-
-    Arguments:
-
-        in_channels : input channels of the feature map, dtype : int
-        num_blocks : number of coupling blocks inside the invertible net, dtype : int
-        activation : activation function used within the layers, dtype : string
-        batch_norm : use of batchnorm after each layer, dtype : bool
-        in_spatial : spatial dimensions of the input feature map, dtype : int
-        net : type of neural network blocks used in coupling layers, dtype : str
-        **kwargs : placeholder for arguments not supported by the function
-
-    Returns:
-
-        Invertible Net model as specified by input arguments
-
-    Note: Currently supported values for net are 'u_net'(default), 'conv_net' and 'res_net'.
-    For choosing 'dense_net' as the network block in coupling layers in_spatial must be set to zero.
-    """
-    if isinstance(in_spatial, tuple):
-        in_spatial = len(in_spatial)
-
-    return InvertibleNet(in_channels, num_blocks, activation, batch_norm, in_spatial, net).to(TORCH.get_default_device().ref)
-
-
-def coupling_layer(in_channels: int,
-                   activation: Union[str, type] = 'ReLU',
-                   batch_norm=False,
-                   reverse_mask=False,
-                   in_spatial: Union[tuple, int] = 2):
-    if isinstance(in_spatial, tuple):
-        in_spatial = len(in_spatial)
-
-    net = CouplingLayer(in_channels, activation, batch_norm, in_spatial, reverse_mask)
-    net = net.to(TORCH.get_default_device().ref)
-    return net
-
-
-##################################################################################################################
-#  Fourier Neural Operators
-#  source: https://github.com/zongyi-li/fourier_neural_operator
-###################################################################################################################
-
-class SpectralConv(nn.Module):
-
-    def __init__(self, in_channels, out_channels, modes, in_spatial):
-
-        super(SpectralConv, self).__init__()
-
-        self.in_channels = in_channels
-        self.out_channels = out_channels
-        self.in_spatial = in_spatial
-        assert in_spatial >= 1 and in_spatial <= 3
-        if isinstance(modes, int):
-            mode = modes
-            modes = [mode for i in range(in_spatial)]
-
-        self.scale = 1 / (in_channels * out_channels)
-
-        self.modes = {i + 1: modes[i] for i in range(len(modes))}
-        self.weights = {}
-
-        rand_shape = [in_channels, out_channels]
-        rand_shape += [self.modes[i] for i in range(1, in_spatial + 1)]
-
-        for i in range(2 ** (in_spatial - 1)):
-            self.weights[f'w{i + 1}'] = nn.Parameter(self.scale * torch.randn(rand_shape, dtype=torch.cfloat))
-            # print('TORCH self.weights:', self.weights_[f'w{i + 1}'].shape)
-            # print(self.weights[f'w{i + 1}'].shape)
-
-    def complex_mul(self, input, weights):
-
-        # print(input.shape)
-        # print(weights.shape)
-        # exit(1)
-        if self.in_spatial == 1:
-            return torch.einsum("bix,iox->box", input, weights)
-        elif self.in_spatial == 2:
-            return torch.einsum("bixy,ioxy->boxy", input, weights)
-        elif self.in_spatial == 3:
-            return torch.einsum("bixyz,ioxyz->boxyz", input, weights)
-
-    def forward(self, x):
-        batch_size = x.shape[0]
-
-        # print('x.shape:', x.shape)
-        ##Convert to Fourier space
-        dims = [-i for i in range(self.in_spatial, 0, -1)]
-        x_ft = torch.fft.rfftn(x, dim=dims)
-        # print('After RFFT torch', x_ft.shape)
-        outft_dims = [batch_size, self.out_channels] + \
-                     [x.size(-i) for i in range(self.in_spatial, 1, -1)] + [x.size(-1) // 2 + 1]
-        out_ft = torch.zeros(outft_dims, dtype=torch.cfloat, device=x.device)
-        # print('outft shape before', out_ft.shape)
-        ##Multiply relevant fourier modes
-        if self.in_spatial == 1:
-            out_ft[:, :, :self.modes[1]] = \
-                self.complex_mul(x_ft[:, :, :self.modes[1]],
-                                 self.weights['w1'].to(x_ft.device))
-        elif self.in_spatial == 2:
-            out_ft[:, :, :self.modes[1], :self.modes[2]] = \
-                self.complex_mul(x_ft[:, :, :self.modes[1], :self.modes[2]],
-                                 self.weights['w1'].to(x_ft.device))
-            out_ft[:, :, -self.modes[1]:, :self.modes[2]] = \
-                self.complex_mul(x_ft[:, :, -self.modes[1]:, :self.modes[2]],
-                                 self.weights['w2'].to(x_ft.device))
-        elif self.in_spatial == 3:
-            out_ft[:, :, :self.modes[1], :self.modes[2], :self.modes[3]] = \
-                self.complex_mul(x_ft[:, :, :self.modes[1], :self.modes[2], :self.modes[3]],
-                                 self.weights['w1'].to(x_ft.device))
-            out_ft[:, :, -self.modes[1]:, :self.modes[2], :self.modes[3]] = \
-                self.complex_mul(x_ft[:, :, -self.modes[1]:, :self.modes[2], :self.modes[3]],
-                                 self.weights['w2'].to(x_ft.device))
-            out_ft[:, :, :self.modes[1], -self.modes[2]:, :self.modes[3]] = \
-                self.complex_mul(x_ft[:, :, :self.modes[1], -self.modes[2]:, :self.modes[3]],
-                                 self.weights['w3'].to(x_ft.device))
-            out_ft[:, :, -self.modes[1]:, -self.modes[2]:, :self.modes[3]] = \
-                self.complex_mul(x_ft[:, :, -self.modes[1]:, -self.modes[2]:, :self.modes[3]],
-                                 self.weights['w4'].to(x_ft.device))
-
-        ##Return to Physical Space
-        x = torch.fft.irfftn(out_ft, s=[x.size(-i) for i in range(self.in_spatial, 0, -1)])
-        return x
-
-
-class FNO(nn.Module):
-
-    def __init__(self, in_channels, out_channels, width, modes, activation, batch_norm, in_spatial):
-        super(FNO, self).__init__()
-
-        """
-        The overall network contains 4 layers of the ["Fourier layer"](https://github.com/zongyi-li/fourier_neural_operator).
-        1. Lift the input to the desire channel dimension by self.fc0 .
-        2. 4 layers of the integral operators u' = (W + K)(u).
-            W defined by self.w; K defined by self.conv .
-        3. Project from the channel space to the output space by self.fc1 and self.fc2.
-        
-        input shape and output shape: (batchsize b, channels c, *spatial)
-        """
-
-        self.activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
-        self.width = width
-        self.in_spatial = in_spatial
-
-        self.fc0 = _bias0(nn.Linear)(in_channels + in_spatial, self.width)
-
-        for i in range(4):
-            self.add_module(f'conv{i}', SpectralConv(self.width, self.width, modes, in_spatial))
-            self.add_module(f'w{i}', CONV[in_spatial](self.width, self.width, kernel_size=1))
-            self.add_module(f'bn{i}', NORM[in_spatial](self.width) if batch_norm else nn.Identity())
-
-        self.fc1 = _bias0(nn.Linear)(self.width, 128)
-        self.fc2 = _bias0(nn.Linear)(128, out_channels)
-
-    # Adding extra spatial channels eg. x, y, z, .... to input x
-    def get_grid(self, shape, device):
-        batch_size = shape[0]
-        grid_channel_sizes = shape[2:]  # shape =  (batch_size, channels, *spatial)
-        self.grid_channels = {}
-        for i in range(self.in_spatial):
-            self.grid_channels[f'dim{i}'] = torch.tensor(torch.linspace(0, 1, grid_channel_sizes[i]),
-                                                         dtype=torch.float)
-            reshape_dim_tuple = [1, 1] + [1 if i != j else grid_channel_sizes[j] for j in range(self.in_spatial)]
-            repeat_dim_tuple = [batch_size, 1] + [1 if i == j else grid_channel_sizes[j] for j in
-                                                  range(self.in_spatial)]
-            self.grid_channels[f'dim{i}'] = self.grid_channels[f'dim{i}'].reshape(reshape_dim_tuple) \
-                .repeat(repeat_dim_tuple)
-
-        return torch.cat([self.grid_channels[f'dim{i}'] for i in range(self.in_spatial)], dim=1).to(device)
-
-    def forward(self, x):
-        grid = self.get_grid(x.shape, x.device)
-        x = torch.cat([x, grid], dim=1)
-
-        permute_tuple = [0] + [2 + i for i in range(self.in_spatial)] + [1]
-        permute_tuple_reverse = [0] + [self.in_spatial + 1] + [i + 1 for i in range(self.in_spatial)]
-
-        # Transpose x such that channels shape lies at the end to pass it through linear layers
-        x = x.permute(permute_tuple)
-
-        x = self.fc0(x)
-
-        # Transpose x back to its original shape to pass it through convolutional layers
-        x = x.permute(permute_tuple_reverse)
-
-        for i in range(4):
-            x1 = getattr(self, f'w{i}')(x)
-            x2 = getattr(self, f'conv{i}')(x)
-            x = getattr(self, f'bn{i}')(x1) + getattr(self, f'bn{i}')(x2)
-            x = self.activation()(x)
-
-        x = x.permute(permute_tuple)
-        x = self.activation()(self.fc1(x))
-        x = self.fc2(x)
-
-        x = x.permute(permute_tuple_reverse)
-
-        return x
-
-
-def fno(in_channels: int,
-        out_channels: int,
-        mid_channels: int,
-        modes: Sequence[int],
-        activation: Union[str, type] = 'ReLU',
-        batch_norm: bool = False,
-        in_spatial: int = 2):
-    """
-    ["Fourier Neural Operator"](https://github.com/zongyi-li/fourier_neural_operator) network contains 4 layers of the Fourier layer.
-    1. Lift the input to the desire channel dimension by self.fc0 .
-    2. 4 layers of the integral operators u' = (W + K)(u). W defined by self.w; K defined by self.conv .
-    3. Project from the channel space to the output space by self.fc1 and self.fc2.
-
-    Arguments:
-
-        in_channels : input channels of the feature map, dtype : int
-        out_channels : output channels of the feature map, dtype : int
-        mid_channels : channels used in Spectral Convolution Layers, dtype : int
-        modes : Fourier modes for each spatial channel, dtype : List[int] or int (in case all number modes are to be the same for each spatial channel)
-        activation : activation function used within the layers, dtype : string
-        batch_norm : use of batchnorm after each conv layer, dtype : bool
-        in_spatial : spatial dimensions of the input feature map, dtype : int
-
-    Returns:
-
-        Fourier Neural Operator model as specified by input arguments.
-    """
-    net = FNO(in_channels, out_channels, mid_channels, modes, activation, batch_norm, in_spatial)
-    net = net.to(TORCH.get_default_device().ref)
-    return net
+"""
+PyTorch implementation of the unified machine learning API.
+Equivalent functions also exist for the other frameworks.
+
+For API documentation, see https://tum-pbs.github.io/PhiFlow/Network_API .
+"""
+from typing import Callable, Union, Sequence
+
+import numpy
+import numpy as np
+import torch
+import torch.nn as nn
+from torch import optim
+
+from . import TORCH
+from ._torch_backend import register_module_call
+from .. import math
+from ..math import channel
+
+
+def parameter_count(model: nn.Module) -> int:
+    """
+    Counts the number of parameters in a model.
+
+    Args:
+        model: PyTorch model
+
+    Returns:
+        `int`
+    """
+    total = 0
+    for parameter in model.parameters():
+        total += numpy.prod(parameter.shape)
+    return int(total)
+
+
+def get_parameters(net: nn.Module, wrap=True) -> dict:
+    if not wrap:
+        return {name: param for name, param in net.named_parameters()}
+    result = {}
+    for name, param in net.named_parameters():
+        if name.endswith('.weight'):
+            if param.ndim == 2:
+                phi_tensor = math.wrap(param, channel('input,output'))
+            elif param.ndim == 3:
+                phi_tensor = math.wrap(param, channel('x,input,output'))
+            elif param.ndim == 4:
+                phi_tensor = math.wrap(param, channel('x,y,input,output'))
+            elif param.ndim == 5:
+                phi_tensor = math.wrap(param, channel('x,y,z,input,output'))
+        elif name.endswith('.bias'):
+            phi_tensor = math.wrap(param, channel('output'))
+        else:
+            raise NotImplementedError
+        result[name] = phi_tensor
+    return result
+
+
+def save_state(obj: Union[nn.Module, optim.Optimizer], path: str):
+    """
+    Write the state of a module or optimizer to a file.
+
+    See Also:
+        `load_state()`
+
+    Args:
+        obj: `torch.nn.Module or torch.optim.Optimizer`
+        path: File path as `str`.
+    """
+    if not path.endswith('.pth'):
+        path += '.pth'
+    torch.save(obj.state_dict(), path)
+
+
+def load_state(obj: Union[nn.Module, optim.Optimizer], path: str):
+    """
+    Read the state of a module or optimizer from a file.
+
+    See Also:
+        `save_state()`
+
+    Args:
+        obj: `torch.nn.Module or torch.optim.Optimizer`
+        path: File path as `str`.
+    """
+    if not path.endswith('.pth'):
+        path += '.pth'
+    obj.load_state_dict(torch.load(path))
+
+
+def update_weights(net: nn.Module, optimizer: optim.Optimizer, loss_function: Callable, *loss_args, check_nan=False, **loss_kwargs):
+    """
+    Computes the gradients of `loss_function` w.r.t. the parameters of `net` and updates its weights using `optimizer`.
+
+    This is the PyTorch version. Analogue functions exist for other learning frameworks.
+
+    Args:
+        net: Learning model.
+        optimizer: Optimizer.
+        loss_function: Loss function, called as `loss_function(*loss_args, **loss_kwargs)`.
+        *loss_args: Arguments given to `loss_function`.
+        **loss_kwargs: Keyword arguments given to `loss_function`.
+
+    Returns:
+        Output of `loss_function`.
+    """
+    optimizer.zero_grad()
+    output = loss_function(*loss_args, **loss_kwargs)
+    loss = output[0] if isinstance(output, tuple) else output
+    loss.sum.backward()
+    if isinstance(optimizer, optim.LBFGS):
+        def closure():
+            result = loss_function(*loss_args, **loss_kwargs)
+            loss_val = result[0] if isinstance(result, tuple) else result
+            return loss_val.sum
+        optimizer.step(closure=closure)
+    else:
+        if check_nan:
+            for p in net.parameters():
+                if not torch.all(torch.isfinite(p.grad)):
+                    raise RuntimeError(f"NaN in network gradient detected. Parameter: {p}")
+        optimizer.step()
+    return output
+
+
+def adam(net: nn.Module, learning_rate: float = 1e-3, betas=(0.9, 0.999), epsilon=1e-07):
+    """
+    Creates an Adam optimizer for `net`, alias for [`torch.optim.Adam`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html).
+    Analogue functions exist for other learning frameworks.
+    """
+    return optim.Adam(net.parameters(), learning_rate, betas, epsilon)
+
+
+def sgd(net: nn.Module, learning_rate: float = 1e-3, momentum=0, dampening=0, weight_decay=0, nesterov=False):
+    """
+    Creates an SGD optimizer for 'net', alias for ['torch.optim.SGD'](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)
+    Analogue functions exist for other learning frameworks.
+    """
+    return optim.SGD(net.parameters(), learning_rate, momentum, dampening, weight_decay, nesterov)
+
+
+def adagrad(net: nn.Module, learning_rate: float = 1e-3, lr_decay=0, weight_decay=0, initial_accumulator_value=0,
+            eps=1e-10):
+    """
+    Creates an Adagrad optimizer for 'net', alias for ['torch.optim.Adagrad'](https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html)
+    Analogue functions exist for other learning frameworks.
+    """
+    return optim.Adagrad(net.parameters(), learning_rate, lr_decay, weight_decay, initial_accumulator_value, eps)
+
+
+def rmsprop(net: nn.Module, learning_rate: float = 1e-3, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0,
+            centered=False):
+    """
+    Creates an RMSProp optimizer for 'net', alias for ['torch.optim.RMSprop'](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html)
+    Analogue functions exist for other learning frameworks.
+    """
+    return optim.RMSprop(net.parameters(), learning_rate, alpha, eps, weight_decay, momentum, centered)
+
+
+def _bias0(conv):
+    def initialize(*args, **kwargs):
+        module = conv(*args, **kwargs)
+        module.bias.data.fill_(0)
+        return module
+    return initialize
+
+
+CONV = [None, _bias0(nn.Conv1d), _bias0(nn.Conv2d), _bias0(nn.Conv3d)]
+NORM = [None, nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d]
+ACTIVATIONS = {'ReLU': nn.ReLU, 'Sigmoid': nn.Sigmoid, 'tanh': nn.Tanh, 'SiLU': nn.SiLU, 'GeLU': nn.GELU}
+
+
+def dense_net(in_channels: int,
+              out_channels: int,
+              layers: Sequence[int],
+              batch_norm=False,
+              activation: Union[str, Callable] = 'ReLU',
+              softmax=False) -> nn.Module:
+    """
+    Fully-connected neural networks are available in Φ-Flow via dense_net().
+    Arguments:
+        in_channels : size of input layer, int
+        out_channels = size of output layer, int
+        layers : tuple of linear layers between input and output neurons, list or tuple
+        activation : activation function used within the layers, string
+        batch_norm : use of batch norm after each linear layer, bool
+
+    Returns:
+        Dense net model as specified by input arguments
+    """
+    layers = [in_channels, *layers, out_channels]
+    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
+    net = DenseNet(layers, activation, batch_norm, softmax)
+    return net.to(TORCH.get_default_device().ref)
+
+
+class DenseNet(nn.Module):
+
+    def __init__(self,
+                 layers: list,
+                 activation: type,
+                 batch_norm: bool,
+                 use_softmax: bool):
+        super(DenseNet, self).__init__()
+        self._layers = layers
+        self._activation = activation
+        self._batch_norm = batch_norm
+        for i, (s1, s2) in enumerate(zip(layers[:-2], layers[1:-1])):
+            self.add_module(f'linear{i}', _bias0(nn.Linear)(s1, s2, bias=True))
+            if batch_norm:
+                self.add_module(f'norm{i}', nn.BatchNorm1d(s2))
+        self.add_module(f'linear_out', _bias0(nn.Linear)(layers[-2], layers[-1], bias=True))
+        self.softmax = nn.Softmax() if use_softmax else None
+
+    def forward(self, x):
+        register_module_call(self)
+        x = TORCH.as_tensor(x)
+        for i in range(len(self._layers) - 2):
+            x = self._activation()(getattr(self, f'linear{i}')(x))
+            if self._batch_norm:
+                x = getattr(self, f'norm{i}')(x)
+        x = getattr(self, f'linear_out')(x)
+        if self.softmax:
+            x = self.softmax(x)
+        return x
+
+
+def u_net(in_channels: int,
+          out_channels: int,
+          levels: int = 4,
+          filters: Union[int, tuple, list] = 16,
+          batch_norm: bool = True,
+          activation: Union[str, type] = 'ReLU',
+          in_spatial: Union[tuple, int] = 2,
+          periodic=False,
+          use_res_blocks: bool = False,
+          **kwargs) -> nn.Module:
+    """
+    ΦFlow provides a built-in U-net architecture, classically popular for Semantic Segmentation in Computer Vision, composed of downsampling and upsampling layers.
+
+    Arguments:
+
+        in_channels: input channels of the feature map, dtype : int
+        out_channels : output channels of the feature map, dtype : int
+        levels : number of levels of down-sampling and upsampling, dtype : int
+        filters : filter sizes at each down/up sampling convolutional layer, if the input is integer all conv layers have the same filter size,
+        activation : activation function used within the layers, dtype : string
+        batch_norm : use of batchnorm after each conv layer, dtype : bool
+        in_spatial : spatial dimensions of the input feature map, dtype : int
+        use_res_blocks : use convolutional blocks with skip connections instead of regular convolutional blocks, dtype : bool
+
+    Returns:
+
+        U-net model as specified by input arguments
+
+    """
+    if isinstance(filters, (tuple, list)):
+        assert len(filters) == levels, f"List of filters has length {len(filters)} but u-net has {levels} levels."
+    else:
+        filters = (filters,) * levels
+    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
+    if isinstance(in_spatial, int):
+        d = in_spatial
+    else:
+        assert isinstance(in_spatial, tuple)
+        d = len(in_spatial)
+    net = UNet(d, in_channels, out_channels, filters, batch_norm, activation, periodic, use_res_blocks)
+    return net.to(TORCH.get_default_device().ref)
+
+
+class UNet(nn.Module):
+
+    def __init__(self, d: int, in_channels: int, out_channels: int, filters: tuple, batch_norm: bool, activation: type, periodic: bool, use_res_blocks: bool):
+        super(UNet, self).__init__()
+        self._levels = len(filters)
+        self._spatial_rank = d
+        if use_res_blocks:
+            self.add_module('inc', resnet_block(d, in_channels, filters[0], batch_norm, activation, periodic))
+        else:
+            self.add_module('inc', DoubleConv(d, in_channels, filters[0], filters[0], batch_norm, activation, periodic))
+        for i in range(1, self._levels):
+            self.add_module(f'down{i}', Down(d, filters[i - 1], filters[i], batch_norm, activation, periodic, use_res_blocks))
+            self.add_module(f'up{i}', Up(d, filters[i] + filters[i - 1], filters[i - 1], batch_norm, activation, periodic, use_res_blocks))
+        self.add_module('outc', CONV[d](filters[0], out_channels, kernel_size=1))
+
+    def forward(self, x):
+        register_module_call(self)
+        x = TORCH.as_tensor(x)
+        x = self.inc(x)
+        xs = [x]
+        for i in range(1, self._levels):
+            x = getattr(self, f'down{i}')(x)
+            xs.insert(0, x)
+        for i in range(1, self._levels):
+            x = getattr(self, f'up{i}')(x, xs[i])
+        x = self.outc(x)
+        return x
+
+
+class DoubleConv(nn.Module):
+    """(convolution => [BN] => ReLU) * 2"""
+
+    def __init__(self, d: int, in_channels: int, out_channels: int, mid_channels: int, batch_norm: bool, activation: type, periodic: bool):
+        super().__init__()
+        self.add_module('double_conv', nn.Sequential(
+            CONV[d](in_channels, mid_channels, kernel_size=3, padding=1, padding_mode='circular' if periodic else 'zeros'),
+            NORM[d](mid_channels) if batch_norm else nn.Identity(),
+            activation(),
+            CONV[d](mid_channels, out_channels, kernel_size=3, padding=1, padding_mode='circular' if periodic else 'zeros'),
+            NORM[d](out_channels) if batch_norm else nn.Identity(),
+            nn.ReLU(inplace=True)
+        ))
+
+    def forward(self, x):
+        return self.double_conv(x)
+
+
+MAX_POOL = [None, nn.MaxPool1d, nn.MaxPool2d, nn.MaxPool3d]
+
+
+class Down(nn.Module):
+    """Downscaling with maxpool then double conv or resnet_block"""
+
+    def __init__(self, d: int, in_channels: int, out_channels: int, batch_norm: bool, activation: Union[str, type], use_res_blocks: bool, periodic):
+        super().__init__()
+        self.add_module('maxpool', MAX_POOL[d](2))
+        if use_res_blocks:
+            self.add_module('conv', resnet_block(d, in_channels, out_channels, batch_norm, activation, periodic))
+        else:
+            self.add_module('conv', DoubleConv(d, in_channels, out_channels, out_channels, batch_norm, activation, periodic))
+
+    def forward(self, x):
+        x = self.maxpool(x)
+        return self.conv(x)
+
+
+class Up(nn.Module):
+    """Upscaling then double conv"""
+
+    _MODES = [None, 'linear', 'bilinear', 'trilinear']
+
+    def __init__(self, d: int, in_channels: int, out_channels: int, batch_norm: bool, activation: type, periodic: bool, use_res_blocks: bool):
+        super().__init__()
+        up = nn.Upsample(scale_factor=2, mode=Up._MODES[d])
+        if use_res_blocks:
+            conv = resnet_block(d, in_channels, out_channels, batch_norm, activation, periodic)
+        else:
+            conv = DoubleConv(d, in_channels, out_channels, in_channels // 2, batch_norm, activation, periodic)
+        self.add_module('up', up)
+        self.add_module('conv', conv)
+
+    def forward(self, x1, x2):
+        x1 = self.up(x1)
+        # input is CHW
+        # diff = [x2.size()[i] - x1.size()[i] for i in range(2, len(x1.shape))]
+        # x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,
+        #                 diffY // 2, diffY - diffY // 2])
+        # if you have padding issues, see
+        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a
+        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd
+        x = torch.cat([x2, x1], dim=1)
+        return self.conv(x)
+
+
+class ConvNet(nn.Module):
+
+    def __init__(self, in_spatial, in_channels, out_channels, layers, batch_norm, activation, periodic: bool):
+        super(ConvNet, self).__init__()
+        activation = ACTIVATIONS[activation]
+        if len(layers) < 1:
+            layers.append(out_channels)
+        self.layers = layers
+        self.add_module(f'Conv_in', nn.Sequential(
+            CONV[in_spatial](in_channels, layers[0], kernel_size=3, padding=1, padding_mode='circular' if periodic else 'zeros'),
+            NORM[in_spatial](layers[0]) if batch_norm else nn.Identity(),
+            activation()))
+        for i in range(1, len(layers)):
+            self.add_module(f'Conv{i}', nn.Sequential(
+                CONV[in_spatial](layers[i - 1], layers[i], kernel_size=3, padding=1, padding_mode='circular' if periodic else 'zeros'),
+                NORM[in_spatial](layers[i]) if batch_norm else nn.Identity(),
+                activation()))
+        self.add_module(f'Conv_out', CONV[in_spatial](layers[len(layers) - 1], out_channels, kernel_size=1))
+
+    def forward(self, x):
+        x = getattr(self, f'Conv_in')(x)
+        for i in range(1, len(self.layers)):
+            x = getattr(self, f'Conv{i}')(x)
+        x = getattr(self, f'Conv_out')(x)
+        return x
+
+
+def conv_net(in_channels: int,
+             out_channels: int,
+             layers: Sequence[int],
+             batch_norm: bool = False,
+             activation: Union[str, type] = 'ReLU',
+             in_spatial: Union[int, tuple] = 2,
+             periodic=False,
+             **kwargs) -> nn.Module:
+    """
+    Built in Conv-Nets are also provided. Contrary to the classical convolutional neural networks, the feature map spatial size remains the same throughout the layers. Each layer of the network is essentially a convolutional block comprising of two conv layers. A filter size of 3 is used in the convolutional layers.
+    Arguments:
+
+        in_channels : input channels of the feature map, dtype : int
+        out_channels : output channels of the feature map, dtype : int
+        layers : list or tuple of output channels for each intermediate layer between the input and final output channels, dtype : list or tuple
+        activation : activation function used within the layers, dtype : string
+        batch_norm : use of batchnorm after each conv layer, dtype : bool
+        in_spatial : spatial dimensions of the input feature map, dtype : int
+
+    Returns:
+
+        Conv-net model as specified by input arguments
+    """
+    if isinstance(in_spatial, int):
+        d = in_spatial
+    else:
+        assert isinstance(in_spatial, tuple)
+        d = len(in_spatial)
+    net = ConvNet(d, in_channels, out_channels, layers, batch_norm, activation, periodic)
+    net = net.to(TORCH.get_default_device().ref)
+    return net
+
+
+class resnet_block(nn.Module):
+
+    def __init__(self, in_spatial, in_channels, out_channels, batch_norm, activation, periodic: bool):
+        # Since in_channels and out_channels might be different
+        # we need a sampling layer for up/down sampling input
+        # in order to add it as a skip connection
+        super(resnet_block, self).__init__()
+        if in_channels != out_channels:
+            self.sample_input = CONV[in_spatial](in_channels, out_channels, kernel_size=1, padding=0)
+            self.bn_sample = NORM[in_spatial](out_channels) if batch_norm else nn.Identity()
+        else:
+            self.sample_input = nn.Identity()
+            self.bn_sample = nn.Identity()
+        self.activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
+        self.bn1 = NORM[in_spatial](out_channels) if batch_norm else nn.Identity()
+        self.conv1 = CONV[in_spatial](in_channels, out_channels, kernel_size=3, padding=1, padding_mode='circular' if periodic else 'zeros')
+        self.bn2 = NORM[in_spatial](out_channels) if batch_norm else nn.Identity()
+        self.conv2 = CONV[in_spatial](out_channels, out_channels, kernel_size=3, padding=1, padding_mode='circular' if periodic else 'zeros')
+
+    def forward(self, x):
+        x = TORCH.as_tensor(x)
+        out = self.activation()(self.bn1(self.conv1(x)))
+        out = self.activation()(self.bn2(self.conv2(out)))
+        out = (out + self.bn_sample(self.sample_input(x)))
+        return out
+
+
+class Dense_resnet_block(nn.Module):
+
+    def __init__(self, in_channels, mid_channels, batch_norm, activation):
+        super(Dense_resnet_block, self).__init__()
+        self.activation = activation
+        self.bn1 = NORM[1](in_channels) if batch_norm else nn.Identity()
+        self.linear1 = _bias0(nn.Linear)(in_channels, mid_channels)
+        self.bn2 = NORM[1](mid_channels) if batch_norm else nn.Identity()
+        self.linear2 = _bias0(nn.Linear)(mid_channels, in_channels)
+
+    def forward(self, x):
+        x = TORCH.as_tensor(x)
+        out = self.activation()(self.bn1(self.linear1(x)))
+        out = self.activation()(self.bn2(self.linear2(out)))
+        out = out + x
+        return out
+
+
+def get_mask(inputs, reverse_mask, data_format='NHWC'):
+    """ Compute mask for slicing input feature map for Invertible Nets """
+    shape = inputs.shape
+    if len(shape) == 2:
+        N = shape[-1]
+        range_n = torch.arange(0, N)
+        even_ind = range_n % 2
+        checker = torch.reshape(even_ind, (-1, N))
+    elif len(shape) == 4:
+        H = shape[2] if data_format == 'NCHW' else shape[1]
+        W = shape[3] if data_format == 'NCHW' else shape[2]
+
+        range_h = torch.arange(0, H)
+        range_w = torch.arange(0, W)
+
+        even_ind_h = range_h % 2
+        even_ind_w = range_w % 2
+
+        ind_h = even_ind_h.unsqueeze(-1).repeat(1, W)
+        ind_w = even_ind_w.unsqueeze(0).repeat(H, 1)
+
+        checker = torch.logical_xor(ind_h, ind_w)
+
+        checker = checker.reshape(1, 1, H, W) if data_format == 'NCHW' else checker.reshape(1, H, W, 1)
+        checker = checker.long()
+
+    else:
+        raise ValueError('Invalid tensor shape. Dimension of the tensor shape must be '
+                         '2 (NxD) or 4 (NxCxHxW or NxHxWxC), got {}.'.format(inputs.get_shape().as_list()))
+
+    if reverse_mask:
+        checker = 1 - checker
+
+    return checker.to(TORCH.get_default_device().ref)
+
+
+class ResNet(nn.Module):
+
+    def __init__(self, in_spatial, in_channels, out_channels, layers, batch_norm, activation, periodic: bool):
+        super(ResNet, self).__init__()
+        self.layers = layers
+        if len(self.layers) < 1:
+            layers.append(out_channels)
+        self.add_module('Res_in', resnet_block(in_spatial, in_channels, layers[0], batch_norm, activation, periodic))
+        for i in range(1, len(layers)):
+            self.add_module(f'Res{i}', resnet_block(in_spatial, layers[i - 1], layers[i], batch_norm, activation, periodic))
+        self.add_module('Res_out', CONV[in_spatial](layers[len(layers) - 1], out_channels, kernel_size=1))
+
+    def forward(self, x):
+        x = TORCH.as_tensor(x)
+        x = getattr(self, 'Res_in')(x)
+        for i in range(1, len(self.layers)):
+            x = getattr(self, f'Res{i}')(x)
+        x = getattr(self, 'Res_out')(x)
+        return x
+
+
+def res_net(in_channels: int,
+            out_channels: int,
+            layers: Sequence[int],
+            batch_norm: bool = False,
+            activation: Union[str, type] = 'ReLU',
+            in_spatial: Union[int, tuple] = 2,
+            periodic=False,
+            **kwargs) -> nn.Module:
+    """
+    Built in Res-Nets are provided in the ΦFlow framework. Similar to the conv-net, the feature map spatial size remains the same throughout the layers.
+    These networks use residual blocks composed of two conv layers with a skip connection added from the input to the output feature map.
+    A default filter size of 3 is used in the convolutional layers.
+
+    Arguments:
+
+        in_channels : input channels of the feature map, dtype : int
+        out_channels : output channels of the feature map, dtype : int
+        layers : list or tuple of output channels for each intermediate layer between the input and final output channels, dtype : list or tuple
+        activation : activation function used within the layers, dtype : string
+        batch_norm : use of batchnorm after each conv layer, dtype : bool
+        in_spatial : spatial dimensions of the input feature map, dtype : int
+
+    Returns:
+
+        Res-net model as specified by input arguments
+
+    """
+    if (isinstance(in_spatial, int)):
+        d = in_spatial
+    else:
+        assert isinstance(in_spatial, tuple)
+        d = len(in_spatial)
+    net = ResNet(d, in_channels, out_channels, layers, batch_norm, activation, periodic)
+    net = net.to(TORCH.get_default_device().ref)
+    return net
+
+
+def conv_classifier(in_features: int,
+                    in_spatial: Union[tuple, list],
+                    num_classes: int,
+                    blocks=(64, 128, 256, 256, 512, 512),
+                    block_sizes=(2, 2, 3, 3, 3),
+                    dense_layers=(4096, 4096, 100),
+                    batch_norm=True,
+                    activation='ReLU',
+                    softmax=True,
+                    periodic=False):
+    """
+    Based on VGG16.
+    """
+    assert isinstance(in_spatial, (tuple, list))
+    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
+    net = ConvClassifier(in_features, in_spatial, num_classes, batch_norm, softmax, blocks, block_sizes, dense_layers, periodic, activation)
+    return net.to(TORCH.get_default_device().ref)
+
+
+class ConvClassifier(nn.Module):
+
+    def __init__(self, in_features, in_spatial: list, num_classes: int, batch_norm: bool, use_softmax: bool, blocks: tuple, block_sizes: tuple, dense_layers: tuple, periodic: bool, activation):
+        super(ConvClassifier, self).__init__()
+        d = len(in_spatial)
+        self.in_spatial = in_spatial
+        self._blocks = blocks
+        self.add_module('maxpool', MAX_POOL[d](2))
+        for i, (prev, next) in enumerate(zip((in_features,) + tuple(blocks[:-1]), blocks)):
+            block_size = block_sizes[i]
+            layers = []
+            for j in range(block_size):
+                layers.append(CONV[d](prev if j == 0 else next, next, kernel_size=3, padding=1, padding_mode='circular' if periodic else 'zeros'))
+                layers.append(NORM[d](next) if batch_norm else nn.Identity())
+                layers.append(activation())
+            self.add_module(f'conv{i+1}', nn.Sequential(*layers))
+        flat_size = int(np.prod(in_spatial) * blocks[-1] / (2**d) ** len(blocks))
+        self.dense_net = dense_net(flat_size, num_classes, dense_layers, batch_norm, activation, use_softmax)
+        self.flatten = nn.Flatten()
+
+    def forward(self, x):
+        for i in range(len(self._blocks)):
+            x = getattr(self, f'conv{i+1}')(x)
+            x = self.maxpool(x)
+        x = self.flatten(x)
+        x = self.dense_net(x)
+        return x
+
+
+NET = {'u_net': u_net, 'res_net': res_net, 'conv_net': conv_net}
+
+
+class CouplingLayer(nn.Module):
+
+    def __init__(self, in_channels, activation, batch_norm, in_spatial, net, reverse_mask):
+        super(CouplingLayer, self).__init__()
+
+        self.activation = activation
+        self.batch_norm = batch_norm
+        self.reverse_mask = reverse_mask
+
+        if in_spatial == 0:  # for in_spatial = 0, use dense layers
+            self.s1 = nn.Sequential(Dense_resnet_block(in_channels, in_channels, batch_norm, activation),
+                                    torch.nn.Tanh())
+            self.t1 = Dense_resnet_block(in_channels, in_channels, batch_norm, activation)
+
+            self.s2 = nn.Sequential(Dense_resnet_block(in_channels, in_channels, batch_norm, activation),
+                                    torch.nn.Tanh())
+            self.t2 = Dense_resnet_block(in_channels, in_channels, batch_norm, activation)
+        else:
+            self.s1 = nn.Sequential(NET[net](in_channels=in_channels, out_channels=in_channels,
+                                             layers=[], batch_norm=batch_norm, activation=activation,
+                                             in_spatial=in_spatial), torch.nn.Tanh())
+            self.t1 = NET[net](in_channels=in_channels, out_channels=in_channels,
+                               layers=[], batch_norm=batch_norm, activation=activation,
+                               in_spatial=in_spatial)
+            self.s2 = nn.Sequential(NET[net](in_channels=in_channels, out_channels=in_channels,
+                                             layers=[], batch_norm=batch_norm, activation=activation,
+                                             in_spatial=in_spatial), torch.nn.Tanh())
+            self.t2 = NET[net](in_channels=in_channels, out_channels=in_channels,
+                               layers=[], batch_norm=batch_norm, activation=activation,
+                               in_spatial=in_spatial)
+
+    def forward(self, x, invert=False):
+        x = TORCH.as_tensor(x)
+        mask = get_mask(x, self.reverse_mask, 'NCHW')
+        if invert:
+            v1 = x * mask
+            v2 = x * (1 - mask)
+            u2 = (1 - mask) * (v2 - self.t1(v1)) * torch.exp(-self.s1(v1))
+            u1 = mask * (v1 - self.t2(u2)) * torch.exp(-self.s2(u2))
+            return u1 + u2
+        else:
+            u1 = x * mask
+            u2 = x * (1 - mask)
+            v1 = mask * (u1 * torch.exp(self.s2(u2)) + self.t2(u2))
+            v2 = (1 - mask) * (u2 * torch.exp(self.s1(v1)) + self.t1(v1))
+            return v1 + v2
+
+
+class InvertibleNet(nn.Module):
+    def __init__(self, in_channels, num_blocks, activation, batch_norm, in_spatial, net):
+        super(InvertibleNet, self).__init__()
+        self.num_blocks = num_blocks
+        for i in range(num_blocks):
+            self.add_module(f'coupling_block{i + 1}',
+                            CouplingLayer(in_channels, activation,
+                                          batch_norm, in_spatial, net, (i % 2 == 0)))
+
+    def forward(self, x, backward=False):
+        if backward:
+            for i in range(self.num_blocks, 0, -1):
+                x = getattr(self, f'coupling_block{i}')(x, backward)
+        else:
+            for i in range(1, self.num_blocks + 1):
+                x = getattr(self, f'coupling_block{i}')(x, backward)
+        return x
+
+
+def invertible_net(in_channels: int,
+                   num_blocks: int,
+                   batch_norm: bool = False,
+                   net: str = 'u_net',
+                   activation: Union[str, type] = 'ReLU',
+                   in_spatial: Union[tuple, int] = 2, **kwargs):
+    """
+    Phiflow also provides invertible neural networks that are capable of inverting the output tensor back to the input tensor initially passed.\ These networks have far reaching applications in predicting input parameters of a problem given its observations.\ Invertible nets are composed of multiple concatenated coupling blocks wherein each such block consists of arbitrary neural networks.
+
+    Currently, these arbitrary neural networks could be set to u_net(default), conv_net, res_net or dense_net blocks with in_channels = out_channels.
+    The architecture used is popularized by ["Real NVP"](https://arxiv.org/abs/1605.08803).
+
+    Arguments:
+
+        in_channels : input channels of the feature map, dtype : int
+        num_blocks : number of coupling blocks inside the invertible net, dtype : int
+        activation : activation function used within the layers, dtype : string
+        batch_norm : use of batchnorm after each layer, dtype : bool
+        in_spatial : spatial dimensions of the input feature map, dtype : int
+        net : type of neural network blocks used in coupling layers, dtype : str
+        **kwargs : placeholder for arguments not supported by the function
+
+    Returns:
+
+        Invertible Net model as specified by input arguments
+
+    Note: Currently supported values for net are 'u_net'(default), 'conv_net' and 'res_net'.
+    For choosing 'dense_net' as the network block in coupling layers in_spatial must be set to zero.
+    """
+    if isinstance(in_spatial, tuple):
+        in_spatial = len(in_spatial)
+
+    return InvertibleNet(in_channels, num_blocks, activation, batch_norm, in_spatial, net).to(TORCH.get_default_device().ref)
+
+
+def coupling_layer(in_channels: int,
+                   activation: Union[str, type] = 'ReLU',
+                   batch_norm=False,
+                   reverse_mask=False,
+                   in_spatial: Union[tuple, int] = 2):
+    if isinstance(in_spatial, tuple):
+        in_spatial = len(in_spatial)
+
+    net = CouplingLayer(in_channels, activation, batch_norm, in_spatial, reverse_mask)
+    net = net.to(TORCH.get_default_device().ref)
+    return net
+
+
+##################################################################################################################
+#  Fourier Neural Operators
+#  source: https://github.com/zongyi-li/fourier_neural_operator
+###################################################################################################################
+
+class SpectralConv(nn.Module):
+
+    def __init__(self, in_channels, out_channels, modes, in_spatial):
+
+        super(SpectralConv, self).__init__()
+
+        self.in_channels = in_channels
+        self.out_channels = out_channels
+        self.in_spatial = in_spatial
+        assert in_spatial >= 1 and in_spatial <= 3
+        if isinstance(modes, int):
+            mode = modes
+            modes = [mode for i in range(in_spatial)]
+
+        self.scale = 1 / (in_channels * out_channels)
+
+        self.modes = {i + 1: modes[i] for i in range(len(modes))}
+        self.weights = {}
+
+        rand_shape = [in_channels, out_channels]
+        rand_shape += [self.modes[i] for i in range(1, in_spatial + 1)]
+
+        for i in range(2 ** (in_spatial - 1)):
+            self.weights[f'w{i + 1}'] = nn.Parameter(self.scale * torch.randn(rand_shape, dtype=torch.cfloat))
+            # print('TORCH self.weights:', self.weights_[f'w{i + 1}'].shape)
+            # print(self.weights[f'w{i + 1}'].shape)
+
+    def complex_mul(self, input, weights):
+
+        # print(input.shape)
+        # print(weights.shape)
+        # exit(1)
+        if self.in_spatial == 1:
+            return torch.einsum("bix,iox->box", input, weights)
+        elif self.in_spatial == 2:
+            return torch.einsum("bixy,ioxy->boxy", input, weights)
+        elif self.in_spatial == 3:
+            return torch.einsum("bixyz,ioxyz->boxyz", input, weights)
+
+    def forward(self, x):
+        batch_size = x.shape[0]
+
+        # print('x.shape:', x.shape)
+        ##Convert to Fourier space
+        dims = [-i for i in range(self.in_spatial, 0, -1)]
+        x_ft = torch.fft.rfftn(x, dim=dims)
+        # print('After RFFT torch', x_ft.shape)
+        outft_dims = [batch_size, self.out_channels] + \
+                     [x.size(-i) for i in range(self.in_spatial, 1, -1)] + [x.size(-1) // 2 + 1]
+        out_ft = torch.zeros(outft_dims, dtype=torch.cfloat, device=x.device)
+        # print('outft shape before', out_ft.shape)
+        ##Multiply relevant fourier modes
+        if self.in_spatial == 1:
+            out_ft[:, :, :self.modes[1]] = \
+                self.complex_mul(x_ft[:, :, :self.modes[1]],
+                                 self.weights['w1'].to(x_ft.device))
+        elif self.in_spatial == 2:
+            out_ft[:, :, :self.modes[1], :self.modes[2]] = \
+                self.complex_mul(x_ft[:, :, :self.modes[1], :self.modes[2]],
+                                 self.weights['w1'].to(x_ft.device))
+            out_ft[:, :, -self.modes[1]:, :self.modes[2]] = \
+                self.complex_mul(x_ft[:, :, -self.modes[1]:, :self.modes[2]],
+                                 self.weights['w2'].to(x_ft.device))
+        elif self.in_spatial == 3:
+            out_ft[:, :, :self.modes[1], :self.modes[2], :self.modes[3]] = \
+                self.complex_mul(x_ft[:, :, :self.modes[1], :self.modes[2], :self.modes[3]],
+                                 self.weights['w1'].to(x_ft.device))
+            out_ft[:, :, -self.modes[1]:, :self.modes[2], :self.modes[3]] = \
+                self.complex_mul(x_ft[:, :, -self.modes[1]:, :self.modes[2], :self.modes[3]],
+                                 self.weights['w2'].to(x_ft.device))
+            out_ft[:, :, :self.modes[1], -self.modes[2]:, :self.modes[3]] = \
+                self.complex_mul(x_ft[:, :, :self.modes[1], -self.modes[2]:, :self.modes[3]],
+                                 self.weights['w3'].to(x_ft.device))
+            out_ft[:, :, -self.modes[1]:, -self.modes[2]:, :self.modes[3]] = \
+                self.complex_mul(x_ft[:, :, -self.modes[1]:, -self.modes[2]:, :self.modes[3]],
+                                 self.weights['w4'].to(x_ft.device))
+
+        ##Return to Physical Space
+        x = torch.fft.irfftn(out_ft, s=[x.size(-i) for i in range(self.in_spatial, 0, -1)])
+        return x
+
+
+class FNO(nn.Module):
+
+    def __init__(self, in_channels, out_channels, width, modes, activation, batch_norm, in_spatial):
+        super(FNO, self).__init__()
+
+        """
+        The overall network contains 4 layers of the ["Fourier layer"](https://github.com/zongyi-li/fourier_neural_operator).
+        1. Lift the input to the desire channel dimension by self.fc0 .
+        2. 4 layers of the integral operators u' = (W + K)(u).
+            W defined by self.w; K defined by self.conv .
+        3. Project from the channel space to the output space by self.fc1 and self.fc2.
+        
+        input shape and output shape: (batchsize b, channels c, *spatial)
+        """
+
+        self.activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
+        self.width = width
+        self.in_spatial = in_spatial
+
+        self.fc0 = _bias0(nn.Linear)(in_channels + in_spatial, self.width)
+
+        for i in range(4):
+            self.add_module(f'conv{i}', SpectralConv(self.width, self.width, modes, in_spatial))
+            self.add_module(f'w{i}', CONV[in_spatial](self.width, self.width, kernel_size=1))
+            self.add_module(f'bn{i}', NORM[in_spatial](self.width) if batch_norm else nn.Identity())
+
+        self.fc1 = _bias0(nn.Linear)(self.width, 128)
+        self.fc2 = _bias0(nn.Linear)(128, out_channels)
+
+    # Adding extra spatial channels eg. x, y, z, .... to input x
+    def get_grid(self, shape, device):
+        batch_size = shape[0]
+        grid_channel_sizes = shape[2:]  # shape =  (batch_size, channels, *spatial)
+        self.grid_channels = {}
+        for i in range(self.in_spatial):
+            self.grid_channels[f'dim{i}'] = torch.tensor(torch.linspace(0, 1, grid_channel_sizes[i]),
+                                                         dtype=torch.float)
+            reshape_dim_tuple = [1, 1] + [1 if i != j else grid_channel_sizes[j] for j in range(self.in_spatial)]
+            repeat_dim_tuple = [batch_size, 1] + [1 if i == j else grid_channel_sizes[j] for j in
+                                                  range(self.in_spatial)]
+            self.grid_channels[f'dim{i}'] = self.grid_channels[f'dim{i}'].reshape(reshape_dim_tuple) \
+                .repeat(repeat_dim_tuple)
+
+        return torch.cat([self.grid_channels[f'dim{i}'] for i in range(self.in_spatial)], dim=1).to(device)
+
+    def forward(self, x):
+        grid = self.get_grid(x.shape, x.device)
+        x = torch.cat([x, grid], dim=1)
+
+        permute_tuple = [0] + [2 + i for i in range(self.in_spatial)] + [1]
+        permute_tuple_reverse = [0] + [self.in_spatial + 1] + [i + 1 for i in range(self.in_spatial)]
+
+        # Transpose x such that channels shape lies at the end to pass it through linear layers
+        x = x.permute(permute_tuple)
+
+        x = self.fc0(x)
+
+        # Transpose x back to its original shape to pass it through convolutional layers
+        x = x.permute(permute_tuple_reverse)
+
+        for i in range(4):
+            x1 = getattr(self, f'w{i}')(x)
+            x2 = getattr(self, f'conv{i}')(x)
+            x = getattr(self, f'bn{i}')(x1) + getattr(self, f'bn{i}')(x2)
+            x = self.activation()(x)
+
+        x = x.permute(permute_tuple)
+        x = self.activation()(self.fc1(x))
+        x = self.fc2(x)
+
+        x = x.permute(permute_tuple_reverse)
+
+        return x
+
+
+def fno(in_channels: int,
+        out_channels: int,
+        mid_channels: int,
+        modes: Sequence[int],
+        activation: Union[str, type] = 'ReLU',
+        batch_norm: bool = False,
+        in_spatial: int = 2):
+    """
+    ["Fourier Neural Operator"](https://github.com/zongyi-li/fourier_neural_operator) network contains 4 layers of the Fourier layer.
+    1. Lift the input to the desire channel dimension by self.fc0 .
+    2. 4 layers of the integral operators u' = (W + K)(u). W defined by self.w; K defined by self.conv .
+    3. Project from the channel space to the output space by self.fc1 and self.fc2.
+
+    Arguments:
+
+        in_channels : input channels of the feature map, dtype : int
+        out_channels : output channels of the feature map, dtype : int
+        mid_channels : channels used in Spectral Convolution Layers, dtype : int
+        modes : Fourier modes for each spatial channel, dtype : List[int] or int (in case all number modes are to be the same for each spatial channel)
+        activation : activation function used within the layers, dtype : string
+        batch_norm : use of batchnorm after each conv layer, dtype : bool
+        in_spatial : spatial dimensions of the input feature map, dtype : int
+
+    Returns:
+
+        Fourier Neural Operator model as specified by input arguments.
+    """
+    net = FNO(in_channels, out_channels, mid_channels, modes, activation, batch_norm, in_spatial)
+    net = net.to(TORCH.get_default_device().ref)
+    return net
```

### Comparing `phiflow-2.3.4/phi/vis/__init__.py` & `phiflow-2.4.0/phi/vis/__init__.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,34 +1,36 @@
-"""
-Visualization: plotting, interactive user interfaces.
-
-Use `view()` to show fields or field variables in an interactive user interface.
-
-Use `plot()` to plot fields using Matplotlib.
-
-See the user interface documentation at https://tum-pbs.github.io/PhiFlow/Visualization.html
-"""
-from ._viewer import Viewer
-from ._matplotlib import plot_scalars
-from ._vis import view, control, show, close, action, plot, overlay, write_image, write_image as savefig
-
-__all__ = [key for key in globals().keys() if not key.startswith('_')]
-
-__pdoc__ = {
-    'Viewer.actions': False,
-    'Viewer.can_progress': False,
-    'Viewer.control_names': False,
-    'Viewer.curve_names': False,
-    'Viewer.field_names': False,
-    'Viewer.get_control': False,
-    'Viewer.get_curve': False,
-    'Viewer.get_field': False,
-    'Viewer.run_action': False,
-    'Viewer.set_control_value': False,
-    'Viewer.log_scalars': False,
-    'Viewer.controls': False,
-    'Viewer.get_control_value': False,
-    'Viewer.info': False,
-    'Viewer.reset': False,
-    'Viewer.progress': False,
-    'Viewer.__init__': False,
-}
+"""
+Visualization: plotting, interactive user interfaces.
+
+Use `view()` to show fields or field variables in an interactive user interface.
+
+Use `plot()` to plot fields using Matplotlib.
+
+See the user interface documentation at https://tum-pbs.github.io/PhiFlow/Visualization.html
+"""
+from ._viewer import Viewer
+from ._matplotlib import plot_scalars
+from ._io import load_scalars
+from ._plot_util import smooth
+from ._vis import view, control, show, close, action, plot, overlay, write_image, write_image as savefig
+
+__all__ = [key for key in globals().keys() if not key.startswith('_')]
+
+__pdoc__ = {
+    'Viewer.actions': False,
+    'Viewer.can_progress': False,
+    'Viewer.control_names': False,
+    'Viewer.curve_names': False,
+    'Viewer.field_names': False,
+    'Viewer.get_control': False,
+    'Viewer.get_curve': False,
+    'Viewer.get_field': False,
+    'Viewer.run_action': False,
+    'Viewer.set_control_value': False,
+    'Viewer.log_scalars': False,
+    'Viewer.controls': False,
+    'Viewer.get_control_value': False,
+    'Viewer.info': False,
+    'Viewer.reset': False,
+    'Viewer.progress': False,
+    'Viewer.__init__': False,
+}
```

### Comparing `phiflow-2.3.4/phi/vis/_console/_console_gui.py` & `phiflow-2.4.0/phi/vis/_console/_console_gui.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,156 +1,156 @@
-import os
-import shutil
-import time
-
-from .._vis_base import Gui, play_async, gui_interrupt, select_channel, get_control_by_name, status_message, \
-    display_name
-from ._console_plot import heatmap, quiver
-
-
-class ConsoleGui(Gui):
-
-    def __init__(self):
-        Gui.__init__(self, asynchronous=True)
-        self.play_status = None
-
-        # def setup(self):
-    #     vis = self.vis
-    #     self.vis.logger.removeHandler(self.vis.console_handler)
-    #     terminal_size = shutil.get_terminal_size(fallback=(80, 20))
-    #
-    #     class CustomHandler(Handler):
-    #
-    #         def emit(self, record: LogRecord) -> None:
-    #             pass
-    #
-    #         def handle(self, record: LogRecord) -> None:
-    #             line = vis.message + " " * (max(1, terminal_size[0]-len(vis.message)-1))
-    #             print(line, end="\r")
-    #
-    #         def createLock(self) -> None:
-    #             pass
-    #
-    #     self.vis.logger.addHandler(CustomHandler())
-
-    def show(self, caller_is_main: bool):
-        print(self.app.name)
-        print(self.app.description)
-        print()
-        print("PhiFlow console interface active. Type 'help' for a list of available commands.")
-        while True:
-            print("PhiFlow>", end="")
-            command = input()
-            if command.strip() == "":
-                time.sleep(.1)
-            else:
-                commands = [s.strip() for s in command.split(';')]
-                for command in commands:
-                    try:
-                        self.process_command(command)
-                    except InterruptedError:
-                        return
-                    except Exception as exc:
-                        print(exc)
-
-    def process_command(self, command: str):
-        if command == 'step':
-            self.app.progress()
-        elif command.startswith('play'):
-            if self.play_status:
-                print("Wait for current step to finish." if self.play_status.paused else "Already playing, command ignored.")
-            else:
-                if command.strip() == 'play':
-                    frames = None
-                else:
-                    frames = int(command[len('play '):].strip())
-                self.play_status = play_async(self.app, frames)
-        elif command == 'pause':
-            if self.play_status:
-                self.play_status.pause()
-        elif command == 'status':
-            print(status_message(self.app, self.play_status))
-        elif command == 'exit':
-            if self.play_status:
-                self.play_status.pause()
-            if self.app.can_progress:
-                if self.app.can_progress:
-                    self.app.pre_step.append(gui_interrupt)
-                    self.app.progress()
-            raise InterruptedError()
-        elif command == 'kill':
-            os._exit(0)
-        elif command == 'show':
-            self.draw()
-        elif command.startswith('show '):
-            fields = command[len('show '):].split(',')
-            fields = [f.strip() for f in fields]
-            self.draw(fields)
-        elif command == 'controls':
-            if self.app.controls:
-                print("Available controls:\n-------------------------------------------")
-                for control in self.app.controls:
-                    value = self.app.get_control_value(control.name)
-                    print(f"{control.name}: {control.control_type.__name__} = {value}  \t(initial value: {control.initial}, \trange {control.value_range})")
-                print("-------------------------------------------")
-                print("You can change a control value by typing '<control_name> = <value>'")
-            else:
-                print("No controls available. Create controls in your Python script using '<control_name> = control(value)'.")
-        elif '=' in command:
-            parts = command.split('=')
-            assert len(parts) == 2, "Invalid command syntax. Use '<control_name> = <value>'. Type 'controls' for a list of available controls."
-            parts = [p.strip() for p in parts]
-            name, value = parts
-            control = get_control_by_name(self.app, name)
-            value = control.control_type(value)  # raises ValueError
-            self.app.set_control_value(name, value)
-        elif command == 'help':
-            print("General Commands:  \t\tstatus, controls, actions, help\n"
-                  "Plotting:  \t\t\t\tshow, show <comma-separated fields>, show <field>.<component>\n"
-                  "Control Execution:  \tplay, play <frames>, pause, step, <control> = <value>, exit, kill\n"
-                  "See https://tum-pbs.github.io/PhiFlow/ConsoleUI.html for a detailed description of available commands.")
-        elif command == 'actions':
-            print("Available actions:\n")
-            for action in self.app.actions:
-                print(f"{display_name(action.name)}  \t(type '{action.name}' to run)")
-                if action.description:
-                    print(f"{action.description}")
-                print()
-        elif command in [a.name for a in self.app.actions]:
-            self.app.run_action(command)
-            print(f"Completed '{display_name(command)}'")
-        else:
-            print(f"Command {command} not recognized.")
-
-    def draw(self, field_names: list = None):
-        if field_names is None:
-            if len(self.app.field_names) == 0:
-                print("Nothing to show.")
-                return
-            field_names = self.app.field_names[:2] if len(self.app.field_names) > 2 else self.app.field_names
-            channel_sel = [None] * len(field_names)
-        else:
-            channel_sel = [n[n.index('.')+1:] if '.' in n else None for n in field_names]
-            field_names = [n[:n.index('.')] if '.' in n else n for n in field_names]
-        values = []
-        for n in field_names:
-            try:
-                values.append(self.app.get_field(n, {}))
-            except KeyError:
-                print(f"The field {n} does not exist. Available fields are {self.app.field_names}")
-                return
-        cols, rows = shutil.get_terminal_size(fallback=(80, 14))
-        plt_width = cols // len(values)
-        plt_height = rows - 1
-        lines = [""] * plt_height
-        for name, v, ch_sel in zip(field_names, values, channel_sel):
-            v = select_channel(v, ch_sel)
-            if v.vector.exists:
-                plt_lines = quiver(v, plt_width, plt_height, name, threshold=0.1, basic_chars=True)
-            else:
-                plt_lines = heatmap(v, plt_width, plt_height, name)
-            lines = [l+p for l, p in zip(lines, plt_lines)]
-        print("\n".join(lines))
-
-    def auto_play(self):
-        framerate = self.config.get('framerate', None)
-        self.play_status = play_async(self.app, framerate=framerate)
+import os
+import shutil
+import time
+
+from .._vis_base import Gui, play_async, gui_interrupt, select_channel, get_control_by_name, status_message, \
+    display_name
+from ._console_plot import heatmap, quiver
+
+
+class ConsoleGui(Gui):
+
+    def __init__(self):
+        Gui.__init__(self, asynchronous=True)
+        self.play_status = None
+
+        # def setup(self):
+    #     vis = self.vis
+    #     self.vis.logger.removeHandler(self.vis.console_handler)
+    #     terminal_size = shutil.get_terminal_size(fallback=(80, 20))
+    #
+    #     class CustomHandler(Handler):
+    #
+    #         def emit(self, record: LogRecord) -> None:
+    #             pass
+    #
+    #         def handle(self, record: LogRecord) -> None:
+    #             line = vis.message + " " * (max(1, terminal_size[0]-len(vis.message)-1))
+    #             print(line, end="\r")
+    #
+    #         def createLock(self) -> None:
+    #             pass
+    #
+    #     self.vis.logger.addHandler(CustomHandler())
+
+    def show(self, caller_is_main: bool):
+        print(self.app.name)
+        print(self.app.description)
+        print()
+        print("PhiFlow console interface active. Type 'help' for a list of available commands.")
+        while True:
+            print("PhiFlow>", end="")
+            command = input()
+            if command.strip() == "":
+                time.sleep(.1)
+            else:
+                commands = [s.strip() for s in command.split(';')]
+                for command in commands:
+                    try:
+                        self.process_command(command)
+                    except InterruptedError:
+                        return
+                    except Exception as exc:
+                        print(exc)
+
+    def process_command(self, command: str):
+        if command == 'step':
+            self.app.progress()
+        elif command.startswith('play'):
+            if self.play_status:
+                print("Wait for current step to finish." if self.play_status.paused else "Already playing, command ignored.")
+            else:
+                if command.strip() == 'play':
+                    frames = None
+                else:
+                    frames = int(command[len('play '):].strip())
+                self.play_status = play_async(self.app, frames)
+        elif command == 'pause':
+            if self.play_status:
+                self.play_status.pause()
+        elif command == 'status':
+            print(status_message(self.app, self.play_status))
+        elif command == 'exit':
+            if self.play_status:
+                self.play_status.pause()
+            if self.app.can_progress:
+                if self.app.can_progress:
+                    self.app.pre_step.append(gui_interrupt)
+                    self.app.progress()
+            raise InterruptedError()
+        elif command == 'kill':
+            os._exit(0)
+        elif command == 'show':
+            self.draw()
+        elif command.startswith('show '):
+            fields = command[len('show '):].split(',')
+            fields = [f.strip() for f in fields]
+            self.draw(fields)
+        elif command == 'controls':
+            if self.app.controls:
+                print("Available controls:\n-------------------------------------------")
+                for control in self.app.controls:
+                    value = self.app.get_control_value(control.name)
+                    print(f"{control.name}: {control.control_type.__name__} = {value}  \t(initial value: {control.initial}, \trange {control.value_range})")
+                print("-------------------------------------------")
+                print("You can change a control value by typing '<control_name> = <value>'")
+            else:
+                print("No controls available. Create controls in your Python script using '<control_name> = control(value)'.")
+        elif '=' in command:
+            parts = command.split('=')
+            assert len(parts) == 2, "Invalid command syntax. Use '<control_name> = <value>'. Type 'controls' for a list of available controls."
+            parts = [p.strip() for p in parts]
+            name, value = parts
+            control = get_control_by_name(self.app, name)
+            value = control.control_type(value)  # raises ValueError
+            self.app.set_control_value(name, value)
+        elif command == 'help':
+            print("General Commands:  \t\tstatus, controls, actions, help\n"
+                  "Plotting:  \t\t\t\tshow, show <comma-separated fields>, show <field>.<component>\n"
+                  "Control Execution:  \tplay, play <frames>, pause, step, <control> = <value>, exit, kill\n"
+                  "See https://tum-pbs.github.io/PhiFlow/ConsoleUI.html for a detailed description of available commands.")
+        elif command == 'actions':
+            print("Available actions:\n")
+            for action in self.app.actions:
+                print(f"{display_name(action.name)}  \t(type '{action.name}' to run)")
+                if action.description:
+                    print(f"{action.description}")
+                print()
+        elif command in [a.name for a in self.app.actions]:
+            self.app.run_action(command)
+            print(f"Completed '{display_name(command)}'")
+        else:
+            print(f"Command {command} not recognized.")
+
+    def draw(self, field_names: list = None):
+        if field_names is None:
+            if len(self.app.field_names) == 0:
+                print("Nothing to show.")
+                return
+            field_names = self.app.field_names[:2] if len(self.app.field_names) > 2 else self.app.field_names
+            channel_sel = [None] * len(field_names)
+        else:
+            channel_sel = [n[n.index('.')+1:] if '.' in n else None for n in field_names]
+            field_names = [n[:n.index('.')] if '.' in n else n for n in field_names]
+        values = []
+        for n in field_names:
+            try:
+                values.append(self.app.get_field(n, {}))
+            except KeyError:
+                print(f"The field {n} does not exist. Available fields are {self.app.field_names}")
+                return
+        cols, rows = shutil.get_terminal_size(fallback=(80, 14))
+        plt_width = cols // len(values)
+        plt_height = rows - 1
+        lines = [""] * plt_height
+        for name, v, ch_sel in zip(field_names, values, channel_sel):
+            v = select_channel(v, ch_sel)
+            if v.vector.exists:
+                plt_lines = quiver(v, plt_width, plt_height, name, threshold=0.1, basic_chars=True)
+            else:
+                plt_lines = heatmap(v, plt_width, plt_height, name)
+            lines = [l+p for l, p in zip(lines, plt_lines)]
+        print("\n".join(lines))
+
+    def auto_play(self):
+        framerate = self.config.get('framerate', None)
+        self.play_status = play_async(self.app, framerate=framerate)
```

### Comparing `phiflow-2.3.4/phi/vis/_console/_console_plot.py` & `phiflow-2.4.0/phi/vis/_console/_console_plot.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,85 +1,85 @@
-from typing import List
-
-import numpy
-
-from phi.field import Grid, CenteredGrid
-from ._console_util import underline, get_arrow
-from .._vis_base import PlottingLibrary
-from ...math import extrapolation, Tensor
-
-
-class ConsolePlots(PlottingLibrary):
-
-    def __init__(self):
-        self.last_figure = ""
-
-    # def plot(self, data: Tensor,
-    #          title=False,
-    #          size=(12, 5),
-    #          same_scale=True,
-    #          show_color_bar=True,
-    #          figure=None,
-    #          **plt_args):
-    #     if v.vector.exists:
-    #         plt_lines = quiver(v, plt_width, plt_height, name, threshold=0.1, basic_chars=True)
-    #     else:
-    #         plt_lines = heatmap(v, plt_width, plt_height, name)
-
-    def show(self, figure: List[str]):
-        print(figure)
-
-    def save(self, figure: List[str], path: str):
-        with open(path, 'w') as file:
-            file.writelines(figure)
-
-
-CONSOLE = ConsolePlots()
-
-
-FILLED = [' ', '.', ':', '-', '=', '+', '*', '#', '%', '@']
-
-
-def heatmap(grid: Grid, cols: int, rows: int, title: str):
-    inner_cols = cols - 10
-    inner_rows = rows - 2
-    grid @= CenteredGrid(0, extrapolation.ZERO, x=inner_cols, y=inner_rows, bounds=grid.bounds)
-    data = grid.values.numpy('y,x')
-    min_, max_ = numpy.min(data), numpy.max(data)
-    col_indices = (data - min_) / (max_ - min_) * len(FILLED)
-    col_indices = numpy.clip(numpy.round(col_indices).astype(numpy.int8), 0, len(FILLED) - 1)
-    lines = []
-    # lines.append("   " + "_" * inner_cols + " ")
-    title = title[:inner_cols]
-    padded_title = " " * ((inner_cols - len(title)) // 2) + title + " " * ((inner_cols - len(title) + 1) // 2)
-    lines.append("   " + underline(padded_title) + "\033[0m ")
-    for index_row in col_indices[::-1]:
-        line = [FILLED[col_index] for col_index in index_row]
-        lines.append("  |" + "".join(line)+"|")
-    lines[-1] = lines[-1][:3] + underline(lines[-1][3:inner_cols+3]) + lines[-1][inner_cols+3:]
-    return lines
-
-
-def quiver(grid: Grid, cols: int, rows: int, title: str, threshold: float, basic_chars=True):
-    inner_cols = cols - 10
-    inner_rows = rows - 2
-    grid @= CenteredGrid(0, extrapolation.ZERO, x=inner_cols, y=inner_rows, bounds=grid.bounds)
-    data = grid.values.numpy('y,x,vector')[::-1]
-    thick_threshold = numpy.max(numpy.sum(data ** 2, -1)) / 4  # half the vector length
-
-    lines = []
-    title = title[:inner_cols]
-    padded_title = " " * ((inner_cols - len(title)) // 2) + title + " " * ((inner_cols - len(title) + 1) // 2)
-    lines.append("   " + underline(padded_title) + "\033[0m ")
-    for y in range(data.shape[0]):
-        line = ""
-        for x in range(data.shape[1]):
-            u, v = data[y, x]
-            len_squared = u ** 2 + v ** 2
-            if len_squared < threshold ** 2:
-                arrow = " "
-            else:
-                arrow = get_arrow(u, v, thick=len_squared >= thick_threshold, basic_char=basic_chars)
-            line += arrow
-        lines.append("  |" + "".join(line)+"|")
-    lines[-1] = lines[-1][:3] + underline(lines[-1][3:inner_cols+3]) + lines[-1][inner_cols+3:]
-    return lines
+from typing import List
+
+import numpy
+
+from phi.field import Grid, CenteredGrid
+from ._console_util import underline, get_arrow
+from .._vis_base import PlottingLibrary
+from ...math import extrapolation, Tensor
+
+
+class ConsolePlots(PlottingLibrary):
+
+    def __init__(self):
+        self.last_figure = ""
+
+    # def plot(self, data: Tensor,
+    #          title=False,
+    #          size=(12, 5),
+    #          same_scale=True,
+    #          show_color_bar=True,
+    #          figure=None,
+    #          **plt_args):
+    #     if v.vector.exists:
+    #         plt_lines = quiver(v, plt_width, plt_height, name, threshold=0.1, basic_chars=True)
+    #     else:
+    #         plt_lines = heatmap(v, plt_width, plt_height, name)
+
+    def show(self, figure: List[str]):
+        print(figure)
+
+    def save(self, figure: List[str], path: str):
+        with open(path, 'w') as file:
+            file.writelines(figure)
+
+
+CONSOLE = ConsolePlots()
+
+
+FILLED = [' ', '.', ':', '-', '=', '+', '*', '#', '%', '@']
+
+
+def heatmap(grid: Grid, cols: int, rows: int, title: str):
+    inner_cols = cols - 10
+    inner_rows = rows - 2
+    grid @= CenteredGrid(0, extrapolation.ZERO, x=inner_cols, y=inner_rows, bounds=grid.bounds)
+    data = grid.values.numpy('y,x')
+    min_, max_ = numpy.min(data), numpy.max(data)
+    col_indices = (data - min_) / (max_ - min_) * len(FILLED)
+    col_indices = numpy.clip(numpy.round(col_indices).astype(numpy.int8), 0, len(FILLED) - 1)
+    lines = []
+    # lines.append("   " + "_" * inner_cols + " ")
+    title = title[:inner_cols]
+    padded_title = " " * ((inner_cols - len(title)) // 2) + title + " " * ((inner_cols - len(title) + 1) // 2)
+    lines.append("   " + underline(padded_title) + "\033[0m ")
+    for index_row in col_indices[::-1]:
+        line = [FILLED[col_index] for col_index in index_row]
+        lines.append("  |" + "".join(line)+"|")
+    lines[-1] = lines[-1][:3] + underline(lines[-1][3:inner_cols+3]) + lines[-1][inner_cols+3:]
+    return lines
+
+
+def quiver(grid: Grid, cols: int, rows: int, title: str, threshold: float, basic_chars=True):
+    inner_cols = cols - 10
+    inner_rows = rows - 2
+    grid @= CenteredGrid(0, extrapolation.ZERO, x=inner_cols, y=inner_rows, bounds=grid.bounds)
+    data = grid.values.numpy('y,x,vector')[::-1]
+    thick_threshold = numpy.max(numpy.sum(data ** 2, -1)) / 4  # half the vector length
+
+    lines = []
+    title = title[:inner_cols]
+    padded_title = " " * ((inner_cols - len(title)) // 2) + title + " " * ((inner_cols - len(title) + 1) // 2)
+    lines.append("   " + underline(padded_title) + "\033[0m ")
+    for y in range(data.shape[0]):
+        line = ""
+        for x in range(data.shape[1]):
+            u, v = data[y, x]
+            len_squared = u ** 2 + v ** 2
+            if len_squared < threshold ** 2:
+                arrow = " "
+            else:
+                arrow = get_arrow(u, v, thick=len_squared >= thick_threshold, basic_char=basic_chars)
+            line += arrow
+        lines.append("  |" + "".join(line)+"|")
+    lines[-1] = lines[-1][:3] + underline(lines[-1][3:inner_cols+3]) + lines[-1][inner_cols+3:]
+    return lines
```

### Comparing `phiflow-2.3.4/phi/vis/_dash/_plotly_plots.py` & `phiflow-2.4.0/phi/vis/_dash/_plotly_plots.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,448 +1,444 @@
-import warnings
-from typing import Tuple, Any, Dict, List, Callable, Union
-
-import numpy
-import numpy as np
-from plotly import graph_objects, figure_factory
-from plotly.subplots import make_subplots
-from plotly.tools import DEFAULT_PLOTLY_COLORS
-
-from phi import math, field
-from phi.field import SampledField, PointCloud, Grid, StaggeredGrid
-from phi.geom import Sphere, BaseBox, Point, Box
-from phi.geom._stack import GeometryStack
-from phi.math import Tensor, spatial, channel, non_channel
-from phi.vis._dash.colormaps import COLORMAPS
-from phi.vis._plot_util import smooth_uniform_curve, down_sample_curve
-from phi.vis._vis_base import PlottingLibrary, Recipe
-
-
-class PlotlyPlots(PlottingLibrary):
-
-    def __init__(self):
-        super().__init__('plotly', [graph_objects.Figure])
-
-    def create_figure(self,
-                      size: tuple,
-                      rows: int,
-                      cols: int,
-                      subplots: Dict[Tuple[int, int], Box],
-                      titles: Dict[Tuple[int, int], str],
-                      log_dims: Tuple[str, ...]) -> Tuple[Any, Dict[Tuple[int, int], Any]]:
-        titles = [titles.get((r, c), None) for r in range(rows) for c in range(cols)]
-        specs = [[{'type': 'xy' if subplots.get((row, col), Box()).spatial_rank < 3 else 'surface'} for col in range(cols)] for row in range(rows)]
-        fig = self.current_figure = make_subplots(rows=rows, cols=cols, subplot_titles=titles, specs=specs)
-        for (row, col), bounds in subplots.items():
-            subplot = fig.get_subplot(row + 1, col + 1)
-            if bounds.spatial_rank == 1:
-                subplot.xaxis.update(title=bounds.vector.item_names[0], range=_get_range(bounds, 0))
-            elif bounds.spatial_rank == 2:
-                subplot.xaxis.update(scaleanchor=f'y{subplot.yaxis.plotly_name[5:]}', scaleratio=1, constrain='domain', title=bounds.vector.item_names[0], range=_get_range(bounds, 0))
-                subplot.yaxis.update(constrain='domain', title=bounds.vector.item_names[1], range=_get_range(bounds, 1))
-            elif bounds.spatial_rank == 3:
-                subplot.xaxis.update(title=bounds.vector.item_names[0], range=_get_range(bounds, 0))
-                subplot.yaxis.update(title=bounds.vector.item_names[1], range=_get_range(bounds, 1))
-                subplot.zaxis.update(title=bounds.vector.item_names[2], range=_get_range(bounds, 2))
-        fig._phi_size = size
-        return fig, {pos: (pos[0]+1, pos[1]+1) for pos in subplots.keys()}
-
-    def animate(self, fig, frames: int, plot_frame_function: Callable, interval: float, repeat: bool):
-        raise NotImplementedError()
-
-    def finalize(self, figure):
-        pass
-
-    def close(self, figure):
-        pass
-
-    def show(self, figure: graph_objects.Figure):
-        figure.show()
-
-    def save(self, figure: graph_objects.Figure, path: str, dpi: float):
-        width, height = figure._phi_size
-        figure.layout.update(margin=dict(l=0, r=0, b=0, t=0))
-        scale = dpi/90.
-        figure.write_image(path, width=width * dpi / scale, height=height * dpi / scale, scale=scale)
-
-
-class LinePlot(Recipe):
-
-    def can_plot(self, data: SampledField, space: Box) -> bool:
-        return data.spatial_rank == 1 and isinstance(data, Grid)
-
-    def plot(self, data: SampledField, figure, subplot, space: Box, min_val: float, max_val: float, show_color_bar: bool, color: Tensor, alpha: Tensor):
-        row, col = subplot
-        subplot = figure.get_subplot(row, col)
-        x = data.points.vector[0].numpy().flatten()
-        channels = data.values.shape.channel
-        if channels.rank == 1 and channels.get_item_names(0) is not None:
-            for i, name in enumerate(channels.get_item_names(0)):
-                y = math.reshaped_native(real_values(data[{channels.name: i}]), [data.shape.spatial], to_numpy=True)
-                figure.add_trace(graph_objects.Scatter(x=x, y=y, mode='lines+markers', name=name), row=row, col=col)
-            figure.update_layout(showlegend=True)
-        else:
-            for ch_idx in channels.meshgrid():
-                y = math.reshaped_native(real_values(data[ch_idx]), [data.shape.spatial], to_numpy=True)
-                figure.add_trace(graph_objects.Scatter(x=x, y=y, mode='lines+markers', name='Multi-channel'), row=row, col=col)
-            figure.update_layout(showlegend=False)
-        if min_val is not None and max_val is not None:
-            subplot.yaxis.update(range=(min_val - .02 * (max_val - min_val), max_val + .02 * (max_val - min_val)))
-
-
-class Heatmap2D(Recipe):
-
-    def can_plot(self, data: SampledField, space: Box) -> bool:
-        return data.spatial_rank == 2 and isinstance(data, Grid) and 'vector' not in data.shape
-
-    def plot(self, data: SampledField, figure, subplot, space: Box, min_val: float, max_val: float, show_color_bar: bool, color: Tensor, alpha: Tensor):
-        row, col = subplot
-        dims = spatial(data)
-        values = real_values(data).numpy(dims.reversed)
-        x = data.points.vector[dims[0].name].dimension(dims[1].name)[0].numpy()
-        y = data.points.vector[dims[1].name].dimension(dims[0].name)[0].numpy()
-        min_val, max_val = numpy.nanmin(values), numpy.nanmax(values)
-        min_val, max_val = min_val if numpy.isfinite(min_val) else 0, max_val if numpy.isfinite(max_val) else 0
-        color_scale = get_div_map(min_val, max_val, equal_scale=True)
-        # color_bar = graph_objects.heatmap.ColorBar(x=1.15)   , colorbar=color_bar
-        figure.add_heatmap(row=row, col=col, x=x, y=y, z=values, zauto=False, zmin=min_val, zmax=max_val, colorscale=color_scale, showscale=show_color_bar)
-
-
-class VectorField2D(Recipe):
-
-    def can_plot(self, data: SampledField, space: Box) -> bool:
-        return data.spatial_rank == 2 and isinstance(data, Grid)
-
-    def plot(self, data: SampledField, figure, subplot, space: Box, min_val: float, max_val: float, show_color_bar: bool, color: Tensor, alpha: Tensor):
-        if isinstance(data, StaggeredGrid):
-            data = data.at_centers()
-        row, col = subplot
-        dims = data.bounds.vector.item_names
-        vector = data.bounds.shape['vector']
-        extra_channels = data.shape.channel.without('vector')
-        x, y = math.reshaped_numpy(data.points.vector[dims], [vector, data.shape.non_channel], force_expand=True)
-        u, v = math.reshaped_numpy(data.values.vector[dims], [vector, extra_channels, data.shape.without(vector)], force_expand=True)
-        for ch in range(u.shape[0]):
-            # quiver = figure_factory.create_quiver(x, y, data_x[ch], data_y[ch], scale=1.0)  # 7 points per arrow
-            # fig.add_trace(quiver, row=row, col=col)
-            u_ch = u[ch]
-            v_ch = v[ch]
-            # lines_y = numpy.stack([y, y + data_y_flat, [None] * len(x)], -1).flatten()  # 3 points per arrow
-            # lines_x = numpy.stack([x, x + data_x_flat, [None] * len(x)], -1).flatten()
-            lines_x = numpy.stack([x, x + u_ch, [None] * len(x)], -1).flatten()
-            lines_y = numpy.stack([y, y + v_ch, [None] * len(x)], -1).flatten()  # 3 points per arrow
-            name = extra_channels.get_item_names(0)[ch] if extra_channels.rank == 1 and extra_channels.get_item_names(0) is not None else None
-            figure.add_scatter(x=lines_x, y=lines_y, mode='lines', row=row, col=col, name=name)
-        if u.shape[0] == 1:
-            figure.update_layout(showlegend=False)
-
-
-class Heatmap3D(Recipe):
-
-    def can_plot(self, data: SampledField, space: Box) -> bool:
-        return data.spatial_rank == 3 and isinstance(data, Grid) and data.shape.channel.volume == 1
-
-    def plot(self, data: SampledField, figure, subplot, space: Box, min_val: float, max_val: float, show_color_bar: bool, color: Tensor, alpha: Tensor):
-        row, col = subplot
-        dims = data.bounds.vector.item_names
-        vector = data.bounds.shape['vector']
-        values = real_values(data).numpy(dims)
-        x, y, z = math.reshaped_numpy(data.points.vector[dims], [vector, *data.points.shape.spatial])
-        min_val, max_val = numpy.nanmin(values), numpy.nanmax(values)
-        min_val, max_val = min_val if numpy.isfinite(min_val) else 0, max_val if numpy.isfinite(max_val) else 0
-        color_scale = get_div_map(min_val, max_val, equal_scale=True)
-        figure.add_volume(x=x.flatten(), y=y.flatten(), z=z.flatten(), value=values.flatten(),
-                          showscale=show_color_bar, colorscale=color_scale, cmin=min_val, cmax=max_val, cauto=False,
-                          isomin=0.1, isomax=0.8,
-                          opacity=0.1,  # needs to be small to see through all surfaces
-                          surface_count=17,  # needs to be a large number for good volume rendering
-                          row=row, col=col)
-        figure.update_layout(uirevision=True)
-
-
-class VectorField3D(Recipe):
-
-    def can_plot(self, data: SampledField, space: Box) -> bool:
-        return isinstance(data, Grid) and data.spatial_rank == 3
-
-    def plot(self, data: SampledField, figure, subplot, space: Box, min_val: float, max_val: float, show_color_bar: bool, color: Tensor, alpha: Tensor):
-        row, col = subplot
-        dims = data.bounds.vector.item_names
-        vector = data.bounds.shape['vector']
-        extra_channels = data.shape.channel.without('vector')
-        if isinstance(data, StaggeredGrid):
-            data = data.at_centers()
-        x, y, z = math.reshaped_numpy(data.points.vector[dims], [vector, data.shape.non_channel])
-        u, v, w = math.reshaped_numpy(data.values.vector[dims], [vector, extra_channels, data.shape.non_channel], force_expand=True)
-        figure.add_cone(x=x.flatten(), y=y.flatten(), z=z.flatten(), u=u.flatten(), v=v.flatten(), w=w.flatten(),
-                        colorscale='Blues',
-                        sizemode="absolute", sizeref=1,
-                        row=row, col=col)
-
-
-class VectorCloud2D(Recipe):
-
-    def can_plot(self, data: SampledField, space: Box) -> bool:
-        return isinstance(data, PointCloud) and data.spatial_rank == 2 and 'vector' in channel(data)
-
-    def plot(self, data: SampledField, figure, subplot, space: Box, min_val: float, max_val: float, show_color_bar: bool, color: Tensor, alpha: Tensor):
-        row, col = subplot
-        vector = data.bounds.shape['vector']
-        x, y = math.reshaped_numpy(data.points, [vector, data.shape.without('vector')])
-        u, v = math.reshaped_numpy(data.values, [vector, data.shape.without('vector')], force_expand=True)
-        quiver = figure_factory.create_quiver(x, y, u, v, scale=1.0).data[0]  # 7 points per arrow
-        if data.color.shape:
-            # color = data.color.numpy(data.shape.non_channel).reshape(-1)
-            warnings.warn("Multi-colored vector plots not yet supported")
-        else:
-            color = data.color.native()
-            quiver.line.update(color=color)
-        figure.add_trace(quiver, row=row, col=col)
-
-
-class PointCloud2D(Recipe):
-
-    def can_plot(self, data: SampledField, space: Box) -> bool:
-        return isinstance(data, PointCloud) and data.spatial_rank == 2
-
-    def plot(self, data: SampledField, figure, subplot, space: Box, min_val: float, max_val: float, show_color_bar: bool, color: Tensor, alpha: Tensor):
-        if isinstance(data.elements, GeometryStack):
-            for idx in data.elements.geometries.shape[0].meshgrid():
-                self.plot(data[idx], figure, subplot, space, min_val, max_val, show_color_bar, color[idx], alpha)
-            return
-        row, col = subplot
-        subplot = figure.get_subplot(row, col)
-        dims = data.bounds.vector.item_names
-        vector = data.bounds.shape['vector']
-        size = figure._phi_size
-        yrange = subplot.yaxis.range
-        if spatial(data):
-            raise NotImplementedError("Plotly does not yet support plotting point clouds with spatial dimensions")
-        for idx in non_channel(data.points).meshgrid(names=True):
-            x, y = math.reshaped_numpy(data[idx].points.vector[dims], [vector, data.shape.non_channel])
-            hex_color = color[idx].native()
-            subplot_height = (subplot.yaxis.domain[1] - subplot.yaxis.domain[0]) * size[1] * 100
-            if isinstance(data.elements, Sphere):
-                symbol = 'circle'
-                marker_size = data.elements.bounding_radius().numpy() * 1.9
-            elif isinstance(data.elements, BaseBox):
-                symbol = 'square'
-                marker_size = math.mean(data.elements.bounding_half_extent(), 'vector').numpy() * 2
-            elif isinstance(data.elements, Point):
-                symbol = 'x'
-                marker_size = 12 / (subplot_height / (yrange[1] - yrange[0]))
-            else:
-                symbol = 'asterisk'
-                marker_size = data.elements.bounding_radius().numpy()
-            marker_size *= subplot_height / (yrange[1] - yrange[0])
-            marker = graph_objects.scatter.Marker(size=marker_size, color=hex_color, sizemode='diameter', symbol=symbol)
-            figure.add_scatter(mode='markers', x=x, y=y, marker=marker, row=row, col=col)
-        figure.update_layout(showlegend=False)
-
-
-class PointCloud3D(Recipe):
-
-    def can_plot(self, data: SampledField, space: Box) -> bool:
-        return isinstance(data, PointCloud) and data.spatial_rank == 3
-
-    def plot(self, data: SampledField, figure, subplot, space: Box, min_val: float, max_val: float, show_color_bar: bool, color: Tensor, alpha: Tensor):
-        row, col = subplot
-        subplot = figure.get_subplot(row, col)
-        dims = data.bounds.vector.item_names
-        vector = data.bounds.shape['vector']
-        size = figure._phi_size
-        yrange = subplot.yaxis.range
-        if data.points.shape.non_channel.rank > 1:
-            data_list = field.unstack(data, data.points.shape.non_channel[0].name)
-            for d in data_list:
-                self.plot(d, figure, (row, col), space, min_val, max_val, show_color_bar, color)
-        else:
-            x, y, z = math.reshaped_numpy(data.points.vector[dims], [vector, data.shape.non_channel])
-            color = color.native()
-            domain_y = figure.layout[subplot.plotly_name].domain.y
-            if isinstance(data.elements, Sphere):
-                symbol = 'circle'
-                marker_size = data.elements.bounding_radius().numpy() * 2
-            elif isinstance(data.elements, BaseBox):
-                symbol = 'square'
-                marker_size = math.mean(data.elements.bounding_half_extent(), 'vector').numpy() * 1
-            elif isinstance(data.elements, Point):
-                symbol = 'x'
-                marker_size = 4 / (size[1] * (domain_y[1] - domain_y[0]) / (yrange[1] - yrange[0]) * 0.5)
-            else:
-                symbol = 'asterisk'
-                marker_size = data.elements.bounding_radius().numpy()
-            marker_size *= size[1] * (domain_y[1] - domain_y[0]) / (yrange[1] - yrange[0]) * 0.5
-            marker = graph_objects.scatter3d.Marker(size=marker_size, color=color, sizemode='diameter', symbol=symbol)
-            figure.add_scatter3d(mode='markers', x=x, y=y, z=z, marker=marker, row=row, col=col)
-        figure.update_layout(showlegend=False)
-
-
-def _get_range(bounds: Box, index: int):
-    lower = bounds.lower.vector[index].numpy()
-    upper = bounds.upper.vector[index].numpy()
-    return lower, upper
-
-
-def real_values(field: SampledField):
-    return field.values if field.values.dtype.kind != complex else abs(field.values)
-
-
-def get_div_map(zmin, zmax, equal_scale=False, colormap: str = None):
-    """
-    Args:
-      colormap(list or array, optional): colormap defined as list of [fraction_val, red_frac, green_frac, blue_frac] (Default value = None)
-      zmin: 
-      zmax: 
-      equal_scale:  (Default value = False)
-    """
-    colormap = COLORMAPS[colormap]
-    # Ensure slicing
-    cm_arr = numpy.array(colormap).astype(numpy.float64)
-    # Centeral color
-    if 0.5 not in cm_arr[:, 0]:
-        central_color = get_color_interpolation(0.5, cm_arr)[1:]
-    else:
-        central_color = cm_arr[cm_arr[:, 0] == 0.5][-1][1:]
-    # Return base
-    if zmin == zmax:
-        central_color = numpy.round(central_color).astype(numpy.int32)
-        return [(0, "rgb({},{},{})".format(*central_color)), (1, "rgb({},{},{})".format(*central_color))]
-    center = abs(zmin / (zmax - zmin))
-    if zmin > 0:
-        center = 0
-    # Rescaling
-    if not equal_scale:
-        # Full range, Zero-centered
-        neg_flag = cm_arr[:, 0] < 0.5
-        pos_flag = cm_arr[:, 0] >= 0.5
-        cm_arr[neg_flag, 0] = cm_arr[neg_flag, 0] * 2 * center  # Scale (0, 0.5) -> (0, center)
-        cm_arr[pos_flag, 0] = (cm_arr[pos_flag, 0] - 0.5) * 2 * (1 - center) + center  # Scale (0.5, 1) -> (center, 0.5)
-        # Drop duplicate zeros. Allow for not center value in original map.
-        if zmin == 0:
-            cm_arr = cm_arr[numpy.max(numpy.arange(len(cm_arr))[cm_arr[:, 0] == 0]):]
-    else:
-        cm_arr[:, 0] = cm_arr[:, 0] - 0.5  # center at zero (-0.5, 0.5)
-        # Scale desired range
-        if zmax > abs(zmin):
-            cm_scale = (1 - center) / (numpy.max(cm_arr[:, 0]))  # scale by plositives
-        else:
-            cm_scale = center / (numpy.max(cm_arr[:, 0]))  # scale by negatives
-        # Scale the maximum to +1 when centered
-        cm_arr[:, 0] *= cm_scale
-        cm_arr[:, 0] += center  # center
-        # Add zero if it doesn't exist
-        if 0 not in cm_arr[:, 0]:
-            new_min = get_color_interpolation(0, cm_arr)
-            cm_arr = numpy.vstack([new_min, cm_arr])
-        # Add one if it doesn't exist
-        if 1 not in cm_arr[:, 0]:
-            new_max = get_color_interpolation(1, cm_arr)
-            cm_arr = numpy.vstack([cm_arr, new_max])
-        # Compare center
-        # new_center = get_color_interpolation(center, cm_arr)
-        # if not all(new_center == [center, *central_color]):
-        #    print("Failed center comparison.")
-        #    print("Center: {}".format(new_center))
-        #    print("Center should be: {}".format([center, *central_color]))
-        #    assert False
-        # Cut to (0, 1)
-        cm_arr = cm_arr[cm_arr[:, 0] >= 0]
-        cm_arr = cm_arr[cm_arr[:, 0] <= 1]
-    cm_arr[:, 1:] = numpy.clip(cm_arr[:, 1:], 0, 255)
-    return [[val, "rgb({:.0f},{:.0f},{:.0f})".format(*colors)] for val, colors in zip(cm_arr[:, 0], cm_arr[:, 1:])]
-
-
-def get_color_interpolation(val, cm_arr):
-    """
-    Weighted average between point smaller and larger than it
-
-    Args:
-      val: 
-      cm_arr: 
-
-    Returns:
-
-    """
-    if 0 in cm_arr[:, 0] - val:
-        center = cm_arr[cm_arr[:, 0] == val][-1]
-    else:
-        offset_positions = cm_arr[:, 0] - val
-        color1 = cm_arr[numpy.argmax(offset_positions[offset_positions < 0])]  # largest value smaller than control
-        color2 = cm_arr[numpy.argmin(offset_positions[offset_positions > 0])]  # smallest value larger than control
-        if color1[0] == color2[0]:
-            center = color1
-        else:
-            x = (val - color1[0]) / (color2[0] - color1[0])  # weight of row2
-            center = color1 * (1 - x) + color2 * x
-    center[0] = val
-    return center
-
-
-def plot_scalars(curves: Union[tuple, list], labels, subplots=True, log_scale='', smooth: int = 1):
-    if not curves:
-        return graph_objects.Figure()
-    if subplots:
-        fig = make_subplots(rows=1, cols=len(curves), subplot_titles=labels)
-        for col, (label, (x, y)) in enumerate(zip(labels, curves)):
-            for trace in _graph(label, x, y, smooth, col):
-                fig.add_trace(trace, row=1, col=1 + col)
-    else:
-        fig = graph_objects.Figure()
-        for col, (label, (x, y)) in enumerate(zip(labels, curves)):
-            for trace in _graph(label, x, y, smooth, col):
-                fig.add_trace(trace)
-    fig.update_layout(showlegend=not subplots, paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)')
-    if 'x' in log_scale:
-        fig.update_xaxes(type='log')
-    if 'y' in log_scale:
-        fig.update_yaxes(type='log')
-    return fig
-
-
-def _graph(label: str, x: np.ndarray, y: np.ndarray, smooth: int, index: int, max_points=2000):
-    color = DEFAULT_PLOTLY_COLORS[index % len(DEFAULT_PLOTLY_COLORS)]
-    if len(x) > len(y):
-        x = x[:len(y)]
-    if len(y) > len(x):
-        y = y[:len(x)]
-    curves = split_curve(np.stack([x, y], -1))
-    low_res = [down_sample_curve(c, max_points) for c in curves]
-    x, y = join_curves(low_res).T
-    if smooth <= 1:
-        return [graph_objects.Scatter(x=x, y=y, name=label, line=graph_objects.scatter.Line(color=color))]
-    else:  # smooth
-        smooth_curves = [smooth_uniform_curve(c, smooth) for c in curves]
-        low_res_smooth = [down_sample_curve(c, max_points) for c in smooth_curves]
-        smooth_x, smooth_y = join_curves(low_res_smooth).T
-        transparent_color = f"rgba{color[3:-1]}, 0.4)"
-        return [
-            graph_objects.Scatter(x=x, y=y, line=graph_objects.scatter.Line(color=transparent_color, width=1), showlegend=False),
-            graph_objects.Scatter(x=smooth_x, y=smooth_y, name=label, line=graph_objects.scatter.Line(color=color, width=3), mode='lines')
-        ]
-
-
-def split_curve(curve: np.ndarray) -> List[np.ndarray]:
-    x = curve[..., 0]
-    backtracks = numpy.argwhere(x[1:] < x[:-1])[:, 0] + 1
-    if len(backtracks) == 0:
-        return [curve]
-    cuts = [0] + list(backtracks) + [curve.shape[-2]]
-    return [curve[s:e] for s, e in zip(cuts[:-1], cuts[1:])]
-
-
-def join_curves(curves: List[np.ndarray]) -> np.ndarray:
-    curves = [np.append(np.array(c, numpy.float), [[numpy.nan, numpy.nan]], -2) for c in curves[:-1]] + [curves[-1]]
-    return np.concatenate(curves, -2)
-
-
-PLOTLY = PlotlyPlots()
-PLOTLY.recipes.extend([
-            LinePlot(),
-            Heatmap2D(),
-            VectorField2D(),
-            VectorField3D(),
-            VectorCloud2D(),
-            Heatmap3D(),
-            PointCloud2D(),
-            PointCloud3D(),
+import warnings
+from typing import Tuple, Any, Dict, List, Callable, Union
+
+import numpy
+import numpy as np
+from plotly import graph_objects, figure_factory
+from plotly.subplots import make_subplots
+from plotly.tools import DEFAULT_PLOTLY_COLORS
+
+from phi import math, field
+from phi.field import SampledField, PointCloud, Grid, StaggeredGrid
+from phi.geom import Sphere, BaseBox, Point, Box
+from phi.geom._stack import GeometryStack
+from phi.math import Tensor, spatial, channel, non_channel
+from phi.vis._dash.colormaps import COLORMAPS
+from phi.vis._plot_util import smooth_uniform_curve, down_sample_curve
+from phi.vis._vis_base import PlottingLibrary, Recipe
+
+
+class PlotlyPlots(PlottingLibrary):
+
+    def __init__(self):
+        super().__init__('plotly', [graph_objects.Figure])
+
+    def create_figure(self,
+                      size: tuple,
+                      rows: int,
+                      cols: int,
+                      subplots: Dict[Tuple[int, int], Box],
+                      titles: Dict[Tuple[int, int], str],
+                      log_dims: Tuple[str, ...]) -> Tuple[Any, Dict[Tuple[int, int], Any]]:
+        titles = [titles.get((r, c), None) for r in range(rows) for c in range(cols)]
+        specs = [[{'type': 'xy' if subplots.get((row, col), Box()).spatial_rank < 3 else 'surface'} for col in range(cols)] for row in range(rows)]
+        fig = self.current_figure = make_subplots(rows=rows, cols=cols, subplot_titles=titles, specs=specs)
+        for (row, col), bounds in subplots.items():
+            subplot = fig.get_subplot(row + 1, col + 1)
+            if bounds.spatial_rank == 1:
+                subplot.xaxis.update(title=bounds.vector.item_names[0], range=_get_range(bounds, 0))
+            elif bounds.spatial_rank == 2:
+                subplot.xaxis.update(scaleanchor=f'y{subplot.yaxis.plotly_name[5:]}', scaleratio=1, constrain='domain', title=bounds.vector.item_names[0], range=_get_range(bounds, 0))
+                subplot.yaxis.update(constrain='domain', title=bounds.vector.item_names[1], range=_get_range(bounds, 1))
+            elif bounds.spatial_rank == 3:
+                subplot.xaxis.update(title=bounds.vector.item_names[0], range=_get_range(bounds, 0))
+                subplot.yaxis.update(title=bounds.vector.item_names[1], range=_get_range(bounds, 1))
+                subplot.zaxis.update(title=bounds.vector.item_names[2], range=_get_range(bounds, 2))
+        fig._phi_size = size
+        return fig, {pos: (pos[0]+1, pos[1]+1) for pos in subplots.keys()}
+
+    def animate(self, fig, frames: int, plot_frame_function: Callable, interval: float, repeat: bool):
+        raise NotImplementedError()
+
+    def finalize(self, figure):
+        pass
+
+    def close(self, figure):
+        pass
+
+    def show(self, figure: graph_objects.Figure):
+        figure.show()
+
+    def save(self, figure: graph_objects.Figure, path: str, dpi: float):
+        width, height = figure._phi_size
+        figure.layout.update(margin=dict(l=0, r=0, b=0, t=0))
+        scale = dpi/90.
+        figure.write_image(path, width=width * dpi / scale, height=height * dpi / scale, scale=scale)
+
+
+class LinePlot(Recipe):
+
+    def can_plot(self, data: SampledField, space: Box) -> bool:
+        return data.spatial_rank == 1 and data.is_grid
+
+    def plot(self, data: SampledField, figure, subplot, space: Box, min_val: float, max_val: float, show_color_bar: bool, color: Tensor, alpha: Tensor, err: Tensor):
+        row, col = subplot
+        subplot = figure.get_subplot(row, col)
+        x = data.points.vector[0].numpy().flatten()
+        channels = data.values.shape.channel
+        if channels.rank == 1 and channels.get_item_names(0) is not None:
+            for i, name in enumerate(channels.get_item_names(0)):
+                y = math.reshaped_native(real_values(data[{channels.name: i}]), [data.shape.spatial], to_numpy=True)
+                figure.add_trace(graph_objects.Scatter(x=x, y=y, mode='lines+markers', name=name), row=row, col=col)
+            figure.update_layout(showlegend=True)
+        else:
+            for ch_idx in channels.meshgrid():
+                y = math.reshaped_native(real_values(data[ch_idx]), [data.shape.spatial], to_numpy=True)
+                figure.add_trace(graph_objects.Scatter(x=x, y=y, mode='lines+markers', name='Multi-channel'), row=row, col=col)
+            figure.update_layout(showlegend=False)
+        if min_val is not None and max_val is not None:
+            subplot.yaxis.update(range=(min_val - .02 * (max_val - min_val), max_val + .02 * (max_val - min_val)))
+
+
+class Heatmap2D(Recipe):
+
+    def can_plot(self, data: SampledField, space: Box) -> bool:
+        return data.spatial_rank == 2 and data.is_grid and 'vector' not in data.shape
+
+    def plot(self, data: SampledField, figure, subplot, space: Box, min_val: float, max_val: float, show_color_bar: bool, color: Tensor, alpha: Tensor, err: Tensor):
+        row, col = subplot
+        dims = spatial(data)
+        values = real_values(data).numpy(dims.reversed)
+        x = data.points.vector[dims[0].name].dimension(dims[1].name)[0].numpy()
+        y = data.points.vector[dims[1].name].dimension(dims[0].name)[0].numpy()
+        min_val, max_val = numpy.nanmin(values), numpy.nanmax(values)
+        min_val, max_val = min_val if numpy.isfinite(min_val) else 0, max_val if numpy.isfinite(max_val) else 0
+        color_scale = get_div_map(min_val, max_val, equal_scale=True)
+        # color_bar = graph_objects.heatmap.ColorBar(x=1.15)   , colorbar=color_bar
+        figure.add_heatmap(row=row, col=col, x=x, y=y, z=values, zauto=False, zmin=min_val, zmax=max_val, colorscale=color_scale, showscale=show_color_bar)
+
+
+class VectorField2D(Recipe):
+
+    def can_plot(self, data: SampledField, space: Box) -> bool:
+        return data.spatial_rank == 2 and data.is_grid
+
+    def plot(self, data: SampledField, figure, subplot, space: Box, min_val: float, max_val: float, show_color_bar: bool, color: Tensor, alpha: Tensor, err: Tensor):
+        if data.is_staggered:
+            data = data.at_centers()
+        row, col = subplot
+        dims = data.bounds.vector.item_names
+        vector = data.bounds.shape['vector']
+        extra_channels = data.shape.channel.without('vector')
+        x, y = math.reshaped_numpy(data.points.vector[dims], [vector, data.shape.non_channel])
+        u, v = math.reshaped_numpy(data.values.vector[dims], [vector, extra_channels, data.shape.without(vector)])
+        for ch in range(u.shape[0]):
+            # quiver = figure_factory.create_quiver(x, y, data_x[ch], data_y[ch], scale=1.0)  # 7 points per arrow
+            # fig.add_trace(quiver, row=row, col=col)
+            u_ch = u[ch]
+            v_ch = v[ch]
+            # lines_y = numpy.stack([y, y + data_y_flat, [None] * len(x)], -1).flatten()  # 3 points per arrow
+            # lines_x = numpy.stack([x, x + data_x_flat, [None] * len(x)], -1).flatten()
+            lines_x = numpy.stack([x, x + u_ch, [None] * len(x)], -1).flatten()
+            lines_y = numpy.stack([y, y + v_ch, [None] * len(x)], -1).flatten()  # 3 points per arrow
+            name = extra_channels.get_item_names(0)[ch] if extra_channels.rank == 1 and extra_channels.get_item_names(0) is not None else None
+            figure.add_scatter(x=lines_x, y=lines_y, mode='lines', row=row, col=col, name=name)
+        if u.shape[0] == 1:
+            figure.update_layout(showlegend=False)
+
+
+class Heatmap3D(Recipe):
+
+    def can_plot(self, data: SampledField, space: Box) -> bool:
+        return data.spatial_rank == 3 and data.is_grid and data.shape.channel.volume == 1
+
+    def plot(self, data: SampledField, figure, subplot, space: Box, min_val: float, max_val: float, show_color_bar: bool, color: Tensor, alpha: Tensor, err: Tensor):
+        row, col = subplot
+        dims = data.bounds.vector.item_names
+        vector = data.bounds.shape['vector']
+        values = real_values(data).numpy(dims)
+        x, y, z = math.reshaped_numpy(data.points.vector[dims], [vector, *data.points.shape.spatial])
+        min_val, max_val = numpy.nanmin(values), numpy.nanmax(values)
+        min_val, max_val = min_val if numpy.isfinite(min_val) else 0, max_val if numpy.isfinite(max_val) else 0
+        color_scale = get_div_map(min_val, max_val, equal_scale=True)
+        figure.add_volume(x=x.flatten(), y=y.flatten(), z=z.flatten(), value=values.flatten(),
+                          showscale=show_color_bar, colorscale=color_scale, cmin=min_val, cmax=max_val, cauto=False,
+                          isomin=0.1, isomax=0.8,
+                          opacity=0.1,  # needs to be small to see through all surfaces
+                          surface_count=17,  # needs to be a large number for good volume rendering
+                          row=row, col=col)
+        figure.update_layout(uirevision=True)
+
+
+class VectorField3D(Recipe):
+
+    def can_plot(self, data: SampledField, space: Box) -> bool:
+        return data.is_grid and data.spatial_rank == 3
+
+    def plot(self, data: SampledField, figure, subplot, space: Box, min_val: float, max_val: float, show_color_bar: bool, color: Tensor, alpha: Tensor, err: Tensor):
+        row, col = subplot
+        dims = data.bounds.vector.item_names
+        vector = data.bounds.shape['vector']
+        extra_channels = data.shape.channel.without('vector')
+        if data.is_staggered:
+            data = data.at_centers()
+        x, y, z = math.reshaped_numpy(data.points.vector[dims], [vector, data.shape.non_channel])
+        u, v, w = math.reshaped_numpy(data.values.vector[dims], [vector, extra_channels, data.shape.non_channel])
+        figure.add_cone(x=x.flatten(), y=y.flatten(), z=z.flatten(), u=u.flatten(), v=v.flatten(), w=w.flatten(),
+                        colorscale='Blues',
+                        sizemode="absolute", sizeref=1,
+                        row=row, col=col)
+
+
+class VectorCloud2D(Recipe):
+
+    def can_plot(self, data: SampledField, space: Box) -> bool:
+        return data.is_point_cloud and data.spatial_rank == 2 and 'vector' in channel(data)
+
+    def plot(self, data: SampledField, figure, subplot, space: Box, min_val: float, max_val: float, show_color_bar: bool, color: Tensor, alpha: Tensor, err: Tensor):
+        row, col = subplot
+        vector = data.bounds.shape['vector']
+        x, y = math.reshaped_numpy(data.points, [vector, data.shape.without('vector')])
+        u, v = math.reshaped_numpy(data.values, [vector, data.shape.without('vector')])
+        quiver = figure_factory.create_quiver(x, y, u, v, scale=1.0).data[0]  # 7 points per arrow
+        if (color != None).all:
+            quiver.line.update(color=color.native())
+        figure.add_trace(quiver, row=row, col=col)
+
+
+class PointCloud2D(Recipe):
+
+    def can_plot(self, data: SampledField, space: Box) -> bool:
+        return data.is_point_cloud and data.spatial_rank == 2
+
+    def plot(self, data: SampledField, figure, subplot, space: Box, min_val: float, max_val: float, show_color_bar: bool, color: Tensor, alpha: Tensor, err: Tensor):
+        if isinstance(data.elements, GeometryStack):
+            for idx in data.elements.geometries.shape[0].meshgrid():
+                self.plot(data[idx], figure, subplot, space, min_val, max_val, show_color_bar, color[idx], alpha, err)
+            return
+        row, col = subplot
+        subplot = figure.get_subplot(row, col)
+        dims = data.bounds.vector.item_names
+        vector = data.bounds.shape['vector']
+        size = figure._phi_size
+        yrange = subplot.yaxis.range
+        if spatial(data):
+            raise NotImplementedError("Plotly does not yet support plotting point clouds with spatial dimensions")
+        for idx in non_channel(data.points).meshgrid(names=True):
+            x, y = math.reshaped_numpy(data[idx].points.vector[dims], [vector, data.shape.non_channel])
+            hex_color = color[idx].native()
+            subplot_height = (subplot.yaxis.domain[1] - subplot.yaxis.domain[0]) * size[1] * 100
+            if isinstance(data.elements, Sphere):
+                symbol = 'circle'
+                marker_size = data.elements.bounding_radius().numpy() * 1.9
+            elif isinstance(data.elements, BaseBox):
+                symbol = 'square'
+                marker_size = math.mean(data.elements.bounding_half_extent(), 'vector').numpy() * 2
+            elif isinstance(data.elements, Point):
+                symbol = 'x'
+                marker_size = 12 / (subplot_height / (yrange[1] - yrange[0]))
+            else:
+                symbol = 'asterisk'
+                marker_size = data.elements.bounding_radius().numpy()
+            marker_size *= subplot_height / (yrange[1] - yrange[0])
+            marker = graph_objects.scatter.Marker(size=marker_size, color=hex_color, sizemode='diameter', symbol=symbol)
+            figure.add_scatter(mode='markers', x=x, y=y, marker=marker, row=row, col=col)
+        figure.update_layout(showlegend=False)
+
+
+class PointCloud3D(Recipe):
+
+    def can_plot(self, data: SampledField, space: Box) -> bool:
+        return data.is_point_cloud and data.spatial_rank == 3
+
+    def plot(self, data: SampledField, figure, subplot, space: Box, min_val: float, max_val: float, show_color_bar: bool, color: Tensor, alpha: Tensor, err: Tensor):
+        row, col = subplot
+        subplot = figure.get_subplot(row, col)
+        dims = data.bounds.vector.item_names
+        vector = data.bounds.shape['vector']
+        size = figure._phi_size
+        yrange = subplot.yaxis.range
+        if data.points.shape.non_channel.rank > 1:
+            data_list = field.unstack(data, data.points.shape.non_channel[0].name)
+            for d in data_list:
+                self.plot(d, figure, (row, col), space, min_val, max_val, show_color_bar, color)
+        else:
+            x, y, z = math.reshaped_numpy(data.points.vector[dims], [vector, data.shape.non_channel])
+            color = color.native()
+            domain_y = figure.layout[subplot.plotly_name].domain.y
+            if isinstance(data.elements, Sphere):
+                symbol = 'circle'
+                marker_size = data.elements.bounding_radius().numpy() * 2
+            elif isinstance(data.elements, BaseBox):
+                symbol = 'square'
+                marker_size = math.mean(data.elements.bounding_half_extent(), 'vector').numpy() * 1
+            elif isinstance(data.elements, Point):
+                symbol = 'x'
+                marker_size = 4 / (size[1] * (domain_y[1] - domain_y[0]) / (yrange[1] - yrange[0]) * 0.5)
+            else:
+                symbol = 'asterisk'
+                marker_size = data.elements.bounding_radius().numpy()
+            marker_size *= size[1] * (domain_y[1] - domain_y[0]) / (yrange[1] - yrange[0]) * 0.5
+            marker = graph_objects.scatter3d.Marker(size=marker_size, color=color, sizemode='diameter', symbol=symbol)
+            figure.add_scatter3d(mode='markers', x=x, y=y, z=z, marker=marker, row=row, col=col)
+        figure.update_layout(showlegend=False)
+
+
+def _get_range(bounds: Box, index: int):
+    lower = bounds.lower.vector[index].numpy()
+    upper = bounds.upper.vector[index].numpy()
+    return lower, upper
+
+
+def real_values(field: SampledField):
+    return field.values if field.values.dtype.kind != complex else abs(field.values)
+
+
+def get_div_map(zmin, zmax, equal_scale=False, colormap: str = None):
+    """
+    Args:
+      colormap(list or array, optional): colormap defined as list of [fraction_val, red_frac, green_frac, blue_frac] (Default value = None)
+      zmin: 
+      zmax: 
+      equal_scale:  (Default value = False)
+    """
+    colormap = COLORMAPS[colormap]
+    # Ensure slicing
+    cm_arr = numpy.array(colormap).astype(numpy.float64)
+    # Centeral color
+    if 0.5 not in cm_arr[:, 0]:
+        central_color = get_color_interpolation(0.5, cm_arr)[1:]
+    else:
+        central_color = cm_arr[cm_arr[:, 0] == 0.5][-1][1:]
+    # Return base
+    if zmin == zmax:
+        central_color = numpy.round(central_color).astype(numpy.int32)
+        return [(0, "rgb({},{},{})".format(*central_color)), (1, "rgb({},{},{})".format(*central_color))]
+    center = abs(zmin / (zmax - zmin))
+    if zmin > 0:
+        center = 0
+    # Rescaling
+    if not equal_scale:
+        # Full range, Zero-centered
+        neg_flag = cm_arr[:, 0] < 0.5
+        pos_flag = cm_arr[:, 0] >= 0.5
+        cm_arr[neg_flag, 0] = cm_arr[neg_flag, 0] * 2 * center  # Scale (0, 0.5) -> (0, center)
+        cm_arr[pos_flag, 0] = (cm_arr[pos_flag, 0] - 0.5) * 2 * (1 - center) + center  # Scale (0.5, 1) -> (center, 0.5)
+        # Drop duplicate zeros. Allow for not center value in original map.
+        if zmin == 0:
+            cm_arr = cm_arr[numpy.max(numpy.arange(len(cm_arr))[cm_arr[:, 0] == 0]):]
+    else:
+        cm_arr[:, 0] = cm_arr[:, 0] - 0.5  # center at zero (-0.5, 0.5)
+        # Scale desired range
+        if zmax > abs(zmin):
+            cm_scale = (1 - center) / (numpy.max(cm_arr[:, 0]))  # scale by plositives
+        else:
+            cm_scale = center / (numpy.max(cm_arr[:, 0]))  # scale by negatives
+        # Scale the maximum to +1 when centered
+        cm_arr[:, 0] *= cm_scale
+        cm_arr[:, 0] += center  # center
+        # Add zero if it doesn't exist
+        if 0 not in cm_arr[:, 0]:
+            new_min = get_color_interpolation(0, cm_arr)
+            cm_arr = numpy.vstack([new_min, cm_arr])
+        # Add one if it doesn't exist
+        if 1 not in cm_arr[:, 0]:
+            new_max = get_color_interpolation(1, cm_arr)
+            cm_arr = numpy.vstack([cm_arr, new_max])
+        # Compare center
+        # new_center = get_color_interpolation(center, cm_arr)
+        # if not all(new_center == [center, *central_color]):
+        #    print("Failed center comparison.")
+        #    print("Center: {}".format(new_center))
+        #    print("Center should be: {}".format([center, *central_color]))
+        #    assert False
+        # Cut to (0, 1)
+        cm_arr = cm_arr[cm_arr[:, 0] >= 0]
+        cm_arr = cm_arr[cm_arr[:, 0] <= 1]
+    cm_arr[:, 1:] = numpy.clip(cm_arr[:, 1:], 0, 255)
+    return [[val, "rgb({:.0f},{:.0f},{:.0f})".format(*colors)] for val, colors in zip(cm_arr[:, 0], cm_arr[:, 1:])]
+
+
+def get_color_interpolation(val, cm_arr):
+    """
+    Weighted average between point smaller and larger than it
+
+    Args:
+      val: 
+      cm_arr: 
+
+    Returns:
+
+    """
+    if 0 in cm_arr[:, 0] - val:
+        center = cm_arr[cm_arr[:, 0] == val][-1]
+    else:
+        offset_positions = cm_arr[:, 0] - val
+        color1 = cm_arr[numpy.argmax(offset_positions[offset_positions < 0])]  # largest value smaller than control
+        color2 = cm_arr[numpy.argmin(offset_positions[offset_positions > 0])]  # smallest value larger than control
+        if color1[0] == color2[0]:
+            center = color1
+        else:
+            x = (val - color1[0]) / (color2[0] - color1[0])  # weight of row2
+            center = color1 * (1 - x) + color2 * x
+    center[0] = val
+    return center
+
+
+def plot_scalars(curves: Union[tuple, list], labels, subplots=True, log_scale='', smooth: int = 1):
+    if not curves:
+        return graph_objects.Figure()
+    if subplots:
+        fig = make_subplots(rows=1, cols=len(curves), subplot_titles=labels)
+        for col, (label, (x, y)) in enumerate(zip(labels, curves)):
+            for trace in _graph(label, x, y, smooth, col):
+                fig.add_trace(trace, row=1, col=1 + col)
+    else:
+        fig = graph_objects.Figure()
+        for col, (label, (x, y)) in enumerate(zip(labels, curves)):
+            for trace in _graph(label, x, y, smooth, col):
+                fig.add_trace(trace)
+    fig.update_layout(showlegend=not subplots, paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)')
+    if 'x' in log_scale:
+        fig.update_xaxes(type='log')
+    if 'y' in log_scale:
+        fig.update_yaxes(type='log')
+    return fig
+
+
+def _graph(label: str, x: np.ndarray, y: np.ndarray, smooth: int, index: int, max_points=2000):
+    color = DEFAULT_PLOTLY_COLORS[index % len(DEFAULT_PLOTLY_COLORS)]
+    if len(x) > len(y):
+        x = x[:len(y)]
+    if len(y) > len(x):
+        y = y[:len(x)]
+    curves = split_curve(np.stack([x, y], -1))
+    low_res = [down_sample_curve(c, max_points) for c in curves]
+    x, y = join_curves(low_res).T
+    if smooth <= 1:
+        return [graph_objects.Scatter(x=x, y=y, name=label, line=graph_objects.scatter.Line(color=color))]
+    else:  # smooth
+        smooth_curves = [smooth_uniform_curve(c, smooth) for c in curves]
+        low_res_smooth = [down_sample_curve(c, max_points) for c in smooth_curves]
+        smooth_x, smooth_y = join_curves(low_res_smooth).T
+        transparent_color = f"rgba{color[3:-1]}, 0.4)"
+        return [
+            graph_objects.Scatter(x=x, y=y, line=graph_objects.scatter.Line(color=transparent_color, width=1), showlegend=False),
+            graph_objects.Scatter(x=smooth_x, y=smooth_y, name=label, line=graph_objects.scatter.Line(color=color, width=3), mode='lines')
+        ]
+
+
+def split_curve(curve: np.ndarray) -> List[np.ndarray]:
+    x = curve[..., 0]
+    backtracks = numpy.argwhere(x[1:] < x[:-1])[:, 0] + 1
+    if len(backtracks) == 0:
+        return [curve]
+    cuts = [0] + list(backtracks) + [curve.shape[-2]]
+    return [curve[s:e] for s, e in zip(cuts[:-1], cuts[1:])]
+
+
+def join_curves(curves: List[np.ndarray]) -> np.ndarray:
+    curves = [np.append(np.array(c, numpy.float), [[numpy.nan, numpy.nan]], -2) for c in curves[:-1]] + [curves[-1]]
+    return np.concatenate(curves, -2)
+
+
+PLOTLY = PlotlyPlots()
+PLOTLY.recipes.extend([
+            LinePlot(),
+            Heatmap2D(),
+            VectorField2D(),
+            VectorField3D(),
+            VectorCloud2D(),
+            Heatmap3D(),
+            PointCloud2D(),
+            PointCloud3D(),
         ])
```

### Comparing `phiflow-2.3.4/phi/vis/_dash/board.py` & `phiflow-2.4.0/phi/vis/_dash/board.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,184 +1,184 @@
-import logging
-import os
-import traceback
-
-import dash_core_components as dcc
-import dash_html_components as html
-from dash.dependencies import Output, Input
-from dash.exceptions import PreventUpdate
-from plotly import graph_objects
-
-from .dash_app import DashApp
-from ._plotly_plots import plot_scalars
-from .player_controls import STEP_COUNT, parse_step_count
-from .._vis_base import display_name, gui_interrupt, benchmark
-
-BENCHMARK_BUTTON = Input('benchmark-button', 'n_clicks')
-PROFILE_BUTTON = Input('profile-button', 'n_clicks')
-
-NO_BENCHMARK_TEXT = '*No benchmarks available.*'
-NO_PROFILES_TEXT = '*No profiles available.*'
-
-REFRESH_GRAPHS_BUTTON = Input('refresh-graphs-button', 'n_clicks')
-
-
-def build_benchmark(dashapp):
-    assert isinstance(dashapp, DashApp)
-
-    layout = html.Div([
-        dcc.Markdown('## Benchmark'),
-        html.Div([
-            html.Button('Benchmark', id=BENCHMARK_BUTTON.component_id)
-        ]),
-        dcc.Markdown(children=NO_BENCHMARK_TEXT, id='run-statistics'),
-    ])
-
-    @dashapp.dash.callback(Output('run-statistics', 'children'), [BENCHMARK_BUTTON], [STEP_COUNT])
-    def run_benchmark(n_clicks, step_count):
-        step_count = parse_step_count(step_count, dashapp, default=1)
-        if n_clicks is None:
-            return NO_BENCHMARK_TEXT
-        if dashapp.play_status:
-            return '*Pause the vis before starting a benchmark.*'
-        # --- Run benchmark ---
-        step_count, time_elapsed = benchmark(dashapp.model, step_count)
-        output = '### Benchmark Results\n'
-        if step_count != step_count:
-            output += 'The benchmark was stopped prematurely.  \n'
-        output += 'Finished %d steps in %.03f seconds.' % (step_count, time_elapsed)
-        output += '  \n*Average*: %.04f seconds per step, %.02f steps per second.' % (
-            time_elapsed / step_count, step_count / time_elapsed)
-        return output
-
-    return layout
-
-
-def build_tf_profiler(dashapp):
-    assert isinstance(dashapp, DashApp)
-
-    layout = html.Div([
-        dcc.Markdown('## TensorFlow Profiler'),
-        html.Div([
-            html.Button('Profile', id=PROFILE_BUTTON.component_id)
-        ]),
-        dcc.Markdown(children=NO_PROFILES_TEXT, id='profile-output'),
-    ])
-
-    @dashapp.dash.callback(Output('profile-output', 'children'), [PROFILE_BUTTON], [STEP_COUNT])
-    def run_benchmark(n_clicks, step_count):
-        step_count = parse_step_count(step_count, dashapp, default=1)
-        if n_clicks is None:
-            return NO_PROFILES_TEXT
-        if dashapp.play_status:
-            return '*Pause the vis before starting a profiled run.*'
-        # --- Profile ---
-        with dashapp.model.session.profiler() as profiler:
-            timeline_file = profiler.timeline_file
-            step_count, time_elapsed = dashapp.model.benchmark(step_count)
-        output = '### Profiling Results\n'
-        if step_count != step_count:
-            output += 'The profiling run was stopped prematurely.  \n'
-        output += 'Finished %d steps in %.03f seconds.' % (step_count, time_elapsed)
-        output += '  \n*Average*: %.04f seconds per step, %.02f steps per second.' % (time_elapsed / step_count, step_count / time_elapsed)
-        output += '  \nProfile saved. Open  \n*chrome://tracing/*  \n and load file  \n *%s*' % timeline_file
-        return output
-    return layout
-
-
-TENSORBOARD_STATUS = Input('tensorboard-status', 'children')
-
-
-def build_tensorboard_launcher(dashapp):
-    assert isinstance(dashapp, DashApp)
-
-    layout = html.Div([
-        html.Div(id='tensorboard-div'),
-        dcc.Interval(id='tensorboard-init', interval=200, max_intervals=1),
-        html.Div(style={'display': 'none'}, id=TENSORBOARD_STATUS.component_id),
-    ])
-
-    @dashapp.dash.callback(Output('tensorboard-div', 'children'), [Input('tensorboard-init', 'n_intervals'), TENSORBOARD_STATUS])
-    def update(*_):
-        if 'tensorboard_url' in dashapp.config:
-            return html.A('TensorBoard', href=dashapp.config['tensorboard_url'], id='tensorboard-href')
-        else:
-            return html.Button('Launch TensorBoard', id='launch-tensorboard')
-
-    @dashapp.dash.callback(Output(TENSORBOARD_STATUS.component_id, TENSORBOARD_STATUS.component_property), [Input('launch-tensorboard', 'n_clicks')])
-    def launch_tensorboard(clicks):
-        if clicks:
-            logging.info('Launching TensorBoard...')
-            logdir = dashapp.model.session.summary_directory
-            import phi.tf._profiling as profiling
-            url = profiling.launch_tensorboard(logdir, port=dashapp.config.get('tensorboard_port', None))
-            dashapp.config['tensorboard_url'] = url
-            logging.info('TensorBoard launched, URL: %s' % url)
-            return 'running'
-        else:
-            raise PreventUpdate()
-
-    return layout
-
-
-def build_system_controls(dashapp):
-    assert isinstance(dashapp, DashApp)
-
-    layout = html.Div([
-        dcc.Markdown('## Application'),
-        html.Button('Exit / Interrupt', id='exit-button'),
-        html.Button('Kill', id='kill-button'),
-    ])
-
-    @dashapp.dash.callback(Output('kill-button', 'style'), [Input('kill-button', 'n_clicks')])
-    def exit_application(n):
-        if n:
-            logging.info('DashGUI: Killing process...')
-            os._exit(0)  # exit() does not work from Dash threads
-
-    @dashapp.dash.callback(Output('exit-button', 'style'), [Input('exit-button', 'n_clicks')])
-    def exit_application(n):
-        if n:
-            dashapp.exit_interrupt()
-
-    return layout
-
-
-def build_graph_view(dashapp):
-    layout = html.Div(style={'width': '90%', 'margin-left': 'auto', 'margin-right': 'auto'}, children=[
-        html.H2("Graphs"),
-        html.Div([
-            html.Button('Refresh now', id=REFRESH_GRAPHS_BUTTON.component_id),
-            dcc.Checklist(id='auto-refresh-checkbox', options=[{'label': 'Auto-refresh', 'value': 'refresh'}], value=['refresh'], style={'display': 'inline-block'}),
-            dcc.Checklist(id='subplots-checkbox', options=[{'label': 'Subplots', 'value': 'subplots'}], value=['subplots'], style={'display': 'inline-block'}),
-            html.Div(style={'display': 'inline-block', 'width': '200px'}, children=[
-                dcc.Slider(id='smooth-slider', min=1, max=10, marks={1: 'Off', 5: '25 steps', 10: '100'}),
-            ]),
-            dcc.Checklist(id='log-graph-checkbox', options=[{'label': 'Log(x)', 'value': 'x'}, {'label': 'Log(y)', 'value': 'y'}], value=['y'], style={'display': 'inline-block'}),
-        ]),
-        dcc.Interval(id='graph-update', interval=5000, disabled=False),
-        html.Div(id='graph-figure-container', style={'height': 600, 'width': '100%'}, children=[
-            dcc.Graph(figure={}, id='board-graph', style={'height': '100%'})
-        ])
-    ])
-
-    @dashapp.dash.callback(Output('board-graph', 'figure'), [Input('subplots-checkbox', 'value'), Input('smooth-slider', 'value'), Input('log-graph-checkbox', 'value'), REFRESH_GRAPHS_BUTTON, Input('graph-update', 'n_intervals')])
-    def update_figure(subplots, smooth, log_scale, _n1, _n2):
-        curves = [dashapp.model.get_curve(n) for n in dashapp.model.curve_names]
-        labels = [display_name(n) for n in dashapp.model.curve_names]
-        try:
-            figure = plot_scalars(curves, labels, subplots=bool(subplots), log_scale=log_scale, smooth=(smooth or 1) ** 2)
-            return figure
-        except BaseException as err:
-            traceback.print_exc()
-            fig = graph_objects.Figure()
-            fig.update_layout(title_text=repr(err))
-            return fig
-
-    @dashapp.dash.callback(Output('graph-update', 'disabled'), [Input('auto-refresh-checkbox', 'value')])
-    def enable_auto_refresh(selected):
-        if selected:
-            return False
-        else:
-            return True
-
+import logging
+import os
+import traceback
+
+import dash_core_components as dcc
+import dash_html_components as html
+from dash.dependencies import Output, Input
+from dash.exceptions import PreventUpdate
+from plotly import graph_objects
+
+from .dash_app import DashApp
+from ._plotly_plots import plot_scalars
+from .player_controls import STEP_COUNT, parse_step_count
+from .._vis_base import display_name, gui_interrupt, benchmark
+
+BENCHMARK_BUTTON = Input('benchmark-button', 'n_clicks')
+PROFILE_BUTTON = Input('profile-button', 'n_clicks')
+
+NO_BENCHMARK_TEXT = '*No benchmarks available.*'
+NO_PROFILES_TEXT = '*No profiles available.*'
+
+REFRESH_GRAPHS_BUTTON = Input('refresh-graphs-button', 'n_clicks')
+
+
+def build_benchmark(dashapp):
+    assert isinstance(dashapp, DashApp)
+
+    layout = html.Div([
+        dcc.Markdown('## Benchmark'),
+        html.Div([
+            html.Button('Benchmark', id=BENCHMARK_BUTTON.component_id)
+        ]),
+        dcc.Markdown(children=NO_BENCHMARK_TEXT, id='run-statistics'),
+    ])
+
+    @dashapp.dash.callback(Output('run-statistics', 'children'), [BENCHMARK_BUTTON], [STEP_COUNT])
+    def run_benchmark(n_clicks, step_count):
+        step_count = parse_step_count(step_count, dashapp, default=1)
+        if n_clicks is None:
+            return NO_BENCHMARK_TEXT
+        if dashapp.play_status:
+            return '*Pause the vis before starting a benchmark.*'
+        # --- Run benchmark ---
+        step_count, time_elapsed = benchmark(dashapp.model, step_count)
+        output = '### Benchmark Results\n'
+        if step_count != step_count:
+            output += 'The benchmark was stopped prematurely.  \n'
+        output += 'Finished %d steps in %.03f seconds.' % (step_count, time_elapsed)
+        output += '  \n*Average*: %.04f seconds per step, %.02f steps per second.' % (
+            time_elapsed / step_count, step_count / time_elapsed)
+        return output
+
+    return layout
+
+
+def build_tf_profiler(dashapp):
+    assert isinstance(dashapp, DashApp)
+
+    layout = html.Div([
+        dcc.Markdown('## TensorFlow Profiler'),
+        html.Div([
+            html.Button('Profile', id=PROFILE_BUTTON.component_id)
+        ]),
+        dcc.Markdown(children=NO_PROFILES_TEXT, id='profile-output'),
+    ])
+
+    @dashapp.dash.callback(Output('profile-output', 'children'), [PROFILE_BUTTON], [STEP_COUNT])
+    def run_benchmark(n_clicks, step_count):
+        step_count = parse_step_count(step_count, dashapp, default=1)
+        if n_clicks is None:
+            return NO_PROFILES_TEXT
+        if dashapp.play_status:
+            return '*Pause the vis before starting a profiled run.*'
+        # --- Profile ---
+        with dashapp.model.session.profiler() as profiler:
+            timeline_file = profiler.timeline_file
+            step_count, time_elapsed = dashapp.model.benchmark(step_count)
+        output = '### Profiling Results\n'
+        if step_count != step_count:
+            output += 'The profiling run was stopped prematurely.  \n'
+        output += 'Finished %d steps in %.03f seconds.' % (step_count, time_elapsed)
+        output += '  \n*Average*: %.04f seconds per step, %.02f steps per second.' % (time_elapsed / step_count, step_count / time_elapsed)
+        output += '  \nProfile saved. Open  \n*chrome://tracing/*  \n and load file  \n *%s*' % timeline_file
+        return output
+    return layout
+
+
+TENSORBOARD_STATUS = Input('tensorboard-status', 'children')
+
+
+def build_tensorboard_launcher(dashapp):
+    assert isinstance(dashapp, DashApp)
+
+    layout = html.Div([
+        html.Div(id='tensorboard-div'),
+        dcc.Interval(id='tensorboard-init', interval=200, max_intervals=1),
+        html.Div(style={'display': 'none'}, id=TENSORBOARD_STATUS.component_id),
+    ])
+
+    @dashapp.dash.callback(Output('tensorboard-div', 'children'), [Input('tensorboard-init', 'n_intervals'), TENSORBOARD_STATUS])
+    def update(*_):
+        if 'tensorboard_url' in dashapp.config:
+            return html.A('TensorBoard', href=dashapp.config['tensorboard_url'], id='tensorboard-href')
+        else:
+            return html.Button('Launch TensorBoard', id='launch-tensorboard')
+
+    @dashapp.dash.callback(Output(TENSORBOARD_STATUS.component_id, TENSORBOARD_STATUS.component_property), [Input('launch-tensorboard', 'n_clicks')])
+    def launch_tensorboard(clicks):
+        if clicks:
+            logging.info('Launching TensorBoard...')
+            logdir = dashapp.model.session.summary_directory
+            import phi.tf._profiling as profiling
+            url = profiling.launch_tensorboard(logdir, port=dashapp.config.get('tensorboard_port', None))
+            dashapp.config['tensorboard_url'] = url
+            logging.info('TensorBoard launched, URL: %s' % url)
+            return 'running'
+        else:
+            raise PreventUpdate()
+
+    return layout
+
+
+def build_system_controls(dashapp):
+    assert isinstance(dashapp, DashApp)
+
+    layout = html.Div([
+        dcc.Markdown('## Application'),
+        html.Button('Exit / Interrupt', id='exit-button'),
+        html.Button('Kill', id='kill-button'),
+    ])
+
+    @dashapp.dash.callback(Output('kill-button', 'style'), [Input('kill-button', 'n_clicks')])
+    def exit_application(n):
+        if n:
+            logging.info('DashGUI: Killing process...')
+            os._exit(0)  # exit() does not work from Dash threads
+
+    @dashapp.dash.callback(Output('exit-button', 'style'), [Input('exit-button', 'n_clicks')])
+    def exit_application(n):
+        if n:
+            dashapp.exit_interrupt()
+
+    return layout
+
+
+def build_graph_view(dashapp):
+    layout = html.Div(style={'width': '90%', 'margin-left': 'auto', 'margin-right': 'auto'}, children=[
+        html.H2("Graphs"),
+        html.Div([
+            html.Button('Refresh now', id=REFRESH_GRAPHS_BUTTON.component_id),
+            dcc.Checklist(id='auto-refresh-checkbox', options=[{'label': 'Auto-refresh', 'value': 'refresh'}], value=['refresh'], style={'display': 'inline-block'}),
+            dcc.Checklist(id='subplots-checkbox', options=[{'label': 'Subplots', 'value': 'subplots'}], value=['subplots'], style={'display': 'inline-block'}),
+            html.Div(style={'display': 'inline-block', 'width': '200px'}, children=[
+                dcc.Slider(id='smooth-slider', min=1, max=10, marks={1: 'Off', 5: '25 steps', 10: '100'}),
+            ]),
+            dcc.Checklist(id='log-graph-checkbox', options=[{'label': 'Log(x)', 'value': 'x'}, {'label': 'Log(y)', 'value': 'y'}], value=['y'], style={'display': 'inline-block'}),
+        ]),
+        dcc.Interval(id='graph-update', interval=5000, disabled=False),
+        html.Div(id='graph-figure-container', style={'height': 600, 'width': '100%'}, children=[
+            dcc.Graph(figure={}, id='board-graph', style={'height': '100%'})
+        ])
+    ])
+
+    @dashapp.dash.callback(Output('board-graph', 'figure'), [Input('subplots-checkbox', 'value'), Input('smooth-slider', 'value'), Input('log-graph-checkbox', 'value'), REFRESH_GRAPHS_BUTTON, Input('graph-update', 'n_intervals')])
+    def update_figure(subplots, smooth, log_scale, _n1, _n2):
+        curves = [dashapp.model.get_curve(n) for n in dashapp.model.curve_names]
+        labels = [display_name(n) for n in dashapp.model.curve_names]
+        try:
+            figure = plot_scalars(curves, labels, subplots=bool(subplots), log_scale=log_scale, smooth=(smooth or 1) ** 2)
+            return figure
+        except BaseException as err:
+            traceback.print_exc()
+            fig = graph_objects.Figure()
+            fig.update_layout(title_text=repr(err))
+            return fig
+
+    @dashapp.dash.callback(Output('graph-update', 'disabled'), [Input('auto-refresh-checkbox', 'value')])
+    def enable_auto_refresh(selected):
+        if selected:
+            return False
+        else:
+            return True
+
     return layout
```

### Comparing `phiflow-2.3.4/phi/vis/_dash/colormaps.py` & `phiflow-2.4.0/phi/vis/_dash/colormaps.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,104 +1,104 @@
-# Scivis Colormaps
-# https://sciviscolor.org/
-import warnings
-
-import numpy as np
-
-from phi.math.backend import PHI_LOGGER
-
-ORANGE_WHITE_BLUE = [
-    [0.      ,  22.00000005,   1.00000035,  76.0000011 ],
-    [0.030334,  28.999875  ,   5.999997  , 114.9999    ],
-    [0.055527,  26.99991   ,  13.000002  , 130.00002   ],
-    [0.073008,  10.0000035 ,  10.0000035 , 142.99992   ],
-    [0.089974,   7.9999875 ,  24.999996  , 153.        ],
-    [0.106427,  11.0000115 ,  42.00003   , 163.00008   ],
-    [0.130077,  14.00001   ,  61.999935  , 172.999905  ],
-    [0.16144 ,  14.00001   ,  80.999985  , 181.00002   ],
-    [0.2     ,  13.000002  , 100.99989   , 188.99988   ],
-    [0.225   ,  10.0000035 , 119.000085  , 195.999885  ],
-    [0.25    ,   7.9999875 , 137.000025  , 200.999925  ],
-    [0.276093,   7.9999875 , 156.99993   , 207.000075  ],
-    [0.302828,   5.999997  , 181.00002   , 212.000115  ],
-    [0.329563,  13.000002  , 204.        , 216.9999    ],
-    [0.351671,  17.999991  , 218.00001   , 221.99994   ],
-    [0.372237,  66.999975  , 230.000055  , 219.999975  ],
-    [0.390231, 107.999895  , 239.99988   , 223.00005   ],
-    [0.417995, 145.999995  , 246.00003   , 212.99997   ],
-    [0.436504, 168.00012   , 249.99996   , 214.999935  ],
-    [0.456041, 195.00003   , 249.99996   , 221.000085  ],
-    [0.468895, 211.000005  , 249.99996   , 226.000125  ],
-    [0.482262, 227.0000004 , 251.99999895, 236.000001  ],
-    [0.492545, 232.999875  , 251.999925  , 239.000025  ],
-    [0.5     , 244.0966743 , 253.51315365, 243.539586  ],  # Inserted
-    [0.501285, 255.        , 255.        , 248.0000001 ],
-    [0.510026, 251.99999895, 251.99999895, 230.99999925],
-    [0.526478, 252.9999993 , 248.0000001 , 205.00000035],
-    [0.539846, 253.000035  , 246.00003   , 181.999875  ],
-    [0.554756, 251.999925  , 244.000065  , 163.999935  ],
-    [0.576864, 249.99996   , 233.999985  , 130.00002   ],
-    [0.599486, 246.999885  , 223.00005   , 103.999965  ],
-    [0.620051, 242.0001    , 209.999895  ,  82.000095  ],
-    [0.636504, 237.00006   , 198.000105  ,  70.999905  ],
-    [0.660668, 232.00002   , 182.999985  ,  59.99997   ],
-    [0.682262, 226.99998   , 168.00012   ,  49.99989   ],
-    [0.7     , 223.999905  , 158.00004   ,  42.999885  ],
-    [0.725   , 221.99994   , 140.0001    ,  40.000065  ],
-    [0.75    , 216.9999    , 121.00005   ,  36.99999   ],
-    [0.775   , 212.000115  , 105.000075  ,  33.999915  ],
-    [0.8     , 207.000075  ,  87.99999   ,  28.999875  ],
-    [0.825   , 200.999925  ,  68.000085  ,  23.999988  ],
-    [0.85    , 188.99988   ,  47.00007   ,  18.999999  ],
-    [0.875   , 175.99998   ,  31.99995   ,  16.0000005 ],
-    [0.9     , 158.00004   ,  16.0000005 ,  11.0000115 ],
-    [0.923393, 140.0001    ,   7.000005  ,  17.999991  ],
-    [0.943959, 119.99994   ,   4.0000065 ,  23.0000055 ],
-    [0.967095, 102.        ,   1.00000035,  26.000055  ],
-    [1.      ,  47.99999895,   0.        ,  18.0000012 ]]
-
-BLUE_WHITE_RED = [[0, 0, 0, 220],
-                  [0.5, 220, 220, 220],
-                  [1,   220,   0,   0]]
-
-
-VIRIDIS_EXTENDED = [
-    [0.0, 255, 200,100],
-    [.13, 255, 153, 51],
-    [.25, 230,  5,  40],
-    [.38, 150,  3,  62],
-    [0.5, 68,   1,  84],
-    [.55, 72,  33, 115],
-    [.59, 67,  62, 133],
-    [.64, 56,  88, 140],
-    [.68, 45, 112, 142],
-    [.73, 37, 133, 142],
-    [.77, 30, 155, 138],
-    [.82, 42, 176, 127],
-    [.86, 82, 197, 105],
-    [.90, 34,  213, 73],
-    [.95, 194, 223, 35],
-    [1.0, 253, 231, 37]
-]
-
-
-COLORMAPS = {
-    None: VIRIDIS_EXTENDED,
-    'OrWhBl': ORANGE_WHITE_BLUE,
-    'viridisx': VIRIDIS_EXTENDED,
-}
-
-
-# --- Load available Matplotlib color maps ---
-try:
-    from matplotlib.pyplot import colormaps
-    from matplotlib.cm import get_cmap
-    from matplotlib.colors import ListedColormap
-
-    for name in colormaps():
-        colormap = get_cmap(name)
-        if isinstance(colormap, ListedColormap):
-            pos = np.expand_dims(np.linspace(0, 1, len(colormap.colors)), axis=-1)
-            cols = np.array(colormap.colors) * 255
-            COLORMAPS[name] = np.concatenate([pos, cols], axis=-1)
-except ImportError:
-    warnings.warn('matplotlib is not installed. Corresponding colormaps are not available.', ImportWarning)
+# Scivis Colormaps
+# https://sciviscolor.org/
+import warnings
+
+import numpy as np
+
+from phi.math.backend import PHI_LOGGER
+
+ORANGE_WHITE_BLUE = [
+    [0.      ,  22.00000005,   1.00000035,  76.0000011 ],
+    [0.030334,  28.999875  ,   5.999997  , 114.9999    ],
+    [0.055527,  26.99991   ,  13.000002  , 130.00002   ],
+    [0.073008,  10.0000035 ,  10.0000035 , 142.99992   ],
+    [0.089974,   7.9999875 ,  24.999996  , 153.        ],
+    [0.106427,  11.0000115 ,  42.00003   , 163.00008   ],
+    [0.130077,  14.00001   ,  61.999935  , 172.999905  ],
+    [0.16144 ,  14.00001   ,  80.999985  , 181.00002   ],
+    [0.2     ,  13.000002  , 100.99989   , 188.99988   ],
+    [0.225   ,  10.0000035 , 119.000085  , 195.999885  ],
+    [0.25    ,   7.9999875 , 137.000025  , 200.999925  ],
+    [0.276093,   7.9999875 , 156.99993   , 207.000075  ],
+    [0.302828,   5.999997  , 181.00002   , 212.000115  ],
+    [0.329563,  13.000002  , 204.        , 216.9999    ],
+    [0.351671,  17.999991  , 218.00001   , 221.99994   ],
+    [0.372237,  66.999975  , 230.000055  , 219.999975  ],
+    [0.390231, 107.999895  , 239.99988   , 223.00005   ],
+    [0.417995, 145.999995  , 246.00003   , 212.99997   ],
+    [0.436504, 168.00012   , 249.99996   , 214.999935  ],
+    [0.456041, 195.00003   , 249.99996   , 221.000085  ],
+    [0.468895, 211.000005  , 249.99996   , 226.000125  ],
+    [0.482262, 227.0000004 , 251.99999895, 236.000001  ],
+    [0.492545, 232.999875  , 251.999925  , 239.000025  ],
+    [0.5     , 244.0966743 , 253.51315365, 243.539586  ],  # Inserted
+    [0.501285, 255.        , 255.        , 248.0000001 ],
+    [0.510026, 251.99999895, 251.99999895, 230.99999925],
+    [0.526478, 252.9999993 , 248.0000001 , 205.00000035],
+    [0.539846, 253.000035  , 246.00003   , 181.999875  ],
+    [0.554756, 251.999925  , 244.000065  , 163.999935  ],
+    [0.576864, 249.99996   , 233.999985  , 130.00002   ],
+    [0.599486, 246.999885  , 223.00005   , 103.999965  ],
+    [0.620051, 242.0001    , 209.999895  ,  82.000095  ],
+    [0.636504, 237.00006   , 198.000105  ,  70.999905  ],
+    [0.660668, 232.00002   , 182.999985  ,  59.99997   ],
+    [0.682262, 226.99998   , 168.00012   ,  49.99989   ],
+    [0.7     , 223.999905  , 158.00004   ,  42.999885  ],
+    [0.725   , 221.99994   , 140.0001    ,  40.000065  ],
+    [0.75    , 216.9999    , 121.00005   ,  36.99999   ],
+    [0.775   , 212.000115  , 105.000075  ,  33.999915  ],
+    [0.8     , 207.000075  ,  87.99999   ,  28.999875  ],
+    [0.825   , 200.999925  ,  68.000085  ,  23.999988  ],
+    [0.85    , 188.99988   ,  47.00007   ,  18.999999  ],
+    [0.875   , 175.99998   ,  31.99995   ,  16.0000005 ],
+    [0.9     , 158.00004   ,  16.0000005 ,  11.0000115 ],
+    [0.923393, 140.0001    ,   7.000005  ,  17.999991  ],
+    [0.943959, 119.99994   ,   4.0000065 ,  23.0000055 ],
+    [0.967095, 102.        ,   1.00000035,  26.000055  ],
+    [1.      ,  47.99999895,   0.        ,  18.0000012 ]]
+
+BLUE_WHITE_RED = [[0, 0, 0, 220],
+                  [0.5, 220, 220, 220],
+                  [1,   220,   0,   0]]
+
+
+VIRIDIS_EXTENDED = [
+    [0.0, 255, 200,100],
+    [.13, 255, 153, 51],
+    [.25, 230,  5,  40],
+    [.38, 150,  3,  62],
+    [0.5, 68,   1,  84],
+    [.55, 72,  33, 115],
+    [.59, 67,  62, 133],
+    [.64, 56,  88, 140],
+    [.68, 45, 112, 142],
+    [.73, 37, 133, 142],
+    [.77, 30, 155, 138],
+    [.82, 42, 176, 127],
+    [.86, 82, 197, 105],
+    [.90, 34,  213, 73],
+    [.95, 194, 223, 35],
+    [1.0, 253, 231, 37]
+]
+
+
+COLORMAPS = {
+    None: VIRIDIS_EXTENDED,
+    'OrWhBl': ORANGE_WHITE_BLUE,
+    'viridisx': VIRIDIS_EXTENDED,
+}
+
+
+# --- Load available Matplotlib color maps ---
+try:
+    from matplotlib.pyplot import colormaps
+    from matplotlib.colors import ListedColormap
+    from matplotlib import colormaps as cms
+
+    for name in colormaps():
+        colormap = cms[name]
+        if isinstance(colormap, ListedColormap):
+            pos = np.expand_dims(np.linspace(0, 1, len(colormap.colors)), axis=-1)
+            cols = np.array(colormap.colors) * 255
+            COLORMAPS[name] = np.concatenate([pos, cols], axis=-1)
+except ImportError:
+    warnings.warn('matplotlib is not installed. Corresponding colormaps are not available.', ImportWarning)
```

### Comparing `phiflow-2.3.4/phi/vis/_dash/dash_app.py` & `phiflow-2.4.0/phi/vis/_dash/dash_app.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,93 +1,93 @@
-import dash
-import dash_core_components as dcc
-import dash_html_components as html
-from dash.dependencies import Input, Output
-from dash.exceptions import PreventUpdate
-
-from phi.vis._vis_base import VisModel, play_async, status_message, gui_interrupt
-
-
-class DashApp:
-
-    def __init__(self, model: VisModel, config: dict, header_layout):
-        self.model = model
-        self.config = config
-        self.dash = dash.Dash(u'PhiFlow')
-        self.dash.config.suppress_callback_exceptions = True
-        self.hrefs = set()
-        self.page_urls = {}
-        self.field_minmax = {}
-        self.minmax_decay = 0.975
-        self.play_status = None
-        
-        # The index page encapsulates the specific pages.
-        self.dash.layout = html.Div([
-            dcc.Location(id='url', refresh=False),
-            header_layout,
-            html.Div(id='page-content')  # Content is set using the URL
-        ], style={'fontFamily': 'arial'})
-
-        @self.dash.callback(Output('page-content', 'children'), [Input('url', 'pathname')])
-        def display_page(pathname):
-            if pathname in self.page_urls:
-                layout = self.page_urls[pathname]
-                if callable(layout):
-                    return layout(self)
-                else:
-                    return layout
-            else:
-                return html.Div([
-                    html.Div('404 - No such page: %s' % pathname),
-                    dcc.Link('Back to main page', href='/'),
-                ])
-
-    @property
-    def server(self):
-        return self.dash.server
-
-    def search_callback(self, output, hrefs):
-        assert isinstance(output, Output)
-        self.hrefs.update(hrefs)
-
-        def decorator(func):
-            @self.dash.callback(output, [Input('url', 'search')])
-            def href_callback(search):
-                if search in hrefs:
-                    func(search)
-                else:
-                    raise PreventUpdate()
-        return decorator
-
-    def consumes(self, href):
-        return href in self.hrefs
-
-    def add_page(self, path, page_layout):
-        self.page_urls[path] = page_layout
-
-    def reset_field_summary(self):
-        self.field_minmax = {}
-
-    def get_minmax(self, field):
-        if field in self.field_minmax:
-            return self.field_minmax[field]
-        else:
-            return 0, 0
-
-    def play(self, max_steps=None):
-        if not self.play_status:
-            framerate = self.config.get('framerate', None)
-            self.play_status = play_async(self.model, max_steps=max_steps, framerate=framerate)
-
-    def pause(self):
-        if self.play_status:
-            self.play_status.pause()
-
-    @property
-    def status_message(self):
-        return status_message(self.model, self.play_status)
-
-    def exit_interrupt(self):
-        self.pause()
-        if self.model.can_progress:
-            self.model.pre_step.append(gui_interrupt)
-            self.model.progress()
+import dash
+import dash_core_components as dcc
+import dash_html_components as html
+from dash.dependencies import Input, Output
+from dash.exceptions import PreventUpdate
+
+from phi.vis._vis_base import VisModel, play_async, status_message, gui_interrupt
+
+
+class DashApp:
+
+    def __init__(self, model: VisModel, config: dict, header_layout):
+        self.model = model
+        self.config = config
+        self.dash = dash.Dash(u'PhiFlow')
+        self.dash.config.suppress_callback_exceptions = True
+        self.hrefs = set()
+        self.page_urls = {}
+        self.field_minmax = {}
+        self.minmax_decay = 0.975
+        self.play_status = None
+        
+        # The index page encapsulates the specific pages.
+        self.dash.layout = html.Div([
+            dcc.Location(id='url', refresh=False),
+            header_layout,
+            html.Div(id='page-content')  # Content is set using the URL
+        ], style={'fontFamily': 'arial'})
+
+        @self.dash.callback(Output('page-content', 'children'), [Input('url', 'pathname')])
+        def display_page(pathname):
+            if pathname in self.page_urls:
+                layout = self.page_urls[pathname]
+                if callable(layout):
+                    return layout(self)
+                else:
+                    return layout
+            else:
+                return html.Div([
+                    html.Div('404 - No such page: %s' % pathname),
+                    dcc.Link('Back to main page', href='/'),
+                ])
+
+    @property
+    def server(self):
+        return self.dash.server
+
+    def search_callback(self, output, hrefs):
+        assert isinstance(output, Output)
+        self.hrefs.update(hrefs)
+
+        def decorator(func):
+            @self.dash.callback(output, [Input('url', 'search')])
+            def href_callback(search):
+                if search in hrefs:
+                    func(search)
+                else:
+                    raise PreventUpdate()
+        return decorator
+
+    def consumes(self, href):
+        return href in self.hrefs
+
+    def add_page(self, path, page_layout):
+        self.page_urls[path] = page_layout
+
+    def reset_field_summary(self):
+        self.field_minmax = {}
+
+    def get_minmax(self, field):
+        if field in self.field_minmax:
+            return self.field_minmax[field]
+        else:
+            return 0, 0
+
+    def play(self, max_steps=None):
+        if not self.play_status:
+            framerate = self.config.get('framerate', None)
+            self.play_status = play_async(self.model, max_steps=max_steps, framerate=framerate)
+
+    def pause(self):
+        if self.play_status:
+            self.play_status.pause()
+
+    @property
+    def status_message(self):
+        return status_message(self.model, self.play_status)
+
+    def exit_interrupt(self):
+        self.pause()
+        if self.model.can_progress:
+            self.model.pre_step.append(gui_interrupt)
+            self.model.progress()
```

### Comparing `phiflow-2.3.4/phi/vis/_dash/info.py` & `phiflow-2.4.0/phi/vis/_dash/info.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,83 +1,83 @@
-# coding=utf-8
-import inspect
-import datetime
-from os.path import dirname, join
-
-import dash_core_components as dcc
-import dash_html_components as html
-import socket
-from dash.dependencies import Input, Output
-
-import phi
-from .dash_app import DashApp
-
-
-def build_app_details(dashapp):
-    assert isinstance(dashapp, DashApp)
-    app = dashapp.model
-    try:
-        app_file = inspect.getfile(app.__class__)
-    except TypeError:
-        app_file = 'Unknown'
-    return dcc.Markdown(f"""
-## Details
-
-Host: {socket.gethostname()}
-
-Script path: {app_file}
-
-Data path: {app.scene}
-    """)
-
-
-def build_description(dashapp):
-    assert isinstance(dashapp, DashApp)
-    app = dashapp.model
-    md_src = _description_markdown_src(app.name, app.description)
-    return dcc.Markdown(children=md_src, id='info_markdown')
-
-
-def _description_markdown_src(title, subtitle=''):
-    if subtitle is not None and len(subtitle) > 0:
-        return """
-# %s
-
----
-
-> **_About this application:_**
-
-%s
-
----""" % (title, subtitle)
-    else:
-        return '# %s' % title
-
-
-def build_phiflow_info(dashapp):
-    root_dir = dirname(dirname(inspect.getfile(phi)))
-    setup_file = join(root_dir, 'setup.py')
-    version = phi.__version__
-    return dcc.Markdown(u"""
-This application is based on the open-source simulation framework [Φ-Flow](https://github.com/tum-pbs/PhiFlow), version %s.
-""" % version)
-
-
-def build_app_time(dashapp):
-    start_time = datetime.datetime.fromtimestamp(dashapp.model.start_time)
-
-    def build_text():
-        now = datetime.datetime.now()
-        elapsed = now - start_time
-        minutes, seconds = divmod(elapsed.seconds, 60)
-        return 'Application started: %s (Running for %d minutes and %d seconds)' % (start_time.ctime(), minutes, seconds)
-
-    layout = html.Div([
-        dcc.Markdown(children=build_text(), id='clock-output'),
-        dcc.Interval(id='clock', interval=1000)
-    ])
-
-    @dashapp.dash.callback(Output('clock-output', 'children'), [Input('clock', 'n_intervals')])
-    def update_clock(_):
-        return build_text()
-
-    return layout
+# coding=utf-8
+import inspect
+import datetime
+from os.path import dirname, join
+
+import dash_core_components as dcc
+import dash_html_components as html
+import socket
+from dash.dependencies import Input, Output
+
+import phi
+from .dash_app import DashApp
+
+
+def build_app_details(dashapp):
+    assert isinstance(dashapp, DashApp)
+    app = dashapp.model
+    try:
+        app_file = inspect.getfile(app.__class__)
+    except TypeError:
+        app_file = 'Unknown'
+    return dcc.Markdown(f"""
+## Details
+
+Host: {socket.gethostname()}
+
+Script path: {app_file}
+
+Data path: {app.scene}
+    """)
+
+
+def build_description(dashapp):
+    assert isinstance(dashapp, DashApp)
+    app = dashapp.model
+    md_src = _description_markdown_src(app.name, app.description)
+    return dcc.Markdown(children=md_src, id='info_markdown')
+
+
+def _description_markdown_src(title, subtitle=''):
+    if subtitle is not None and len(subtitle) > 0:
+        return """
+# %s
+
+---
+
+> **_About this application:_**
+
+%s
+
+---""" % (title, subtitle)
+    else:
+        return '# %s' % title
+
+
+def build_phiflow_info(dashapp):
+    root_dir = dirname(dirname(inspect.getfile(phi)))
+    setup_file = join(root_dir, 'setup.py')
+    version = phi.__version__
+    return dcc.Markdown(u"""
+This application is based on the open-source simulation framework [Φ-Flow](https://github.com/tum-pbs/PhiFlow), version %s.
+""" % version)
+
+
+def build_app_time(dashapp):
+    start_time = datetime.datetime.fromtimestamp(dashapp.model.start_time)
+
+    def build_text():
+        now = datetime.datetime.now()
+        elapsed = now - start_time
+        minutes, seconds = divmod(elapsed.seconds, 60)
+        return 'Application started: %s (Running for %d minutes and %d seconds)' % (start_time.ctime(), minutes, seconds)
+
+    layout = html.Div([
+        dcc.Markdown(children=build_text(), id='clock-output'),
+        dcc.Interval(id='clock', interval=1000)
+    ])
+
+    @dashapp.dash.callback(Output('clock-output', 'children'), [Input('clock', 'n_intervals')])
+    def update_clock(_):
+        return build_text()
+
+    return layout
```

### Comparing `phiflow-2.3.4/phi/vis/_dash/log.py` & `phiflow-2.4.0/phi/vis/_dash/log.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,34 +1,34 @@
-
-import dash_core_components as dcc
-import dash_html_components as html
-
-from .player_controls import STEP_COMPLETE
-from .dash_app import DashApp
-from dash.dependencies import Input, Output
-
-
-def build_log(dashapp):
-    assert isinstance(dashapp, DashApp)
-
-    layout = html.Div([
-        html.Button('Refresh', id='log-refresh'),
-        html.Div(id='log-dump', style={'background-color': '#F0F0F0'}, children=[
-        ]),
-        dcc.Interval(id='initialize-log', interval=200, max_intervals=1)
-    ])
-
-    @dashapp.dash.callback(Output('log-dump', 'children'), [STEP_COMPLETE, Input('initialize-log', 'n_intervals'), Input('log-refresh', 'n_clicks')])
-    def refresh_log(*args):
-        try:
-            log_file = dashapp.model.log_file
-            if log_file:
-                with open(log_file, 'r') as stream:
-                    log_text = stream.read()
-                paragraphs = log_text.split('\n')
-                return [html.P(paragraph) for paragraph in paragraphs]
-            else:
-                return "Log no available. Set scene=True or pass an existing Scene to view() to enable logging."
-        except BaseException as exc:
-            return 'Could not load log file: %s' % exc
-
-    return layout
+
+import dash_core_components as dcc
+import dash_html_components as html
+
+from .player_controls import STEP_COMPLETE
+from .dash_app import DashApp
+from dash.dependencies import Input, Output
+
+
+def build_log(dashapp):
+    assert isinstance(dashapp, DashApp)
+
+    layout = html.Div([
+        html.Button('Refresh', id='log-refresh'),
+        html.Div(id='log-dump', style={'background-color': '#F0F0F0'}, children=[
+        ]),
+        dcc.Interval(id='initialize-log', interval=200, max_intervals=1)
+    ])
+
+    @dashapp.dash.callback(Output('log-dump', 'children'), [STEP_COMPLETE, Input('initialize-log', 'n_intervals'), Input('log-refresh', 'n_clicks')])
+    def refresh_log(*args):
+        try:
+            log_file = dashapp.model.log_file
+            if log_file:
+                with open(log_file, 'r') as stream:
+                    log_text = stream.read()
+                paragraphs = log_text.split('\n')
+                return [html.P(paragraph) for paragraph in paragraphs]
+            else:
+                return "Log no available. Set scene=True or pass an existing Scene to view() to enable logging."
+        except BaseException as exc:
+            return 'Could not load log file: %s' % exc
+
+    return layout
```

### Comparing `phiflow-2.3.4/phi/vis/_dash/model_controls.py` & `phiflow-2.4.0/phi/vis/_dash/model_controls.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,112 +1,112 @@
-import numpy as np
-
-import dash_core_components as dcc
-import dash_html_components as html
-from dash.dependencies import Input, Output
-
-from .dash_app import DashApp
-from .._vis_base import display_name, value_range, is_log_control
-
-
-def all_controls(app: DashApp):
-    return tuple(Input(control.name, 'value') for control in app.model.controls)
-
-
-def build_model_controls(app: DashApp):
-    controls = app.model.controls
-    if not controls:
-        return html.Div()
-    model_floats = [control for control in controls if control.control_type == float]
-    model_bools = [control for control in controls if control.control_type == bool]
-    model_ints = [control for control in controls if control.control_type == int]
-    model_texts = [control for control in controls if control.control_type == str]
-
-    # MODEL_ACTIONS.extend([Input(action.name, 'n_clicks') for action in actions])
-
-    layout = html.Div(style={'width': '75%', 'margin-left': 'auto', 'margin-right': 'auto', 'background-color': '#F0F0F0'}, children=[
-        html.Div(id='control-div', style={'width': '95%', 'height': '90%', 'margin-left': 'auto', 'margin-right': 'auto', 'margin-top': 15, 'margin-bottom': 'auto'}, children=[
-            dcc.Interval(id='initialize-controls', interval=200, max_intervals=1)
-        ]),
-    ])
-
-    @app.dash.callback(Output('control-div', 'children'), [Input('initialize-controls', 'n_intervals')])
-    def build_controls(_):
-        model_sliders_float = create_sliders(model_floats)
-        model_sliders_int = create_sliders(model_ints)
-        model_checkboxes = [dcc.Checklist(options=[{'label': display_name(control.name), 'value': control.name}], value=[control.name] if control.initial else [], id=control.name)
-                            for control in model_bools]
-        model_textfields = []
-        for control in model_texts:
-            if not control.value_range:
-                text_area = dcc.Textarea(placeholder=control.initial, id=control.name, value=control.initial, rows=1, style={'width': '600px', 'display': 'inline-block'})
-                model_textfields.append(html.Div([display_name(control.name) + '  ', text_area]))
-            else:
-                options = [{'label': o, 'value': o} for o in value_range(control)[1]]
-                dropdown = dcc.Dropdown(id=control.name, options=options, value=control.initial, style={'display': 'inline-block', 'width': 200})
-                model_textfields.append(html.Div([display_name(control.name) + '  ', dropdown]))
-        return [
-            dcc.Markdown('### Model'),
-            *model_sliders_float,
-            *model_sliders_int,
-            *model_textfields,
-            *model_checkboxes
-        ]
-
-    for control in model_floats:
-        @app.dash.callback(Output(control.name, 'disabled'), [Input(control.name, 'value')])
-        def set_model_value(slider_value, control=control):
-            if is_log_control(control):
-                value = 10.0 ** slider_value
-                if value * 0.99 <= value_range(control)[0]:
-                    value = value_range(control)[0]
-            else:
-                value = slider_value
-            app.model.set_control_value(control.name, value)
-            return False
-
-    for control in model_ints:
-        @app.dash.callback(Output(control.name, 'step'), [Input(control.name, 'value')])
-        def set_model_value(value, control=control):
-            app.model.set_control_value(control.name, value)
-            return 1
-
-    for control in model_bools:
-        @app.dash.callback(Output(control.name, 'style'), [Input(control.name, 'value')])
-        def set_model_bool(values, control=control):
-            app.model.set_control_value(control.name, True if values else False)
-            return {}
-
-    for control in model_texts:
-        @app.dash.callback(Output(control.name, 'disabled'), [Input(control.name, 'value')])
-        def set_model_text(value, control=control):
-            if value is not None:
-                app.model.set_control_value(control.name, value)
-            return False
-
-    return layout
-
-
-def create_sliders(controls):
-    sliders = []
-    for control in controls:
-        val = control.initial
-        lower, upper = value_range(control)
-        use_log = is_log_control(control)
-        if use_log:
-            magn = np.log10(val)
-            slider_min = np.log10(lower)
-            slider_max = np.log10(upper)
-            stepsize_magn = 0.1
-            marks = {e: '{:.1e}'.format(np.power(10.0, e)) for e in range(-20, 20) if slider_min <= e <= slider_max}
-            slider = dcc.Slider(min=slider_min, max=slider_max, value=magn, id=control.name, step=stepsize_magn, updatemode='drag', marks=marks)
-        else:
-            if control.control_type == int:
-                marks = {v: str(v) for v in range(lower, upper + 1)}
-                step = 1
-            else:
-                marks = {float(v): str(round(v, 4)) for v in np.linspace(lower, upper, 21)}
-                step = (upper-lower) / (len(marks)-1)
-            slider = dcc.Slider(min=lower, max=upper, value=val, id=control.name, step=step, marks=marks, updatemode='drag')
-        slider_container = html.Div(style={'height': '50px', 'width': '100%', 'display': 'inline-block'}, children=[display_name(control.name), slider])
-        sliders.append(slider_container)
-    return sliders
+import numpy as np
+
+import dash_core_components as dcc
+import dash_html_components as html
+from dash.dependencies import Input, Output
+
+from .dash_app import DashApp
+from .._vis_base import display_name, value_range, is_log_control
+
+
+def all_controls(app: DashApp):
+    return tuple(Input(control.name, 'value') for control in app.model.controls)
+
+
+def build_model_controls(app: DashApp):
+    controls = app.model.controls
+    if not controls:
+        return html.Div()
+    model_floats = [control for control in controls if control.control_type == float]
+    model_bools = [control for control in controls if control.control_type == bool]
+    model_ints = [control for control in controls if control.control_type == int]
+    model_texts = [control for control in controls if control.control_type == str]
+
+    # MODEL_ACTIONS.extend([Input(action.name, 'n_clicks') for action in actions])
+
+    layout = html.Div(style={'width': '75%', 'margin-left': 'auto', 'margin-right': 'auto', 'background-color': '#F0F0F0'}, children=[
+        html.Div(id='control-div', style={'width': '95%', 'height': '90%', 'margin-left': 'auto', 'margin-right': 'auto', 'margin-top': 15, 'margin-bottom': 'auto'}, children=[
+            dcc.Interval(id='initialize-controls', interval=200, max_intervals=1)
+        ]),
+    ])
+
+    @app.dash.callback(Output('control-div', 'children'), [Input('initialize-controls', 'n_intervals')])
+    def build_controls(_):
+        model_sliders_float = create_sliders(model_floats)
+        model_sliders_int = create_sliders(model_ints)
+        model_checkboxes = [dcc.Checklist(options=[{'label': display_name(control.name), 'value': control.name}], value=[control.name] if control.initial else [], id=control.name)
+                            for control in model_bools]
+        model_textfields = []
+        for control in model_texts:
+            if not control.value_range:
+                text_area = dcc.Textarea(placeholder=control.initial, id=control.name, value=control.initial, rows=1, style={'width': '600px', 'display': 'inline-block'})
+                model_textfields.append(html.Div([display_name(control.name) + '  ', text_area]))
+            else:
+                options = [{'label': o, 'value': o} for o in value_range(control)[1]]
+                dropdown = dcc.Dropdown(id=control.name, options=options, value=control.initial, style={'display': 'inline-block', 'width': 200})
+                model_textfields.append(html.Div([display_name(control.name) + '  ', dropdown]))
+        return [
+            dcc.Markdown('### Model'),
+            *model_sliders_float,
+            *model_sliders_int,
+            *model_textfields,
+            *model_checkboxes
+        ]
+
+    for control in model_floats:
+        @app.dash.callback(Output(control.name, 'disabled'), [Input(control.name, 'value')])
+        def set_model_value(slider_value, control=control):
+            if is_log_control(control):
+                value = 10.0 ** slider_value
+                if value * 0.99 <= value_range(control)[0]:
+                    value = value_range(control)[0]
+            else:
+                value = slider_value
+            app.model.set_control_value(control.name, value)
+            return False
+
+    for control in model_ints:
+        @app.dash.callback(Output(control.name, 'step'), [Input(control.name, 'value')])
+        def set_model_value(value, control=control):
+            app.model.set_control_value(control.name, value)
+            return 1
+
+    for control in model_bools:
+        @app.dash.callback(Output(control.name, 'style'), [Input(control.name, 'value')])
+        def set_model_bool(values, control=control):
+            app.model.set_control_value(control.name, True if values else False)
+            return {}
+
+    for control in model_texts:
+        @app.dash.callback(Output(control.name, 'disabled'), [Input(control.name, 'value')])
+        def set_model_text(value, control=control):
+            if value is not None:
+                app.model.set_control_value(control.name, value)
+            return False
+
+    return layout
+
+
+def create_sliders(controls):
+    sliders = []
+    for control in controls:
+        val = control.initial
+        lower, upper = value_range(control)
+        use_log = is_log_control(control)
+        if use_log:
+            magn = np.log10(val)
+            slider_min = np.log10(lower)
+            slider_max = np.log10(upper)
+            stepsize_magn = 0.1
+            marks = {e: '{:.1e}'.format(np.power(10.0, e)) for e in range(-20, 20) if slider_min <= e <= slider_max}
+            slider = dcc.Slider(min=slider_min, max=slider_max, value=magn, id=control.name, step=stepsize_magn, updatemode='drag', marks=marks)
+        else:
+            if control.control_type == int:
+                marks = {v: str(v) for v in range(lower, upper + 1)}
+                step = 1
+            else:
+                marks = {float(v): str(round(v, 4)) for v in np.linspace(lower, upper, 21)}
+                step = (upper-lower) / (len(marks)-1)
+            slider = dcc.Slider(min=lower, max=upper, value=val, id=control.name, step=step, marks=marks, updatemode='drag')
+        slider_container = html.Div(style={'height': '50px', 'width': '100%', 'display': 'inline-block'}, children=[display_name(control.name), slider])
+        sliders.append(slider_container)
+    return sliders
```

### Comparing `phiflow-2.3.4/phi/vis/_dash/player_controls.py` & `phiflow-2.4.0/phi/vis/_dash/player_controls.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,96 +1,96 @@
-from typing import Union
-
-import dash_core_components as dcc
-import dash_html_components as html
-from dash.dependencies import Output, Input, State
-from dash.exceptions import PreventUpdate
-
-from .dash_app import DashApp
-from .._vis_base import display_name
-
-
-REFRESH_INTERVAL = Input('playing-refresh-interval', 'n_intervals')
-
-
-def build_status_bar(app: DashApp):
-    layout = html.Div([
-        html.Div(id='status-bar', children=['Loading status...'], style={'backgroundColor': '#E0E0FF'}),
-        dcc.Interval(id='status-interval', interval=500),
-    ])
-
-    @app.dash.callback(Output('status-bar', 'children'), [Input('status-interval', 'n_intervals'), STEP_COMPLETE, PLAYING])
-    def update_status_bar(*_):
-        return [app.status_message]
-
-    return layout
-
-
-PLAY_BUTTON = Input('play-button', 'n_clicks')
-PLAYING = Input(PLAY_BUTTON.component_id, 'style')
-PAUSE_BUTTON = Input('pause-button', 'n_clicks')
-STEP_BUTTON = Input('step-button', 'n_clicks')
-STEP_COMPLETE = Input('step-complete', 'children')
-STEP_COUNT = State('step-count', 'value')
-
-
-def all_actions(app: DashApp):
-    return tuple(Input(f'action_{action.name}', 'n_clicks') for action in app.model.actions)
-
-
-def build_player_controls(app: DashApp):
-
-    layout = html.Div(style={'height': '30px'}, children=[
-        html.Button('Play', id=PLAY_BUTTON.component_id),
-        html.Button('Pause', id=PAUSE_BUTTON.component_id),
-        html.Button('Step', id=STEP_BUTTON.component_id),
-        dcc.Textarea(placeholder='#steps', id=STEP_COUNT.component_id, value='', rows=1, style={'width': 70}),
-        *[html.Button(display_name(action.name), id=f'action_{action.name}') for action in app.model.actions],
-        html.Div(style={'display': 'none'}, id=STEP_COMPLETE.component_id),
-    ])
-
-    @app.dash.callback(Output(PLAY_BUTTON.component_id, 'style'), inputs=[PLAY_BUTTON], state=[STEP_COUNT])
-    def play(n_clicks, step_count):
-        if n_clicks and not app.play_status:
-            step_count = parse_step_count(step_count, app, default=None)
-            app.play(max_steps=step_count)
-        else:
-            raise PreventUpdate()
-
-    @app.dash.callback(Output(PAUSE_BUTTON.component_id, 'style'), [PAUSE_BUTTON])
-    def pause_simulation(n_clicks):
-        if n_clicks:
-            app.pause()
-        raise PreventUpdate()
-
-    @app.dash.callback(Output(STEP_BUTTON.component_id, 'style'), [STEP_BUTTON])
-    def simulation_step(n_clicks):
-        if n_clicks and not app.play_status:
-            app.model.progress()
-        raise PreventUpdate()
-
-    @app.dash.callback(Output(STEP_COMPLETE.component_id, 'children'), [STEP_BUTTON, PAUSE_BUTTON])
-    def simulation_step(step, pause):
-        return ['%s / %s' % (step, pause)]
-
-    for action in app.model.actions:
-        @app.dash.callback(Output(f'action_{action.name}', 'disabled'), [Input(f'action_{action.name}', 'n_clicks')])
-        def perform_action(n_clicks, action=action):
-            if n_clicks is not None:
-                app.model.run_action(action.name)
-            raise PreventUpdate()
-
-    return layout
-
-
-def parse_step_count(step_count, app, default: Union[int, None] = 1):
-    if step_count is None:
-        return default
-    try:
-        step_count = step_count.strip()
-        if step_count.startswith('*'):
-            step_count = app.model.sequence_stride * int(step_count[1:].strip())
-        else:
-            step_count = int(step_count)
-        return step_count
-    except ValueError:
-        return default
+from typing import Union
+
+import dash_core_components as dcc
+import dash_html_components as html
+from dash.dependencies import Output, Input, State
+from dash.exceptions import PreventUpdate
+
+from .dash_app import DashApp
+from .._vis_base import display_name
+
+
+REFRESH_INTERVAL = Input('playing-refresh-interval', 'n_intervals')
+
+
+def build_status_bar(app: DashApp):
+    layout = html.Div([
+        html.Div(id='status-bar', children=['Loading status...'], style={'backgroundColor': '#E0E0FF'}),
+        dcc.Interval(id='status-interval', interval=500),
+    ])
+
+    @app.dash.callback(Output('status-bar', 'children'), [Input('status-interval', 'n_intervals'), STEP_COMPLETE, PLAYING])
+    def update_status_bar(*_):
+        return [app.status_message]
+
+    return layout
+
+
+PLAY_BUTTON = Input('play-button', 'n_clicks')
+PLAYING = Input(PLAY_BUTTON.component_id, 'style')
+PAUSE_BUTTON = Input('pause-button', 'n_clicks')
+STEP_BUTTON = Input('step-button', 'n_clicks')
+STEP_COMPLETE = Input('step-complete', 'children')
+STEP_COUNT = State('step-count', 'value')
+
+
+def all_actions(app: DashApp):
+    return tuple(Input(f'action_{action.name}', 'n_clicks') for action in app.model.actions)
+
+
+def build_player_controls(app: DashApp):
+
+    layout = html.Div(style={'height': '30px'}, children=[
+        html.Button('Play', id=PLAY_BUTTON.component_id),
+        html.Button('Pause', id=PAUSE_BUTTON.component_id),
+        html.Button('Step', id=STEP_BUTTON.component_id),
+        dcc.Textarea(placeholder='#steps', id=STEP_COUNT.component_id, value='', rows=1, style={'width': 70}),
+        *[html.Button(display_name(action.name), id=f'action_{action.name}') for action in app.model.actions],
+        html.Div(style={'display': 'none'}, id=STEP_COMPLETE.component_id),
+    ])
+
+    @app.dash.callback(Output(PLAY_BUTTON.component_id, 'style'), inputs=[PLAY_BUTTON], state=[STEP_COUNT])
+    def play(n_clicks, step_count):
+        if n_clicks and not app.play_status:
+            step_count = parse_step_count(step_count, app, default=None)
+            app.play(max_steps=step_count)
+        else:
+            raise PreventUpdate()
+
+    @app.dash.callback(Output(PAUSE_BUTTON.component_id, 'style'), [PAUSE_BUTTON])
+    def pause_simulation(n_clicks):
+        if n_clicks:
+            app.pause()
+        raise PreventUpdate()
+
+    @app.dash.callback(Output(STEP_BUTTON.component_id, 'style'), [STEP_BUTTON])
+    def simulation_step(n_clicks):
+        if n_clicks and not app.play_status:
+            app.model.progress()
+        raise PreventUpdate()
+
+    @app.dash.callback(Output(STEP_COMPLETE.component_id, 'children'), [STEP_BUTTON, PAUSE_BUTTON])
+    def simulation_step(step, pause):
+        return ['%s / %s' % (step, pause)]
+
+    for action in app.model.actions:
+        @app.dash.callback(Output(f'action_{action.name}', 'disabled'), [Input(f'action_{action.name}', 'n_clicks')])
+        def perform_action(n_clicks, action=action):
+            if n_clicks is not None:
+                app.model.run_action(action.name)
+            raise PreventUpdate()
+
+    return layout
+
+
+def parse_step_count(step_count, app, default: Union[int, None] = 1):
+    if step_count is None:
+        return default
+    try:
+        step_count = step_count.strip()
+        if step_count.startswith('*'):
+            step_count = app.model.sequence_stride * int(step_count[1:].strip())
+        else:
+            step_count = int(step_count)
+        return step_count
+    except ValueError:
+        return default
```

### Comparing `phiflow-2.3.4/phi/vis/_dash/viewer.py` & `phiflow-2.4.0/phi/vis/_dash/viewer.py`

 * *Ordering differences only*

 * *Files 25% similar despite different names*

```diff
@@ -1,55 +1,55 @@
-import traceback
-
-import dash_core_components as dcc
-import dash_html_components as html
-from dash.dependencies import Input, Output
-from plotly import graph_objects
-
-from .dash_app import DashApp
-from .model_controls import all_controls
-from .player_controls import STEP_COMPLETE, all_actions, REFRESH_INTERVAL
-from .viewsettings import parse_view_settings, all_view_settings
-from .. import plot
-from .._vis_base import select_channel
-from ...field import SampledField
-
-
-def build_viewers(app: DashApp, count: int, height: int, viewer_group: str):
-    field_names = app.model.field_names + ('None',) * max(0, 4 - len(app.model.field_names))
-    result = []
-    ids = [f'{viewer_group}_{i}' for i in range(count)]
-    field_selections = tuple(Input(f'{id}-field-select', 'value') for id in ids)
-    for id, field_name in zip(ids, field_names):
-        result.append(build_viewer(app, height, field_name, id, viewer_group))
-    return result, field_selections
-
-
-def build_viewer(app: DashApp, height: int, initial_field_name: str, id: str, viewer_group: str):
-    field_options = [{'label': item, 'value': item} for item in app.model.field_names] + [{'label': '<None>', 'value': 'None'}]
-
-    layout = html.Div(style={'height': '100%'}, children=[
-        html.Div(style={'width': '100%', 'height': '5%', 'display': 'inline-block', 'vertical-align': 'middle'}, children=[
-            dcc.Dropdown(options=field_options, value=initial_field_name, id=id+'-field-select'),
-        ]),
-        html.Div(id=id+'-figure-container', style={'height': '95%', 'width': '100%', 'display': 'inline-block'}, children=[
-            dcc.Graph(figure={}, id=id + '-graph', style={'height': '100%'})
-        ]),
-    ])
-
-    @app.dash.callback(Output(id+'-graph', 'figure'), (Input(f'{id}-field-select', 'value'), STEP_COMPLETE, REFRESH_INTERVAL, *all_view_settings(app, viewer_group), *all_controls(app), *all_actions(app)))
-    def update_figure(field, _0, _1, *settings):
-        if field is None or field == 'None':
-            fig = graph_objects.Figure()
-            fig.update_layout(title_text="None", paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)')
-            return fig
-        selection = parse_view_settings(app, *settings)
-        value = app.model.get_field(field, selection['select'])
-        try:
-            value = select_channel(value, selection.get('component', None))
-            return plot(value, lib='plotly', size=(height, height), same_scale=False).native()
-        except ValueError as err:
-            fig = graph_objects.Figure()
-            fig.update_layout(title_text=str(value), paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)')
-            return fig
-
-    return layout
+import traceback
+
+import dash_core_components as dcc
+import dash_html_components as html
+from dash.dependencies import Input, Output
+from plotly import graph_objects
+
+from .dash_app import DashApp
+from .model_controls import all_controls
+from .player_controls import STEP_COMPLETE, all_actions, REFRESH_INTERVAL
+from .viewsettings import parse_view_settings, all_view_settings
+from .. import plot
+from .._vis_base import select_channel
+from ...field import SampledField
+
+
+def build_viewers(app: DashApp, count: int, height: int, viewer_group: str):
+    field_names = app.model.field_names + ('None',) * max(0, 4 - len(app.model.field_names))
+    result = []
+    ids = [f'{viewer_group}_{i}' for i in range(count)]
+    field_selections = tuple(Input(f'{id}-field-select', 'value') for id in ids)
+    for id, field_name in zip(ids, field_names):
+        result.append(build_viewer(app, height, field_name, id, viewer_group))
+    return result, field_selections
+
+
+def build_viewer(app: DashApp, height: int, initial_field_name: str, id: str, viewer_group: str):
+    field_options = [{'label': item, 'value': item} for item in app.model.field_names] + [{'label': '<None>', 'value': 'None'}]
+
+    layout = html.Div(style={'height': '100%'}, children=[
+        html.Div(style={'width': '100%', 'height': '5%', 'display': 'inline-block', 'vertical-align': 'middle'}, children=[
+            dcc.Dropdown(options=field_options, value=initial_field_name, id=id+'-field-select'),
+        ]),
+        html.Div(id=id+'-figure-container', style={'height': '95%', 'width': '100%', 'display': 'inline-block'}, children=[
+            dcc.Graph(figure={}, id=id + '-graph', style={'height': '100%'})
+        ]),
+    ])
+
+    @app.dash.callback(Output(id+'-graph', 'figure'), (Input(f'{id}-field-select', 'value'), STEP_COMPLETE, REFRESH_INTERVAL, *all_view_settings(app, viewer_group), *all_controls(app), *all_actions(app)))
+    def update_figure(field, _0, _1, *settings):
+        if field is None or field == 'None':
+            fig = graph_objects.Figure()
+            fig.update_layout(title_text="None", paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)')
+            return fig
+        selection = parse_view_settings(app, *settings)
+        value = app.model.get_field(field, selection['select'])
+        try:
+            value = select_channel(value, selection.get('component', None))
+            return plot(value, lib='plotly', size=(height, height), same_scale=False).native()
+        except ValueError as err:
+            fig = graph_objects.Figure()
+            fig.update_layout(title_text=str(value), paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)')
+            return fig
+
+    return layout
```

### Comparing `phiflow-2.3.4/phi/vis/_log.py` & `phiflow-2.4.0/phi/vis/_log.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,80 +1,84 @@
-import logging
-import sys
-from os.path import isfile
-from typing import Union
-
-import numpy as np
-
-from phi import math
-from phi.field import Scene
-
-
-class SceneLog:
-
-    def __init__(self, scene: Scene):
-        self.scene = scene
-        self._scalars = {}  # name -> (frame, value)
-        self._scalar_streams = {}
-        root_logger = logging.getLogger()
-        root_logger.setLevel(logging.WARNING)
-        self.logger = logging.Logger("vis", logging.DEBUG)
-        console_handler = self.console_handler = logging.StreamHandler(sys.stdout)
-        log_formatter = logging.Formatter("%(message)s (%(levelname)s), %(asctime)sn\n")
-        console_handler.setFormatter(log_formatter)
-        console_handler.setLevel(logging.INFO)
-        self.logger.addHandler(console_handler)
-        if self.scene is not None:
-            if not isfile(self.scene.subpath("info.log")):
-                log_file = self.scene.subpath("info.log")
-            else:
-                index = 2
-                while True:
-                    log_file = self.scene.subpath("info_%d.log" % index)
-                    if not isfile(log_file):
-                        break
-                    else:
-                        index += 1
-            self.log_file = log_file
-            file_handler = self.file_handler = logging.FileHandler(log_file)
-            file_handler.setFormatter(log_formatter)
-            self.logger.addHandler(file_handler)
-        else:
-            self.log_file = None
-
-    def log(self, message):
-        self.logger.info(message)
-
-    def log_scalars(self, frame: int, **values: Union[float, math.Tensor]):
-        """
-        Adds `values` to the curves by name.
-        This can be used to log the evolution of scalar quantities or summaries.
-
-        The values are stored in a text file within the scene directory.
-        The curves may also be directly viewed in the user interface.
-
-        Args:
-            frame: step
-            values: Values and names to append to the curves, must be numbers or `phi.math.Tensor`.
-                If a curve does not yet exists, a new one is created.
-        """
-        for name, value in values.items():
-            assert isinstance(name, str)
-            value = float(math.mean(value).mean)
-            if name not in self._scalars:
-                self._scalars[name] = []
-                if self.scene is not None:
-                    path = self.scene.subpath(f"log_{name}.txt")
-                    self._scalar_streams[name] = open(path, "w")
-            self._scalars[name].append((frame, value))
-            if self.scene is not None:
-                self._scalar_streams[name].write(f"{frame} {value}\n")
-                self._scalar_streams[name].flush()
-
-    def get_scalar_curve(self, name) -> tuple:
-        frames = np.array([item[0] for item in self._scalars[name]])
-        values = np.array([item[1] for item in self._scalars[name]])
-        return frames, values
-
-    @property
-    def scalar_curve_names(self) -> tuple:
-        return tuple(self._scalars.keys())
+import logging
+import sys
+from numbers import Number
+from os.path import isfile
+from typing import Union, Callable, Optional
+
+import numpy as np
+
+from phi import math
+from phi.field import Scene
+
+
+class SceneLog:
+
+    def __init__(self, scene: Scene):
+        self.scene = scene
+        self._scalars = {}  # name -> (frame, value)
+        self._scalar_streams = {}
+        root_logger = logging.getLogger()
+        root_logger.setLevel(logging.WARNING)
+        self.logger = logging.Logger("vis", logging.DEBUG)
+        console_handler = self.console_handler = logging.StreamHandler(sys.stdout)
+        log_formatter = logging.Formatter("%(message)s (%(levelname)s), %(asctime)sn\n")
+        console_handler.setFormatter(log_formatter)
+        console_handler.setLevel(logging.INFO)
+        self.logger.addHandler(console_handler)
+        if self.scene is not None:
+            if not isfile(self.scene.subpath("info.log")):
+                log_file = self.scene.subpath("info.log")
+            else:
+                index = 2
+                while True:
+                    log_file = self.scene.subpath("info_%d.log" % index)
+                    if not isfile(log_file):
+                        break
+                    else:
+                        index += 1
+            self.log_file = log_file
+            file_handler = self.file_handler = logging.FileHandler(log_file)
+            file_handler.setFormatter(log_formatter)
+            self.logger.addHandler(file_handler)
+        else:
+            self.log_file = None
+
+    def log(self, message):
+        self.logger.info(message)
+
+    def log_scalars(self, frame: int, reduce: Optional[Callable], **values: Union[float, math.Tensor]):
+        """
+        Adds `values` to the curves by name.
+        This can be used to log the evolution of scalar quantities or summaries.
+
+        The values are stored in a text file within the scene directory.
+        The curves may also be directly viewed in the user interface.
+
+        Args:
+            frame: step
+            values: Values and names to append to the curves, must be numbers or `phi.math.Tensor`.
+                If a curve does not yet exists, a new one is created.
+        """
+        for name, value in values.items():
+            assert isinstance(name, str)
+            if reduce:
+                value = float(reduce(value, math.shape(value)))
+            else:
+                value = math.convert(value, math.NUMPY)
+            if name not in self._scalars:
+                self._scalars[name] = []
+                if self.scene is not None:
+                    path = self.scene.subpath(f"log_{name}.txt")
+                    self._scalar_streams[name] = open(path, "w")
+            self._scalars[name].append((frame, value))
+            if self.scene is not None:
+                self._scalar_streams[name].write(f"{frame} {value if isinstance(value, Number) else ' '.join([str(float(v)) for v in value])}\n")
+                self._scalar_streams[name].flush()
+
+    def get_scalar_curve(self, name) -> tuple:
+        frames = np.array([item[0] for item in self._scalars[name]])
+        values = np.array([float(item[1].mean) if isinstance(item[1], math.Tensor) else item[1] for item in self._scalars[name]])
+        return frames, values
+
+    @property
+    def scalar_curve_names(self) -> tuple:
+        return tuple(self._scalars.keys())
```

### Comparing `phiflow-2.3.4/phi/vis/_matplotlib/_scalars.py` & `phiflow-2.4.0/phi/vis/_matplotlib/_scalars.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,178 +1,180 @@
-import os
-from numbers import Number
-from typing import Callable, Union
-
-import matplotlib.pyplot as plt
-import numpy
-import numpy as np
-
-from phi import math
-from phi.field import Scene
-from phi.field._scene import _str
-from phi.math import Tensor, batch
-from phi.math.backend import PHI_LOGGER
-from phi.vis._plot_util import smooth_uniform_curve
-from phi.vis._vis_base import display_name
-from ._matplotlib_plots import MATPLOTLIB
-
-
-def plot_scalars(scene: Union[str, tuple, list, Scene, math.Tensor],
-                 names: Union[str, tuple, list, math.Tensor] = None,
-                 reduce: Union[str, tuple, list, math.Shape] = 'names',
-                 down='',
-                 smooth=1,
-                 smooth_alpha=0.2,
-                 smooth_linewidth=2.,
-                 size=(8, 6),
-                 transform: Callable = None,
-                 tight_layout=True,
-                 grid: Union[str, dict] = 'y',
-                 log_scale='',
-                 legend='upper right',
-                 x='steps',
-                 xlim=None,
-                 ylim=None,
-                 titles=True,
-                 labels: math.Tensor = None,
-                 xlabel: str = None,
-                 ylabel: str = None,
-                 colors: math.Tensor = 'default',
-                 dashed: math.Tensor = False):
-    """
-
-    Args:
-        scene: `str` or `Tensor`. Scene paths containing the data to plot.
-        names: Data files to plot for each scene. The file must be located inside the scene directory and have the name `log_<name>.txt`.
-        reduce: Tensor dimensions along which all curves are plotted in the same diagram.
-        down: Tensor dimensions along which diagrams are ordered top-to-bottom instead of left-to-right.
-        smooth: `int` or `Tensor`. Number of data points to average, -1 for all.
-        smooth_alpha: Opacity of the non-smoothed curves under the smoothed curves.
-        smooth_linewidth: Line width of the smoothed curves.
-        size: Figure size in inches.
-        transform: Function `T(x,y) -> (x,y)` transforming the curves.
-        tight_layout:
-        grid:
-        log_scale:
-        legend:
-        x:
-        xlim:
-        ylim:
-        titles:
-        labels:
-        xlabel:
-        ylabel:
-        colors: Line colors as `str`, `int` or `Tensor`. Integers are interpreted as indices of the default color list.
-
-    Returns:
-        MatPlotLib [figure](https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure)
-    """
-    scene = Scene.at(scene)
-    additional_reduce = ()
-    if names is None:
-        first_path = next(iter(math.flatten(scene.paths)))
-        names = [_str(n) for n in os.listdir(first_path)]
-        names = [n[4:-4] for n in names if n.endswith('.txt') and n.startswith('log_')]
-        names = math.wrap(names, batch('names'))
-        additional_reduce = ['names']
-    elif isinstance(names, str):
-        names = math.wrap(names)
-    elif isinstance(names, (tuple, list)):
-        names = math.wrap(names, batch('names'))
-    else:
-        assert isinstance(names, math.Tensor), f"Invalid argument 'names': {type(names)}"
-    colors = math.wrap(colors)
-    dashed = math.wrap(dashed)
-    if xlabel is None:
-        xlabel = 'Iterations' if x == 'steps' else 'Time (s)'
-
-    shape = (scene.shape & names.shape)
-    batches = shape.without(reduce).without(additional_reduce)
-
-    cycle = list(plt.rcParams['axes.prop_cycle'].by_key()['color'])
-    fig, axes = plt.subplots(batches.only(down).volume, batches.without(down).volume, figsize=size)
-    MATPLOTLIB.current_figure = fig
-    axes = axes if isinstance(axes, numpy.ndarray) else np.array(axes)
-
-    for b, axis in zip(math.concat_shapes(batches.only(down), batches.without(down)).meshgrid(), axes.flatten()):
-        assert isinstance(axis, plt.Axes)
-        names_equal = names[b].rank == 0
-        paths_equal = scene.paths[b].rank == 0
-        if titles is not None and titles is not False:
-            if isinstance(titles, str):
-                axis.set_title(titles)
-            elif isinstance(titles, Tensor):
-                axis.set_title(titles[b].native())
-            elif names_equal:
-                axis.set_title(display_name(names[b].native()))
-            elif paths_equal:
-                axis.set_title(os.path.basename(scene.paths[b].native()))
-        if labels is not None:
-            curve_labels = labels
-        elif names_equal:
-            curve_labels = math.map(os.path.basename, scene.paths[b])
-        elif paths_equal:
-            curve_labels = names[b]
-        else:
-            curve_labels = math.map(lambda p, n: f"{os.path.basename(p)} - {n}", scene.paths[b], names[b])
-
-        def single_plot(name, path, label, i, color, dashed_, smooth):
-            PHI_LOGGER.debug(f"Reading {os.path.join(path, f'log_{name}.txt')}")
-            curve = numpy.loadtxt(os.path.join(path, f"log_{name}.txt"))
-            if curve.ndim == 2:
-                x_values, values, *_ = curve.T
-            else:
-                values = curve
-                x_values = np.arange(len(values))
-            if x == 'steps':
-                pass
-            else:
-                assert x == 'time', f"x must be 'steps' or 'time' but got {x}"
-                PHI_LOGGER.debug(f"Reading {os.path.join(path, 'log_step_time.txt')}")
-                _, x_values, *_ = numpy.loadtxt(os.path.join(path, "log_step_time.txt")).T
-                values = values[:len(x_values+1)]
-                x_values = np.cumsum(x_values[:len(values)-1])
-                x_values = np.concatenate([[0.], x_values])
-            if transform:
-                x_values, values = transform(np.stack([x_values, values]))
-            if color == 'default':
-                color = cycle[i]
-            try:
-                color = int(color)
-            except ValueError:
-                pass
-            if isinstance(color, Number):
-                color = cycle[int(color)]
-            PHI_LOGGER.debug(f"Plotting curve {label}")
-            if smooth > 1:
-                axis.plot(x_values, values, color=color, alpha=smooth_alpha, linewidth=1)
-                curve = np.stack([x_values, values], -1)
-                axis.plot(*smooth_uniform_curve(curve, smooth).T, *(['--'] if dashed_ else []), color=color, linewidth=smooth_linewidth, label=label)
-            else:
-                axis.plot(x_values, values, *(['--'] if dashed_ else []), color=color, linewidth=1, label=label)
-            if grid:
-                if isinstance(grid, dict):
-                    axis.grid(**grid)
-                else:
-                    grid_axis = 'both' if 'x' in grid and 'y' in grid else grid
-                    axis.grid(which='both', axis=grid_axis, linestyle='--', linewidth=size[1] * 0.3)
-            if 'x' in log_scale:
-                axis.set_xscale('log')
-            if 'y' in log_scale:
-                axis.set_yscale('log')
-            if xlim:
-                axis.set_xlim(xlim)
-            if ylim:
-                axis.set_ylim(ylim)
-            if xlabel:
-                axis.set_xlabel(xlabel)
-            if ylabel:
-                axis.set_ylabel(ylabel)
-            return name
-
-        math.map(single_plot, names[b], scene.paths[b], curve_labels, math.range_tensor(shape.after_gather(b)), colors, dashed, smooth)
-        if legend:
-            axis.legend(loc=legend)
-    # Final touches
-    if tight_layout:
-        plt.tight_layout()
-    return fig
+import os
+import warnings
+from numbers import Number
+from typing import Callable, Union
+
+import matplotlib.pyplot as plt
+import numpy
+import numpy as np
+
+from phi import math
+from phi.field import Scene
+from phi.field._scene import _str
+from phi.math import Tensor, batch
+from phi.math.backend import PHI_LOGGER
+from phi.vis._plot_util import smooth_uniform_curve
+from phi.vis._vis_base import display_name
+from ._matplotlib_plots import MATPLOTLIB
+
+
+def plot_scalars(scene: Union[str, tuple, list, Scene, math.Tensor],
+                 names: Union[str, tuple, list, math.Tensor] = None,
+                 reduce: Union[str, tuple, list, math.Shape] = 'names',
+                 down='',
+                 smooth=1,
+                 smooth_alpha=0.2,
+                 smooth_linewidth=2.,
+                 size=(8, 6),
+                 transform: Callable = None,
+                 tight_layout=True,
+                 grid: Union[str, dict] = 'y',
+                 log_scale='',
+                 legend='upper right',
+                 x='steps',
+                 xlim=None,
+                 ylim=None,
+                 titles=True,
+                 labels: math.Tensor = None,
+                 xlabel: str = None,
+                 ylabel: str = None,
+                 colors: math.Tensor = 'default',
+                 dashed: math.Tensor = False):
+    """
+
+    Args:
+        scene: `str` or `Tensor`. Scene paths containing the data to plot.
+        names: Data files to plot for each scene. The file must be located inside the scene directory and have the name `log_<name>.txt`.
+        reduce: Tensor dimensions along which all curves are plotted in the same diagram.
+        down: Tensor dimensions along which diagrams are ordered top-to-bottom instead of left-to-right.
+        smooth: `int` or `Tensor`. Number of data points to average, -1 for all.
+        smooth_alpha: Opacity of the non-smoothed curves under the smoothed curves.
+        smooth_linewidth: Line width of the smoothed curves.
+        size: Figure size in inches.
+        transform: Function `T(x,y) -> (x,y)` transforming the curves.
+        tight_layout:
+        grid:
+        log_scale:
+        legend:
+        x:
+        xlim:
+        ylim:
+        titles:
+        labels:
+        xlabel:
+        ylabel:
+        colors: Line colors as `str`, `int` or `Tensor`. Integers are interpreted as indices of the default color list.
+
+    Returns:
+        MatPlotLib [figure](https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure)
+    """
+    warnings.warn("plot_scalars is deprecated. Use load_scalars() and plot() instead.", DeprecationWarning, stacklevel=2)
+    scene = Scene.at(scene)
+    additional_reduce = ()
+    if names is None:
+        first_path = next(iter(math.flatten(scene.paths)))
+        names = [_str(n) for n in os.listdir(first_path)]
+        names = [n[4:-4] for n in names if n.endswith('.txt') and n.startswith('log_')]
+        names = math.wrap(names, batch('names'))
+        additional_reduce = ['names']
+    elif isinstance(names, str):
+        names = math.wrap(names)
+    elif isinstance(names, (tuple, list)):
+        names = math.wrap(names, batch('names'))
+    else:
+        assert isinstance(names, math.Tensor), f"Invalid argument 'names': {type(names)}"
+    colors = math.wrap(colors)
+    dashed = math.wrap(dashed)
+    if xlabel is None:
+        xlabel = 'Iterations' if x == 'steps' else 'Time (s)'
+
+    shape = (scene.shape & names.shape)
+    batches = shape.without(reduce).without(additional_reduce)
+
+    cycle = list(plt.rcParams['axes.prop_cycle'].by_key()['color'])
+    fig, axes = plt.subplots(batches.only(down).volume, batches.without(down).volume, figsize=size)
+    MATPLOTLIB.current_figure = fig
+    axes = axes if isinstance(axes, numpy.ndarray) else np.array(axes)
+
+    for b, axis in zip(math.concat_shapes(batches.only(down), batches.without(down)).meshgrid(), axes.flatten()):
+        assert isinstance(axis, plt.Axes)
+        names_equal = names[b].rank == 0
+        paths_equal = scene.paths[b].rank == 0
+        if titles is not None and titles is not False:
+            if isinstance(titles, str):
+                axis.set_title(titles)
+            elif isinstance(titles, Tensor):
+                axis.set_title(titles[b].native())
+            elif names_equal:
+                axis.set_title(display_name(names[b].native()))
+            elif paths_equal:
+                axis.set_title(os.path.basename(scene.paths[b].native()))
+        if labels is not None:
+            curve_labels = labels
+        elif names_equal:
+            curve_labels = math.map(os.path.basename, scene.paths[b])
+        elif paths_equal:
+            curve_labels = names[b]
+        else:
+            curve_labels = math.map(lambda p, n: f"{os.path.basename(p)} - {n}", scene.paths[b], names[b])
+
+        def single_plot(name, path, label, i, color, dashed_, smooth):
+            PHI_LOGGER.debug(f"Reading {os.path.join(path, f'log_{name}.txt')}")
+            curve = numpy.loadtxt(os.path.join(path, f"log_{name}.txt"))
+            if curve.ndim == 2:
+                x_values, values, *_ = curve.T
+            else:
+                values = curve
+                x_values = np.arange(len(values))
+            if x == 'steps':
+                pass
+            else:
+                assert x == 'time', f"x must be 'steps' or 'time' but got {x}"
+                PHI_LOGGER.debug(f"Reading {os.path.join(path, 'log_step_time.txt')}")
+                _, x_values, *_ = numpy.loadtxt(os.path.join(path, "log_step_time.txt")).T
+                values = values[:len(x_values+1)]
+                x_values = np.cumsum(x_values[:len(values)-1])
+                x_values = np.concatenate([[0.], x_values])
+            if transform:
+                x_values, values = transform(np.stack([x_values, values]))
+            if color == 'default':
+                color = cycle[i]
+            try:
+                color = int(color)
+            except ValueError:
+                pass
+            if isinstance(color, Number):
+                color = cycle[int(color)]
+            PHI_LOGGER.debug(f"Plotting curve {label}")
+            if smooth > 1:
+                axis.plot(x_values, values, color=color, alpha=smooth_alpha, linewidth=1)
+                curve = np.stack([x_values, values], -1)
+                axis.plot(*smooth_uniform_curve(curve, smooth).T, *(['--'] if dashed_ else []), color=color, linewidth=smooth_linewidth, label=label)
+            else:
+                axis.plot(x_values, values, *(['--'] if dashed_ else []), color=color, linewidth=1, label=label)
+            if grid:
+                if isinstance(grid, dict):
+                    axis.grid(**grid)
+                else:
+                    grid_axis = 'both' if 'x' in grid and 'y' in grid else grid
+                    axis.grid(which='both', axis=grid_axis, linestyle='--', linewidth=size[1] * 0.3)
+            if 'x' in log_scale:
+                axis.set_xscale('log')
+            if 'y' in log_scale:
+                axis.set_yscale('log')
+            if xlim:
+                axis.set_xlim(xlim)
+            if ylim:
+                axis.set_ylim(ylim)
+            if xlabel:
+                axis.set_xlabel(xlabel)
+            if ylabel:
+                axis.set_ylabel(ylabel)
+            return name
+
+        math.map(single_plot, names[b], scene.paths[b], curve_labels, math.range_tensor(shape.after_gather(b)), colors, dashed, smooth)
+        if legend:
+            axis.legend(loc=legend)
+    # Final touches
+    if tight_layout:
+        plt.tight_layout()
+    return fig
```

### Comparing `phiflow-2.3.4/phi/vis/_user_namespace.py` & `phiflow-2.4.0/phi/vis/_user_namespace.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,212 +1,212 @@
-import inspect
-import os
-import sys
-from typing import List
-
-from phi.vis._vis_base import display_name
-
-
-class UserNamespace:
-
-    def list_variables(self, only_public=False, only_current_scope=False) -> dict:
-        raise NotImplementedError(self)
-
-    def get_variable(self, name: str, default=None):  # __getitem__
-        raise NotImplementedError(self)
-
-    def set_variable(self, name: str, value):  # __setitem__
-        raise NotImplementedError(self)
-
-    def get_title(self):  # __repr__()
-        raise NotImplementedError(self)
-
-    def get_description(self):  # __doc__?
-        raise NotImplementedError(self)
-
-    def get_reference(self):  # __str__() / __repr__()
-        """ Used to determine the default directory name to which data is written. """
-        raise NotImplementedError(self)
-
-
-def global_user_namespace(frames: List[inspect.FrameInfo]) -> UserNamespace:
-    if 'ipykernel' in sys.modules:
-        return JupyterNamespace()
-    else:
-        for frame in frames:
-            if frame.function == '<module>':
-                module = inspect.getmodule(frame.frame)
-                return ModuleNamespace(module)
-        raise AssertionError('No module found in call stack.')
-
-
-def get_user_namespace(ignore_stack_frames=0, frames: List[inspect.FrameInfo] = None) -> UserNamespace:
-    if not frames:
-        frames = inspect.stack()[ignore_stack_frames + 1:]  # 1 for this function
-    if frames[0].function == '<module>':
-        return global_user_namespace(frames)
-    else:
-        return LocalNamespace(frames)
-
-
-class ModuleNamespace(UserNamespace):
-
-    def __init__(self, module):
-        self.module = module
-
-    def get_title(self):
-        doc = self.module.__doc__
-        if doc is None:
-            return self.get_reference()
-        else:
-            end_of_line = doc.index('\n')
-            name = doc[:end_of_line].strip()
-            return name if name else self.get_reference()
-
-    def get_reference(self):
-        return os.path.basename(self.module.__file__)[:-3]
-
-    def get_description(self):
-        doc = self.module.__doc__
-        if doc is None:
-            return doc or self.module.__file__
-        else:
-            end_of_line = doc.index('\n')
-            return doc[end_of_line:].strip() or ""
-
-    def list_variables(self, only_public=False, only_current_scope=False) -> dict:
-        # all variables are in the current scope
-        variables = {name: getattr(self.module, name) for name in dir(self.module)}
-        if only_public:
-            variables = {n: v for n, v in variables.items() if not n.startswith('_')}
-        if only_current_scope:
-            variables = {n: v for n, v in variables.items() if is_value(v) or inspect.getmodule(v) == self.module}
-        return variables
-
-    def get_variable(self, name: str, default=None):
-        return getattr(self.module, name, default)
-
-    def set_variable(self, name: str, value):
-        setattr(self.module, name, value)
-
-
-def is_value(obj):
-    if isinstance(obj, type):
-        return False
-    # return type(open).__name__ not in ('function', 'builtin_function_or_method', 'module')
-    # if isinstance(obj, (type, function, builtin_function_or_method))
-    if inspect.isfunction(obj):
-        return False
-    return True
-
-
-class JupyterNamespace(UserNamespace):
-
-    def get_title(self):
-        return "Notebook"
-
-    def get_reference(self):
-        return "notebooks"
-
-    def get_description(self):
-        if 'google.colab' not in sys.modules:
-            return "Google Colab Notebook"
-        else:
-            import ipykernel
-            version = ipykernel.version_info
-            client_name = ipykernel.write_connection_file.__module__.split('.')[0]
-            return f"{client_name} ({version})"
-
-    def list_variables(self, only_public=False, only_current_scope=False) -> dict:
-        from IPython import get_ipython
-        shell = get_ipython()
-        variables = shell.user_ns
-        if only_public:
-            hidden = shell.user_ns_hidden
-            variables = {n: v for n, v in variables.items() if not n.startswith('_') and n not in hidden}
-        if only_current_scope:
-            text = shell.user_ns['In'][-1]
-            variables = {n: v for n, v in variables.items() if n in text}  # TODO parse text, only show names with assignment
-        return variables
-
-    def get_variable(self, name: str, default=None):
-        from IPython import get_ipython
-        shell = get_ipython()
-        return shell.user_ns.get(name, default)
-
-    def set_variable(self, name: str, value):
-        from IPython import get_ipython
-        shell = get_ipython()
-        shell.user_ns[name] = value
-
-
-class LocalNamespace(UserNamespace):
-
-    def __init__(self, frames: List[inspect.FrameInfo]):
-        self.frame = frames[0].frame
-        self.function_name: str = frames[0].function
-        self.module = inspect.getmodule(self.frame)
-        if self.module:
-            if hasattr(self.module, self.function_name):
-                self.function = getattr(self.module, self.function_name)
-            else:
-                self.function = None
-        else:
-            assert 'ipykernel' in sys.modules, f"Unable to locate file in which {self.function_name} is declared."
-            self.function = JupyterNamespace().get_variable(self.function_name)
-
-    def list_variables(self, only_public=False, only_current_scope=False) -> dict:
-        return self.frame.f_locals
-
-    def get_variable(self, name: str, default=None):
-        return self.frame.f_locals.get(name, default)
-
-    def set_variable(self, name: str, value):
-        import ctypes
-        self.frame.f_locals[name] = value
-        ctypes.pythonapi.PyFrame_LocalsToFast(ctypes.py_object(self.frame), ctypes.c_int(0))
-
-    def get_title(self):
-        return display_name(self.function_name)
-
-    def get_description(self):
-        if self.function is not None and self.function.__doc__:
-            return self.function.__doc__
-        else:
-            return f"Function `{self.function_name}()` in '{self.module.__file__}'"
-
-    def get_reference(self):
-        return f"{os.path.basename(self.module.__file__)[:-3]}_{self.function_name}"
-
-
-class DictNamespace(UserNamespace):
-
-    def __init__(self,
-                 variables: dict,
-                 title: str = "Unknown",
-                 description: str = "Custom namespace, unknown source.",
-                 reference: str = 'unknown'):
-        self.variables = variables
-        self.title = title
-        self.description = description
-        self.reference = reference
-
-    def list_variables(self, only_public=False, only_current_scope=False) -> dict:
-        variables = self.variables
-        if only_public:
-            variables = {n: v for n, v in variables.items() if not n.startswith('_')}
-        return variables
-
-    def get_variable(self, name: str, default=None):
-        return self.variables.get(name, default)
-
-    def set_variable(self, name: str, value):
-        self.variables[name] = value
-
-    def get_title(self):
-        return self.title
-
-    def get_description(self):
-        return self.description
-
-    def get_reference(self):
-        return self.reference
+import inspect
+import os
+import sys
+from typing import List
+
+from phi.vis._vis_base import display_name
+
+
+class UserNamespace:
+
+    def list_variables(self, only_public=False, only_current_scope=False) -> dict:
+        raise NotImplementedError(self)
+
+    def get_variable(self, name: str, default=None):  # __getitem__
+        raise NotImplementedError(self)
+
+    def set_variable(self, name: str, value):  # __setitem__
+        raise NotImplementedError(self)
+
+    def get_title(self):  # __repr__()
+        raise NotImplementedError(self)
+
+    def get_description(self):  # __doc__?
+        raise NotImplementedError(self)
+
+    def get_reference(self):  # __str__() / __repr__()
+        """ Used to determine the default directory name to which data is written. """
+        raise NotImplementedError(self)
+
+
+def global_user_namespace(frames: List[inspect.FrameInfo]) -> UserNamespace:
+    if 'ipykernel' in sys.modules:
+        return JupyterNamespace()
+    else:
+        for frame in frames:
+            if frame.function == '<module>':
+                module = inspect.getmodule(frame.frame)
+                return ModuleNamespace(module)
+        raise AssertionError('No module found in call stack.')
+
+
+def get_user_namespace(ignore_stack_frames=0, frames: List[inspect.FrameInfo] = None) -> UserNamespace:
+    if not frames:
+        frames = inspect.stack()[ignore_stack_frames + 1:]  # 1 for this function
+    if frames[0].function == '<module>':
+        return global_user_namespace(frames)
+    else:
+        return LocalNamespace(frames)
+
+
+class ModuleNamespace(UserNamespace):
+
+    def __init__(self, module):
+        self.module = module
+
+    def get_title(self):
+        doc = self.module.__doc__
+        if doc is None:
+            return self.get_reference()
+        else:
+            end_of_line = doc.index('\n')
+            name = doc[:end_of_line].strip()
+            return name if name else self.get_reference()
+
+    def get_reference(self):
+        return os.path.basename(self.module.__file__)[:-3]
+
+    def get_description(self):
+        doc = self.module.__doc__
+        if doc is None:
+            return doc or self.module.__file__
+        else:
+            end_of_line = doc.index('\n')
+            return doc[end_of_line:].strip() or ""
+
+    def list_variables(self, only_public=False, only_current_scope=False) -> dict:
+        # all variables are in the current scope
+        variables = {name: getattr(self.module, name) for name in dir(self.module)}
+        if only_public:
+            variables = {n: v for n, v in variables.items() if not n.startswith('_')}
+        if only_current_scope:
+            variables = {n: v for n, v in variables.items() if is_value(v) or inspect.getmodule(v) == self.module}
+        return variables
+
+    def get_variable(self, name: str, default=None):
+        return getattr(self.module, name, default)
+
+    def set_variable(self, name: str, value):
+        setattr(self.module, name, value)
+
+
+def is_value(obj):
+    if isinstance(obj, type):
+        return False
+    # return type(open).__name__ not in ('function', 'builtin_function_or_method', 'module')
+    # if isinstance(obj, (type, function, builtin_function_or_method))
+    if inspect.isfunction(obj):
+        return False
+    return True
+
+
+class JupyterNamespace(UserNamespace):
+
+    def get_title(self):
+        return "Notebook"
+
+    def get_reference(self):
+        return "notebooks"
+
+    def get_description(self):
+        if 'google.colab' not in sys.modules:
+            return "Google Colab Notebook"
+        else:
+            import ipykernel
+            version = ipykernel.version_info
+            client_name = ipykernel.write_connection_file.__module__.split('.')[0]
+            return f"{client_name} ({version})"
+
+    def list_variables(self, only_public=False, only_current_scope=False) -> dict:
+        from IPython import get_ipython
+        shell = get_ipython()
+        variables = shell.user_ns
+        if only_public:
+            hidden = shell.user_ns_hidden
+            variables = {n: v for n, v in variables.items() if not n.startswith('_') and n not in hidden}
+        if only_current_scope:
+            text = shell.user_ns['In'][-1]
+            variables = {n: v for n, v in variables.items() if n in text}  # TODO parse text, only show names with assignment
+        return variables
+
+    def get_variable(self, name: str, default=None):
+        from IPython import get_ipython
+        shell = get_ipython()
+        return shell.user_ns.get(name, default)
+
+    def set_variable(self, name: str, value):
+        from IPython import get_ipython
+        shell = get_ipython()
+        shell.user_ns[name] = value
+
+
+class LocalNamespace(UserNamespace):
+
+    def __init__(self, frames: List[inspect.FrameInfo]):
+        self.frame = frames[0].frame
+        self.function_name: str = frames[0].function
+        self.module = inspect.getmodule(self.frame)
+        if self.module:
+            if hasattr(self.module, self.function_name):
+                self.function = getattr(self.module, self.function_name)
+            else:
+                self.function = None
+        else:
+            assert 'ipykernel' in sys.modules, f"Unable to locate file in which {self.function_name} is declared."
+            self.function = JupyterNamespace().get_variable(self.function_name)
+
+    def list_variables(self, only_public=False, only_current_scope=False) -> dict:
+        return self.frame.f_locals
+
+    def get_variable(self, name: str, default=None):
+        return self.frame.f_locals.get(name, default)
+
+    def set_variable(self, name: str, value):
+        import ctypes
+        self.frame.f_locals[name] = value
+        ctypes.pythonapi.PyFrame_LocalsToFast(ctypes.py_object(self.frame), ctypes.c_int(0))
+
+    def get_title(self):
+        return display_name(self.function_name)
+
+    def get_description(self):
+        if self.function is not None and self.function.__doc__:
+            return self.function.__doc__
+        else:
+            return f"Function `{self.function_name}()` in '{self.module.__file__}'"
+
+    def get_reference(self):
+        return f"{os.path.basename(self.module.__file__)[:-3]}_{self.function_name}"
+
+
+class DictNamespace(UserNamespace):
+
+    def __init__(self,
+                 variables: dict,
+                 title: str = "Unknown",
+                 description: str = "Custom namespace, unknown source.",
+                 reference: str = 'unknown'):
+        self.variables = variables
+        self.title = title
+        self.description = description
+        self.reference = reference
+
+    def list_variables(self, only_public=False, only_current_scope=False) -> dict:
+        variables = self.variables
+        if only_public:
+            variables = {n: v for n, v in variables.items() if not n.startswith('_')}
+        return variables
+
+    def get_variable(self, name: str, default=None):
+        return self.variables.get(name, default)
+
+    def set_variable(self, name: str, value):
+        self.variables[name] = value
+
+    def get_title(self):
+        return self.title
+
+    def get_description(self):
+        return self.description
+
+    def get_reference(self):
+        return self.reference
```

### Comparing `phiflow-2.3.4/phi/vis/_vis.py` & `phiflow-2.4.0/phi/vis/_vis.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,595 +1,661 @@
-import inspect
-import os
-import sys
-import warnings
-from contextlib import contextmanager
-from threading import Thread
-from typing import Tuple, List, Dict, Union
-
-from ._user_namespace import get_user_namespace, UserNamespace, DictNamespace
-from ._viewer import create_viewer, Viewer
-from ._vis_base import Control, value_range, Action, VisModel, Gui, PlottingLibrary, tensor_as_field, common_index, index_label, title_label
-from .. import math
-from ..field import SampledField, Scene, Field, PointCloud
-from ..field._scene import _slugify_filename
-from ..geom import Geometry, Box, embed
-from ..math import Tensor, layout, batch, Shape, wrap, merge_shapes, EMPTY_SHAPE
-from ..math._shape import parse_dim_order, DimFilter
-
-
-def show(*model: Union[VisModel, SampledField, tuple, list, Tensor, Geometry],
-         play=True,
-         gui: Union[Gui, str] = None,
-         lib: Union[Gui, str] = None,
-         keep_alive=True,
-         **config):
-    """
-    If `model` is a user interface model, launches the registered user interface.
-    This will typically be the Dash web interface or the console interface if dash is not available.
-    This method prepares the `model` before showing it. No more fields should be added to the vis after this method is invoked.
-
-    See Also:
-        `view()`.
-
-    If `model` is plottable, e.g. a `SampledField` or `Tensor`, a figure is created and shown.
-    If `model` is a figure, it is simply shown.
-
-    See Also:
-        `plot()`.
-
-    This method may block until the GUI or plot window is closed.
-
-    Also see the user interface documentation at https://tum-pbs.github.io/PhiFlow/Visualization.html
-
-    Args:
-      model: (Optional) `VisModel`, the application or plottable object to display.
-        If unspecified, shows the most recently plotted figure.
-      play: If true, invokes `App.play()`. The default value is False unless "autorun" is passed as a command line argument.
-      gui: Deprecated. Use `lib` instead. (optional) class of GUI to use
-      lib: Gui class or plotting library as `str`, e.g. `'matplotlib'` or `'plotly'`
-      keep_alive: Whether the GUI keeps the vis alive. If `False`, the program will exit when the main script is finished.
-      **config: additional GUI configuration parameters.
-        For a full list of parameters, see the respective GUI documentation at https://tum-pbs.github.io/PhiFlow/Visualization.html
-    """
-    lib = lib if lib is not None else gui
-    if len(model) == 1 and isinstance(model[0], VisModel):
-        model[0].prepare()
-        # --- Setup Gui ---
-        gui = default_gui() if lib is None else get_gui(lib)
-        gui.configure(config)
-        gui.setup(model[0])
-        if play:  # this needs to be done even if model cannot progress right now
-            gui.auto_play()
-        if gui.asynchronous:
-            display_thread = Thread(target=lambda: gui.show(True), name="AsyncGui", daemon=not keep_alive)
-            display_thread.start()
-        else:
-            gui.show(True)  # may be blocking call
-    elif len(model) == 0:
-        plots = default_plots() if lib is None else get_plots(lib)
-        return plots.show(plots.current_figure)
-    else:
-        plots = default_plots() if lib is None else get_plots(lib)
-        fig_tensor = plot(*model, lib=plots, **config)
-        if isinstance(fig_tensor, Tensor):
-            for fig in fig_tensor:
-                plots.show(fig)
-        else:
-            return plots.show(fig_tensor)
-
-
-def close(figure=None):
-    """
-    Close and destroy a figure.
-
-    Args:
-        figure: (Optional) A figure that was created using `plot()`.
-            If not specified, closes the figure created most recently.
-    """
-    if figure is None:
-        figure = LAST_FIGURE[0]
-    if isinstance(figure, Tensor):
-        for fig in figure:
-            close(fig)
-    else:
-        plots = get_plots_by_figure(figure)
-        plots.close(figure)
-
-
-
-RECORDINGS = {}
-
-
-def record(*fields: Union[str, SampledField]) -> Viewer:
-    user_namespace = get_user_namespace(1)
-    variables = _default_field_variables(user_namespace, fields)
-    viewer = create_viewer(user_namespace, variables, "record", "", scene=None, asynchronous=False, controls=(),
-                           actions={}, log_performance=False)
-    viewer.post_step.append(lambda viewer: print(viewer.steps, end=" "))
-    viewer.progress_unavailable.append(lambda viewer: print())
-    return viewer
-
-
-def view(*fields: Union[str, SampledField],
-         play: bool = True,
-         gui=None,
-         name: str = None,
-         description: str = None,
-         scene: Union[bool, Scene] = False,
-         keep_alive=True,
-         select: Union[str, tuple, list] = '',
-         framerate=None,
-         namespace=None,
-         log_performance=True,
-         **config) -> Viewer:
-    """
-    Show `fields` in a graphical user interface.
-
-    `fields` may contain instances of `Field` or variable names of top-level variables (main module or Jupyter notebook).
-    During loops, e.g. `view().range()`, the variable status is tracked and the GUI is updated.
-
-    When called from a Python script, name and description may be specified in the module docstring (string before imports).
-    The first line is interpreted as the name, the rest as the subtitle.
-    If not specified, a generic name and description is chosen.
-
-    Args:
-        *fields: (Optional) Contents to be displayed. Either variable names or values.
-            For field instances, all variables referencing the value will be shown.
-            If not provided, the user namespace is searched for Field variables.
-        play: Whether to immediately start executing loops.
-        gui: (Optional) Name of GUI as `str` or GUI class.
-            Built-in GUIs can be selected via `'dash'`, `'console'`.
-            See https://tum-pbs.github.io/PhiFlow/Visualization.html
-        name: (Optional) Name to display in GUI and use for the output directory if `scene=True`.
-            Will be generated from the top-level script if not provided.
-        description: (Optional) Description to be displayed in the GUI.
-            Will be generated from the top-level script if not provided.
-        scene: Existing `Scene` to write into or `bool`. If `True`, creates a new Scene in `~/phi/<name>`
-        keep_alive: Whether the GUI should keep running even after the main thread finishes.
-        framerate: Target frame rate in Hz. Play will not step faster than the framerate. `None` for unlimited frame rate.
-        select: Dimension names along which one item to show is selected.
-            Dimensions may be passed as `tuple` of `str` or as comma-separated names in a single `str`.
-            For each `select` dimension, an associated selection slider will be created.
-        log_performance: Whether to measure and log the time each step takes.
-            If `True`, will be logged as `step_time` to `log_step_time.txt`.
-        **config: Additional GUI configuration arguments.
-
-    Returns:
-        `Viewer`
-    """
-    default_namespace = get_user_namespace(1)
-    user_namespace = default_namespace if namespace is None else DictNamespace(namespace,
-                                                                               title=default_namespace.get_title(),
-                                                                               description=default_namespace.get_description(),
-                                                                               reference=default_namespace.get_reference())
-    variables = _default_field_variables(user_namespace, fields)
-    actions = dict(ACTIONS)
-    ACTIONS.clear()
-    if scene is False:
-        scene = None
-    elif scene is True:
-        scene = Scene.create(os.path.join("~", "phi", _slugify_filename(name or user_namespace.get_reference())))
-        print(f"Created scene at {scene}")
-    else:
-        assert isinstance(scene, Scene)
-    name = name or user_namespace.get_title()
-    description = description or user_namespace.get_description()
-    gui = default_gui() if gui is None else get_gui(gui)
-    controls = tuple(c for c in sorted(CONTROL_VARS.values(), key=lambda c: c.name) if
-                     user_namespace.get_variable(c.name) is not None)
-    CONTROL_VARS.clear()
-    viewer = create_viewer(user_namespace, variables, name, description, scene, asynchronous=gui.asynchronous,
-                           controls=controls, actions=actions, log_performance=log_performance)
-    show(viewer, play=play, gui=gui, keep_alive=keep_alive, framerate=framerate, select=select, **config)
-    return viewer
-
-
-def _default_field_variables(user_namespace: UserNamespace, fields: tuple):
-    names = []
-    values = []
-    if len(fields) == 0:  # view all Fields
-        user_variables = user_namespace.list_variables(only_public=True, only_current_scope=True)
-        for name, val in user_variables.items():
-            if isinstance(val, SampledField):
-                names.append(name)
-                values.append(val)
-    else:  # find variable names
-        user_variables = user_namespace.list_variables()
-        for field in fields:
-            if isinstance(field, str):
-                split = [n.strip() for n in field.split(',')]
-                names.extend(split)
-                values.extend([user_namespace.get_variable(n, default=None) for n in split])
-            else:
-                for name, val in user_variables.items():
-                    if val is field:
-                        names.append(name)
-                        values.append(field)
-    return {n: v for n, v in zip(names, values)}
-
-
-CONTROL_VARS = {}
-
-
-def control(value, range: tuple = None, description="", **kwargs):
-    """
-    Mark a variable as controllable by any GUI created via `view()`.
-
-    Example:
-    >>> dt = control(1.0, (0.1, 10), name="Time increment (dt)")
-
-    This will cause a control component (slider, checkbox, text field, drop-down, etc.) to be generated in the user interface.
-    Changes to that component will immediately be reflected in the Python variable assigned to the control.
-    The Python variable will always hold a primitive type, such as `int`, `float´, `bool` or `str`.
-
-    Args:
-        value: Initial value. Must be either `int`, `float`, `bool` or `str`.
-        range: (Optional) Specify range of possible values as `(min, max)`. Only for `int`, `float` and `str` values.
-        description: Human-readable description.
-        **kwargs: Additional arguments to determine the appearance of the GUI component,
-            e.g. `rows` for text fields or `log=False` for float sliders.
-
-    Returns:
-        `value`
-    """
-    assert type(value) in (int, float, bool, str), f"Value must be one of (int, float, bool, str) but {type(value)}"
-    calling_code = inspect.stack()[1].code_context[0]
-    assert 'control' in calling_code and '=' in calling_code, f"control() must be used in a variable assignment statement but context is: {calling_code}"
-    calling_code = calling_code[:calling_code.index('control')]
-    var_names = [var.strip() for var in calling_code.split('=')[:-1]]
-    var_names = [n for n in var_names if n]
-    for var_name in var_names:
-        ctrl = Control(var_name, type(value), value, range, description, kwargs)
-        value_range(ctrl)  # checks if valid
-        CONTROL_VARS[var_name] = ctrl
-    return value
-
-
-ACTIONS = {}
-
-
-def action(fun):
-    doc = inspect.getdoc(fun)
-    ACTIONS[Action(fun.__name__, doc)] = fun
-    return fun
-
-
-LAST_FIGURE = [None]  # reference to last figure (1-element list)
-
-
-def get_current_figure():
-    """
-    Returns the figure that was most recently created using `plot()`.
-
-    The type of the figure depends on which library was used, e.g. `matplotlib.figure.Figure` or `plotly.graph_objs.Figure`.
-    """
-    return LAST_FIGURE[0]
-
-
-def plot(*fields: Union[SampledField, Tensor, Geometry],
-         lib: Union[str, PlottingLibrary] = None,
-         row_dims: DimFilter = None,
-         col_dims: DimFilter = batch,
-         animate: DimFilter = None,
-         overlay: DimFilter = 'overlay',
-         title: Union[str, Tensor] = None,
-         size=(12, 5),
-         same_scale=True,
-         log_dims: Union[str, tuple, list, Shape] = '',
-         show_color_bar=True,
-         color: Union[str, int, Tensor] = None,
-         alpha: Union[float, Tensor] = 1.,
-         frame_time=100,
-         repeat=True):
-    """
-    Creates one or multiple figures and sub-figures and plots the given fields.
-
-    To show the figures, use `show()`.
-
-    The arguments `row_dims`, `col_dims`, `animate` and `overlay` control how data is presented.
-    Each accepts dimensions as a `str`, `Shape`, tuple, list or type function.
-    In addition to the dimensions present on the data to be plotted, the dimensions `args` is created if multiple arguments are passed,
-    and `tuple`, `list`, `dict` are generated for corresponding objects to be plotted.
-
-    Args:
-        fields: Fields or Tensors to plot.
-        lib: Plotting library name or reference. Valid names are `'matplotlib'`, `'plotly'` and `'console'`.
-        row_dims: Batch dimensions along which sub-figures should be laid out vertically.
-            `Shape` or comma-separated names as `str`, `tuple` or `list`.
-        col_dims: Batch dimensions along which sub-figures should be laid out horizontally.
-            `Shape` or comma-separated names as `str`, `tuple` or `list`.
-        title: String `Tensor` with dimensions `rows` and `cols`.
-        size: Figure size in inches, `(width, height)`.
-        same_scale: Whether to use the same axis limits for all sub-figures.
-        log_dims: Dimensions for which the plot axes should be scaled logarithmically.
-            Can be given as a comma-separated `str`, a sequence of dimension names or a `Shape`.
-            Use `'_'` to scale unnamed axes logarithmically, e.g. the y-axis of scalar functions.
-        show_color_bar: Whether to display color bars for heat maps.
-        color: `Tensor` of line / marker colors.
-            The color can be specified either as a cycle index (int tensor) or as a hex code (str tensor).
-            The color of different lines and markers can vary.
-        alpha: Opacity as `float` or `Tensor`.
-            This affects all elements, not only line plots.
-            Opacity can vary between lines and markers.
-        animate: Time dimension to animate.
-            If not present in the data, will produce a regular plot instead.
-        overlay: Dimensions along which elements should be overlaid in the same subplot.
-            The default is only the `overlay` dimension which is created by `overlay()`.
-        frame_time: Interval between frames in the animation.
-        repeat: Whether the animation should loop.
-
-    Returns:
-        `Tensor` of figure objects.
-        The tensor contains those dimensions of `fields` that were not reduced by `row_dims`, `col_dims` or `animate`.
-        Currently, only single-figure plots are supported.
-
-        In case of an animation, a displayable animation object will be returned instead of a `Tensor`.
-    """
-    positioning = {}
-    indices: Dict[Tuple[int, int], List[dict]] = {}
-    nrows, ncols, fig_shape, reduced_shape = layout_sub_figures(layout(fields, batch('args')), row_dims, col_dims, animate, overlay, 0, 0, positioning, indices, {})
-    animate = fig_shape.only(animate)
-    fig_shape = fig_shape.without(animate)
-    plots = default_plots() if lib is None else get_plots(lib)
-    # --- Process arguments ---
-    if same_scale:
-        if any([f.values.dtype.kind == complex for l in positioning.values() for f in l]):
-            min_val = 0
-            max_val = max([float(abs(f.values).finite_max) for l in positioning.values() for f in l])
-        else:
-            min_val = min([float(f.values.finite_min) for l in positioning.values() for f in l])
-            max_val = max([float(f.values.finite_max) for l in positioning.values() for f in l])
-    else:
-        min_val = max_val = None
-    subplots = {pos: _space(fields, animate) for pos, fields in positioning.items()}
-    if isinstance(title, str):
-        title = {pos: title for pos in positioning}
-    elif isinstance(title, Tensor):
-        title = {(row, col): title.rows[row].cols[col].native() for (row, col) in positioning}
-    else:
-        assert title is None, f"title must be a str or Tensor but got {title}"
-        title = {pos: title_label(common_index(*i, exclude=reduced_shape.singleton)) for pos, i in indices.items()}
-    log_dims = parse_dim_order(log_dims) or ()
-    color = layout_pytree_node(color, wrap_leaf=True)
-    alpha = layout_pytree_node(alpha, wrap_leaf=True)
-    # --- animate or plot ---
-    if fig_shape.volume == 1:
-        figure, axes = plots.create_figure(size, nrows, ncols, subplots, title, log_dims)
-        if animate:
-            def plot_frame(frame: int):
-                for pos, fields in positioning.items():
-                    for i, f in enumerate(fields):
-                        idx = indices[pos][i]
-                        f = f[{animate.name: frame}]
-                        plots.plot(f, figure, axes[pos], subplots[pos], min_val, max_val, show_color_bar, color[idx], alpha[idx])
-                plots.finalize(figure)
-            anim = plots.animate(figure, animate.size, plot_frame, frame_time, repeat)
-            LAST_FIGURE[0] = anim
-            plots.close(figure)
-            return anim
-        else:
-            for pos, fields in positioning.items():
-                for i, f in enumerate(fields):
-                    idx = indices[pos][i]
-                    plots.plot(f, figure, axes[pos], subplots[pos], min_val, max_val, show_color_bar, color[idx], alpha[idx])
-            plots.finalize(figure)
-            LAST_FIGURE[0] = figure
-            return layout(figure)
-    else:
-        raise NotImplementedError(f"Figure batches not yet supported. Use rows and cols to reduce all batch dimensions. Not reduced. {fig_shape}")
-
-
-def layout_pytree_node(data, wrap_leaf=False):
-    if isinstance(data, tuple):
-        return layout(data, batch('tuple'))
-    elif isinstance(data, list):
-        return layout(data, batch('list'))
-    elif isinstance(data, dict):
-        return layout(data, batch('dict'))
-    return wrap(data) if wrap_leaf else data
-
-
-def layout_sub_figures(data: Union[Tensor, SampledField],
-                       row_dims: DimFilter,
-                       col_dims: DimFilter,
-                       animate: DimFilter,  # do not reduce these dims, has priority
-                       overlay: DimFilter,
-                       offset_row: int,
-                       offset_col: int,
-                       positioning: Dict[Tuple[int, int], List],
-                       indices: Dict[Tuple[int, int], List[dict]],
-                       base_index: Dict[str, Union[int, str]]) -> Tuple[int, int, Shape, Shape]:  # rows, cols
-    if data is None:
-        raise ValueError(f"Cannot layout figure for '{data}'")
-    data = layout_pytree_node(data)
-    if isinstance(data, Tensor) and data.dtype.kind == object:  # layout
-        rows, cols = 0, 0
-        non_reduced = math.EMPTY_SHAPE
-        dim0 = reduced = data.shape[0]
-        if dim0.only(overlay):
-            for overlay_index in dim0.only(overlay).meshgrid(names=True):  # overlay these fields
-                e_rows, e_cols, d_non_reduced, d_reduced = layout_sub_figures(data[overlay_index].native(), row_dims, col_dims, animate, overlay, offset_row, offset_col, positioning, indices, {**base_index, **overlay_index})
-                rows = max(rows, e_rows)
-                cols = max(cols, e_cols)
-                non_reduced &= d_non_reduced
-                reduced = merge_shapes(reduced, d_reduced, allow_varying_sizes=True)
-        elif dim0.only(animate):
-            data = math.stack(data.native(), dim0)
-            return layout_sub_figures(data, row_dims, col_dims, animate, overlay, offset_row, offset_col, positioning, indices, base_index)
-        else:
-            elements = data.unstack(dim0.name)
-            for item_name, e in zip(dim0.get_item_names(dim0.name) or range(dim0.size), elements):
-                index = dict(base_index, **{dim0.name: item_name})
-                if dim0.only(row_dims):
-                    e_rows, e_cols, e_non_reduced, e_reduced = layout_sub_figures(e.native(), row_dims, col_dims, animate, overlay, offset_row + rows, offset_col, positioning, indices, index)
-                    rows += e_rows
-                    cols = max(cols, e_cols)
-                elif dim0.only(col_dims):
-                    e_rows, e_cols, e_non_reduced, e_reduced = layout_sub_figures(e.native(), row_dims, col_dims, animate, overlay, offset_row, offset_col + cols, positioning, indices, index)
-                    cols += e_cols
-                    rows = max(rows, e_rows)
-                else:
-                    e_rows, e_cols, e_non_reduced, e_reduced = layout_sub_figures(e.native(), row_dims, col_dims, animate, overlay, offset_row, offset_col, positioning, indices, index)
-                    cols = max(cols, e_cols)
-                    rows = max(rows, e_rows)
-                non_reduced &= e_non_reduced
-                reduced = merge_shapes(reduced, e_reduced, allow_varying_sizes=True)
-        return rows, cols, non_reduced, reduced
-    else:
-        if isinstance(data, Tensor):
-            data = tensor_as_field(data)
-        elif isinstance(data, Geometry):
-            data = PointCloud(data)
-        assert isinstance(data, Field), f"Cannot plot {type(data)}. Only tensors, geometries and fields can be plotted."
-        overlay = data.shape.only(overlay)
-        animate = data.shape.only(animate).without(overlay)
-        row_shape = data.shape.only(row_dims).without(animate).without(overlay)
-        col_shape = data.shape.only(col_dims).without(row_dims).without(animate).without(overlay)
-        non_reduced: Shape = batch(data).without(row_dims).without(col_dims) & animate
-        for ri, r in enumerate(row_shape.meshgrid(names=True)):
-            for ci, c in enumerate(col_shape.meshgrid(names=True)):
-                for o in overlay.meshgrid(names=True):
-                    sub_data = data[r][c][o]
-                    positioning.setdefault((offset_row + ri, offset_col + ci), []).append(sub_data)
-                    indices.setdefault((offset_row + ri, offset_col + ci), []).append(dict(base_index, **r, **c, **o))
-        return row_shape.volume, col_shape.volume, non_reduced, EMPTY_SHAPE
-
-
-def _space(fields: Tuple[Field, ...], ignore_dims: Shape) -> Box:
-    all_dims = []
-    for f in fields:
-        for dim in f.bounds.vector.item_names:
-            if dim not in all_dims and dim not in ignore_dims:
-                all_dims.append(dim)
-    all_bounds = [embed(f.bounds.without(ignore_dims.names), all_dims) for f in fields]
-    if len(all_bounds) == 1:
-        return all_bounds[0]
-    bounds: Box = math.stack(all_bounds, batch('_fields'))
-    lower = math.finite_min(bounds.lower, '_fields', default=-math.INF)
-    upper = math.finite_max(bounds.upper, '_fields', default=math.INF)
-    return Box(lower, upper)
-
-
-def overlay(*fields: Union[SampledField, Tensor]) -> Tensor:
-    """
-    Specify that multiple fields should be drawn on top of one another in the same figure.
-    The fields will be plotted in the order they are given, i.e. the last field on top.
-
-    >>> plot(vis.overlay(heatmap, points, velocity))
-
-    Args:
-        *fields: `SampledField` or `Tensor` instances
-
-    Returns:
-        Plottable object
-    """
-    return layout(fields, math.channel('overlay'))
-
-
-def write_image(path: str, figure=None, dpi=120.):
-    """
-    Save a figure to an image file.
-
-    Args:
-        figure: Matplotlib or Plotly figure or text.
-        path: File path.
-        dpi: Pixels per inch.
-    """
-    figure = figure or LAST_FIGURE[0]
-    if figure is None:
-        figure = default_plots().current_figure
-    assert figure is not None, "No figure to save."
-    lib = get_plots_by_figure(figure)
-    lib.save(figure, path, dpi)
-
-
-def default_gui() -> Gui:
-    if GUI_OVERRIDES:
-        return GUI_OVERRIDES[-1]
-    if 'google.colab' in sys.modules or 'ipykernel' in sys.modules:
-        raise NotImplementedError("There is currently no GUI support for Python notebooks. Use `vis.plot()` to display plots or animations instead.")
-    else:
-        options = ['dash', 'console']
-    for option in options:
-        try:
-            return get_gui(option)
-        except ImportError as import_error:
-            warnings.warn(f"{option} user interface is unavailable because of missing dependency: {import_error}.", ImportWarning)
-    raise RuntimeError("No user interface available.")
-
-
-def get_gui(gui: Union[str, Gui]) -> Gui:
-    if GUI_OVERRIDES:
-        return GUI_OVERRIDES[-1]
-    if isinstance(gui, str):
-        if gui == 'dash':
-            from ._dash.dash_gui import DashGui
-            return DashGui()
-        elif gui == 'console':
-            from ._console import ConsoleGui
-            return ConsoleGui()
-        else:
-            raise NotImplementedError(f"No display available with name {gui}")
-    elif isinstance(gui, Gui):
-        return gui
-    else:
-        raise ValueError(gui)
-
-
-GUI_OVERRIDES = []
-
-
-@contextmanager
-def force_use_gui(gui: Gui):
-    GUI_OVERRIDES.append(gui)
-    try:
-        yield None
-    finally:
-        assert GUI_OVERRIDES.pop(-1) is gui
-
-
-_LOADED_PLOTTING_LIBRARIES: List[PlottingLibrary] = []
-
-
-def default_plots() -> PlottingLibrary:
-    if 'google.colab' in sys.modules or 'ipykernel' in sys.modules:
-        options = ['matplotlib']
-    else:
-        options = ['matplotlib', 'plotly', 'ascii']
-    for option in options:
-        try:
-            return get_plots(option)
-        except ImportError as import_error:
-            warnings.warn(f"{option} user interface is unavailable because of missing dependency: {import_error}.", ImportWarning)
-    raise RuntimeError("No user interface available.")
-
-
-def get_plots(lib: Union[str, PlottingLibrary]) -> PlottingLibrary:
-    if isinstance(lib, PlottingLibrary):
-        return lib
-    for loaded_lib in _LOADED_PLOTTING_LIBRARIES:
-        if loaded_lib.name == lib:
-            return loaded_lib
-    if lib == 'matplotlib':
-        from ._matplotlib._matplotlib_plots import MATPLOTLIB
-        _LOADED_PLOTTING_LIBRARIES.append(MATPLOTLIB)
-        return MATPLOTLIB
-    elif lib == 'plotly':
-        from ._dash._plotly_plots import PLOTLY
-        _LOADED_PLOTTING_LIBRARIES.append(PLOTLY)
-        return PLOTLY
-    elif lib == 'ascii':
-        from ._console._console_plot import CONSOLE
-        _LOADED_PLOTTING_LIBRARIES.append(CONSOLE)
-        return CONSOLE
-    else:
-        raise NotImplementedError(f"No plotting library available with name {lib}")
-
-
-def get_plots_by_figure(figure):
-    for loaded_lib in _LOADED_PLOTTING_LIBRARIES:
-        if loaded_lib.is_figure(figure):
-            return loaded_lib
-    else:
-        raise ValueError(f"No library found matching figure {figure} from list {_LOADED_PLOTTING_LIBRARIES}")
-
+import inspect
+import os
+import sys
+import warnings
+from contextlib import contextmanager
+from threading import Thread
+from typing import Tuple, List, Dict, Union
+
+from ._user_namespace import get_user_namespace, UserNamespace, DictNamespace
+from ._viewer import create_viewer, Viewer
+from ._vis_base import Control, value_range, Action, VisModel, Gui, PlottingLibrary, common_index, to_field, get_default_limits
+from ._vis_base import title_label
+from .. import math
+from ..field import SampledField, Scene, Field, PointCloud
+from ..field._scene import _slugify_filename
+from ..geom import Geometry, Box, embed
+from ..math import Tensor, layout, batch, Shape, vec, stack, concat
+from ..math import wrap
+from ..math._shape import parse_dim_order, DimFilter, EMPTY_SHAPE, merge_shapes, shape, non_batch
+from ..math._tensors import Layout
+
+
+def show(*model: Union[VisModel, SampledField, tuple, list, Tensor, Geometry],
+         play=True,
+         gui: Union[Gui, str] = None,
+         lib: Union[Gui, str] = None,
+         keep_alive=True,
+         **config):
+    """
+    If `model` is a user interface model, launches the registered user interface.
+    This will typically be the Dash web interface or the console interface if dash is not available.
+    This method prepares the `model` before showing it. No more fields should be added to the vis after this method is invoked.
+
+    See Also:
+        `view()`.
+
+    If `model` is plottable, e.g. a `SampledField` or `Tensor`, a figure is created and shown.
+    If `model` is a figure, it is simply shown.
+
+    See Also:
+        `plot()`.
+
+    This method may block until the GUI or plot window is closed.
+
+    Also see the user interface documentation at https://tum-pbs.github.io/PhiFlow/Visualization.html
+
+    Args:
+      model: (Optional) `VisModel`, the application or plottable object to display.
+        If unspecified, shows the most recently plotted figure.
+      play: If true, invokes `App.play()`. The default value is False unless "autorun" is passed as a command line argument.
+      gui: Deprecated. Use `lib` instead. (optional) class of GUI to use
+      lib: Gui class or plotting library as `str`, e.g. `'matplotlib'` or `'plotly'`
+      keep_alive: Whether the GUI keeps the vis alive. If `False`, the program will exit when the main script is finished.
+      **config: additional GUI configuration parameters.
+        For a full list of parameters, see the respective GUI documentation at https://tum-pbs.github.io/PhiFlow/Visualization.html
+    """
+    lib = lib if lib is not None else gui
+    if len(model) == 1 and isinstance(model[0], VisModel):
+        model[0].prepare()
+        # --- Setup Gui ---
+        gui = default_gui() if lib is None else get_gui(lib)
+        gui.configure(config)
+        gui.setup(model[0])
+        if play:  # this needs to be done even if model cannot progress right now
+            gui.auto_play()
+        if gui.asynchronous:
+            display_thread = Thread(target=lambda: gui.show(True), name="AsyncGui", daemon=not keep_alive)
+            display_thread.start()
+        else:
+            gui.show(True)  # may be blocking call
+    elif len(model) == 0:
+        plots = default_plots() if lib is None else get_plots(lib)
+        return plots.show(plots.current_figure)
+    else:
+        plots = default_plots() if lib is None else get_plots(lib)
+        fig_tensor = plot(*model, lib=plots, **config)
+        if isinstance(fig_tensor, Tensor):
+            for fig in fig_tensor:
+                plots.show(fig)
+        else:
+            return plots.show(fig_tensor)
+
+
+def close(figure=None):
+    """
+    Close and destroy a figure.
+
+    Args:
+        figure: (Optional) A figure that was created using `plot()`.
+            If not specified, closes the figure created most recently.
+    """
+    if figure is None:
+        figure = LAST_FIGURE[0]
+    if isinstance(figure, Tensor):
+        for fig in figure:
+            close(fig)
+    else:
+        plots = get_plots_by_figure(figure)
+        plots.close(figure)
+
+
+close_ = close
+
+
+RECORDINGS = {}
+
+
+def record(*fields: Union[str, SampledField]) -> Viewer:
+    user_namespace = get_user_namespace(1)
+    variables = _default_field_variables(user_namespace, fields)
+    viewer = create_viewer(user_namespace, variables, "record", "", scene=None, asynchronous=False, controls=(),
+                           actions={}, log_performance=False)
+    viewer.post_step.append(lambda viewer: print(viewer.steps, end=" "))
+    viewer.progress_unavailable.append(lambda viewer: print())
+    return viewer
+
+
+def view(*fields: Union[str, SampledField],
+         play: bool = True,
+         gui=None,
+         name: str = None,
+         description: str = None,
+         scene: Union[bool, Scene] = False,
+         keep_alive=True,
+         select: Union[str, tuple, list] = '',
+         framerate=None,
+         namespace=None,
+         log_performance=True,
+         **config) -> Viewer:
+    """
+    Show `fields` in a graphical user interface.
+
+    `fields` may contain instances of `Field` or variable names of top-level variables (main module or Jupyter notebook).
+    During loops, e.g. `view().range()`, the variable status is tracked and the GUI is updated.
+
+    When called from a Python script, name and description may be specified in the module docstring (string before imports).
+    The first line is interpreted as the name, the rest as the subtitle.
+    If not specified, a generic name and description is chosen.
+
+    Args:
+        *fields: (Optional) Contents to be displayed. Either variable names or values.
+            For field instances, all variables referencing the value will be shown.
+            If not provided, the user namespace is searched for Field variables.
+        play: Whether to immediately start executing loops.
+        gui: (Optional) Name of GUI as `str` or GUI class.
+            Built-in GUIs can be selected via `'dash'`, `'console'`.
+            See https://tum-pbs.github.io/PhiFlow/Visualization.html
+        name: (Optional) Name to display in GUI and use for the output directory if `scene=True`.
+            Will be generated from the top-level script if not provided.
+        description: (Optional) Description to be displayed in the GUI.
+            Will be generated from the top-level script if not provided.
+        scene: Existing `Scene` to write into or `bool`. If `True`, creates a new Scene in `~/phi/<name>`
+        keep_alive: Whether the GUI should keep running even after the main thread finishes.
+        framerate: Target frame rate in Hz. Play will not step faster than the framerate. `None` for unlimited frame rate.
+        select: Dimension names along which one item to show is selected.
+            Dimensions may be passed as `tuple` of `str` or as comma-separated names in a single `str`.
+            For each `select` dimension, an associated selection slider will be created.
+        log_performance: Whether to measure and log the time each step takes.
+            If `True`, will be logged as `step_time` to `log_step_time.txt`.
+        **config: Additional GUI configuration arguments.
+
+    Returns:
+        `Viewer`
+    """
+    default_namespace = get_user_namespace(1)
+    user_namespace = default_namespace if namespace is None else DictNamespace(namespace,
+                                                                               title=default_namespace.get_title(),
+                                                                               description=default_namespace.get_description(),
+                                                                               reference=default_namespace.get_reference())
+    variables = _default_field_variables(user_namespace, fields)
+    actions = dict(ACTIONS)
+    ACTIONS.clear()
+    if scene is False:
+        scene = None
+    elif scene is True:
+        scene = Scene.create(os.path.join("~", "phi", _slugify_filename(name or user_namespace.get_reference())))
+        print(f"Created scene at {scene}")
+    else:
+        assert isinstance(scene, Scene)
+    name = name or user_namespace.get_title()
+    description = description or user_namespace.get_description()
+    gui = default_gui() if gui is None else get_gui(gui)
+    controls = tuple(c for c in sorted(CONTROL_VARS.values(), key=lambda c: c.name) if
+                     user_namespace.get_variable(c.name) is not None)
+    CONTROL_VARS.clear()
+    viewer = create_viewer(user_namespace, variables, name, description, scene, asynchronous=gui.asynchronous,
+                           controls=controls, actions=actions, log_performance=log_performance)
+    show(viewer, play=play, gui=gui, keep_alive=keep_alive, framerate=framerate, select=select, **config)
+    return viewer
+
+
+def _default_field_variables(user_namespace: UserNamespace, fields: tuple):
+    names = []
+    values = []
+    if len(fields) == 0:  # view all Fields
+        user_variables = user_namespace.list_variables(only_public=True, only_current_scope=True)
+        for name, val in user_variables.items():
+            if isinstance(val, SampledField):
+                names.append(name)
+                values.append(val)
+    else:  # find variable names
+        user_variables = user_namespace.list_variables()
+        for field in fields:
+            if isinstance(field, str):
+                split = [n.strip() for n in field.split(',')]
+                names.extend(split)
+                values.extend([user_namespace.get_variable(n, default=None) for n in split])
+            else:
+                for name, val in user_variables.items():
+                    if val is field:
+                        names.append(name)
+                        values.append(field)
+    return {n: v for n, v in zip(names, values)}
+
+
+CONTROL_VARS = {}
+
+
+def control(value, range: tuple = None, description="", **kwargs):
+    """
+    Mark a variable as controllable by any GUI created via `view()`.
+
+    Example:
+    >>> dt = control(1.0, (0.1, 10), name="Time increment (dt)")
+
+    This will cause a control component (slider, checkbox, text field, drop-down, etc.) to be generated in the user interface.
+    Changes to that component will immediately be reflected in the Python variable assigned to the control.
+    The Python variable will always hold a primitive type, such as `int`, `float´, `bool` or `str`.
+
+    Args:
+        value: Initial value. Must be either `int`, `float`, `bool` or `str`.
+        range: (Optional) Specify range of possible values as `(min, max)`. Only for `int`, `float` and `str` values.
+        description: Human-readable description.
+        **kwargs: Additional arguments to determine the appearance of the GUI component,
+            e.g. `rows` for text fields or `log=False` for float sliders.
+
+    Returns:
+        `value`
+    """
+    assert type(value) in (int, float, bool, str), f"Value must be one of (int, float, bool, str) but {type(value)}"
+    calling_code = inspect.stack()[1].code_context[0]
+    assert 'control' in calling_code and '=' in calling_code, f"control() must be used in a variable assignment statement but context is: {calling_code}"
+    calling_code = calling_code[:calling_code.index('control')]
+    var_names = [var.strip() for var in calling_code.split('=')[:-1]]
+    var_names = [n for n in var_names if n]
+    for var_name in var_names:
+        ctrl = Control(var_name, type(value), value, range, description, kwargs)
+        value_range(ctrl)  # checks if valid
+        CONTROL_VARS[var_name] = ctrl
+    return value
+
+
+ACTIONS = {}
+
+
+def action(fun):
+    doc = inspect.getdoc(fun)
+    ACTIONS[Action(fun.__name__, doc)] = fun
+    return fun
+
+
+LAST_FIGURE = [None]  # reference to last figure (1-element list)
+
+
+def get_current_figure():
+    """
+    Returns the figure that was most recently created using `plot()`.
+
+    The type of the figure depends on which library was used, e.g. `matplotlib.figure.Figure` or `plotly.graph_objs.Figure`.
+    """
+    return LAST_FIGURE[0]
+
+
+def plot(*fields: Union[SampledField, Tensor, Geometry, list, tuple, dict],
+         lib: Union[str, PlottingLibrary] = None,
+         row_dims: DimFilter = None,
+         col_dims: DimFilter = batch,
+         animate: DimFilter = None,
+         overlay: DimFilter = 'overlay',
+         title: Union[str, Tensor, list, tuple] = None,
+         size=(12, 5),
+         same_scale: Union[bool, Shape, tuple, list, str] = True,
+         log_dims: Union[str, tuple, list, Shape] = '',
+         show_color_bar=True,
+         color: Union[str, int, Tensor, list, tuple] = None,
+         alpha: Union[float, Tensor, list, tuple] = 1.,
+         err: Union[Tensor, tuple, list, float] = 0.,
+         frame_time=100,
+         repeat=True):
+    """
+    Creates one or multiple figures and sub-figures and plots the given fields.
+
+    To show the figures, use `show()`.
+
+    The arguments `row_dims`, `col_dims`, `animate` and `overlay` control how data is presented.
+    Each accepts dimensions as a `str`, `Shape`, tuple, list or type function.
+    In addition to the dimensions present on the data to be plotted, the dimensions `args` is created if multiple arguments are passed,
+    and `tuple`, `list`, `dict` are generated for corresponding objects to be plotted.
+
+    Args:
+        fields: Fields or Tensors to plot.
+        lib: Plotting library name or reference. Valid names are `'matplotlib'`, `'plotly'` and `'console'`.
+        row_dims: Batch dimensions along which sub-figures should be laid out vertically.
+            `Shape` or comma-separated names as `str`, `tuple` or `list`.
+        col_dims: Batch dimensions along which sub-figures should be laid out horizontally.
+            `Shape` or comma-separated names as `str`, `tuple` or `list`.
+        title: `str` for figures with a single subplot.
+            For subplots, pass a string `Tensor` matching the content dimensions, i.e. `row_dims` and `col_dims`.
+            Passing a `tuple`, `list` or `dict`, will create a tensor with these names internally.
+        size: Figure size in inches, `(width, height)`.
+        same_scale: Whether to use the same axis limits for all sub-figures.
+        log_dims: Dimensions for which the plot axes should be scaled logarithmically.
+            Can be given as a comma-separated `str`, a sequence of dimension names or a `Shape`.
+            Use `'_'` to scale unnamed axes logarithmically, e.g. the y-axis of scalar functions.
+        show_color_bar: Whether to display color bars for heat maps.
+        color: `Tensor` of line / marker colors.
+            The color can be specified either as a cycle index (int tensor) or as a hex code (str tensor).
+            The color of different lines and markers can vary.
+        alpha: Opacity as `float` or `Tensor`.
+            This affects all elements, not only line plots.
+            Opacity can vary between lines and markers.
+        err: Expected deviation from the value given in `fields`.
+            For supported plots, adds error bars of size *2·err*.
+            If the plotted data is the mean of some distribution, a good choice for `err` is the standard deviation along the mean dims.
+        animate: Time dimension to animate.
+            If not present in the data, will produce a regular plot instead.
+        overlay: Dimensions along which elements should be overlaid in the same subplot.
+            The default is only the `overlay` dimension which is created by `overlay()`.
+        frame_time: Interval between frames in the animation.
+        repeat: Whether the animation should loop.
+
+    Returns:
+        `Tensor` of figure objects.
+        The tensor contains those dimensions of `fields` that were not reduced by `row_dims`, `col_dims` or `animate`.
+        Currently, only single-figure plots are supported.
+
+        In case of an animation, a displayable animation object will be returned instead of a `Tensor`.
+    """
+    positioning = {}
+    indices: Dict[Tuple[int, int], List[dict]] = {}
+    nrows, ncols, fig_shape, reduced_shape = layout_sub_figures(layout(fields, batch('args')), row_dims, col_dims, animate, overlay, 0, 0, positioning, indices, {})
+    animate = fig_shape.only(animate)
+    fig_shape = fig_shape.without(animate)
+    plots = default_plots() if lib is None else get_plots(lib)
+    # --- Process arguments ---
+    if title is None:
+        title_by_subplot = {pos: title_label(common_index(*i, exclude=reduced_shape.singleton)) for pos, i in indices.items()}
+    elif isinstance(title, Tensor) and ('rows' in title.shape or 'cols' in title.shape):
+        title_by_subplot = {(row, col): title.rows[row].cols[col].native() for (row, col) in positioning}
+    else:
+        title = layout_pytree_node(title, wrap_leaf=True)
+        title_by_subplot = {pos: _title(title, i[0]) for pos, i in indices.items()}
+    log_dims = parse_dim_order(log_dims) or ()
+    color = layout_pytree_node(color, wrap_leaf=True)
+    alpha = layout_pytree_node(alpha, wrap_leaf=True)
+    err = layout_pytree_node(err, wrap_leaf=True)
+    if same_scale is True:
+        same_scale = '_'
+    elif same_scale is False or same_scale is None:
+        same_scale = ''
+    same_scale = parse_dim_order(same_scale)
+    if '_' in same_scale:
+        if any([f.values.dtype.kind == complex for l in positioning.values() for f in l]):
+            min_val = 0
+            max_val = max([float(abs(f.values).finite_max) for l in positioning.values() for f in l])
+        else:
+            min_val = min([float(f.values.finite_min) for l in positioning.values() for f in l])
+            max_val = max([float(f.values.finite_max) for l in positioning.values() for f in l])
+    else:
+        min_val = max_val = None
+    # --- Layout ---
+    subplots = {pos: _space(*fields, ignore_dims=animate) for pos, fields in positioning.items()}
+    subplots = {pos: _insert_value_dim(space, pos, subplots, min_val, max_val) for pos, space in subplots.items()}
+    if same_scale:
+        shared_lim: Box = share_axes(*subplots.values(), axes=same_scale)
+        subplots = {pos: replace_bounds(lim, shared_lim) for pos, lim in subplots.items()}
+    # --- animate or plot ---
+    if fig_shape.volume == 1:
+        figure, axes = plots.create_figure(size, nrows, ncols, subplots, title_by_subplot, log_dims)
+        if animate:
+            def plot_frame(frame: int):
+                for pos, fields in positioning.items():
+                    for i, f in enumerate(fields):
+                        idx = indices[pos][i]
+                        f = f[{animate.name: frame}]
+                        plots.plot(f, figure, axes[pos], subplots[pos], min_val, max_val, show_color_bar, color[idx], alpha[idx], err[idx])
+                plots.finalize(figure)
+            anim = plots.animate(figure, animate.size, plot_frame, frame_time, repeat)
+            LAST_FIGURE[0] = anim
+            plots.close(figure)
+            return anim
+        else:
+            for pos, fields in positioning.items():
+                for i, f in enumerate(fields):
+                    idx = indices[pos][i]
+                    err_ = err[idx]
+                    while isinstance(err_, Layout) and not err_.shape and isinstance(err_.native(), Tensor):
+                        err_ = err_.native()[idx]
+                    color_ = color[idx]
+                    while isinstance(color_, Layout) and not color_.shape and isinstance(color_.native(), Tensor):
+                        color_ = color_.native()[idx]
+                    plots.plot(f, figure, axes[pos], subplots[pos], min_val, max_val, show_color_bar, color_, alpha[idx], err_)
+            plots.finalize(figure)
+            LAST_FIGURE[0] = figure
+            return layout(figure)
+    else:
+        raise NotImplementedError(f"Figure batches not yet supported. Use rows and cols to reduce all batch dimensions. Not reduced. {fig_shape}")
+
+
+def layout_pytree_node(data, wrap_leaf=False):
+    if isinstance(data, tuple):
+        return layout(data, batch('tuple'))
+    elif isinstance(data, list):
+        return layout(data, batch('list'))
+    elif isinstance(data, dict):
+        return layout(data, batch('dict'))
+    return wrap(data) if wrap_leaf else data
+
+
+def layout_sub_figures(data: Union[Tensor, SampledField],
+                       row_dims: DimFilter,
+                       col_dims: DimFilter,
+                       animate: DimFilter,  # do not reduce these dims, has priority
+                       overlay: DimFilter,
+                       offset_row: int,
+                       offset_col: int,
+                       positioning: Dict[Tuple[int, int], List],
+                       indices: Dict[Tuple[int, int], List[dict]],
+                       base_index: Dict[str, Union[int, str]]) -> Tuple[int, int, Shape, Shape]:  # rows, cols
+    if data is None:
+        raise ValueError(f"Cannot layout figure for '{data}'")
+    data = layout_pytree_node(data, wrap_leaf=False)
+    if isinstance(data, Tensor) and data.dtype.kind == object:  # layout
+        rows, cols = 0, 0
+        non_reduced = math.EMPTY_SHAPE
+        dim0 = reduced = data.shape[0]
+        if dim0.only(overlay):
+            for overlay_index in dim0.only(overlay).meshgrid(names=True):  # overlay these fields
+                e_rows, e_cols, d_non_reduced, d_reduced = layout_sub_figures(data[overlay_index].native(), row_dims, col_dims, animate, overlay, offset_row, offset_col, positioning, indices, {**base_index, **overlay_index})
+                rows = max(rows, e_rows)
+                cols = max(cols, e_cols)
+                non_reduced &= d_non_reduced
+                reduced = merge_shapes(reduced, d_reduced, allow_varying_sizes=True)
+        elif dim0.only(animate):
+            data = math.stack(data.native(), dim0)
+            return layout_sub_figures(data, row_dims, col_dims, animate, overlay, offset_row, offset_col, positioning, indices, base_index)
+        else:
+            elements = data.unstack(dim0.name)
+            for item_name, e in zip(dim0.get_item_names(dim0.name) or range(dim0.size), elements):
+                index = dict(base_index, **{dim0.name: item_name})
+                if dim0.only(row_dims):
+                    e_rows, e_cols, e_non_reduced, e_reduced = layout_sub_figures(e.native(), row_dims, col_dims, animate, overlay, offset_row + rows, offset_col, positioning, indices, index)
+                    rows += e_rows
+                    cols = max(cols, e_cols)
+                elif dim0.only(col_dims):
+                    e_rows, e_cols, e_non_reduced, e_reduced = layout_sub_figures(e.native(), row_dims, col_dims, animate, overlay, offset_row, offset_col + cols, positioning, indices, index)
+                    cols += e_cols
+                    rows = max(rows, e_rows)
+                else:
+                    e_rows, e_cols, e_non_reduced, e_reduced = layout_sub_figures(e.native(), row_dims, col_dims, animate, overlay, offset_row, offset_col, positioning, indices, index)
+                    cols = max(cols, e_cols)
+                    rows = max(rows, e_rows)
+                non_reduced &= e_non_reduced
+                reduced = merge_shapes(reduced, e_reduced, allow_varying_sizes=True)
+        return rows, cols, non_reduced, reduced
+    else:
+        data = to_field(data)
+        overlay = data.shape.only(overlay)
+        animate = data.shape.only(animate).without(overlay)
+        row_shape = data.shape.only(row_dims).without(animate).without(overlay)
+        col_shape = data.shape.only(col_dims).without(row_dims).without(animate).without(overlay)
+        non_reduced: Shape = batch(data).without(row_dims).without(col_dims) & animate
+        for ri, r in enumerate(row_shape.meshgrid(names=True)):
+            for ci, c in enumerate(col_shape.meshgrid(names=True)):
+                for o in overlay.meshgrid(names=True):
+                    sub_data = data[r][c][o]
+                    positioning.setdefault((offset_row + ri, offset_col + ci), []).append(sub_data)
+                    indices.setdefault((offset_row + ri, offset_col + ci), []).append(dict(base_index, **r, **c, **o))
+        return row_shape.volume, col_shape.volume, non_reduced, EMPTY_SHAPE
+
+
+def _space(*values: Field or Tensor, ignore_dims: Shape) -> Box:
+    all_dims = []
+    for f in values:
+        for dim in f.bounds.vector.item_names:
+            if dim not in all_dims and dim not in ignore_dims:
+                all_dims.append(dim)
+    all_bounds = [embed(get_default_limits(f).without(ignore_dims.names).largest(shape), all_dims) for f in values]
+    bounds: Box = math.stack(all_bounds, batch('_fields'))
+    lower = math.finite_min(bounds.lower, bounds.shape.without('vector'), default=-math.INF)
+    upper = math.finite_max(bounds.upper, bounds.shape.without('vector'), default=math.INF)
+    return Box(lower, upper)
+
+
+def _insert_value_dim(space: Box, pos: Tuple[int, int], subplots: dict, min_val, max_val):
+    row, col = pos
+    axis = space.vector.item_names[0]
+    new_axis = Box(_=(min_val, max_val))
+    if space.vector.size <= 1:
+        for (r, c), other_space in subplots.items():
+            dims: tuple = other_space.vector.item_names
+            if r == row and axis in dims and len(dims) == 2 and dims.index(axis) == 1:
+                return concat([new_axis, space], 'vector')  # values along X
+        return concat([space, new_axis], 'vector')  # values along Y (standard)
+    else:
+        return space
+
+
+def overlay(*fields: Union[SampledField, Tensor]) -> Tensor:
+    """
+    Specify that multiple fields should be drawn on top of one another in the same figure.
+    The fields will be plotted in the order they are given, i.e. the last field on top.
+
+    >>> plot(vis.overlay(heatmap, points, velocity))
+
+    Args:
+        *fields: `SampledField` or `Tensor` instances
+
+    Returns:
+        Plottable object
+    """
+    return layout(fields, math.channel('overlay'))
+
+
+def write_image(path: str, figure=None, dpi=120., close=False):
+    """
+    Save a figure to an image file.
+
+    Args:
+        figure: Matplotlib or Plotly figure or text.
+        path: File path.
+        dpi: Pixels per inch.
+        close: Whether to close the figure after saving it.
+    """
+    figure = figure or LAST_FIGURE[0]
+    if figure is None:
+        figure = default_plots().current_figure
+    assert figure is not None, "No figure to save."
+    lib = get_plots_by_figure(figure)
+    path = os.path.expanduser(path)
+    lib.save(figure, path, dpi)
+    if close:
+        close_(figure=figure)
+
+
+def default_gui() -> Gui:
+    if GUI_OVERRIDES:
+        return GUI_OVERRIDES[-1]
+    if 'google.colab' in sys.modules or 'ipykernel' in sys.modules:
+        raise NotImplementedError("There is currently no GUI support for Python notebooks. Use `vis.plot()` to display plots or animations instead.")
+    else:
+        options = ['dash', 'console']
+    for option in options:
+        try:
+            return get_gui(option)
+        except ImportError as import_error:
+            warnings.warn(f"{option} user interface is unavailable because of missing dependency: {import_error}.", ImportWarning)
+    raise RuntimeError("No user interface available.")
+
+
+def get_gui(gui: Union[str, Gui]) -> Gui:
+    if GUI_OVERRIDES:
+        return GUI_OVERRIDES[-1]
+    if isinstance(gui, str):
+        if gui == 'dash':
+            from ._dash.dash_gui import DashGui
+            return DashGui()
+        elif gui == 'console':
+            from ._console import ConsoleGui
+            return ConsoleGui()
+        else:
+            raise NotImplementedError(f"No display available with name {gui}")
+    elif isinstance(gui, Gui):
+        return gui
+    else:
+        raise ValueError(gui)
+
+
+GUI_OVERRIDES = []
+
+
+@contextmanager
+def force_use_gui(gui: Gui):
+    GUI_OVERRIDES.append(gui)
+    try:
+        yield None
+    finally:
+        assert GUI_OVERRIDES.pop(-1) is gui
+
+
+_LOADED_PLOTTING_LIBRARIES: List[PlottingLibrary] = []
+
+
+def default_plots() -> PlottingLibrary:
+    if 'google.colab' in sys.modules or 'ipykernel' in sys.modules:
+        options = ['matplotlib']
+    else:
+        options = ['matplotlib', 'plotly', 'ascii']
+    for option in options:
+        try:
+            return get_plots(option)
+        except ImportError as import_error:
+            warnings.warn(f"{option} user interface is unavailable because of missing dependency: {import_error}.", ImportWarning)
+    raise RuntimeError("No user interface available.")
+
+
+def get_plots(lib: Union[str, PlottingLibrary]) -> PlottingLibrary:
+    if isinstance(lib, PlottingLibrary):
+        return lib
+    for loaded_lib in _LOADED_PLOTTING_LIBRARIES:
+        if loaded_lib.name == lib:
+            return loaded_lib
+    if lib == 'matplotlib':
+        from ._matplotlib._matplotlib_plots import MATPLOTLIB
+        _LOADED_PLOTTING_LIBRARIES.append(MATPLOTLIB)
+        return MATPLOTLIB
+    elif lib == 'plotly':
+        from ._dash._plotly_plots import PLOTLY
+        _LOADED_PLOTTING_LIBRARIES.append(PLOTLY)
+        return PLOTLY
+    elif lib == 'ascii':
+        from ._console._console_plot import CONSOLE
+        _LOADED_PLOTTING_LIBRARIES.append(CONSOLE)
+        return CONSOLE
+    else:
+        raise NotImplementedError(f"No plotting library available with name {lib}")
+
+
+def get_plots_by_figure(figure):
+    for loaded_lib in _LOADED_PLOTTING_LIBRARIES:
+        if loaded_lib.is_figure(figure):
+            return loaded_lib
+    else:
+        raise ValueError(f"No library found matching figure {figure} from list {_LOADED_PLOTTING_LIBRARIES}")
+
+
+def share_axes(*lims: Box, axes: Tuple[str]) -> Box or None:
+    lower = {}
+    upper = {}
+    for axis in axes:
+        if any(axis in box.vector.item_names for box in lims):
+            lower[axis] = math.min([box.lower.vector[axis] for box in lims if axis in box.vector.item_names], shape)
+            upper[axis] = math.max([box.upper.vector[axis] for box in lims if axis in box.vector.item_names], shape)
+    return Box(vec(**lower), vec(**upper)) if lower else None
+
+
+def replace_bounds(box: Box, replace: Box):
+    if replace is None:
+        return box
+    lower = {axis: replace.lower.vector[axis] if axis in replace.vector.item_names else box.lower.vector[axis] for axis in box.vector.item_names}
+    upper = {axis: replace.upper.vector[axis] if axis in replace.vector.item_names else box.upper.vector[axis] for axis in box.vector.item_names}
+    return Box(vec(**lower), vec(**upper))
+
+
+def _title(obj: Tensor, idx: dict):
+    obj = obj[idx]
+    while isinstance(obj, Layout) and not obj.shape and isinstance(obj.native(), Tensor):
+        obj = obj.native()[idx]
+    if not obj.shape:
+        return obj.native()
+    return ", ".join(obj)
```

### Comparing `phiflow-2.3.4/phi/vis/_vis_base.py` & `phiflow-2.4.0/phi/vis/_vis_base.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,507 +1,543 @@
-import threading
-import time
-from collections import namedtuple
-from math import log10
-from threading import Lock
-from typing import Tuple, Any, Optional, Dict, Callable, Union
-
-from phi import field, math
-from phi.field import SampledField, Scene, PointCloud, CenteredGrid
-from phi.field._field_math import data_bounds
-from phi.geom import Box, Cuboid
-from phi.math import Shape, EMPTY_SHAPE, Tensor, spatial, instance, wrap, channel
-
-Control = namedtuple('Control', [
-    'name',
-    'control_type',  # type (float, int, str, bool)
-    'initial',
-    'value_range',  # (lo, hi) or ("
-    'description',  # str
-    'kwargs'  # dict
-])
-
-Action = namedtuple('Action', ['name', 'description'])
-
-
-def value_range(control: Control) -> tuple:
-    if control.control_type == float:
-        if isinstance(control.value_range, tuple):
-            assert len(control.value_range) == 2, f"Tuple must be (min, max) but got length {len(control.value_range)}"
-            return control.value_range
-        log_scale = is_log_control(control)
-        if log_scale:
-            magn = log10(control.initial)
-            val_range = (10.0 ** (magn - 3.2), 10.0 ** (magn + 2.2))
-        else:
-            if control.initial == 0.0:
-                val_range = (-10.0, 10.0)
-            elif control.initial > 0:
-                val_range = (0., 4. * control.initial)
-            else:
-                val_range = (2. * control.initial, -2. * control.initial)
-    elif control.control_type == int:
-        if isinstance(control.value_range, tuple):
-            assert len(control.value_range) == 2, f"Tuple must be (min, max) but got length {len(control.value_range)}"
-            return control.value_range
-        if control.initial == 0:
-            val_range = (-10, 10)
-        elif control.initial > 0:
-            val_range = (0, 4 * control.initial)
-        else:
-            val_range = (2 * control.initial, -2 * control.initial)
-    elif control.control_type == bool:
-        assert control.value_range is None, "Specifying range for bool controls is not allowed."
-        return False, True
-    elif control.control_type == str:
-        if isinstance(control.value_range, tuple):
-            return "", control.value_range
-        return "", ""
-    else:
-        raise AssertionError(f"Not a numeric control: {control}")
-    return val_range
-
-
-def is_log_control(control: Control):
-    if control.control_type != float:
-        return False
-    log_scale = control.kwargs.get('log')
-    if log_scale is not None:
-        return log_scale
-    else:
-        if control.value_range is None:
-            return True
-        else:
-            if 0 in control.value_range:
-                return False
-            return control.value_range[1] / float(control.value_range[0]) > 10
-
-
-class VisModel:
-
-    def __init__(self, name: str = None, description: str = "", scene: Scene = None):
-        self.start_time = time.time()
-        """ Time of creation (`App` constructor invocation) """
-        self.name = name if name is not None else self.__class__.__name__
-        """ Human-readable name. """
-        self.description = description
-        """ Description to be displayed. """
-        self.scene = scene
-        """ Directory to which data and logging information should be written as `Scene` instance. """
-        self.uses_existing_scene = scene.exist_properties() if scene is not None else False
-        self.steps = 0
-        """ Counts the number of times `step()` has been called. May be set by the user. """
-        self.progress_lock = Lock()
-        self.pre_step = []  # callback(vis)
-        self.post_step = []  # callback(vis)
-        self.progress_available = []  # callback(vis)
-        self.progress_unavailable = []  # callback(vis)
-        self.growing_dims = ()  # tuple or list, used by GUI to determine whether to scroll to last element
-        self.message = None
-        self.log_file = None
-
-    def progress(self):
-        pass
-
-    @property
-    def is_progressing(self) -> bool:
-        return self.progress_lock.locked()
-
-    @property
-    def can_progress(self) -> bool:
-        raise NotImplementedError(self)
-
-    def prepare(self):
-        pass
-
-    @property
-    def field_names(self) -> tuple:
-        raise NotImplementedError(self)
-
-    def get_field(self, name: str, dim_selection: dict) -> SampledField:
-        """
-        Returns the current value of a field.
-        The name must be part of `VisModel.field_names`.
-
-        Raises:
-            `KeyError` if `field_name` is not a valid field.
-
-        Args:
-            name: Registered name of the field.
-            dim_selection: Slices the field according to `selection`. `dict` mapping dimension names to `int` or `slice`.
-
-        Returns:
-            `SampledField`
-        """
-        raise NotImplementedError(self)
-
-    def get_field_shape(self, name: str) -> Shape:
-        value = self.get_field(name, {})
-        if isinstance(value, (Tensor, SampledField)):
-            return value.shape
-        else:
-            return EMPTY_SHAPE
-
-    @property
-    def curve_names(self) -> tuple:
-        raise NotImplementedError(self)
-
-    def get_curve(self, name: str) -> tuple:
-        raise NotImplementedError(self)
-
-    @property
-    def controls(self) -> Tuple[Control]:
-        raise NotImplementedError(self)
-
-    def get_control_value(self, name):
-        raise NotImplementedError(self)
-
-    def set_control_value(self, name, value):
-        raise NotImplementedError(self)
-
-    @property
-    def actions(self) -> Tuple[Action]:
-        raise NotImplementedError(self)
-
-    def run_action(self, name):
-        raise NotImplementedError(self)
-
-    # Implemented methods
-
-    def _call(self, observers):
-        for obs in observers:
-            obs(self)
-
-
-def get_control_by_name(model: VisModel, control_name: str):
-    assert isinstance(control_name, str)
-    for control in model.controls:
-        if control.name == control_name:
-            return control
-    raise KeyError(f"No control with name '{control_name}'. Available controls: {model.controls}")
-
-
-def _step_and_wait(model: VisModel, framerate=None):
-    t = time.time()
-    model.progress()
-    if framerate is not None:
-        remaining_time = 1.0 / framerate - (time.time() - t)
-        if remaining_time > 0:
-            time.sleep(remaining_time)
-
-
-class AsyncPlay:
-
-    def __init__(self, model: VisModel, max_steps, framerate):
-        self.model = model
-        self.max_steps = max_steps
-        self.framerate = framerate
-        self.paused = False
-        self._finished = False
-
-    def start(self):
-        thread = threading.Thread(target=self, name='AsyncPlay')
-        thread.start()
-
-    def __call__(self):
-        step_count = 0
-        while not self.paused:
-            _step_and_wait(self.model, framerate=self.framerate)
-            step_count += 1
-            if self.max_steps and step_count >= self.max_steps:
-                break
-        self._finished = True
-
-    def pause(self):
-        self.paused = True
-
-    def __bool__(self):
-        return not self._finished
-
-    def __repr__(self):
-        return status_message(self.model, self)
-
-
-def status_message(model: VisModel, play_status: Union[AsyncPlay, None]):
-    pausing = "/Pausing" if (play_status and play_status.paused) else ""
-    current_action = "Running" if model.is_progressing else "Waiting"
-    action = current_action if play_status else "Idle"
-    message = f" - {model.message}" if model.message else ""
-    return f"{action}{pausing} ({model.steps} steps){message}"
-
-
-def play_async(model: VisModel, max_steps=None, framerate=None):
-    """
-    Run a number of steps.
-
-    Args:
-        model: Model to progress
-        max_steps: (optional) stop when this many steps have been completed (independent of the `steps` variable) or `pause()` is called.
-        framerate: Target frame rate in Hz.
-    """
-    assert model.can_progress
-    play = AsyncPlay(model, max_steps, framerate)
-    play.start()
-    return play
-
-
-def benchmark(model: VisModel, sequence_count):
-    # self._pause = False  # TODO allow premature stop
-    step_count = 0
-    t = time.time()
-    for i in range(sequence_count):
-        model.progress()
-        step_count += 1
-        # if self._pause:
-        #     break
-    time_elapsed = time.time() - t
-    return step_count, time_elapsed
-
-
-class Gui:
-
-    def __init__(self, asynchronous=False):
-        """
-        Creates a display for the given vis and initializes the configuration.
-        This method does not set up the display. It only sets up the Gui object and returns as quickly as possible.
-        """
-        self.app: Optional[VisModel] = None
-        self.asynchronous = asynchronous
-        self.config = {}
-
-    def configure(self, config: dict):
-        """
-        Updates the GUI configuration.
-        This method may only be called while the GUI is not yet visible, i.e. before show() is called.
-
-        Args:
-            config: Complete or partial GUI-specific configuration. dictionary mapping from strings to JSON serializable values
-        """
-        self.config.update(config)
-
-    def get_configuration(self) -> dict:
-        """
-        Returns the current configuration of the GUI.
-        The returned dictionary may only contain serializable values and all keys must be strings.
-        The configuration can be passed to another instance of this class using set_configuration().
-        """
-        return self.config
-
-    def setup(self, app: VisModel):
-        """
-        Sets up all necessary GUI components.
-        
-        The GUI can register callbacks with the vis to be informed about vis-state changes induced externally.
-        The vis can be assumed to be prepared when this method is called.
-        
-        This method is called after set_configuration() but before show()
-
-        Args:
-          app: vis to be displayed, may not be prepared or be otherwise invalid at this point.
-        """
-        self.app = app
-
-    def show(self, caller_is_main: bool):
-        """
-        Displays the previously setup GUI.
-        This method is blocking and returns only when the GUI is hidden.
-
-        This method will always be called after setup().
-
-        Args:
-            caller_is_main: True if the calling script is the __main__ module.
-        """
-        pass
-
-    def auto_play(self):
-        """
-        Called if `autorun=True`.
-        If no Gui is specified, `App.run()` is called instead.
-        """
-        raise NotImplementedError(self)
-
-
-class PlottingLibrary:
-
-    def __init__(self, name: str, figure_classes: Union[tuple, list]):
-        self.name = name
-        self.figure_classes = tuple(figure_classes)
-        self.current_figure = None
-        self.recipes = []
-
-    def __repr__(self):
-        return self.name
-
-    def is_figure(self, obj):
-        if isinstance(obj, (tuple, list)):
-            return isinstance(obj[0], self.figure_classes)
-        return isinstance(obj, self.figure_classes)
-
-    def create_figure(self,
-                      size: tuple,
-                      rows: int,
-                      cols: int,
-                      spaces: Dict[Tuple[int, int], Box],
-                      titles: Dict[Tuple[int, int], str],
-                      log_dims: Tuple[str, ...]) -> Tuple[Any, Dict[Tuple[int, int], Any]]:
-        """
-        Args:
-            size: Figure size in inches.
-            rows: Number of sub-figures laid out vertically.
-            cols: Number of sub-figures laid out horizontally.
-            spaces: Axes and range per sub-plot: `(x,y) -> Box`. Only subplot locations contained as keys should be plotted.
-                To indicate automatic limit, the box will have a lower or upper limit of -inf or inf, respectively.
-            titles: Subplot titles.
-            log_dims: Dimensions along which axes should be log-scaled
-
-        Returns:
-            figure: Native figure object
-            subfigures: Native sub-figures by subplot location.
-        """
-        raise NotImplementedError
-
-    def animate(self, fig, frames: int, plot_frame_function: Callable, interval: float, repeat: bool):
-        raise NotImplementedError
-
-    def finalize(self, figure):
-        raise NotImplementedError
-
-    def close(self, figure):
-        raise NotImplementedError
-
-    def show(self, figure):
-        raise NotImplementedError
-
-    def save(self, figure, path: str, dpi: float):
-        raise NotImplementedError
-
-    def plot(self, data, figure, subplot, space, *args, **kwargs):
-        for recipe in self.recipes:
-            if recipe.can_plot(data, space):
-                recipe.plot(data, figure, subplot, space, *args, **kwargs)
-                return
-        raise NotImplementedError(f"No {self.name} recipe found for {data}. Recipes: {self.recipes}")
-
-
-class Recipe:
-
-    def can_plot(self, data: SampledField, space: Box) -> bool:
-        raise NotImplementedError
-
-    def plot(self,
-             data: SampledField,
-             figure,
-             subplot,
-             space: Box,
-             min_val: float,
-             max_val: float,
-             show_color_bar: bool,
-             color: Tensor,
-             alpha: Tensor):
-        raise NotImplementedError
-
-    def __repr__(self):
-        return self.__class__.__name__
-
-
-class GuiInterrupt(KeyboardInterrupt):
-    pass
-
-
-def gui_interrupt(*args, **kwargs):
-    raise GuiInterrupt()
-
-
-def display_name(python_name: Any):
-    if isinstance(python_name, (int, bool)):
-        return str(python_name)
-    n = list(python_name)
-    n[0] = n[0].upper()
-    for i in range(1, len(n)):
-        if n[i] == "_":
-            n[i] = " "
-            if len(n) > i + 1:
-                n[i + 1] = n[i + 1].upper()
-    text = "".join(n)
-    if "Reset" in text:
-        return f"⏮ {text}"
-    else:
-        return text
-
-
-def index_label(idx: dict) -> Union[str, None]:
-    if len(idx) == 0:
-        return None
-    elif len(idx) == 1:
-        return display_name(next(iter(idx.values())))
-    else:
-        number_unlabelled_dims = len([1 for k, v in idx.items() if isinstance(v, int)])
-        if number_unlabelled_dims <= 1:
-            return " ".join([display_name(n) for n in idx.values()])
-        else:
-            return ", ".join(f'{k}={display_name(v)}' for k, v in idx.items())
-
-
-def title_label(idx: dict):
-    idx = {k: v for k, v in idx.items() if k not in ['tuple', 'list', 'dict', 'args'] or isinstance(v, str)}
-    if len(idx) == 0:
-        return None
-    elif len(idx) == 1:
-        for name, value in idx.items():
-            if isinstance(value, int):
-                return f"{display_name(name)} {display_name(value)}"
-            else:
-                return display_name(value)
-    else:
-        return index_label(idx)
-
-
-
-def common_index(*indices: dict, exclude=()):
-    return {k: v for k, v in indices[0].items() if k not in exclude and all(i[k] == v for i in indices)}
-
-
-def select_channel(value: Union[SampledField, Tensor, tuple, list], channel: Union[str, None]):
-    if isinstance(value, (tuple, list)):
-        return [select_channel(v, channel) for v in value]
-    if channel is None:
-        return value
-    elif channel == 'abs':
-        if value.vector.exists:
-            return field.vec_abs(value) if isinstance(value, SampledField) else math.vec_length(value)
-        else:
-            return value
-    else:  # x, y, z
-        if channel in value.shape.spatial and 'vector' in value.shape:
-            return value.vector[channel]
-        elif 'vector' in value.shape:
-            raise ValueError(
-                f"No {channel} component present. Available dimensions: {', '.join(value.shape.spatial.names)}")
-        else:
-            return value
-
-
-def tensor_as_field(t: Tensor):
-    """
-    Interpret a `Tensor` as a `CenteredGrid` or `PointCloud` depending on its dimensions.
-
-    Unlike the `CenteredGrid` constructor, this function will have the values sampled at integer points for each spatial dimension.
-
-    Args:
-        t: `Tensor` with either `spatial` or `instance` dimensions.
-
-    Returns:
-        `CenteredGrid` or `PointCloud`
-    """
-    arbitrary_lines_1d = spatial(t).rank == 1 and 'vector' in t.shape
-    if instance(t) or arbitrary_lines_1d or arbitrary_lines_1d:
-        bounds = data_bounds(t)
-        extended_bounds = Cuboid(bounds.center, bounds.half_size * 1.2).box()
-        lower = math.where(extended_bounds.lower * bounds.lower <= 0, bounds.lower * .9, extended_bounds.lower)
-        upper = math.where(extended_bounds.upper * bounds.upper <= 0, bounds.lower * .9, extended_bounds.upper)
-        return PointCloud(t, bounds=Box(lower, upper))
-    elif spatial(t):
-        return CenteredGrid(t, 0, bounds=Box(math.const_vec(-0.5, spatial(t)), wrap(spatial(t), channel('vector')) - 0.5))
-    elif 'vector' in t.shape:
-        return PointCloud(math.expand(t, instance(points=1)), bounds=Cuboid(t, half_size=math.const_vec(1, t.shape['vector'])).box())
-    else:
-        raise ValueError(f"Cannot create field from tensor with shape {t.shape}. Requires at least one spatial, instance or vector dimension.")
+import threading
+import time
+from collections import namedtuple
+from math import log10
+from threading import Lock
+from typing import Tuple, Any, Optional, Dict, Callable, Union
+
+from phi import field, math
+from phi.field import SampledField, Scene, PointCloud, CenteredGrid
+from phi.field._field_math import data_bounds
+from phi.geom import Box, Cuboid, Geometry, Point
+from phi.math import Shape, EMPTY_SHAPE, Tensor, spatial, instance, wrap, channel, expand, non_batch
+
+Control = namedtuple('Control', [
+    'name',
+    'control_type',  # type (float, int, str, bool)
+    'initial',
+    'value_range',  # (lo, hi) or ("
+    'description',  # str
+    'kwargs'  # dict
+])
+
+Action = namedtuple('Action', ['name', 'description'])
+
+
+def value_range(control: Control) -> tuple:
+    if control.control_type == float:
+        if isinstance(control.value_range, tuple):
+            assert len(control.value_range) == 2, f"Tuple must be (min, max) but got length {len(control.value_range)}"
+            return control.value_range
+        log_scale = is_log_control(control)
+        if log_scale:
+            magn = log10(control.initial)
+            val_range = (10.0 ** (magn - 3.2), 10.0 ** (magn + 2.2))
+        else:
+            if control.initial == 0.0:
+                val_range = (-10.0, 10.0)
+            elif control.initial > 0:
+                val_range = (0., 4. * control.initial)
+            else:
+                val_range = (2. * control.initial, -2. * control.initial)
+    elif control.control_type == int:
+        if isinstance(control.value_range, tuple):
+            assert len(control.value_range) == 2, f"Tuple must be (min, max) but got length {len(control.value_range)}"
+            return control.value_range
+        if control.initial == 0:
+            val_range = (-10, 10)
+        elif control.initial > 0:
+            val_range = (0, 4 * control.initial)
+        else:
+            val_range = (2 * control.initial, -2 * control.initial)
+    elif control.control_type == bool:
+        assert control.value_range is None, "Specifying range for bool controls is not allowed."
+        return False, True
+    elif control.control_type == str:
+        if isinstance(control.value_range, tuple):
+            return "", control.value_range
+        return "", ""
+    else:
+        raise AssertionError(f"Not a numeric control: {control}")
+    return val_range
+
+
+def is_log_control(control: Control):
+    if control.control_type != float:
+        return False
+    log_scale = control.kwargs.get('log')
+    if log_scale is not None:
+        return log_scale
+    else:
+        if control.value_range is None:
+            return True
+        else:
+            if 0 in control.value_range:
+                return False
+            return control.value_range[1] / float(control.value_range[0]) > 10
+
+
+class VisModel:
+
+    def __init__(self, name: str = None, description: str = "", scene: Scene = None):
+        self.start_time = time.time()
+        """ Time of creation (`App` constructor invocation) """
+        self.name = name if name is not None else self.__class__.__name__
+        """ Human-readable name. """
+        self.description = description
+        """ Description to be displayed. """
+        self.scene = scene
+        """ Directory to which data and logging information should be written as `Scene` instance. """
+        self.uses_existing_scene = scene.exist_properties() if scene is not None else False
+        self.steps = 0
+        """ Counts the number of times `step()` has been called. May be set by the user. """
+        self.progress_lock = Lock()
+        self.pre_step = []  # callback(vis)
+        self.post_step = []  # callback(vis)
+        self.progress_available = []  # callback(vis)
+        self.progress_unavailable = []  # callback(vis)
+        self.growing_dims = ()  # tuple or list, used by GUI to determine whether to scroll to last element
+        self.message = None
+        self.log_file = None
+
+    def progress(self):
+        pass
+
+    @property
+    def is_progressing(self) -> bool:
+        return self.progress_lock.locked()
+
+    @property
+    def can_progress(self) -> bool:
+        raise NotImplementedError(self)
+
+    def prepare(self):
+        pass
+
+    @property
+    def field_names(self) -> tuple:
+        raise NotImplementedError(self)
+
+    def get_field(self, name: str, dim_selection: dict) -> SampledField:
+        """
+        Returns the current value of a field.
+        The name must be part of `VisModel.field_names`.
+
+        Raises:
+            `KeyError` if `field_name` is not a valid field.
+
+        Args:
+            name: Registered name of the field.
+            dim_selection: Slices the field according to `selection`. `dict` mapping dimension names to `int` or `slice`.
+
+        Returns:
+            `SampledField`
+        """
+        raise NotImplementedError(self)
+
+    def get_field_shape(self, name: str) -> Shape:
+        value = self.get_field(name, {})
+        if isinstance(value, (Tensor, SampledField)):
+            return value.shape
+        else:
+            return EMPTY_SHAPE
+
+    @property
+    def curve_names(self) -> tuple:
+        raise NotImplementedError(self)
+
+    def get_curve(self, name: str) -> tuple:
+        raise NotImplementedError(self)
+
+    @property
+    def controls(self) -> Tuple[Control]:
+        raise NotImplementedError(self)
+
+    def get_control_value(self, name):
+        raise NotImplementedError(self)
+
+    def set_control_value(self, name, value):
+        raise NotImplementedError(self)
+
+    @property
+    def actions(self) -> Tuple[Action]:
+        raise NotImplementedError(self)
+
+    def run_action(self, name):
+        raise NotImplementedError(self)
+
+    # Implemented methods
+
+    def _call(self, observers):
+        for obs in observers:
+            obs(self)
+
+
+def get_control_by_name(model: VisModel, control_name: str):
+    assert isinstance(control_name, str)
+    for control in model.controls:
+        if control.name == control_name:
+            return control
+    raise KeyError(f"No control with name '{control_name}'. Available controls: {model.controls}")
+
+
+def _step_and_wait(model: VisModel, framerate=None):
+    t = time.time()
+    model.progress()
+    if framerate is not None:
+        remaining_time = 1.0 / framerate - (time.time() - t)
+        if remaining_time > 0:
+            time.sleep(remaining_time)
+
+
+class AsyncPlay:
+
+    def __init__(self, model: VisModel, max_steps, framerate):
+        self.model = model
+        self.max_steps = max_steps
+        self.framerate = framerate
+        self.paused = False
+        self._finished = False
+
+    def start(self):
+        thread = threading.Thread(target=self, name='AsyncPlay')
+        thread.start()
+
+    def __call__(self):
+        step_count = 0
+        while not self.paused:
+            _step_and_wait(self.model, framerate=self.framerate)
+            step_count += 1
+            if self.max_steps and step_count >= self.max_steps:
+                break
+        self._finished = True
+
+    def pause(self):
+        self.paused = True
+
+    def __bool__(self):
+        return not self._finished
+
+    def __repr__(self):
+        return status_message(self.model, self)
+
+
+def status_message(model: VisModel, play_status: Union[AsyncPlay, None]):
+    pausing = "/Pausing" if (play_status and play_status.paused) else ""
+    current_action = "Running" if model.is_progressing else "Waiting"
+    action = current_action if play_status else "Idle"
+    message = f" - {model.message}" if model.message else ""
+    return f"{action}{pausing} ({model.steps} steps){message}"
+
+
+def play_async(model: VisModel, max_steps=None, framerate=None):
+    """
+    Run a number of steps.
+
+    Args:
+        model: Model to progress
+        max_steps: (optional) stop when this many steps have been completed (independent of the `steps` variable) or `pause()` is called.
+        framerate: Target frame rate in Hz.
+    """
+    assert model.can_progress
+    play = AsyncPlay(model, max_steps, framerate)
+    play.start()
+    return play
+
+
+def benchmark(model: VisModel, sequence_count):
+    # self._pause = False  # TODO allow premature stop
+    step_count = 0
+    t = time.time()
+    for i in range(sequence_count):
+        model.progress()
+        step_count += 1
+        # if self._pause:
+        #     break
+    time_elapsed = time.time() - t
+    return step_count, time_elapsed
+
+
+class Gui:
+
+    def __init__(self, asynchronous=False):
+        """
+        Creates a display for the given vis and initializes the configuration.
+        This method does not set up the display. It only sets up the Gui object and returns as quickly as possible.
+        """
+        self.app: Optional[VisModel] = None
+        self.asynchronous = asynchronous
+        self.config = {}
+
+    def configure(self, config: dict):
+        """
+        Updates the GUI configuration.
+        This method may only be called while the GUI is not yet visible, i.e. before show() is called.
+
+        Args:
+            config: Complete or partial GUI-specific configuration. dictionary mapping from strings to JSON serializable values
+        """
+        self.config.update(config)
+
+    def get_configuration(self) -> dict:
+        """
+        Returns the current configuration of the GUI.
+        The returned dictionary may only contain serializable values and all keys must be strings.
+        The configuration can be passed to another instance of this class using set_configuration().
+        """
+        return self.config
+
+    def setup(self, app: VisModel):
+        """
+        Sets up all necessary GUI components.
+        
+        The GUI can register callbacks with the vis to be informed about vis-state changes induced externally.
+        The vis can be assumed to be prepared when this method is called.
+        
+        This method is called after set_configuration() but before show()
+
+        Args:
+          app: vis to be displayed, may not be prepared or be otherwise invalid at this point.
+        """
+        self.app = app
+
+    def show(self, caller_is_main: bool):
+        """
+        Displays the previously setup GUI.
+        This method is blocking and returns only when the GUI is hidden.
+
+        This method will always be called after setup().
+
+        Args:
+            caller_is_main: True if the calling script is the __main__ module.
+        """
+        pass
+
+    def auto_play(self):
+        """
+        Called if `autorun=True`.
+        If no Gui is specified, `App.run()` is called instead.
+        """
+        raise NotImplementedError(self)
+
+
+class PlottingLibrary:
+
+    def __init__(self, name: str, figure_classes: Union[tuple, list]):
+        self.name = name
+        self.figure_classes = tuple(figure_classes)
+        self.current_figure = None
+        self.recipes = []
+
+    def __repr__(self):
+        return self.name
+
+    def is_figure(self, obj):
+        if isinstance(obj, (tuple, list)):
+            return isinstance(obj[0], self.figure_classes)
+        return isinstance(obj, self.figure_classes)
+
+    def create_figure(self,
+                      size: tuple,
+                      rows: int,
+                      cols: int,
+                      spaces: Dict[Tuple[int, int], Box],
+                      titles: Dict[Tuple[int, int], str],
+                      log_dims: Tuple[str, ...]) -> Tuple[Any, Dict[Tuple[int, int], Any]]:
+        """
+        Args:
+            size: Figure size in inches.
+            rows: Number of sub-figures laid out vertically.
+            cols: Number of sub-figures laid out horizontally.
+            spaces: Axes and range per sub-plot: `(x,y) -> Box`. Only subplot locations contained as keys should be plotted.
+                To indicate automatic limit, the box will have a lower or upper limit of -inf or inf, respectively.
+            titles: Subplot titles.
+            log_dims: Dimensions along which axes should be log-scaled
+
+        Returns:
+            figure: Native figure object
+            subfigures: Native sub-figures by subplot location.
+        """
+        raise NotImplementedError
+
+    def animate(self, fig, frames: int, plot_frame_function: Callable, interval: float, repeat: bool):
+        raise NotImplementedError
+
+    def finalize(self, figure):
+        raise NotImplementedError
+
+    def close(self, figure):
+        raise NotImplementedError
+
+    def show(self, figure):
+        raise NotImplementedError
+
+    def save(self, figure, path: str, dpi: float):
+        raise NotImplementedError
+
+    def plot(self, data, figure, subplot, space, *args, **kwargs):
+        for recipe in self.recipes:
+            if recipe.can_plot(data, space):
+                recipe.plot(data, figure, subplot, space, *args, **kwargs)
+                return
+        raise NotImplementedError(f"No {self.name} recipe found for {data}. Recipes: {self.recipes}")
+
+
+class Recipe:
+
+    def can_plot(self, data: SampledField, space: Box) -> bool:
+        raise NotImplementedError
+
+    def plot(self,
+             data: SampledField,
+             figure,
+             subplot,
+             space: Box,
+             min_val: float,
+             max_val: float,
+             show_color_bar: bool,
+             color: Tensor,
+             alpha: Tensor,
+             err: Tensor):
+        raise NotImplementedError
+
+    def __repr__(self):
+        return self.__class__.__name__
+
+
+class GuiInterrupt(KeyboardInterrupt):
+    pass
+
+
+def gui_interrupt(*args, **kwargs):
+    raise GuiInterrupt()
+
+
+def display_name(python_name: Any):
+    if isinstance(python_name, (int, bool)):
+        return str(python_name)
+    assert isinstance(python_name, str), f"name must be a str, int or bool but got {type(python_name)}"
+    if python_name == '_':
+        return ""
+    n = list(python_name)
+    n[0] = n[0].upper()
+    for i in range(1, len(n)):
+        if n[i] == "_":
+            n[i] = " "
+            if len(n) > i + 1:
+                n[i + 1] = n[i + 1].upper()
+    text = "".join(n)
+    if "Reset" in text:
+        return f"⏮ {text}"
+    else:
+        return text
+
+
+def index_label(idx: dict, always_include_names: bool = False) -> Union[str, None]:
+    if len(idx) == 0:
+        return None
+    if len(idx) == 1:
+        if always_include_names:
+            for name, value in idx.items():
+                return f"{display_name(name)} {display_name(value)}"
+        else:
+            return display_name(next(iter(idx.values())))
+    else:
+        number_unlabelled_dims = len([1 for k, v in idx.items() if isinstance(v, int)])
+        if number_unlabelled_dims <= 1:
+            return " ".join([display_name(n) for n in idx.values()])
+        else:
+            return ", ".join(f'{k}={display_name(v)}' for k, v in idx.items())
+
+
+def title_label(idx: dict):
+    idx = {k: v for k, v in idx.items() if k not in ['tuple', 'list', 'dict', 'args'] or isinstance(v, str)}
+    if len(idx) == 0:
+        return None
+    elif len(idx) == 1:
+        for name, value in idx.items():
+            if isinstance(value, int):
+                return f"{display_name(name)} {display_name(value)}"
+            else:
+                return display_name(value)
+    else:
+        return index_label(idx)
+
+
+
+def common_index(*indices: dict, exclude=()):
+    return {k: v for k, v in indices[0].items() if k not in exclude and all([i[k] == v for i in indices])}
+
+
+def select_channel(value: Union[SampledField, Tensor, tuple, list], channel: Union[str, None]):
+    if isinstance(value, (tuple, list)):
+        return [select_channel(v, channel) for v in value]
+    if channel is None:
+        return value
+    elif channel == 'abs':
+        if value.vector.exists:
+            return field.vec_abs(value) if isinstance(value, SampledField) else math.vec_length(value)
+        else:
+            return value
+    else:  # x, y, z
+        if channel in value.shape.spatial and 'vector' in value.shape:
+            return value.vector[channel]
+        elif 'vector' in value.shape:
+            raise ValueError(
+                f"No {channel} component present. Available dimensions: {', '.join(value.shape.spatial.names)}")
+        else:
+            return value
+
+
+def to_field(obj):
+    if isinstance(obj, SampledField):
+        return obj
+    if isinstance(obj, Geometry):
+        return PointCloud(obj)
+    if isinstance(obj, Tensor):
+        arbitrary_lines_1d = spatial(obj).rank == 1 and 'vector' in obj.shape
+        point_cloud = instance(obj) and 'vector' in obj.shape
+        if point_cloud or arbitrary_lines_1d:
+            return PointCloud(obj)
+        elif spatial(obj):
+            return CenteredGrid(obj, 0, bounds=Box(math.const_vec(-0.5, spatial(obj)), wrap(spatial(obj), channel('vector')) - 0.5))
+        elif 'vector' in obj.shape:
+            return PointCloud(math.expand(obj, instance(points=1)), bounds=Cuboid(obj, half_size=math.const_vec(1e-3, obj.shape['vector'])).box())
+        elif instance(obj) and not spatial(obj):
+            assert instance(obj).rank == 1, "Bar charts must have only one instance dimension"
+            vector = channel(vector=instance(obj).names)
+            equal_spacing = math.range_tensor(instance(obj), vector)
+            lower = expand(-.5, vector)
+            upper = expand(equal_spacing.max + .5, vector)
+            return PointCloud(equal_spacing, values=obj, bounds=Box(lower, upper))
+            # positions = math.layout(instance(obj).item_names, instance(obj))
+            # positions = expand(positions, vector)
+            # return PointCloud(positions, values=obj)
+    raise ValueError(f"Cannot plot {obj}. Tensors, geometries and fields can be plotted.")
+
+
+def get_default_limits(f: SampledField) -> Box:
+    if f._bounds is not None:
+        return f.bounds
+    # --- Determine element size ---
+    if (f.elements.bounding_half_extent() > 0).any:
+        size = 2 * f.elements.bounding_half_extent()
+    elif isinstance(f, PointCloud) and f.spatial_rank == 1:
+        bounds = f.bounds
+        count = non_batch(f).non_dual.non_channel.volume
+        return Box(bounds.lower - bounds.size / count / 2, bounds.upper + bounds.size / count / 2)
+    # elif instance(f) and f.spatial_rank == 1:
+    #     lower = expand(-.5, vector)
+    #     upper = expand(equal_spacing.max + .5, vector)
+    #     size =
+    else:
+        size = expand(0, f.elements.shape['vector'])
+    if (size == 0).all:
+        size = math.const_vec(.1, f.elements.shape['vector'])
+    bounds = data_bounds(f.elements.center).largest(channel)
+    extended_bounds = Cuboid(bounds.center, bounds.half_size + size * 0.6)
+    extended_bounds = Box(math.min(extended_bounds.lower, size.shape.without('vector')), math.max(extended_bounds.upper, size.shape.without('vector')))
+    if isinstance(f.elements, Point):
+        lower = math.where(extended_bounds.lower * bounds.lower < 0, bounds.lower * .9, extended_bounds.lower)
+        upper = math.where(extended_bounds.upper * bounds.upper < 0, bounds.lower * .9, extended_bounds.upper)
+        extended_bounds = Box(lower, upper)
+    return extended_bounds
```

### Comparing `phiflow-2.3.4/phiflow.egg-info/PKG-INFO` & `phiflow-2.4.0/phiflow.egg-info/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 Metadata-Version: 2.1
 Name: phiflow
-Version: 2.3.4
+Version: 2.4.0
 Summary: Differentiable PDE solving framework for machine learning
 Home-page: https://github.com/tum-pbs/PhiFlow
 Author: Philipp Holl
 Author-email: philipp.holl@tum.de
 License: MIT
-Download-URL: https://github.com/tum-pbs/PhiFlow/archive/2.3.4.tar.gz
+Download-URL: https://github.com/tum-pbs/PhiFlow/archive/2.4.0.tar.gz
 Description: # PhiFlow
         
         [**Homepage**](https://github.com/tum-pbs/PhiFlow)
         &nbsp;&nbsp;&nbsp; [**Documentation**](https://tum-pbs.github.io/PhiFlow/)
         &nbsp;&nbsp;&nbsp; [**API**](https://tum-pbs.github.io/PhiFlow/phi)
         &nbsp;&nbsp;&nbsp; [**Demos**](https://github.com/tum-pbs/PhiFlow/tree/master/demos)
         &nbsp;&nbsp;&nbsp; [<img src="https://www.tensorflow.org/images/colab_logo_32px.png" height=16> **Fluids Tutorial**](https://colab.research.google.com/github/tum-pbs/PhiFlow/blob/develop/docs/Fluids_Tutorial.ipynb#offline=true&sandboxMode=true)
```

### Comparing `phiflow-2.3.4/phiflow.egg-info/SOURCES.txt` & `phiflow-2.4.0/phiflow.egg-info/SOURCES.txt`

 * *Files 14% similar despite different names*

```diff
@@ -10,20 +10,22 @@
 phi/field/_angular_velocity.py
 phi/field/_embed.py
 phi/field/_field.py
 phi/field/_field_io.py
 phi/field/_field_math.py
 phi/field/_grid.py
 phi/field/_mask.py
+phi/field/_mesh.py
 phi/field/_noise.py
 phi/field/_point_cloud.py
 phi/field/_scene.py
 phi/geom/__init__.py
 phi/geom/_box.py
 phi/geom/_geom.py
+phi/geom/_poly_surface.py
 phi/geom/_sphere.py
 phi/geom/_stack.py
 phi/geom/_transform.py
 phi/geom/_union.py
 phi/jax/__init__.py
 phi/jax/_jax_backend.py
 phi/jax/flow.py
@@ -45,14 +47,17 @@
 phi/math/magic.py
 phi/math/backend/__init__.py
 phi/math/backend/_backend.py
 phi/math/backend/_dtype.py
 phi/math/backend/_linalg.py
 phi/math/backend/_minimize.py
 phi/math/backend/_numpy_backend.py
+phi/math/backend/_object.py
+phi/math/backend/_partition.py
+phi/math/backend/_partition_draft.py
 phi/math/backend/_profile.py
 phi/physics/__init__.py
 phi/physics/_boundaries.py
 phi/physics/advect.py
 phi/physics/diffuse.py
 phi/physics/fluid.py
 phi/tf/__init__.py
@@ -63,14 +68,15 @@
 phi/tf/flow.py
 phi/tf/nets.py
 phi/torch/__init__.py
 phi/torch/_torch_backend.py
 phi/torch/flow.py
 phi/torch/nets.py
 phi/vis/__init__.py
+phi/vis/_io.py
 phi/vis/_log.py
 phi/vis/_plot_util.py
 phi/vis/_user_namespace.py
 phi/vis/_viewer.py
 phi/vis/_vis.py
 phi/vis/_vis_base.py
 phi/vis/_console/__init__.py
```

### Comparing `phiflow-2.3.4/setup.py` & `phiflow-2.4.0/setup.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,189 +1,190 @@
-import distutils.cmd
-import distutils.log
-import subprocess
-import os
-import sys
-from os.path import join, isfile, abspath, isdir, dirname
-from setuptools import setup
-
-
-def check_tf_cuda_compatibility():
-    import tensorflow
-    build = tensorflow.sysconfig.get_build_info()  # is_rocm_build, cuda_compute_capabilities
-    tf_gcc = build['cpu_compiler']
-    is_cuda_build = build['is_cuda_build']
-    print(f"TensorFlow compiler: {tf_gcc}.")
-    if not is_cuda_build:
-        raise AssertionError("Your TensorFlow build does not support CUDA.")
-    else:
-        cuda_version = build['cuda_version']
-        cudnn_version = build['cudnn_version']
-        print(f"TensorFlow was compiled against CUDA {cuda_version} and cuDNN {cudnn_version}.")
-        return tf_gcc
-
-
-def compile_cuda(file_names, nvcc, source_dir, target_dir, logfile):
-    import tensorflow
-    tf_cflags = tensorflow.sysconfig.get_compile_flags()
-    command = [
-            nvcc,
-            join(source_dir, f'{file_names}.cu.cc'),
-            '-o', join(target_dir, f'{file_names}.cu.o'),
-            '-std=c++11',
-            '-c',
-            '-D GOOGLE_CUDA=1',
-            '-x', 'cu',
-            '-Xcompiler',
-            '-fPIC',
-            '--expt-relaxed-constexpr',
-            '-DNDEBUG',
-            '-O3'
-        ] + tf_cflags
-    print(f"nvcc {file_names}")
-    logfile.writelines(["\n", " ".join(command), "\n"])
-    subprocess.check_call(command, stdout=logfile, stderr=logfile)
-
-
-def compile_gcc(file_names, gcc, source_dir, target_dir, cuda_lib, logfile):
-    import tensorflow
-    from packaging import version
-    if version.parse(tensorflow.__version__) >= version.parse('2.5.0'):
-        cpp_version, gcc_version = '14', '7.5'
-    else:
-        cpp_version, gcc_version = '11', '4.8'
-    tf_cflags = tensorflow.sysconfig.get_compile_flags()
-    tf_lflags = tensorflow.sysconfig.get_link_flags()
-    link_cuda_lib = '-L' + cuda_lib
-    command = [
-                gcc,
-                join(source_dir, f'{file_names}.cc'),
-                join(target_dir, f'{file_names}.cu.o'),
-                '-o', join(target_dir, f'{file_names}.so'),
-                f'-std=c++{cpp_version}',
-                '-shared',
-                '-fPIC',
-                '-lcudart',
-                '-O3',
-                link_cuda_lib
-            ] + tf_cflags + tf_lflags
-    print(f"gcc {file_names}")
-    logfile.writelines(["\n", " ".join(command), "\n"])
-    subprocess.check_call(command, stdout=logfile, stderr=logfile)
-
-
-class CudaCommand(distutils.cmd.Command):
-    description = 'Compile CUDA sources'
-    user_options = [
-        ('gcc=', None, 'Path to the gcc compiler.'),
-        ('nvcc=', None, 'Path to the Nvidia nvcc compiler.'),
-        ('cuda-lib=', None, 'Path to the CUDA libraries.'),
-    ]
-
-    def initialize_options(self):
-        tf_gcc = check_tf_cuda_compatibility()
-        self.gcc = tf_gcc if isfile(tf_gcc) else 'gcc'
-        self.nvcc = '/usr/local/cuda/bin/nvcc' if isfile('/usr/local/cuda/bin/nvcc') else 'nvcc'
-        self.cuda_lib = '/usr/local/cuda/lib64/'
-
-    def finalize_options(self) -> None:
-        pass
-
-    def run(self):
-        src_path = abspath('./phi/tf/cuda/src')
-        build_path = abspath('./phi/tf/cuda/build')
-        logfile_path = abspath('./phi/tf/cuda/log.txt')
-        print("Source Path:\t" + src_path)
-        print("Build Path:\t" + build_path)
-        print("GCC:\t\t" + self.gcc)
-        print("NVCC:\t\t" + self.nvcc)
-        print("CUDA lib:\t" + self.cuda_lib)
-        print("----------------------------")
-        # Remove old build files
-        if isdir(build_path):
-            print('Removing old build files from %s' % build_path)
-            for file in os.listdir(build_path):
-                os.remove(join(build_path, file))
-        else:
-            print('Creating build directory at %s' % build_path)
-            os.mkdir(build_path)
-        print('Compiling CUDA code...')
-        with open(logfile_path, "w") as logfile:
-            try:
-                compile_cuda('resample', self.nvcc, src_path, build_path, logfile=logfile)
-                compile_gcc('resample', self.gcc, src_path, build_path, self.cuda_lib, logfile=logfile)
-                compile_cuda('resample_gradient', self.nvcc, src_path, build_path, logfile=logfile)
-                compile_gcc('resample_gradient', self.gcc, src_path, build_path, self.cuda_lib, logfile=logfile)
-                # compile_cuda('bicgstab_ilu_linear_solve_op', self.nvcc, src_path, build_path, logfile=logfile)
-                # compile_gcc('bicgstab_ilu_linear_solve_op', self.gcc, src_path, build_path, self.cuda_lib, logfile=logfile)
-            except BaseException as err:
-                print(f"Compilation failed. See {logfile_path} for details.")
-                raise err
-        print(f"Compilation complete. See {logfile_path} for details.")
-
-
-try:
-    with open(join(dirname(__file__), 'docs/Package_Info.md'), 'r') as readme:
-        long_description = readme.read()
-except FileNotFoundError:
-    long_description = ""
-    pass
-
-with open(join(dirname(__file__), 'phi', 'VERSION'), 'r') as version_file:
-    version = version_file.read()
-
-setup(
-    name='phiflow',
-    version=version,
-    download_url='https://github.com/tum-pbs/PhiFlow/archive/%s.tar.gz' % version,
-    packages=['phi',
-              'phi.field',
-              'phi.geom',
-              'phi.jax',
-              'phi.jax.stax',
-              'phi.math',
-              'phi.math.backend',
-              'phi.physics',
-              'phi.tf',
-              'phi.torch',
-              'phi.vis',
-              'phi.vis._console',
-              'phi.vis._dash',
-              'phi.vis._matplotlib',
-          ],
-    cmdclass={
-        'tf_cuda': CudaCommand,
-    },
-    description='Differentiable PDE solving framework for machine learning',
-    long_description=long_description,
-    long_description_content_type='text/markdown',
-    keywords=['Differentiable', 'Simulation', 'Fluid', 'Machine Learning', 'Deep Learning'],
-    license='MIT',
-    author='Philipp Holl',
-    author_email='philipp.holl@tum.de',
-    url='https://github.com/tum-pbs/PhiFlow',
-    include_package_data=True,
-    install_requires=[
-        'numpy',  # 1.20 causes TensorFlow tracing errors: NotImplementedError: Cannot convert a symbolic Tensor to a numpy array.
-        'scipy>=1.5.4',
-        'matplotlib'  # also required by dash for color maps
-    ],
-    # Optional packages:
-    # - dash + plotly (included in dash)
-    # - torch
-    # - tensorflow
-    # - jax
-    #
-    # phi.verify() should detect missing packages.
-    classifiers=[
-        'Development Status :: 5 - Production/Stable',
-        'Intended Audience :: Developers',
-        'Topic :: Software Development :: Build Tools',
-        'License :: OSI Approved :: MIT License',
-        'Programming Language :: Python :: 3',
-        'Programming Language :: Python :: 3.6',
-        'Programming Language :: Python :: 3.7',
-        'Programming Language :: Python :: 3.8',
-        'Programming Language :: Python :: 3.9',
-        'Programming Language :: Python :: 3.10',
-    ],
-)
+import distutils.cmd
+import distutils.log
+import subprocess
+import os
+import sys
+from os.path import join, isfile, abspath, isdir, dirname
+from setuptools import setup
+
+
+def check_tf_cuda_compatibility():
+    import tensorflow
+    build = tensorflow.sysconfig.get_build_info()  # is_rocm_build, cuda_compute_capabilities
+    tf_gcc = build['cpu_compiler']
+    is_cuda_build = build['is_cuda_build']
+    print(f"TensorFlow compiler: {tf_gcc}.")
+    if not is_cuda_build:
+        raise AssertionError("Your TensorFlow build does not support CUDA.")
+    else:
+        cuda_version = build['cuda_version']
+        cudnn_version = build['cudnn_version']
+        print(f"TensorFlow was compiled against CUDA {cuda_version} and cuDNN {cudnn_version}.")
+        return tf_gcc
+
+
+def compile_cuda(file_names, nvcc, source_dir, target_dir, logfile):
+    import tensorflow
+    tf_cflags = tensorflow.sysconfig.get_compile_flags()
+    command = [
+            nvcc,
+            join(source_dir, f'{file_names}.cu.cc'),
+            '-o', join(target_dir, f'{file_names}.cu.o'),
+            '-std=c++11',
+            '-c',
+            '-D GOOGLE_CUDA=1',
+            '-x', 'cu',
+            '-Xcompiler',
+            '-fPIC',
+            '--expt-relaxed-constexpr',
+            '-DNDEBUG',
+            '-O3'
+        ] + tf_cflags
+    print(f"nvcc {file_names}")
+    logfile.writelines(["\n", " ".join(command), "\n"])
+    subprocess.check_call(command, stdout=logfile, stderr=logfile)
+
+
+def compile_gcc(file_names, gcc, source_dir, target_dir, cuda_lib, logfile):
+    import tensorflow
+    from packaging import version
+    if version.parse(tensorflow.__version__) >= version.parse('2.5.0'):
+        cpp_version, gcc_version = '14', '7.5'
+    else:
+        cpp_version, gcc_version = '11', '4.8'
+    tf_cflags = tensorflow.sysconfig.get_compile_flags()
+    tf_lflags = tensorflow.sysconfig.get_link_flags()
+    link_cuda_lib = '-L' + cuda_lib
+    command = [
+                gcc,
+                join(source_dir, f'{file_names}.cc'),
+                join(target_dir, f'{file_names}.cu.o'),
+                '-o', join(target_dir, f'{file_names}.so'),
+                f'-std=c++{cpp_version}',
+                '-shared',
+                '-fPIC',
+                '-lcudart',
+                '-O3',
+                link_cuda_lib
+            ] + tf_cflags + tf_lflags
+    print(f"gcc {file_names}")
+    logfile.writelines(["\n", " ".join(command), "\n"])
+    subprocess.check_call(command, stdout=logfile, stderr=logfile)
+
+
+class CudaCommand(distutils.cmd.Command):
+    description = 'Compile CUDA sources'
+    user_options = [
+        ('gcc=', None, 'Path to the gcc compiler.'),
+        ('nvcc=', None, 'Path to the Nvidia nvcc compiler.'),
+        ('cuda-lib=', None, 'Path to the CUDA libraries.'),
+    ]
+
+    def initialize_options(self):
+        tf_gcc = check_tf_cuda_compatibility()
+        self.gcc = tf_gcc if isfile(tf_gcc) else 'gcc'
+        self.nvcc = '/usr/local/cuda/bin/nvcc' if isfile('/usr/local/cuda/bin/nvcc') else 'nvcc'
+        self.cuda_lib = '/usr/local/cuda/lib64/'
+
+    def finalize_options(self) -> None:
+        pass
+
+    def run(self):
+        src_path = abspath('./phi/tf/cuda/src')
+        build_path = abspath('./phi/tf/cuda/build')
+        logfile_path = abspath('./phi/tf/cuda/log.txt')
+        print("Source Path:\t" + src_path)
+        print("Build Path:\t" + build_path)
+        print("GCC:\t\t" + self.gcc)
+        print("NVCC:\t\t" + self.nvcc)
+        print("CUDA lib:\t" + self.cuda_lib)
+        print("----------------------------")
+        # Remove old build files
+        if isdir(build_path):
+            print('Removing old build files from %s' % build_path)
+            for file in os.listdir(build_path):
+                os.remove(join(build_path, file))
+        else:
+            print('Creating build directory at %s' % build_path)
+            os.mkdir(build_path)
+        print('Compiling CUDA code...')
+        with open(logfile_path, "w") as logfile:
+            try:
+                compile_cuda('resample', self.nvcc, src_path, build_path, logfile=logfile)
+                compile_gcc('resample', self.gcc, src_path, build_path, self.cuda_lib, logfile=logfile)
+                compile_cuda('resample_gradient', self.nvcc, src_path, build_path, logfile=logfile)
+                compile_gcc('resample_gradient', self.gcc, src_path, build_path, self.cuda_lib, logfile=logfile)
+                # compile_cuda('bicgstab_ilu_linear_solve_op', self.nvcc, src_path, build_path, logfile=logfile)
+                # compile_gcc('bicgstab_ilu_linear_solve_op', self.gcc, src_path, build_path, self.cuda_lib, logfile=logfile)
+            except BaseException as err:
+                print(f"Compilation failed. See {logfile_path} for details.")
+                raise err
+        print(f"Compilation complete. See {logfile_path} for details.")
+
+
+try:
+    with open(join(dirname(__file__), 'docs/Package_Info.md'), 'r') as readme:
+        long_description = readme.read()
+except FileNotFoundError:
+    long_description = ""
+    pass
+
+with open(join(dirname(__file__), 'phi', 'VERSION'), 'r') as version_file:
+    version = version_file.read()
+
+setup(
+    name='phiflow',
+    version=version,
+    download_url='https://github.com/tum-pbs/PhiFlow/archive/%s.tar.gz' % version,
+    packages=['phi',
+              'phi.field',
+              'phi.geom',
+              'phi.jax',
+              'phi.jax.stax',
+              'phi.math',
+              'phi.math.backend',
+              'phi.physics',
+              'phi.tf',
+              'phi.torch',
+              'phi.vis',
+              'phi.vis._console',
+              'phi.vis._dash',
+              'phi.vis._matplotlib',
+          ],
+    cmdclass={
+        'tf_cuda': CudaCommand,
+    },
+    description='Differentiable PDE solving framework for machine learning',
+    long_description=long_description,
+    long_description_content_type='text/markdown',
+    keywords=['Differentiable', 'Simulation', 'Fluid', 'Machine Learning', 'Deep Learning'],
+    license='MIT',
+    author='Philipp Holl',
+    author_email='philipp.holl@tum.de',
+    url='https://github.com/tum-pbs/PhiFlow',
+    include_package_data=True,
+    install_requires=[
+        'numpy',  # 1.20 causes TensorFlow tracing errors: NotImplementedError: Cannot convert a symbolic Tensor to a numpy array.
+        'scipy>=1.5.4',
+        'matplotlib>=3.5.0',  # also required by dash for color maps
+        'packaging',
+    ],
+    # Optional packages:
+    # - dash + plotly (included in dash)
+    # - torch
+    # - tensorflow
+    # - jax
+    #
+    # phi.verify() should detect missing packages.
+    classifiers=[
+        'Development Status :: 5 - Production/Stable',
+        'Intended Audience :: Developers',
+        'Topic :: Software Development :: Build Tools',
+        'License :: OSI Approved :: MIT License',
+        'Programming Language :: Python :: 3',
+        'Programming Language :: Python :: 3.6',
+        'Programming Language :: Python :: 3.7',
+        'Programming Language :: Python :: 3.8',
+        'Programming Language :: Python :: 3.9',
+        'Programming Language :: Python :: 3.10',
+    ],
+)
```

