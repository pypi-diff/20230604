# Comparing `tmp/torchrl_nightly-2023.6.2-cp39-cp39-win_amd64.whl.zip` & `tmp/torchrl_nightly-2023.6.3-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,131 +1,131 @@
-Zip file size: 580680 bytes, number of entries: 129
--rw-rw-rw-  2.0 fat      183 b- defN 23-Jun-02 11:19 build_tools/__init__.py
--rw-rw-rw-  2.0 fat      245 b- defN 23-Jun-02 11:19 build_tools/setup_helpers/__init__.py
--rw-rw-rw-  2.0 fat     6085 b- defN 23-Jun-02 11:19 build_tools/setup_helpers/extension.py
--rw-rw-rw-  2.0 fat      957 b- defN 23-Jun-02 11:19 torchrl/__init__.py
--rw-rw-rw-  2.0 fat      868 b- defN 23-Jun-02 11:19 torchrl/_extension.py
--rw-rw-rw-  2.0 fat   360448 b- defN 23-Jun-02 11:21 torchrl/_torchrl.pyd
--rw-rw-rw-  2.0 fat    17746 b- defN 23-Jun-02 11:19 torchrl/_utils.py
--rw-rw-rw-  2.0 fat       84 b- defN 23-Jun-02 11:21 torchrl/version.py
--rw-rw-rw-  2.0 fat      366 b- defN 23-Jun-02 11:19 torchrl/collectors/__init__.py
--rw-rw-rw-  2.0 fat    97515 b- defN 23-Jun-02 11:19 torchrl/collectors/collectors.py
--rw-rw-rw-  2.0 fat     3847 b- defN 23-Jun-02 11:19 torchrl/collectors/utils.py
--rw-rw-rw-  2.0 fat      412 b- defN 23-Jun-02 11:19 torchrl/collectors/distributed/__init__.py
--rw-rw-rw-  2.0 fat      674 b- defN 23-Jun-02 11:19 torchrl/collectors/distributed/default_configs.py
--rw-rw-rw-  2.0 fat    34086 b- defN 23-Jun-02 11:19 torchrl/collectors/distributed/generic.py
--rw-rw-rw-  2.0 fat    27837 b- defN 23-Jun-02 11:19 torchrl/collectors/distributed/ray.py
--rw-rw-rw-  2.0 fat    27697 b- defN 23-Jun-02 11:19 torchrl/collectors/distributed/rpc.py
--rw-rw-rw-  2.0 fat    20055 b- defN 23-Jun-02 11:19 torchrl/collectors/distributed/sync.py
--rw-rw-rw-  2.0 fat     6244 b- defN 23-Jun-02 11:19 torchrl/collectors/distributed/utils.py
--rw-rw-rw-  2.0 fat      974 b- defN 23-Jun-02 11:19 torchrl/data/__init__.py
--rw-rw-rw-  2.0 fat   122048 b- defN 23-Jun-02 11:19 torchrl/data/tensor_specs.py
--rw-rw-rw-  2.0 fat     2061 b- defN 23-Jun-02 11:19 torchrl/data/utils.py
--rw-rw-rw-  2.0 fat       84 b- defN 23-Jun-02 11:19 torchrl/data/datasets/__init__.py
--rw-rw-rw-  2.0 fat    12415 b- defN 23-Jun-02 11:19 torchrl/data/datasets/d4rl.py
--rw-rw-rw-  2.0 fat     6333 b- defN 23-Jun-02 11:19 torchrl/data/datasets/openml.py
--rw-rw-rw-  2.0 fat      219 b- defN 23-Jun-02 11:19 torchrl/data/postprocs/__init__.py
--rw-rw-rw-  2.0 fat     9361 b- defN 23-Jun-02 11:19 torchrl/data/postprocs/postprocs.py
--rw-rw-rw-  2.0 fat      648 b- defN 23-Jun-02 11:19 torchrl/data/replay_buffers/__init__.py
--rw-rw-rw-  2.0 fat    42205 b- defN 23-Jun-02 11:19 torchrl/data/replay_buffers/replay_buffers.py
--rw-rw-rw-  2.0 fat    11535 b- defN 23-Jun-02 11:19 torchrl/data/replay_buffers/samplers.py
--rw-rw-rw-  2.0 fat    17810 b- defN 23-Jun-02 11:19 torchrl/data/replay_buffers/storages.py
--rw-rw-rw-  2.0 fat     1904 b- defN 23-Jun-02 11:19 torchrl/data/replay_buffers/utils.py
--rw-rw-rw-  2.0 fat     3640 b- defN 23-Jun-02 11:19 torchrl/data/replay_buffers/writers.py
--rw-rw-rw-  2.0 fat     1539 b- defN 23-Jun-02 11:19 torchrl/envs/__init__.py
--rw-rw-rw-  2.0 fat    61326 b- defN 23-Jun-02 11:19 torchrl/envs/common.py
--rw-rw-rw-  2.0 fat     7603 b- defN 23-Jun-02 11:19 torchrl/envs/env_creator.py
--rw-rw-rw-  2.0 fat    12655 b- defN 23-Jun-02 11:19 torchrl/envs/gym_like.py
--rw-rw-rw-  2.0 fat    18687 b- defN 23-Jun-02 11:19 torchrl/envs/utils.py
--rw-rw-rw-  2.0 fat    55328 b- defN 23-Jun-02 11:19 torchrl/envs/vec_env.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-Jun-02 11:19 torchrl/envs/libs/__init__.py
--rw-rw-rw-  2.0 fat    15376 b- defN 23-Jun-02 11:19 torchrl/envs/libs/brax.py
--rw-rw-rw-  2.0 fat    11253 b- defN 23-Jun-02 11:19 torchrl/envs/libs/dm_control.py
--rw-rw-rw-  2.0 fat    25980 b- defN 23-Jun-02 11:19 torchrl/envs/libs/gym.py
--rw-rw-rw-  2.0 fat     2857 b- defN 23-Jun-02 11:19 torchrl/envs/libs/habitat.py
--rw-rw-rw-  2.0 fat     4699 b- defN 23-Jun-02 11:19 torchrl/envs/libs/jax_utils.py
--rw-rw-rw-  2.0 fat    12877 b- defN 23-Jun-02 11:19 torchrl/envs/libs/jumanji.py
--rw-rw-rw-  2.0 fat     5260 b- defN 23-Jun-02 11:19 torchrl/envs/libs/openml.py
--rw-rw-rw-  2.0 fat     5316 b- defN 23-Jun-02 11:19 torchrl/envs/libs/utils.py
--rw-rw-rw-  2.0 fat    18538 b- defN 23-Jun-02 11:19 torchrl/envs/libs/vmas.py
--rw-rw-rw-  2.0 fat      224 b- defN 23-Jun-02 11:19 torchrl/envs/model_based/__init__.py
--rw-rw-rw-  2.0 fat     8168 b- defN 23-Jun-02 11:19 torchrl/envs/model_based/common.py
--rw-rw-rw-  2.0 fat     2875 b- defN 23-Jun-02 11:19 torchrl/envs/model_based/dreamer.py
--rw-rw-rw-  2.0 fat     1077 b- defN 23-Jun-02 11:19 torchrl/envs/transforms/__init__.py
--rw-rw-rw-  2.0 fat     1485 b- defN 23-Jun-02 11:19 torchrl/envs/transforms/functional.py
--rw-rw-rw-  2.0 fat    13723 b- defN 23-Jun-02 11:19 torchrl/envs/transforms/r3m.py
--rw-rw-rw-  2.0 fat     8280 b- defN 23-Jun-02 11:19 torchrl/envs/transforms/rlhf.py
--rw-rw-rw-  2.0 fat   171431 b- defN 23-Jun-02 11:19 torchrl/envs/transforms/transforms.py
--rw-rw-rw-  2.0 fat      408 b- defN 23-Jun-02 11:19 torchrl/envs/transforms/utils.py
--rw-rw-rw-  2.0 fat    14084 b- defN 23-Jun-02 11:19 torchrl/envs/transforms/vip.py
--rw-rw-rw-  2.0 fat     1437 b- defN 23-Jun-02 11:19 torchrl/modules/__init__.py
--rw-rw-rw-  2.0 fat      600 b- defN 23-Jun-02 11:19 torchrl/modules/distributions/__init__.py
--rw-rw-rw-  2.0 fat    21386 b- defN 23-Jun-02 11:19 torchrl/modules/distributions/continuous.py
--rw-rw-rw-  2.0 fat     9293 b- defN 23-Jun-02 11:19 torchrl/modules/distributions/discrete.py
--rw-rw-rw-  2.0 fat     5988 b- defN 23-Jun-02 11:19 torchrl/modules/distributions/truncated_normal.py
--rw-rw-rw-  2.0 fat     6861 b- defN 23-Jun-02 11:19 torchrl/modules/distributions/utils.py
--rw-rw-rw-  2.0 fat      580 b- defN 23-Jun-02 11:19 torchrl/modules/models/__init__.py
--rw-rw-rw-  2.0 fat    20516 b- defN 23-Jun-02 11:19 torchrl/modules/models/exploration.py
--rw-rw-rw-  2.0 fat    11863 b- defN 23-Jun-02 11:19 torchrl/modules/models/model_based.py
--rw-rw-rw-  2.0 fat    45664 b- defN 23-Jun-02 11:19 torchrl/modules/models/models.py
--rw-rw-rw-  2.0 fat     4023 b- defN 23-Jun-02 11:19 torchrl/modules/models/utils.py
--rw-rw-rw-  2.0 fat      281 b- defN 23-Jun-02 11:19 torchrl/modules/planners/__init__.py
--rw-rw-rw-  2.0 fat     9572 b- defN 23-Jun-02 11:19 torchrl/modules/planners/cem.py
--rw-rw-rw-  2.0 fat     2455 b- defN 23-Jun-02 11:19 torchrl/modules/planners/common.py
--rw-rw-rw-  2.0 fat    10683 b- defN 23-Jun-02 11:19 torchrl/modules/planners/mppi.py
--rw-rw-rw-  2.0 fat      882 b- defN 23-Jun-02 11:19 torchrl/modules/tensordict_module/__init__.py
--rw-rw-rw-  2.0 fat    78666 b- defN 23-Jun-02 11:19 torchrl/modules/tensordict_module/actors.py
--rw-rw-rw-  2.0 fat    17209 b- defN 23-Jun-02 11:19 torchrl/modules/tensordict_module/common.py
--rw-rw-rw-  2.0 fat    24439 b- defN 23-Jun-02 11:19 torchrl/modules/tensordict_module/exploration.py
--rw-rw-rw-  2.0 fat    11468 b- defN 23-Jun-02 11:19 torchrl/modules/tensordict_module/probabilistic.py
--rw-rw-rw-  2.0 fat    18299 b- defN 23-Jun-02 11:19 torchrl/modules/tensordict_module/rnn.py
--rw-rw-rw-  2.0 fat     5835 b- defN 23-Jun-02 11:19 torchrl/modules/tensordict_module/sequence.py
--rw-rw-rw-  2.0 fat     1360 b- defN 23-Jun-02 11:19 torchrl/modules/tensordict_module/world_models.py
--rw-rw-rw-  2.0 fat     3984 b- defN 23-Jun-02 11:19 torchrl/modules/utils/__init__.py
--rw-rw-rw-  2.0 fat     2435 b- defN 23-Jun-02 11:19 torchrl/modules/utils/mappings.py
--rw-rw-rw-  2.0 fat     1368 b- defN 23-Jun-02 11:19 torchrl/modules/utils/utils.py
--rw-rw-rw-  2.0 fat      895 b- defN 23-Jun-02 11:19 torchrl/objectives/__init__.py
--rw-rw-rw-  2.0 fat    11362 b- defN 23-Jun-02 11:19 torchrl/objectives/a2c.py
--rw-rw-rw-  2.0 fat    26566 b- defN 23-Jun-02 11:19 torchrl/objectives/common.py
--rw-rw-rw-  2.0 fat     8685 b- defN 23-Jun-02 11:19 torchrl/objectives/ddpg.py
--rw-rw-rw-  2.0 fat    13051 b- defN 23-Jun-02 11:19 torchrl/objectives/deprecated.py
--rw-rw-rw-  2.0 fat    22531 b- defN 23-Jun-02 11:19 torchrl/objectives/dqn.py
--rw-rw-rw-  2.0 fat    17407 b- defN 23-Jun-02 11:19 torchrl/objectives/dreamer.py
--rw-rw-rw-  2.0 fat     2119 b- defN 23-Jun-02 11:19 torchrl/objectives/functional.py
--rw-rw-rw-  2.0 fat    12319 b- defN 23-Jun-02 11:19 torchrl/objectives/iql.py
--rw-rw-rw-  2.0 fat    31728 b- defN 23-Jun-02 11:19 torchrl/objectives/ppo.py
--rw-rw-rw-  2.0 fat    16103 b- defN 23-Jun-02 11:19 torchrl/objectives/redq.py
--rw-rw-rw-  2.0 fat     9266 b- defN 23-Jun-02 11:19 torchrl/objectives/reinforce.py
--rw-rw-rw-  2.0 fat    34866 b- defN 23-Jun-02 11:19 torchrl/objectives/sac.py
--rw-rw-rw-  2.0 fat    11303 b- defN 23-Jun-02 11:19 torchrl/objectives/td3.py
--rw-rw-rw-  2.0 fat    16464 b- defN 23-Jun-02 11:19 torchrl/objectives/utils.py
--rw-rw-rw-  2.0 fat      371 b- defN 23-Jun-02 11:19 torchrl/objectives/value/__init__.py
--rw-rw-rw-  2.0 fat    47539 b- defN 23-Jun-02 11:19 torchrl/objectives/value/advantages.py
--rw-rw-rw-  2.0 fat    40208 b- defN 23-Jun-02 11:19 torchrl/objectives/value/functional.py
--rw-rw-rw-  2.0 fat      317 b- defN 23-Jun-02 11:19 torchrl/objectives/value/pg.py
--rw-rw-rw-  2.0 fat    13251 b- defN 23-Jun-02 11:19 torchrl/objectives/value/utils.py
--rw-rw-rw-  2.0 fat     1724 b- defN 23-Jun-02 11:19 torchrl/objectives/value/vtrace.py
--rw-rw-rw-  2.0 fat      242 b- defN 23-Jun-02 11:19 torchrl/record/__init__.py
--rw-rw-rw-  2.0 fat     6839 b- defN 23-Jun-02 11:19 torchrl/record/recorder.py
--rw-rw-rw-  2.0 fat      413 b- defN 23-Jun-02 11:19 torchrl/record/loggers/__init__.py
--rw-rw-rw-  2.0 fat     1130 b- defN 23-Jun-02 11:19 torchrl/record/loggers/common.py
--rw-rw-rw-  2.0 fat     4709 b- defN 23-Jun-02 11:19 torchrl/record/loggers/csv.py
--rw-rw-rw-  2.0 fat     4344 b- defN 23-Jun-02 11:19 torchrl/record/loggers/mlflow.py
--rw-rw-rw-  2.0 fat     3412 b- defN 23-Jun-02 11:19 torchrl/record/loggers/tensorboard.py
--rw-rw-rw-  2.0 fat     2248 b- defN 23-Jun-02 11:19 torchrl/record/loggers/utils.py
--rw-rw-rw-  2.0 fat     6025 b- defN 23-Jun-02 11:19 torchrl/record/loggers/wandb.py
--rw-rw-rw-  2.0 fat      467 b- defN 23-Jun-02 11:19 torchrl/trainers/__init__.py
--rw-rw-rw-  2.0 fat    52042 b- defN 23-Jun-02 11:19 torchrl/trainers/trainers.py
--rw-rw-rw-  2.0 fat      942 b- defN 23-Jun-02 11:19 torchrl/trainers/helpers/__init__.py
--rw-rw-rw-  2.0 fat    19309 b- defN 23-Jun-02 11:19 torchrl/trainers/helpers/collectors.py
--rw-rw-rw-  2.0 fat    22620 b- defN 23-Jun-02 11:19 torchrl/trainers/helpers/envs.py
--rw-rw-rw-  2.0 fat     1206 b- defN 23-Jun-02 11:19 torchrl/trainers/helpers/logger.py
--rw-rw-rw-  2.0 fat    11052 b- defN 23-Jun-02 11:19 torchrl/trainers/helpers/losses.py
--rw-rw-rw-  2.0 fat    77459 b- defN 23-Jun-02 11:19 torchrl/trainers/helpers/models.py
--rw-rw-rw-  2.0 fat     1939 b- defN 23-Jun-02 11:19 torchrl/trainers/helpers/replay_buffer.py
--rw-rw-rw-  2.0 fat    12076 b- defN 23-Jun-02 11:19 torchrl/trainers/helpers/trainers.py
--rw-rw-rw-  2.0 fat     1119 b- defN 23-Jun-02 11:21 torchrl_nightly-2023.6.2.dist-info/LICENSE
--rw-rw-rw-  2.0 fat    28565 b- defN 23-Jun-02 11:21 torchrl_nightly-2023.6.2.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 23-Jun-02 11:21 torchrl_nightly-2023.6.2.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       20 b- defN 23-Jun-02 11:21 torchrl_nightly-2023.6.2.dist-info/top_level.txt
--rw-rw-r--  2.0 fat    11568 b- defN 23-Jun-02 11:21 torchrl_nightly-2023.6.2.dist-info/RECORD
-129 files, 2242886 bytes uncompressed, 562370 bytes compressed:  75.0%
+Zip file size: 577778 bytes, number of entries: 129
+-rw-rw-rw-  2.0 fat      183 b- defN 23-Jun-03 11:17 build_tools/__init__.py
+-rw-rw-rw-  2.0 fat      245 b- defN 23-Jun-03 11:17 build_tools/setup_helpers/__init__.py
+-rw-rw-rw-  2.0 fat     6085 b- defN 23-Jun-03 11:17 build_tools/setup_helpers/extension.py
+-rw-rw-rw-  2.0 fat      957 b- defN 23-Jun-03 11:17 torchrl/__init__.py
+-rw-rw-rw-  2.0 fat      868 b- defN 23-Jun-03 11:17 torchrl/_extension.py
+-rw-rw-rw-  2.0 fat   360448 b- defN 23-Jun-03 11:20 torchrl/_torchrl.pyd
+-rw-rw-rw-  2.0 fat    17746 b- defN 23-Jun-03 11:17 torchrl/_utils.py
+-rw-rw-rw-  2.0 fat       84 b- defN 23-Jun-03 11:19 torchrl/version.py
+-rw-rw-rw-  2.0 fat      366 b- defN 23-Jun-03 11:17 torchrl/collectors/__init__.py
+-rw-rw-rw-  2.0 fat    97515 b- defN 23-Jun-03 11:17 torchrl/collectors/collectors.py
+-rw-rw-rw-  2.0 fat     3847 b- defN 23-Jun-03 11:17 torchrl/collectors/utils.py
+-rw-rw-rw-  2.0 fat      412 b- defN 23-Jun-03 11:17 torchrl/collectors/distributed/__init__.py
+-rw-rw-rw-  2.0 fat      674 b- defN 23-Jun-03 11:17 torchrl/collectors/distributed/default_configs.py
+-rw-rw-rw-  2.0 fat    34086 b- defN 23-Jun-03 11:17 torchrl/collectors/distributed/generic.py
+-rw-rw-rw-  2.0 fat    27837 b- defN 23-Jun-03 11:17 torchrl/collectors/distributed/ray.py
+-rw-rw-rw-  2.0 fat    27697 b- defN 23-Jun-03 11:17 torchrl/collectors/distributed/rpc.py
+-rw-rw-rw-  2.0 fat    20055 b- defN 23-Jun-03 11:17 torchrl/collectors/distributed/sync.py
+-rw-rw-rw-  2.0 fat     6244 b- defN 23-Jun-03 11:17 torchrl/collectors/distributed/utils.py
+-rw-rw-rw-  2.0 fat      974 b- defN 23-Jun-03 11:17 torchrl/data/__init__.py
+-rw-rw-rw-  2.0 fat   122535 b- defN 23-Jun-03 11:17 torchrl/data/tensor_specs.py
+-rw-rw-rw-  2.0 fat     2061 b- defN 23-Jun-03 11:17 torchrl/data/utils.py
+-rw-rw-rw-  2.0 fat       84 b- defN 23-Jun-03 11:17 torchrl/data/datasets/__init__.py
+-rw-rw-rw-  2.0 fat    12415 b- defN 23-Jun-03 11:17 torchrl/data/datasets/d4rl.py
+-rw-rw-rw-  2.0 fat     6333 b- defN 23-Jun-03 11:17 torchrl/data/datasets/openml.py
+-rw-rw-rw-  2.0 fat      219 b- defN 23-Jun-03 11:17 torchrl/data/postprocs/__init__.py
+-rw-rw-rw-  2.0 fat     9361 b- defN 23-Jun-03 11:17 torchrl/data/postprocs/postprocs.py
+-rw-rw-rw-  2.0 fat      648 b- defN 23-Jun-03 11:17 torchrl/data/replay_buffers/__init__.py
+-rw-rw-rw-  2.0 fat    42205 b- defN 23-Jun-03 11:17 torchrl/data/replay_buffers/replay_buffers.py
+-rw-rw-rw-  2.0 fat    11535 b- defN 23-Jun-03 11:17 torchrl/data/replay_buffers/samplers.py
+-rw-rw-rw-  2.0 fat    17810 b- defN 23-Jun-03 11:17 torchrl/data/replay_buffers/storages.py
+-rw-rw-rw-  2.0 fat     1904 b- defN 23-Jun-03 11:17 torchrl/data/replay_buffers/utils.py
+-rw-rw-rw-  2.0 fat     3640 b- defN 23-Jun-03 11:17 torchrl/data/replay_buffers/writers.py
+-rw-rw-rw-  2.0 fat     1539 b- defN 23-Jun-03 11:17 torchrl/envs/__init__.py
+-rw-rw-rw-  2.0 fat    61326 b- defN 23-Jun-03 11:17 torchrl/envs/common.py
+-rw-rw-rw-  2.0 fat     7603 b- defN 23-Jun-03 11:17 torchrl/envs/env_creator.py
+-rw-rw-rw-  2.0 fat    12655 b- defN 23-Jun-03 11:17 torchrl/envs/gym_like.py
+-rw-rw-rw-  2.0 fat    18687 b- defN 23-Jun-03 11:17 torchrl/envs/utils.py
+-rw-rw-rw-  2.0 fat    55328 b- defN 23-Jun-03 11:17 torchrl/envs/vec_env.py
+-rw-rw-rw-  2.0 fat      183 b- defN 23-Jun-03 11:17 torchrl/envs/libs/__init__.py
+-rw-rw-rw-  2.0 fat    15376 b- defN 23-Jun-03 11:17 torchrl/envs/libs/brax.py
+-rw-rw-rw-  2.0 fat    11253 b- defN 23-Jun-03 11:17 torchrl/envs/libs/dm_control.py
+-rw-rw-rw-  2.0 fat    25980 b- defN 23-Jun-03 11:17 torchrl/envs/libs/gym.py
+-rw-rw-rw-  2.0 fat     2857 b- defN 23-Jun-03 11:17 torchrl/envs/libs/habitat.py
+-rw-rw-rw-  2.0 fat     4699 b- defN 23-Jun-03 11:17 torchrl/envs/libs/jax_utils.py
+-rw-rw-rw-  2.0 fat    12877 b- defN 23-Jun-03 11:17 torchrl/envs/libs/jumanji.py
+-rw-rw-rw-  2.0 fat     5260 b- defN 23-Jun-03 11:17 torchrl/envs/libs/openml.py
+-rw-rw-rw-  2.0 fat     5316 b- defN 23-Jun-03 11:17 torchrl/envs/libs/utils.py
+-rw-rw-rw-  2.0 fat    18538 b- defN 23-Jun-03 11:17 torchrl/envs/libs/vmas.py
+-rw-rw-rw-  2.0 fat      224 b- defN 23-Jun-03 11:17 torchrl/envs/model_based/__init__.py
+-rw-rw-rw-  2.0 fat     8168 b- defN 23-Jun-03 11:17 torchrl/envs/model_based/common.py
+-rw-rw-rw-  2.0 fat     2875 b- defN 23-Jun-03 11:17 torchrl/envs/model_based/dreamer.py
+-rw-rw-rw-  2.0 fat     1077 b- defN 23-Jun-03 11:17 torchrl/envs/transforms/__init__.py
+-rw-rw-rw-  2.0 fat     1485 b- defN 23-Jun-03 11:17 torchrl/envs/transforms/functional.py
+-rw-rw-rw-  2.0 fat    13723 b- defN 23-Jun-03 11:17 torchrl/envs/transforms/r3m.py
+-rw-rw-rw-  2.0 fat     8280 b- defN 23-Jun-03 11:17 torchrl/envs/transforms/rlhf.py
+-rw-rw-rw-  2.0 fat   171431 b- defN 23-Jun-03 11:17 torchrl/envs/transforms/transforms.py
+-rw-rw-rw-  2.0 fat      408 b- defN 23-Jun-03 11:17 torchrl/envs/transforms/utils.py
+-rw-rw-rw-  2.0 fat    14084 b- defN 23-Jun-03 11:17 torchrl/envs/transforms/vip.py
+-rw-rw-rw-  2.0 fat     1437 b- defN 23-Jun-03 11:17 torchrl/modules/__init__.py
+-rw-rw-rw-  2.0 fat      600 b- defN 23-Jun-03 11:17 torchrl/modules/distributions/__init__.py
+-rw-rw-rw-  2.0 fat    21386 b- defN 23-Jun-03 11:17 torchrl/modules/distributions/continuous.py
+-rw-rw-rw-  2.0 fat     9293 b- defN 23-Jun-03 11:17 torchrl/modules/distributions/discrete.py
+-rw-rw-rw-  2.0 fat     5988 b- defN 23-Jun-03 11:17 torchrl/modules/distributions/truncated_normal.py
+-rw-rw-rw-  2.0 fat     6861 b- defN 23-Jun-03 11:17 torchrl/modules/distributions/utils.py
+-rw-rw-rw-  2.0 fat      580 b- defN 23-Jun-03 11:17 torchrl/modules/models/__init__.py
+-rw-rw-rw-  2.0 fat    20516 b- defN 23-Jun-03 11:17 torchrl/modules/models/exploration.py
+-rw-rw-rw-  2.0 fat    11863 b- defN 23-Jun-03 11:17 torchrl/modules/models/model_based.py
+-rw-rw-rw-  2.0 fat    45664 b- defN 23-Jun-03 11:17 torchrl/modules/models/models.py
+-rw-rw-rw-  2.0 fat     4023 b- defN 23-Jun-03 11:17 torchrl/modules/models/utils.py
+-rw-rw-rw-  2.0 fat      281 b- defN 23-Jun-03 11:17 torchrl/modules/planners/__init__.py
+-rw-rw-rw-  2.0 fat     9572 b- defN 23-Jun-03 11:17 torchrl/modules/planners/cem.py
+-rw-rw-rw-  2.0 fat     2455 b- defN 23-Jun-03 11:17 torchrl/modules/planners/common.py
+-rw-rw-rw-  2.0 fat    10683 b- defN 23-Jun-03 11:17 torchrl/modules/planners/mppi.py
+-rw-rw-rw-  2.0 fat      882 b- defN 23-Jun-03 11:17 torchrl/modules/tensordict_module/__init__.py
+-rw-rw-rw-  2.0 fat    78665 b- defN 23-Jun-03 11:17 torchrl/modules/tensordict_module/actors.py
+-rw-rw-rw-  2.0 fat    17209 b- defN 23-Jun-03 11:17 torchrl/modules/tensordict_module/common.py
+-rw-rw-rw-  2.0 fat    24200 b- defN 23-Jun-03 11:17 torchrl/modules/tensordict_module/exploration.py
+-rw-rw-rw-  2.0 fat    11468 b- defN 23-Jun-03 11:17 torchrl/modules/tensordict_module/probabilistic.py
+-rw-rw-rw-  2.0 fat    18299 b- defN 23-Jun-03 11:17 torchrl/modules/tensordict_module/rnn.py
+-rw-rw-rw-  2.0 fat     5835 b- defN 23-Jun-03 11:17 torchrl/modules/tensordict_module/sequence.py
+-rw-rw-rw-  2.0 fat     1360 b- defN 23-Jun-03 11:17 torchrl/modules/tensordict_module/world_models.py
+-rw-rw-rw-  2.0 fat     3984 b- defN 23-Jun-03 11:17 torchrl/modules/utils/__init__.py
+-rw-rw-rw-  2.0 fat     2435 b- defN 23-Jun-03 11:17 torchrl/modules/utils/mappings.py
+-rw-rw-rw-  2.0 fat     1368 b- defN 23-Jun-03 11:17 torchrl/modules/utils/utils.py
+-rw-rw-rw-  2.0 fat      895 b- defN 23-Jun-03 11:17 torchrl/objectives/__init__.py
+-rw-rw-rw-  2.0 fat    11362 b- defN 23-Jun-03 11:17 torchrl/objectives/a2c.py
+-rw-rw-rw-  2.0 fat    26566 b- defN 23-Jun-03 11:17 torchrl/objectives/common.py
+-rw-rw-rw-  2.0 fat     8685 b- defN 23-Jun-03 11:17 torchrl/objectives/ddpg.py
+-rw-rw-rw-  2.0 fat    13051 b- defN 23-Jun-03 11:17 torchrl/objectives/deprecated.py
+-rw-rw-rw-  2.0 fat    22531 b- defN 23-Jun-03 11:17 torchrl/objectives/dqn.py
+-rw-rw-rw-  2.0 fat    17407 b- defN 23-Jun-03 11:17 torchrl/objectives/dreamer.py
+-rw-rw-rw-  2.0 fat     2119 b- defN 23-Jun-03 11:17 torchrl/objectives/functional.py
+-rw-rw-rw-  2.0 fat    12319 b- defN 23-Jun-03 11:17 torchrl/objectives/iql.py
+-rw-rw-rw-  2.0 fat    31728 b- defN 23-Jun-03 11:17 torchrl/objectives/ppo.py
+-rw-rw-rw-  2.0 fat    16103 b- defN 23-Jun-03 11:17 torchrl/objectives/redq.py
+-rw-rw-rw-  2.0 fat     9266 b- defN 23-Jun-03 11:17 torchrl/objectives/reinforce.py
+-rw-rw-rw-  2.0 fat    34859 b- defN 23-Jun-03 11:17 torchrl/objectives/sac.py
+-rw-rw-rw-  2.0 fat    13144 b- defN 23-Jun-03 11:17 torchrl/objectives/td3.py
+-rw-rw-rw-  2.0 fat    16464 b- defN 23-Jun-03 11:17 torchrl/objectives/utils.py
+-rw-rw-rw-  2.0 fat      371 b- defN 23-Jun-03 11:17 torchrl/objectives/value/__init__.py
+-rw-rw-rw-  2.0 fat    47539 b- defN 23-Jun-03 11:17 torchrl/objectives/value/advantages.py
+-rw-rw-rw-  2.0 fat    40208 b- defN 23-Jun-03 11:17 torchrl/objectives/value/functional.py
+-rw-rw-rw-  2.0 fat      317 b- defN 23-Jun-03 11:17 torchrl/objectives/value/pg.py
+-rw-rw-rw-  2.0 fat    13251 b- defN 23-Jun-03 11:17 torchrl/objectives/value/utils.py
+-rw-rw-rw-  2.0 fat     1724 b- defN 23-Jun-03 11:17 torchrl/objectives/value/vtrace.py
+-rw-rw-rw-  2.0 fat      242 b- defN 23-Jun-03 11:17 torchrl/record/__init__.py
+-rw-rw-rw-  2.0 fat     6839 b- defN 23-Jun-03 11:17 torchrl/record/recorder.py
+-rw-rw-rw-  2.0 fat      413 b- defN 23-Jun-03 11:17 torchrl/record/loggers/__init__.py
+-rw-rw-rw-  2.0 fat     1130 b- defN 23-Jun-03 11:17 torchrl/record/loggers/common.py
+-rw-rw-rw-  2.0 fat     4709 b- defN 23-Jun-03 11:17 torchrl/record/loggers/csv.py
+-rw-rw-rw-  2.0 fat     4344 b- defN 23-Jun-03 11:17 torchrl/record/loggers/mlflow.py
+-rw-rw-rw-  2.0 fat     3412 b- defN 23-Jun-03 11:17 torchrl/record/loggers/tensorboard.py
+-rw-rw-rw-  2.0 fat     2248 b- defN 23-Jun-03 11:17 torchrl/record/loggers/utils.py
+-rw-rw-rw-  2.0 fat     6025 b- defN 23-Jun-03 11:17 torchrl/record/loggers/wandb.py
+-rw-rw-rw-  2.0 fat      467 b- defN 23-Jun-03 11:17 torchrl/trainers/__init__.py
+-rw-rw-rw-  2.0 fat    52042 b- defN 23-Jun-03 11:17 torchrl/trainers/trainers.py
+-rw-rw-rw-  2.0 fat      734 b- defN 23-Jun-03 11:17 torchrl/trainers/helpers/__init__.py
+-rw-rw-rw-  2.0 fat    19309 b- defN 23-Jun-03 11:17 torchrl/trainers/helpers/collectors.py
+-rw-rw-rw-  2.0 fat    22620 b- defN 23-Jun-03 11:17 torchrl/trainers/helpers/envs.py
+-rw-rw-rw-  2.0 fat     1206 b- defN 23-Jun-03 11:17 torchrl/trainers/helpers/logger.py
+-rw-rw-rw-  2.0 fat     6920 b- defN 23-Jun-03 11:17 torchrl/trainers/helpers/losses.py
+-rw-rw-rw-  2.0 fat    32462 b- defN 23-Jun-03 11:17 torchrl/trainers/helpers/models.py
+-rw-rw-rw-  2.0 fat     1939 b- defN 23-Jun-03 11:17 torchrl/trainers/helpers/replay_buffer.py
+-rw-rw-rw-  2.0 fat    12076 b- defN 23-Jun-03 11:17 torchrl/trainers/helpers/trainers.py
+-rw-rw-rw-  2.0 fat     1119 b- defN 23-Jun-03 11:20 torchrl_nightly-2023.6.3.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat    28718 b- defN 23-Jun-03 11:20 torchrl_nightly-2023.6.3.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      100 b- defN 23-Jun-03 11:20 torchrl_nightly-2023.6.3.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       20 b- defN 23-Jun-03 11:20 torchrl_nightly-2023.6.3.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat    11567 b- defN 23-Jun-03 11:20 torchrl_nightly-2023.6.3.dist-info/RECORD
+129 files, 2195782 bytes uncompressed, 559468 bytes compressed:  74.5%
```

## zipnote {}

```diff
@@ -366,23 +366,23 @@
 
 Filename: torchrl/trainers/helpers/replay_buffer.py
 Comment: 
 
 Filename: torchrl/trainers/helpers/trainers.py
 Comment: 
 
-Filename: torchrl_nightly-2023.6.2.dist-info/LICENSE
+Filename: torchrl_nightly-2023.6.3.dist-info/LICENSE
 Comment: 
 
-Filename: torchrl_nightly-2023.6.2.dist-info/METADATA
+Filename: torchrl_nightly-2023.6.3.dist-info/METADATA
 Comment: 
 
-Filename: torchrl_nightly-2023.6.2.dist-info/WHEEL
+Filename: torchrl_nightly-2023.6.3.dist-info/WHEEL
 Comment: 
 
-Filename: torchrl_nightly-2023.6.2.dist-info/top_level.txt
+Filename: torchrl_nightly-2023.6.3.dist-info/top_level.txt
 Comment: 
 
-Filename: torchrl_nightly-2023.6.2.dist-info/RECORD
+Filename: torchrl_nightly-2023.6.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## torchrl/version.py

```diff
@@ -1,2 +1,2 @@
-__version__ = '2023.6.2'
-git_version = '2485344bfd6c477c298ab511bffc145d3acd6ae4'
+__version__ = '2023.6.3'
+git_version = 'e955cfc50318158d3490cb8183925562593e492c'
```

## torchrl/data/tensor_specs.py

```diff
@@ -1463,29 +1463,44 @@
             shape=self.shape,
             device=self.device,
             dtype=self.dtype,
         )
 
     def __getitem__(self, idx: SHAPE_INDEX_TYPING):
         """Indexes the current TensorSpec based on the provided index."""
-        raise NotImplementedError(
-            "Pending resolution of https://github.com/pytorch/pytorch/issues/100080."
-        )
+        if _is_nested_list(idx):
+            raise NotImplementedError(
+                "Pending resolution of https://github.com/pytorch/pytorch/issues/100080."
+            )
 
         indexed_shape = torch.Size(_shape_indexing(self.shape, idx))
         # Expand is required as pytorch.tensor indexing
         return self.__class__(
             minimum=self.space.minimum[idx].clone().expand(indexed_shape),
             maximum=self.space.maximum[idx].clone().expand(indexed_shape),
             shape=indexed_shape,
             device=self.device,
             dtype=self.dtype,
         )
 
 
+def _is_nested_list(index, notuple=False):
+    if not notuple and isinstance(index, tuple):
+        for idx in index:
+            if _is_nested_list(idx, notuple=True):
+                return True
+    elif isinstance(index, list):
+        for idx in index:
+            if isinstance(idx, list):
+                return True
+        else:
+            return False
+    return False
+
+
 @dataclass(repr=False)
 class UnboundedContinuousTensorSpec(TensorSpec):
     """An unbounded continuous tensor spec.
 
     Args:
         device (str, int or torch.device, optional): device of the tensors.
         dtype (str or torch.dtype, optional): dtype of the tensors
@@ -2364,17 +2379,18 @@
         nvec = self.nvec.unsqueeze(dim)
         return self.__class__(
             nvec=nvec, shape=shape, device=self.device, dtype=self.dtype
         )
 
     def __getitem__(self, idx: SHAPE_INDEX_TYPING):
         """Indexes the current TensorSpec based on the provided index."""
-        raise NotImplementedError(
-            "Pending resolution of https://github.com/pytorch/pytorch/issues/100080."
-        )
+        if _is_nested_list(idx):
+            raise NotImplementedError(
+                "Pending resolution of https://github.com/pytorch/pytorch/issues/100080."
+            )
 
         return self.__class__(
             nvec=self.nvec[idx].clone(),
             shape=None,
             device=self.device,
             dtype=self.dtype,
         )
```

## torchrl/modules/tensordict_module/actors.py

```diff
@@ -1689,15 +1689,15 @@
         else:
             # if one spec is present, we assume it is the same for all keys
             spec = CompositeSpec(
                 {out_key: spec for out_key in out_keys},
             )
 
         leaf_specs = [spec[out_key] for out_key in self.out_keys]
-        self._spec = spec
+        self.spec = spec
         self.non_trivial = {}
         for out_key, leaf_spec in zip(out_keys, leaf_specs):
             _low, _high = self._make_low_high(low, high, leaf_spec)
             key = out_key if isinstance(out_key, str) else "_".join(out_key)
             self.register_buffer(f"{key}_low", _low)
             self.register_buffer(f"{key}_high", _high)
             self.non_trivial[out_key] = (_high != 1).any() or (_low != -1).any()
```

## torchrl/modules/tensordict_module/exploration.py

```diff
@@ -7,19 +7,15 @@
 
 import numpy as np
 import torch
 from tensordict.nn import TensorDictModule, TensorDictModuleWrapper
 from tensordict.tensordict import TensorDictBase
 from tensordict.utils import expand_as_right
 
-from torchrl.data.tensor_specs import (
-    CompositeSpec,
-    TensorSpec,
-    UnboundedContinuousTensorSpec,
-)
+from torchrl.data.tensor_specs import CompositeSpec, TensorSpec
 from torchrl.envs.utils import exploration_type, ExplorationType
 from torchrl.modules.tensordict_module.common import _forward_hook_safe_action
 
 __all__ = [
     "EGreedyWrapper",
     "AdditiveGaussianWrapper",
     "OrnsteinUhlenbeckProcessWrapper",
@@ -418,36 +414,32 @@
             )
         self.annealing_num_steps = annealing_num_steps
         self.register_buffer("eps", torch.tensor([eps_init]))
         self.out_keys = list(self.td_module.out_keys) + self.ou.out_keys
         noise_key = self.ou.noise_key
         steps_key = self.ou.steps_key
 
-        ou_specs = {
-            noise_key: None,
-            steps_key: UnboundedContinuousTensorSpec(
-                shape=(*self.td_module._spec.shape, 1),
-                device=self.td_module._spec.device,
-                dtype=torch.int64,
-            ),
-        }
         if spec is not None:
             if not isinstance(spec, CompositeSpec) and len(self.out_keys) >= 1:
                 spec = CompositeSpec({action_key: spec}, shape=spec.shape[:-1])
             self._spec = spec
         elif hasattr(self.td_module, "_spec"):
             self._spec = self.td_module._spec.clone()
             if action_key not in self._spec.keys():
                 self._spec[action_key] = None
         elif hasattr(self.td_module, "spec"):
             self._spec = self.td_module.spec.clone()
             if action_key not in self._spec.keys():
                 self._spec[action_key] = None
         else:
             self._spec = CompositeSpec({key: None for key in policy.out_keys})
+        ou_specs = {
+            noise_key: None,
+            steps_key: None,
+        }
         self._spec.update(ou_specs)
         if len(set(self.out_keys)) != len(self.out_keys):
             raise RuntimeError(f"Got multiple identical output keys: {self.out_keys}")
         self.safe = safe
         if self.safe:
             self.register_forward_hook(_forward_hook_safe_action)
```

## torchrl/objectives/sac.py

```diff
@@ -448,15 +448,15 @@
             return target_value
 
     def _loss_qvalue_v2(self, tensordict: TensorDictBase) -> Tuple[Tensor, Tensor]:
         # we pass the alpha value to the tensordict. Since it's a scalar, we must erase the batch-size first.
         target_value = self._get_value_v2(
             tensordict,
             self._alpha,
-            self.target_actor_network_params,
+            self.actor_network_params,
             self.target_qvalue_network_params,
         )
 
         tensordict_expand = vmap(self.qvalue_network, (None, 0))(
             tensordict.select(*self.qvalue_network.in_keys),
             self.qvalue_network_params,
         )
```

## torchrl/objectives/td3.py

```diff
@@ -1,19 +1,21 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 import warnings
 from dataclasses import dataclass
+from typing import Optional, Tuple
 
 import torch
 from tensordict.nn import TensorDictModule
 
 from tensordict.tensordict import TensorDict, TensorDictBase
 from tensordict.utils import NestedKey
+from torchrl.data import BoundedTensorSpec, CompositeSpec, TensorSpec
 
 from torchrl.envs.utils import step_mdp
 from torchrl.objectives.common import LossModule
 from torchrl.objectives.utils import (
     _GAMMA_LMBDA_DEPREC_WARNING,
     default_value_kwargs,
     distance_loss,
@@ -37,14 +39,21 @@
 class TD3Loss(LossModule):
     """TD3 Loss module.
 
     Args:
         actor_network (TensorDictModule): the actor to be trained
         qvalue_network (TensorDictModule): a single Q-value network that will
             be multiplicated as many times as needed.
+
+    Keyword Args:
+        bounds (tuple of float, optional): the bounds of the action space.
+            Exclusive with action_spec. Either this or ``action_spec`` must
+            be provided.
+        action_spec (TensorSpec, optional): the action spec.
+            Exclusive with bounds. Either this or ``bounds`` must be provided.
         num_qvalue_nets (int, optional): Number of Q-value networks to be
             trained. Default is ``10``.
         policy_noise (float, optional): Standard deviation for the target
             policy action noise. Default is ``0.2``.
         noise_clip (float, optional): Clipping range value for the sampled
             target policy action noise. Default is ``0.5``.
         priority_key (str, optional): Key where to write the priority value
@@ -85,14 +94,16 @@
     default_value_estimator = ValueEstimators.TD0
 
     def __init__(
         self,
         actor_network: TensorDictModule,
         qvalue_network: TensorDictModule,
         *,
+        action_spec: TensorSpec = None,
+        bounds: Optional[Tuple[float]] = None,
         num_qvalue_nets: int = 2,
         policy_noise: float = 0.2,
         noise_clip: float = 0.5,
         loss_function: str = "smooth_l1",
         delay_actor: bool = True,
         delay_qvalue: bool = True,
         gamma: float = None,
@@ -119,21 +130,50 @@
             qvalue_network,
             "qvalue_network",
             num_qvalue_nets,
             create_target_params=self.delay_qvalue,
             compare_against=list(actor_network.parameters()),
         )
 
+        for p in self.parameters():
+            device = p.device
+            break
+        else:
+            device = None
         self.num_qvalue_nets = num_qvalue_nets
         self.loss_function = loss_function
         self.policy_noise = policy_noise
         self.noise_clip = noise_clip
-        self.max_action = (
-            actor_network.spec[self.tensor_keys.action].space.maximum.max().item()
-        )
+        if not ((action_spec is not None) ^ (bounds is not None)):
+            raise ValueError(
+                "One of 'bounds' and 'action_spec' must be provided, "
+                f"but not both. Got bounds={bounds} and action_spec={action_spec}."
+            )
+        elif action_spec is not None:
+            if isinstance(action_spec, CompositeSpec):
+                action_spec = action_spec[self.tensor_keys.action]
+            if not isinstance(action_spec, BoundedTensorSpec):
+                raise ValueError(
+                    f"action_spec is not of type BoundedTensorSpec but {type(action_spec)}."
+                )
+            low = action_spec.space.minimum
+            high = action_spec.space.maximum
+        else:
+            low, high = bounds
+        if not isinstance(low, torch.Tensor):
+            low = torch.tensor(low)
+        if not isinstance(high, torch.Tensor):
+            high = torch.tensor(high, device=low.device, dtype=low.dtype)
+        if (low > high).any():
+            raise ValueError("Got a low bound higher than a high bound.")
+        if device is not None:
+            low = low.to(device)
+            high = high.to(device)
+        self.register_buffer("max_action", high)
+        self.register_buffer("min_action", low)
         if gamma is not None:
             warnings.warn(_GAMMA_LMBDA_DEPREC_WARNING, category=DeprecationWarning)
             self.gamma = gamma
 
     def _forward_value_estimator_keys(self, **kwargs) -> None:
         if self._value_estimator is not None:
             self._value_estimator.set_keys(
@@ -167,15 +207,15 @@
         noise = torch.normal(
             mean=torch.zeros(action.shape),
             std=torch.full(action.shape, self.policy_noise),
         ).to(action.device)
         noise = noise.clamp(-self.noise_clip, self.noise_clip)
 
         next_action = (actor_output_td[1][self.tensor_keys.action] + noise).clamp(
-            -self.max_action, self.max_action
+            self.min_action, self.max_action
         )
         actor_output_td[1].set(self.tensor_keys.action, next_action)
         tensordict_actor.set(
             self.tensor_keys.action,
             actor_output_td.get(self.tensor_keys.action),
         )
```

## torchrl/trainers/helpers/__init__.py

```diff
@@ -12,27 +12,11 @@
 from .envs import (
     correct_for_frame_skip,
     get_stats_random_rollout,
     parallel_env_constructor,
     transformed_env_constructor,
 )
 from .logger import LoggerConfig
-from .losses import (
-    make_a2c_loss,
-    make_ddpg_loss,
-    make_dqn_loss,
-    make_ppo_loss,
-    make_redq_loss,
-    make_sac_loss,
-    make_target_updater,
-)
-from .models import (
-    make_a2c_model,
-    make_ddpg_actor,
-    make_dqn_actor,
-    make_dreamer,
-    make_ppo_model,
-    make_redq_model,
-    make_sac_model,
-)
+from .losses import make_dqn_loss, make_redq_loss, make_target_updater
+from .models import make_dqn_actor, make_dreamer, make_redq_model
 from .replay_buffer import make_replay_buffer
 from .trainers import make_trainer
```

## torchrl/trainers/helpers/losses.py

```diff
@@ -3,31 +3,18 @@
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
 from dataclasses import dataclass
 from typing import Any, Optional, Tuple
 
 from torchrl.modules import ActorCriticOperator, ActorValueOperator
-from torchrl.objectives import (
-    A2CLoss,
-    ClipPPOLoss,
-    DDPGLoss,
-    DistributionalDQNLoss,
-    DQNLoss,
-    HardUpdate,
-    KLPENPPOLoss,
-    PPOLoss,
-    SACLoss,
-    SoftUpdate,
-)
+from torchrl.objectives import DistributionalDQNLoss, DQNLoss, HardUpdate, SoftUpdate
 from torchrl.objectives.common import LossModule
 from torchrl.objectives.deprecated import REDQLoss_deprecated
 
-# from torchrl.objectives.redq import REDQLoss
-
 from torchrl.objectives.utils import TargetNetUpdater
 
 
 def make_target_updater(
     cfg: "DictConfig", loss_module: LossModule  # noqa: F821
 ) -> Optional[TargetNetUpdater]:
     """Builds a target network weight update object."""
@@ -47,67 +34,14 @@
                 "hard/soft-update are supposed to be used with double SAC loss. "
                 "Consider using --loss=double or discarding the hard_update flag."
             )
         target_net_updater = None
     return target_net_updater
 
 
-def make_sac_loss(model, cfg) -> Tuple[SACLoss, Optional[TargetNetUpdater]]:
-    """Builds the SAC loss module."""
-    loss_kwargs = {}
-    if hasattr(cfg, "distributional") and cfg.distributional:
-        raise NotImplementedError
-    else:
-        loss_kwargs.update({"loss_function": cfg.loss_function})
-        loss_kwargs.update(
-            {
-                "target_entropy": cfg.target_entropy
-                if cfg.target_entropy is not None
-                else "auto"
-            }
-        )
-        loss_class = SACLoss
-        if cfg.loss == "double":
-            loss_kwargs.update(
-                {
-                    "delay_actor": False,
-                    "delay_qvalue": True,
-                    "delay_value": True,
-                }
-            )
-        elif cfg.loss == "single":
-            loss_kwargs.update(
-                {
-                    "delay_actor": False,
-                    "delay_qvalue": False,
-                    "delay_value": False,
-                }
-            )
-        else:
-            raise NotImplementedError(
-                f"cfg.loss {cfg.loss} unsupported. Consider chosing from 'double' or 'single'"
-            )
-
-    actor_model, qvalue_model, value_model = model
-
-    loss_module = loss_class(
-        actor_network=actor_model,
-        qvalue_network=qvalue_model,
-        value_network=value_model,
-        num_qvalue_nets=cfg.num_q_values,
-        **loss_kwargs,
-    )
-    loss_module.make_value_estimator(gamma=cfg.gamma)
-    if cfg.loss == "double":
-        target_net_updater = make_target_updater(cfg, loss_module)
-    else:
-        target_net_updater = None
-    return loss_module, target_net_updater
-
-
 def make_redq_loss(
     model, cfg
 ) -> Tuple[REDQLoss_deprecated, Optional[TargetNetUpdater]]:
     """Builds the REDQ loss module."""
     loss_kwargs = {}
     if hasattr(cfg, "distributional") and cfg.distributional:
         raise NotImplementedError
@@ -136,33 +70,14 @@
         **loss_kwargs,
     )
     loss_module.make_value_estimator(gamma=cfg.gamma)
     target_net_updater = make_target_updater(cfg, loss_module)
     return loss_module, target_net_updater
 
 
-def make_ddpg_loss(model, cfg) -> Tuple[DDPGLoss, Optional[TargetNetUpdater]]:
-    """Builds the DDPG loss module."""
-    actor, value_net = model
-    loss_kwargs = {}
-    if cfg.distributional:
-        raise NotImplementedError
-    else:
-        loss_kwargs.update({"loss_function": cfg.loss_function})
-        loss_class = DDPGLoss
-    if cfg.loss not in ("single", "double"):
-        raise NotImplementedError
-    double_loss = cfg.loss == "double"
-    loss_kwargs.update({"delay_actor": double_loss, "delay_value": double_loss})
-    loss_module = loss_class(actor, value_net, **loss_kwargs)
-    loss_module.make_value_estimator(gamma=cfg.gamma)
-    target_net_updater = make_target_updater(cfg, loss_module)
-    return loss_module, target_net_updater
-
-
 def make_dqn_loss(model, cfg) -> Tuple[DQNLoss, Optional[TargetNetUpdater]]:
     """Builds the DQN loss module."""
     loss_kwargs = {}
     if cfg.distributional:
         loss_class = DistributionalDQNLoss
     else:
         loss_kwargs.update({"loss_function": cfg.loss_function})
@@ -172,70 +87,14 @@
     loss_kwargs.update({"delay_value": cfg.loss == "double"})
     loss_module = loss_class(model, **loss_kwargs)
     loss_module.make_value_estimator(gamma=cfg.gamma)
     target_net_updater = make_target_updater(cfg, loss_module)
     return loss_module, target_net_updater
 
 
-def make_a2c_loss(model, cfg) -> A2CLoss:
-    """Builds the A2C loss module."""
-    actor_model = model.get_policy_operator()
-    critic_model = model.get_value_operator()
-
-    kwargs = {
-        "actor": actor_model,
-        "critic": critic_model,
-        "loss_critic_type": cfg.critic_loss_function,
-        "entropy_coef": cfg.entropy_coef,
-    }
-
-    loss_module = A2CLoss(**kwargs)
-
-    return loss_module
-
-
-def make_ppo_loss(model, cfg) -> PPOLoss:
-    """Builds the PPO loss module."""
-    loss_dict = {
-        "clip": ClipPPOLoss,
-        "kl": KLPENPPOLoss,
-        "base": PPOLoss,
-        "": PPOLoss,
-    }
-    actor_model = model.get_policy_operator()
-    critic_model = model.get_value_operator()
-
-    kwargs = {
-        "actor": actor_model,
-        "critic": critic_model,
-        "loss_critic_type": cfg.loss_function,
-        "entropy_coef": cfg.entropy_coef,
-    }
-
-    if cfg.loss == "clip":
-        kwargs.update(
-            {
-                "clip_epsilon": cfg.clip_epsilon,
-            }
-        )
-    elif cfg.loss == "kl":
-        kwargs.update(
-            {
-                "dtarg": cfg.dtarg,
-                "beta": cfg.beta,
-                "increment": cfg.increment,
-                "decrement": cfg.decrement,
-                "samples_mc_kl": cfg.samples_mc_kl,
-            }
-        )
-
-    loss_module = loss_dict[cfg.loss](**kwargs)
-    return loss_module
-
-
 @dataclass
 class LossConfig:
     """Generic Loss config struct."""
 
     loss: str = "double"
     # whether double or single SAC loss should be used. Default=double
     hard_update: bool = False
```

## torchrl/trainers/helpers/models.py

```diff
@@ -18,60 +18,50 @@
 )
 from torchrl.data.utils import DEVICE_TYPING
 from torchrl.envs import TensorDictPrimer, TransformedEnv
 from torchrl.envs.common import EnvBase
 from torchrl.envs.model_based.dreamer import DreamerEnv
 from torchrl.envs.utils import ExplorationType, set_exploration_type
 from torchrl.modules import (
-    ActorValueOperator,
     NoisyLinear,
     NormalParamWrapper,
     SafeModule,
     SafeProbabilisticModule,
     SafeProbabilisticTensorDictSequential,
     SafeSequential,
 )
 from torchrl.modules.distributions import (
     Delta,
     OneHotCategorical,
     TanhDelta,
     TanhNormal,
-    TruncatedNormal,
 )
 from torchrl.modules.distributions.continuous import SafeTanhTransform
 from torchrl.modules.models.exploration import LazygSDEModule
 from torchrl.modules.models.model_based import (
     DreamerActor,
     ObsDecoder,
     ObsEncoder,
     RSSMPosterior,
     RSSMPrior,
     RSSMRollout,
 )
 from torchrl.modules.models.models import (
-    ConvNet,
     DdpgCnnActor,
     DdpgCnnQNet,
-    DdpgMlpActor,
-    DdpgMlpQNet,
     DuelingCnnDQNet,
     DuelingMlpDQNet,
-    LSTMNet,
     MLP,
 )
 from torchrl.modules.tensordict_module import (
     Actor,
     DistributionalQValueActor,
     QValueActor,
 )
-from torchrl.modules.tensordict_module.actors import (
-    ActorCriticWrapper,
-    ProbabilisticActor,
-    ValueOperator,
-)
+from torchrl.modules.tensordict_module.actors import ProbabilisticActor, ValueOperator
 from torchrl.modules.tensordict_module.world_models import WorldModelWrapper
 from torchrl.trainers.helpers import transformed_env_constructor
 
 DISTRIBUTIONS = {
     "delta": Delta,
     "tanh-normal": TanhNormal,
     "categorical": OneHotCategorical,
@@ -211,1023 +201,14 @@
     # init
     with torch.no_grad():
         td = proof_environment.rollout(max_steps=1000)
         model(td.to(device))
     return model
 
 
-def make_ddpg_actor(
-    proof_environment: EnvBase,
-    cfg: "DictConfig",  # noqa: F821
-    actor_net_kwargs: Optional[dict] = None,
-    value_net_kwargs: Optional[dict] = None,
-    device: DEVICE_TYPING = "cpu",
-) -> torch.nn.ModuleList:
-    """DDPG constructor helper function.
-
-    Args:
-        proof_environment (EnvBase): a dummy environment to retrieve the observation and action spec
-        cfg (DictConfig): contains arguments of the DDPG script
-        actor_net_kwargs (dict, optional): kwargs to be used for the policy network (either DdpgCnnActor or
-            DdpgMlpActor).
-        value_net_kwargs (dict, optional): kwargs to be used for the policy network (either DdpgCnnQNet or
-            DdpgMlpQNet).
-        device (torch.device, optional): device on which the model must be cast. Default is "cpu".
-
-    Returns:
-         An actor and a value operators for DDPG.
-
-    For more details on DDPG, refer to "CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING",
-    https://arxiv.org/pdf/1509.02971.pdf.
-
-    Examples:
-        >>> from torchrl.trainers.helpers.models import make_ddpg_actor
-        >>> from torchrl.envs.libs.gym import GymEnv
-        >>> from torchrl.envs.transforms import CatTensors, TransformedEnv, DoubleToFloat, Compose
-        >>> import hydra
-        >>> from hydra.core.config_store import ConfigStore
-        >>> import dataclasses
-        >>> proof_environment = TransformedEnv(GymEnv("HalfCheetah-v2"), Compose(DoubleToFloat(["observation"]),
-        ...    CatTensors(["observation"], "observation_vector")))
-        >>> device = torch.device("cpu")
-        >>> config_fields = [(config_field.name, config_field.type, config_field) for config_cls in
-        ...                    (DDPGModelConfig, EnvConfig)
-        ...                   for config_field in dataclasses.fields(config_cls)]
-        >>> Config = dataclasses.make_dataclass(cls_name="Config", fields=config_fields)
-        >>> cs = ConfigStore.instance()
-        >>> cs.store(name="config", node=Config)
-        >>> with initialize(config_path=None):
-        >>>     cfg = compose(config_name="config")
-        >>> actor, value = make_ddpg_actor(
-        ...     proof_environment,
-        ...     device=device,
-        ...     cfg=cfg)
-        >>> td = proof_environment.reset()
-        >>> print(actor(td))
-        TensorDict(
-            fields={
-                done: Tensor(torch.Size([1]), dtype=torch.bool),
-                observation_vector: Tensor(torch.Size([17]), dtype=torch.float32),
-                param: Tensor(torch.Size([6]), dtype=torch.float32),
-                action: Tensor(torch.Size([6]), dtype=torch.float32)},
-            batch_size=torch.Size([]),
-            device=cpu,
-            is_shared=False)
-        >>> print(value(td))
-        TensorDict(
-            fields={
-                done: Tensor(torch.Size([1]), dtype=torch.bool),
-                observation_vector: Tensor(torch.Size([17]), dtype=torch.float32),
-                param: Tensor(torch.Size([6]), dtype=torch.float32),
-                action: Tensor(torch.Size([6]), dtype=torch.float32),
-                state_action_value: Tensor(torch.Size([1]), dtype=torch.float32)},
-            batch_size=torch.Size([]),
-            device=cpu,
-            is_shared=False)
-    """
-    # TODO: https://arxiv.org/pdf/1804.08617.pdf
-
-    from_pixels = cfg.from_pixels
-    noisy = cfg.noisy
-
-    actor_net_kwargs = actor_net_kwargs if actor_net_kwargs is not None else {}
-    value_net_kwargs = value_net_kwargs if value_net_kwargs is not None else {}
-
-    linear_layer_class = torch.nn.Linear if not noisy else NoisyLinear
-
-    env_specs = proof_environment.specs
-    out_features = env_specs["input_spec", "_action_spec", "action"].shape[0]
-
-    actor_net_default_kwargs = {
-        "action_dim": out_features,
-        "mlp_net_kwargs": {
-            "layer_class": linear_layer_class,
-            "activation_class": ACTIVATIONS[cfg.activation],
-        },
-    }
-    actor_net_default_kwargs.update(actor_net_kwargs)
-    if from_pixels:
-        in_keys = ["pixels"]
-        actor_net_default_kwargs["conv_net_kwargs"] = {
-            "activation_class": ACTIVATIONS[cfg.activation]
-        }
-        actor_net = DdpgCnnActor(**actor_net_default_kwargs)
-        gSDE_state_key = "hidden"
-        out_keys = ["param", "hidden"]
-    else:
-        in_keys = ["observation_vector"]
-        actor_net = DdpgMlpActor(**actor_net_default_kwargs)
-        gSDE_state_key = "observation_vector"
-        out_keys = ["param"]
-    actor_module = SafeModule(actor_net, in_keys=in_keys, out_keys=out_keys)
-
-    if cfg.gSDE:
-        min = env_specs["input_spec", "_action_spec", "action"].space.minimum
-        max = env_specs["input_spec", "_action_spec", "action"].space.maximum
-        transform = SafeTanhTransform()
-        if (min != -1).any() or (max != 1).any():
-            transform = d.ComposeTransform(
-                transform, d.AffineTransform(loc=(max + min) / 2, scale=(max - min) / 2)
-            )
-        actor_module = SafeSequential(
-            actor_module,
-            SafeModule(
-                LazygSDEModule(transform=transform, learn_sigma=False),
-                in_keys=["param", gSDE_state_key, "_eps_gSDE"],
-                out_keys=["loc", "scale", "action", "_eps_gSDE"],
-            ),
-        )
-
-    # We use a ProbabilisticActor to make sure that we map the network output to the right space using a TanhDelta
-    # distribution.
-    actor = ProbabilisticActor(
-        module=actor_module,
-        in_keys=["param"],
-        spec=CompositeSpec(action=env_specs["input_spec", "_action_spec", "action"]),
-        safe=True,
-        distribution_class=TanhDelta,
-        distribution_kwargs={
-            "min": env_specs["input_spec", "_action_spec", "action"].space.minimum,
-            "max": env_specs["input_spec", "_action_spec", "action"].space.maximum,
-        },
-    )
-
-    state_class = ValueOperator
-    if from_pixels:
-        value_net_default_kwargs = {
-            "mlp_net_kwargs": {
-                "layer_class": linear_layer_class,
-                "activation_class": ACTIVATIONS[cfg.activation],
-            }
-        }
-        value_net_default_kwargs.update(value_net_kwargs)
-
-        in_keys = ["pixels", "action"]
-        out_keys = ["state_action_value"]
-        q_net = DdpgCnnQNet(**value_net_default_kwargs)
-    else:
-        value_net_default_kwargs1 = {"activation_class": ACTIVATIONS[cfg.activation]}
-        value_net_default_kwargs1.update(
-            value_net_kwargs.get(
-                "mlp_net_kwargs_net1",
-                {
-                    "layer_class": linear_layer_class,
-                    "activation_class": ACTIVATIONS[cfg.activation],
-                    "bias_last_layer": True,
-                },
-            )
-        )
-        value_net_default_kwargs2 = {
-            "num_cells": [400, 300],
-            "activation_class": ACTIVATIONS[cfg.activation],
-            "bias_last_layer": True,
-        }
-        value_net_default_kwargs2.update(
-            value_net_kwargs.get(
-                "mlp_net_kwargs_net2",
-                {
-                    "layer_class": linear_layer_class,
-                },
-            )
-        )
-        in_keys = ["observation_vector", "action"]
-        out_keys = ["state_action_value"]
-        q_net = DdpgMlpQNet(
-            mlp_net_kwargs_net1=value_net_default_kwargs1,
-            mlp_net_kwargs_net2=value_net_default_kwargs2,
-        )
-
-    value = state_class(
-        in_keys=in_keys,
-        out_keys=out_keys,
-        module=q_net,
-    )
-
-    module = torch.nn.ModuleList([actor, value]).to(device)
-
-    # init
-    with torch.no_grad(), set_exploration_type(ExplorationType.RANDOM):
-        td = proof_environment.rollout(max_steps=1000)
-        td = td.to(device)
-        module[0](td)
-        module[1](td)
-
-    return module
-
-
-def make_a2c_model(
-    proof_environment: EnvBase,
-    cfg: "DictConfig",  # noqa: F821
-    device: DEVICE_TYPING,
-    in_keys_actor: Optional[Sequence[str]] = None,
-    observation_key=None,
-    **kwargs,
-) -> ActorValueOperator:
-    """Actor-value model constructor helper function.
-
-    Currently constructs MLP networks with immutable default arguments as described in "Proximal Policy Optimization
-    Algorithms", https://arxiv.org/abs/1707.06347
-    Other configurations can easily be implemented by modifying this function at will.
-
-    Args:
-        proof_environment (EnvBase): a dummy environment to retrieve the observation and action spec
-        cfg (DictConfig): contains arguments of the PPO script
-        device (torch.device): device on which the model must be cast.
-        in_keys_actor (iterable of strings, optional): observation key to be read by the actor, usually one of
-            `'observation_vector'` or `'pixels'`. If none is provided, one of these two keys is chosen based on
-            the `cfg.from_pixels` argument.
-
-    Returns:
-         A joined ActorCriticOperator.
-
-    Examples:
-        >>> from torchrl.trainers.helpers.envs import parser_env_args
-        >>> from torchrl.trainers.helpers.models import make_ppo_model, parser_model_args_continuous
-        >>> from torchrl.envs.libs.gym import GymEnv
-        >>> from torchrl.envs.transforms import CatTensors, TransformedEnv, DoubleToFloat, Compose
-        >>> import hydra
-        >>> from hydra.core.config_store import ConfigStore
-        >>> import dataclasses
-        >>> proof_environment = TransformedEnv(GymEnv("HalfCheetah-v2"), Compose(DoubleToFloat(["observation"]),
-        ...    CatTensors(["observation"], "observation_vector")))
-        >>> device = torch.device("cpu")
-        >>> config_fields = [(config_field.name, config_field.type, config_field) for config_cls in
-        ...                    (PPOModelConfig, EnvConfig)
-        ...                   for config_field in dataclasses.fields(config_cls)]
-        >>> Config = dataclasses.make_dataclass(cls_name="Config", fields=config_fields)
-        >>> cs = ConfigStore.instance()
-        >>> cs.store(name="config", node=Config)
-        >>> with initialize(config_path=None):
-        >>>     cfg = compose(config_name="config")
-        >>> actor_value = make_ppo_model(
-        ...     proof_environment,
-        ...     device=device,
-        ...     cfg=cfg,
-        ...     )
-        >>> actor = actor_value.get_policy_operator()
-        >>> value = actor_value.get_value_operator()
-        >>> td = proof_environment.reset()
-        >>> print(actor(td.clone()))
-        TensorDict(
-            fields={
-                done: Tensor(torch.Size([1]), dtype=torch.bool),
-                observation_vector: Tensor(torch.Size([17]), dtype=torch.float32),
-                hidden: Tensor(torch.Size([300]), dtype=torch.float32),
-                loc: Tensor(torch.Size([6]), dtype=torch.float32),
-                scale: Tensor(torch.Size([6]), dtype=torch.float32),
-                action: Tensor(torch.Size([6]), dtype=torch.float32),
-                sample_log_prob: Tensor(torch.Size([1]), dtype=torch.float32)},
-            batch_size=torch.Size([]),
-            device=cpu,
-            is_shared=False)
-        >>> print(value(td.clone()))
-        TensorDict(
-            fields={
-                done: Tensor(torch.Size([1]), dtype=torch.bool),
-                observation_vector: Tensor(torch.Size([17]), dtype=torch.float32),
-                hidden: Tensor(torch.Size([300]), dtype=torch.float32),
-                state_value: Tensor(torch.Size([1]), dtype=torch.float32)},
-            batch_size=torch.Size([]),
-            device=cpu,
-            is_shared=False)
-
-    """
-    # proof_environment.set_seed(cfg.seed)
-    specs = proof_environment.specs  # TODO: use env.sepcs
-    action_spec = specs["input_spec", "_action_spec", "action"]
-
-    if in_keys_actor is None and proof_environment.from_pixels:
-        in_keys_actor = ["pixels"]
-        in_keys_critic = ["pixels"]
-    elif in_keys_actor is None:
-        in_keys_actor = ["observation_vector"]
-        in_keys_critic = ["observation_vector"]
-    out_keys = ["action"]
-
-    if action_spec.domain == "continuous":
-        dist_in_keys = ["loc", "scale"]
-        out_features = (2 - cfg.gSDE) * action_spec.shape[-1]
-        if cfg.distribution == "tanh_normal":
-            policy_distribution_kwargs = {
-                "min": action_spec.space.minimum,
-                "max": action_spec.space.maximum,
-                "tanh_loc": cfg.tanh_loc,
-            }
-            policy_distribution_class = TanhNormal
-        elif cfg.distribution == "truncated_normal":
-            policy_distribution_kwargs = {
-                "min": action_spec.space.minimum,
-                "max": action_spec.space.maximum,
-                "tanh_loc": cfg.tanh_loc,
-            }
-            policy_distribution_class = TruncatedNormal
-    elif action_spec.domain == "discrete":
-        out_features = action_spec.shape[-1]
-        policy_distribution_kwargs = {}
-        policy_distribution_class = OneHotCategorical
-        dist_in_keys = ["logits"]
-    else:
-        raise NotImplementedError(
-            f"actions with domain {action_spec.domain} are not supported"
-        )
-
-    if cfg.shared_mapping:
-        hidden_features = 300
-        if proof_environment.from_pixels:
-            if in_keys_actor is None:
-                in_keys_actor = ["pixels"]
-            common_module = ConvNet(
-                bias_last_layer=True,
-                depth=None,
-                num_cells=[32, 64, 64],
-                kernel_sizes=[8, 4, 3],
-                strides=[4, 2, 1],
-            )
-        else:
-            if cfg.lstm:
-                raise NotImplementedError(
-                    "lstm not yet compatible with shared mapping for A2C"
-                )
-            common_module = MLP(
-                num_cells=[
-                    64,
-                ],
-                out_features=hidden_features,
-                activate_last_layer=True,
-            )
-        common_operator = SafeModule(
-            spec=None,
-            module=common_module,
-            in_keys=in_keys_actor,
-            out_keys=["hidden"],
-        )
-
-        policy_net = MLP(
-            num_cells=[64],
-            out_features=out_features,
-        )
-
-        shared_out_keys = ["hidden"]
-        if not cfg.gSDE:
-            if action_spec.domain == "continuous":
-                policy_net = NormalParamWrapper(
-                    policy_net,
-                    scale_mapping=f"biased_softplus_{cfg.default_policy_scale}",
-                )
-            actor_module = SafeModule(
-                policy_net, in_keys=shared_out_keys, out_keys=dist_in_keys
-            )
-        else:
-            gSDE_state_key = "hidden"
-            actor_module = SafeModule(
-                policy_net,
-                in_keys=shared_out_keys,
-                out_keys=["action"],  # will be overwritten
-            )
-
-            if action_spec.domain == "continuous":
-                min = action_spec.space.minimum
-                max = action_spec.space.maximum
-                transform = SafeTanhTransform()
-                if (min != -1).any() or (max != 1).any():
-                    transform = d.ComposeTransform(
-                        transform,
-                        d.AffineTransform(loc=(max + min) / 2, scale=(max - min) / 2),
-                    )
-            else:
-                raise RuntimeError("cannot use gSDE with discrete actions")
-
-            actor_module = SafeSequential(
-                actor_module,
-                SafeModule(
-                    LazygSDEModule(transform=transform),
-                    in_keys=["action", gSDE_state_key, "_eps_gSD"],
-                    out_keys=["loc", "scale", "action", "_eps_gSDE"],
-                ),
-            )
-
-        policy_operator = ProbabilisticActor(
-            spec=CompositeSpec(action=action_spec),
-            module=actor_module,
-            in_keys=dist_in_keys,
-            default_interaction_type=InteractionType.RANDOM,
-            distribution_class=policy_distribution_class,
-            distribution_kwargs=policy_distribution_kwargs,
-            return_log_prob=True,
-        )
-        value_net = MLP(
-            num_cells=[64],
-            out_features=1,
-        )
-        value_operator = ValueOperator(value_net, in_keys=shared_out_keys)
-        actor_value = ActorValueOperator(
-            common_operator=common_operator,
-            policy_operator=policy_operator,
-            value_operator=value_operator,
-        ).to(device)
-    else:
-        if cfg.from_pixels:
-            raise RuntimeError(
-                "A2C learnt from pixels require the shared_mapping to be set to True."
-            )
-        if cfg.lstm:
-            policy_net = LSTMNet(
-                out_features=out_features,
-                lstm_kwargs={"input_size": 64, "hidden_size": 64},
-                mlp_kwargs={"num_cells": [64, 64], "out_features": 64},
-            )
-            in_keys_actor += ["hidden0", "hidden1"]
-            out_keys += ["hidden0", "hidden1", ("next", "hidden0"), ("next", "hidden1")]
-        else:
-            policy_net = MLP(
-                num_cells=[64, 64],
-                out_features=out_features,
-            )
-
-        if not cfg.gSDE:
-            if action_spec.domain == "continuous":
-                policy_net = NormalParamWrapper(
-                    policy_net,
-                    scale_mapping=f"biased_softplus_{cfg.default_policy_scale}",
-                )
-            actor_module = SafeModule(
-                policy_net, in_keys=in_keys_actor, out_keys=dist_in_keys
-            )
-        else:
-            in_keys = in_keys_actor
-            gSDE_state_key = in_keys_actor[0]
-            actor_module = SafeModule(
-                policy_net,
-                in_keys=in_keys,
-                out_keys=["action"],  # will be overwritten
-            )
-
-            if action_spec.domain == "continuous":
-                min = action_spec.space.minimum
-                max = action_spec.space.maximum
-                transform = SafeTanhTransform()
-                if (min != -1).any() or (max != 1).any():
-                    transform = d.ComposeTransform(
-                        transform,
-                        d.AffineTransform(loc=(max + min) / 2, scale=(max - min) / 2),
-                    )
-            else:
-                raise RuntimeError("cannot use gSDE with discrete actions")
-
-            actor_module = SafeSequential(
-                actor_module,
-                SafeModule(
-                    LazygSDEModule(transform=transform),
-                    in_keys=["action", gSDE_state_key, "_eps_gSDE"],
-                    out_keys=["loc", "scale", "action", "_eps_gSDE"],
-                ),
-            )
-
-        policy_po = ProbabilisticActor(
-            actor_module,
-            spec=action_spec,
-            in_keys=dist_in_keys,
-            distribution_class=policy_distribution_class,
-            distribution_kwargs=policy_distribution_kwargs,
-            return_log_prob=True,
-            default_interaction_type=InteractionType.RANDOM,
-        )
-
-        value_net = MLP(
-            num_cells=[64, 64],
-            out_features=1,
-        )
-        value_po = ValueOperator(
-            value_net,
-            in_keys=in_keys_critic,
-        )
-        actor_value = ActorCriticWrapper(policy_po, value_po).to(device)
-
-    with torch.no_grad(), set_exploration_type(ExplorationType.RANDOM):
-        td = proof_environment.rollout(max_steps=1000)
-        td_device = td.to(device)
-        td_device = actor_value(td_device)  # for init
-    return actor_value
-
-
-def make_ppo_model(
-    proof_environment: EnvBase,
-    cfg: "DictConfig",  # noqa: F821
-    device: DEVICE_TYPING,
-    in_keys_actor: Optional[Sequence[str]] = None,
-    observation_key=None,
-    **kwargs,
-) -> ActorValueOperator:
-    """Actor-value model constructor helper function.
-
-    Currently constructs MLP networks with immutable default arguments as described in "Proximal Policy Optimization
-    Algorithms", https://arxiv.org/abs/1707.06347
-    Other configurations can easily be implemented by modifying this function at will.
-
-    Args:
-        proof_environment (EnvBase): a dummy environment to retrieve the observation and action spec
-        cfg (DictConfig): contains arguments of the PPO script
-        device (torch.device): device on which the model must be cast.
-        in_keys_actor (iterable of strings, optional): observation key to be read by the actor, usually one of
-            `'observation_vector'` or `'pixels'`. If none is provided, one of these two keys is chosen based on
-            the `cfg.from_pixels` argument.
-
-    Returns:
-         A joined ActorCriticOperator.
-
-    Examples:
-        >>> from torchrl.trainers.helpers.envs import parser_env_args
-        >>> from torchrl.trainers.helpers.models import make_ppo_model, parser_model_args_continuous
-        >>> from torchrl.envs.libs.gym import GymEnv
-        >>> from torchrl.envs.transforms import CatTensors, TransformedEnv, DoubleToFloat, Compose
-        >>> import hydra
-        >>> from hydra.core.config_store import ConfigStore
-        >>> import dataclasses
-        >>> proof_environment = TransformedEnv(GymEnv("HalfCheetah-v2"), Compose(DoubleToFloat(["observation"]),
-        ...    CatTensors(["observation"], "observation_vector")))
-        >>> device = torch.device("cpu")
-        >>> config_fields = [(config_field.name, config_field.type, config_field) for config_cls in
-        ...                    (PPOModelConfig, EnvConfig)
-        ...                   for config_field in dataclasses.fields(config_cls)]
-        >>> Config = dataclasses.make_dataclass(cls_name="Config", fields=config_fields)
-        >>> cs = ConfigStore.instance()
-        >>> cs.store(name="config", node=Config)
-        >>> with initialize(config_path=None):
-        >>>     cfg = compose(config_name="config")
-        >>> actor_value = make_ppo_model(
-        ...     proof_environment,
-        ...     device=device,
-        ...     cfg=cfg,
-        ...     )
-        >>> actor = actor_value.get_policy_operator()
-        >>> value = actor_value.get_value_operator()
-        >>> td = proof_environment.reset()
-        >>> print(actor(td.clone()))
-        TensorDict(
-            fields={
-                done: Tensor(torch.Size([1]), dtype=torch.bool),
-                observation_vector: Tensor(torch.Size([17]), dtype=torch.float32),
-                hidden: Tensor(torch.Size([300]), dtype=torch.float32),
-                loc: Tensor(torch.Size([6]), dtype=torch.float32),
-                scale: Tensor(torch.Size([6]), dtype=torch.float32),
-                action: Tensor(torch.Size([6]), dtype=torch.float32),
-                sample_log_prob: Tensor(torch.Size([1]), dtype=torch.float32)},
-            batch_size=torch.Size([]),
-            device=cpu,
-            is_shared=False)
-        >>> print(value(td.clone()))
-        TensorDict(
-            fields={
-                done: Tensor(torch.Size([1]), dtype=torch.bool),
-                observation_vector: Tensor(torch.Size([17]), dtype=torch.float32),
-                hidden: Tensor(torch.Size([300]), dtype=torch.float32),
-                state_value: Tensor(torch.Size([1]), dtype=torch.float32)},
-            batch_size=torch.Size([]),
-            device=cpu,
-            is_shared=False)
-
-    """
-    # proof_environment.set_seed(cfg.seed)
-    specs = proof_environment.specs  # TODO: use env.sepcs
-    action_spec = specs["input_spec", "_action_spec", "action"]
-
-    if in_keys_actor is None and proof_environment.from_pixels:
-        in_keys_actor = ["pixels"]
-        in_keys_critic = ["pixels"]
-    elif in_keys_actor is None:
-        in_keys_actor = ["observation_vector"]
-        in_keys_critic = ["observation_vector"]
-    out_keys = ["action"]
-
-    if action_spec.domain == "continuous":
-        dist_in_keys = ["loc", "scale"]
-        out_features = (2 - cfg.gSDE) * action_spec.shape[-1]
-        if cfg.distribution == "tanh_normal":
-            policy_distribution_kwargs = {
-                "min": action_spec.space.minimum,
-                "max": action_spec.space.maximum,
-                "tanh_loc": cfg.tanh_loc,
-            }
-            policy_distribution_class = TanhNormal
-        elif cfg.distribution == "truncated_normal":
-            policy_distribution_kwargs = {
-                "min": action_spec.space.minimum,
-                "max": action_spec.space.maximum,
-                "tanh_loc": cfg.tanh_loc,
-            }
-            policy_distribution_class = TruncatedNormal
-    elif action_spec.domain == "discrete":
-        out_features = action_spec.shape[-1]
-        policy_distribution_kwargs = {}
-        policy_distribution_class = OneHotCategorical
-        dist_in_keys = ["logits"]
-    else:
-        raise NotImplementedError(
-            f"actions with domain {action_spec.domain} are not supported"
-        )
-
-    if cfg.shared_mapping:
-        hidden_features = 300
-        if proof_environment.from_pixels:
-            if in_keys_actor is None:
-                in_keys_actor = ["pixels"]
-            common_module = ConvNet(
-                bias_last_layer=True,
-                depth=None,
-                num_cells=[32, 64, 64],
-                kernel_sizes=[8, 4, 3],
-                strides=[4, 2, 1],
-            )
-        else:
-            if cfg.lstm:
-                raise NotImplementedError(
-                    "lstm not yet compatible with shared mapping for PPO"
-                )
-            common_module = MLP(
-                num_cells=[
-                    400,
-                ],
-                out_features=hidden_features,
-                activate_last_layer=True,
-            )
-        common_operator = SafeModule(
-            spec=None,
-            module=common_module,
-            in_keys=in_keys_actor,
-            out_keys=["hidden"],
-        )
-
-        policy_net = MLP(
-            num_cells=[200],
-            out_features=out_features,
-        )
-
-        shared_out_keys = ["hidden"]
-        if not cfg.gSDE:
-            if action_spec.domain == "continuous":
-                policy_net = NormalParamWrapper(
-                    policy_net,
-                    scale_mapping=f"biased_softplus_{cfg.default_policy_scale}",
-                )
-            actor_module = SafeModule(
-                policy_net, in_keys=shared_out_keys, out_keys=dist_in_keys
-            )
-        else:
-            gSDE_state_key = "hidden"
-            actor_module = SafeModule(
-                policy_net,
-                in_keys=shared_out_keys,
-                out_keys=["action"],  # will be overwritten
-            )
-
-            if action_spec.domain == "continuous":
-                min = action_spec.space.minimum
-                max = action_spec.space.maximum
-                transform = SafeTanhTransform()
-                if (min != -1).any() or (max != 1).any():
-                    transform = d.ComposeTransform(
-                        transform,
-                        d.AffineTransform(loc=(max + min) / 2, scale=(max - min) / 2),
-                    )
-            else:
-                raise RuntimeError("cannot use gSDE with discrete actions")
-
-            actor_module = SafeSequential(
-                actor_module,
-                SafeModule(
-                    LazygSDEModule(transform=transform),
-                    in_keys=["action", gSDE_state_key, "_eps_gSD"],
-                    out_keys=["loc", "scale", "action", "_eps_gSDE"],
-                ),
-            )
-
-        policy_operator = ProbabilisticActor(
-            spec=CompositeSpec(action=action_spec),
-            module=actor_module,
-            in_keys=dist_in_keys,
-            default_interaction_type=InteractionType.RANDOM,
-            distribution_class=policy_distribution_class,
-            distribution_kwargs=policy_distribution_kwargs,
-            return_log_prob=True,
-        )
-        value_net = MLP(
-            num_cells=[200],
-            out_features=1,
-        )
-        value_operator = ValueOperator(value_net, in_keys=shared_out_keys)
-        actor_value = ActorValueOperator(
-            common_operator=common_operator,
-            policy_operator=policy_operator,
-            value_operator=value_operator,
-        ).to(device)
-    else:
-        if cfg.from_pixels:
-            raise RuntimeError(
-                "PPO learnt from pixels require the shared_mapping to be set to True."
-            )
-        if cfg.lstm:
-            policy_net = LSTMNet(
-                out_features=out_features,
-                lstm_kwargs={"input_size": 256, "hidden_size": 256},
-                mlp_kwargs={"num_cells": [256, 256], "out_features": 256},
-            )
-            in_keys_actor += ["hidden0", "hidden1"]
-            out_keys += ["hidden0", "hidden1", ("next", "hidden0"), ("next", "hidden1")]
-        else:
-            policy_net = MLP(
-                num_cells=[400, 300],
-                out_features=out_features,
-            )
-
-        if not cfg.gSDE:
-            if action_spec.domain == "continuous":
-                policy_net = NormalParamWrapper(
-                    policy_net,
-                    scale_mapping=f"biased_softplus_{cfg.default_policy_scale}",
-                )
-            actor_module = SafeModule(
-                policy_net, in_keys=in_keys_actor, out_keys=dist_in_keys
-            )
-        else:
-            in_keys = in_keys_actor
-            gSDE_state_key = in_keys_actor[0]
-            actor_module = SafeModule(
-                policy_net,
-                in_keys=in_keys,
-                out_keys=["action"],  # will be overwritten
-            )
-
-            if action_spec.domain == "continuous":
-                min = action_spec.space.minimum
-                max = action_spec.space.maximum
-                transform = SafeTanhTransform()
-                if (min != -1).any() or (max != 1).any():
-                    transform = d.ComposeTransform(
-                        transform,
-                        d.AffineTransform(loc=(max + min) / 2, scale=(max - min) / 2),
-                    )
-            else:
-                raise RuntimeError("cannot use gSDE with discrete actions")
-
-            actor_module = SafeSequential(
-                actor_module,
-                SafeModule(
-                    LazygSDEModule(transform=transform),
-                    in_keys=["action", gSDE_state_key, "_eps_gSDE"],
-                    out_keys=["loc", "scale", "action", "_eps_gSDE"],
-                ),
-            )
-
-        policy_po = ProbabilisticActor(
-            actor_module,
-            spec=action_spec,
-            in_keys=dist_in_keys,
-            distribution_class=policy_distribution_class,
-            distribution_kwargs=policy_distribution_kwargs,
-            return_log_prob=True,
-            default_interaction_type=InteractionType.RANDOM,
-        )
-
-        value_net = MLP(
-            num_cells=[400, 300],
-            out_features=1,
-        )
-        value_po = ValueOperator(
-            value_net,
-            in_keys=in_keys_critic,
-        )
-        actor_value = ActorCriticWrapper(policy_po, value_po).to(device)
-
-    with torch.no_grad(), set_exploration_type(ExplorationType.RANDOM):
-        td = proof_environment.rollout(max_steps=1000)
-        td_device = td.to(device)
-        td_device = actor_value(td_device)  # for init
-    return actor_value
-
-
-def make_sac_model(
-    proof_environment: EnvBase,
-    cfg: "DictConfig",  # noqa: F821
-    device: DEVICE_TYPING = "cpu",
-    in_keys: Optional[Sequence[str]] = None,
-    actor_net_kwargs=None,
-    qvalue_net_kwargs=None,
-    value_net_kwargs=None,
-    observation_key=None,
-    **kwargs,
-) -> nn.ModuleList:
-    """Actor, Q-value and value model constructor helper function for SAC.
-
-    Follows default parameters proposed in SAC original paper: https://arxiv.org/pdf/1801.01290.pdf.
-    Other configurations can easily be implemented by modifying this function at will.
-
-    Args:
-        proof_environment (EnvBase): a dummy environment to retrieve the observation and action spec
-        cfg (DictConfig): contains arguments of the SAC script
-        device (torch.device, optional): device on which the model must be cast. Default is "cpu".
-        in_keys (iterable of strings, optional): observation key to be read by the actor, usually one of
-            `'observation_vector'` or `'pixels'`. If none is provided, one of these two keys is chosen
-             based on the `cfg.from_pixels` argument.
-        actor_net_kwargs (dict, optional): kwargs of the actor MLP.
-        qvalue_net_kwargs (dict, optional): kwargs of the qvalue MLP.
-        value_net_kwargs (dict, optional): kwargs of the value MLP.
-
-    Returns:
-         A nn.ModuleList containing the actor, qvalue operator(s) and the value operator.
-
-    Examples:
-        >>> from torchrl.trainers.helpers.models import make_sac_model, parser_model_args_continuous
-        >>> from torchrl.envs.libs.gym import GymEnv
-        >>> from torchrl.envs.transforms import CatTensors, TransformedEnv, DoubleToFloat, Compose
-        >>> import hydra
-        >>> from hydra.core.config_store import ConfigStore
-        >>> import dataclasses
-        >>> proof_environment = TransformedEnv(GymEnv("HalfCheetah-v2"), Compose(DoubleToFloat(["observation"]),
-        ...    CatTensors(["observation"], "observation_vector")))
-        >>> device = torch.device("cpu")
-        >>> config_fields = [(config_field.name, config_field.type, config_field) for config_cls in
-        ...                    (SACModelConfig, EnvConfig)
-        ...                   for config_field in dataclasses.fields(config_cls)]
-        >>> Config = dataclasses.make_dataclass(cls_name="Config", fields=config_fields)
-        >>> cs = ConfigStore.instance()
-        >>> cs.store(name="config", node=Config)
-        >>> with initialize(config_path=None):
-        >>>     cfg = compose(config_name="config")
-        >>> model = make_sac_model(
-        ...     proof_environment,
-        ...     device=device,
-        ...     cfg=cfg,
-        ...     )
-        >>> actor, qvalue, value = model
-        >>> td = proof_environment.reset()
-        >>> print(actor(td))
-        TensorDict(
-            fields={
-                done: Tensor(torch.Size([1]), dtype=torch.bool),
-                observation_vector: Tensor(torch.Size([17]), dtype=torch.float32),
-                loc: Tensor(torch.Size([6]), dtype=torch.float32),
-                scale: Tensor(torch.Size([6]), dtype=torch.float32),
-                action: Tensor(torch.Size([6]), dtype=torch.float32)},
-            batch_size=torch.Size([]),
-            device=cpu,
-            is_shared=False)
-        >>> print(qvalue(td.clone()))
-        TensorDict(
-            fields={
-                done: Tensor(torch.Size([1]), dtype=torch.bool),
-                observation_vector: Tensor(torch.Size([17]), dtype=torch.float32),
-                loc: Tensor(torch.Size([6]), dtype=torch.float32),
-                scale: Tensor(torch.Size([6]), dtype=torch.float32),
-                action: Tensor(torch.Size([6]), dtype=torch.float32),
-                state_action_value: Tensor(torch.Size([1]), dtype=torch.float32)},
-            batch_size=torch.Size([]),
-            device=cpu,
-            is_shared=False)
-        >>> print(value(td.clone()))
-        TensorDict(
-            fields={
-                done: Tensor(torch.Size([1]), dtype=torch.bool),
-                observation_vector: Tensor(torch.Size([17]), dtype=torch.float32),
-                loc: Tensor(torch.Size([6]), dtype=torch.float32),
-                scale: Tensor(torch.Size([6]), dtype=torch.float32),
-                action: Tensor(torch.Size([6]), dtype=torch.float32),
-                state_value: Tensor(torch.Size([1]), dtype=torch.float32)},
-            batch_size=torch.Size([]),
-            device=cpu,
-            is_shared=False)
-
-    """
-    tanh_loc = cfg.tanh_loc
-    default_policy_scale = cfg.default_policy_scale
-    gSDE = cfg.gSDE
-
-    proof_environment.reset()
-    action_spec = proof_environment.action_spec
-
-    if actor_net_kwargs is None:
-        actor_net_kwargs = {}
-    if value_net_kwargs is None:
-        value_net_kwargs = {}
-    if qvalue_net_kwargs is None:
-        qvalue_net_kwargs = {}
-
-    if in_keys is None:
-        in_keys = ["observation_vector"]
-
-    actor_net_kwargs_default = {
-        "num_cells": [cfg.actor_cells, cfg.actor_cells],
-        "out_features": (2 - gSDE) * action_spec.shape[-1],
-        "activation_class": ACTIVATIONS[cfg.activation],
-    }
-    actor_net_kwargs_default.update(actor_net_kwargs)
-    actor_net = MLP(**actor_net_kwargs_default)
-
-    qvalue_net_kwargs_default = {
-        "num_cells": [cfg.qvalue_cells, cfg.qvalue_cells],
-        "out_features": 1,
-        "activation_class": ACTIVATIONS[cfg.activation],
-    }
-    qvalue_net_kwargs_default.update(qvalue_net_kwargs)
-    qvalue_net = MLP(
-        **qvalue_net_kwargs_default,
-    )
-
-    value_net_kwargs_default = {
-        "num_cells": [cfg.value_cells, cfg.value_cells],
-        "out_features": 1,
-        "activation_class": ACTIVATIONS[cfg.activation],
-    }
-    value_net_kwargs_default.update(value_net_kwargs)
-    value_net = MLP(
-        **value_net_kwargs_default,
-    )
-
-    dist_class = TanhNormal
-    dist_kwargs = {
-        "min": action_spec.space.minimum,
-        "max": action_spec.space.maximum,
-        "tanh_loc": tanh_loc,
-    }
-
-    if not gSDE:
-        actor_net = NormalParamWrapper(
-            actor_net,
-            scale_mapping=f"biased_softplus_{default_policy_scale}",
-            scale_lb=cfg.scale_lb,
-        )
-        in_keys_actor = in_keys
-        actor_module = SafeModule(
-            actor_net,
-            in_keys=in_keys_actor,
-            out_keys=[
-                "loc",
-                "scale",
-            ],
-        )
-
-    else:
-        gSDE_state_key = in_keys[0]
-        actor_module = SafeModule(
-            actor_net,
-            in_keys=in_keys,
-            out_keys=["action"],  # will be overwritten
-        )
-
-        if action_spec.domain == "continuous":
-            min = action_spec.space.minimum
-            max = action_spec.space.maximum
-            transform = SafeTanhTransform()
-            if (min != -1).any() or (max != 1).any():
-                transform = d.ComposeTransform(
-                    transform,
-                    d.AffineTransform(loc=(max + min) / 2, scale=(max - min) / 2),
-                )
-        else:
-            raise RuntimeError("cannot use gSDE with discrete actions")
-
-        actor_module = SafeSequential(
-            actor_module,
-            SafeModule(
-                LazygSDEModule(transform=transform),
-                in_keys=["action", gSDE_state_key, "_eps_gSDE"],
-                out_keys=["loc", "scale", "action", "_eps_gSDE"],
-            ),
-        )
-
-    actor = ProbabilisticActor(
-        spec=action_spec,
-        in_keys=["loc", "scale"],
-        module=actor_module,
-        distribution_class=dist_class,
-        distribution_kwargs=dist_kwargs,
-        default_interaction_type=InteractionType.RANDOM,
-        return_log_prob=False,
-    )
-
-    qvalue = ValueOperator(
-        in_keys=["action"] + in_keys,
-        module=qvalue_net,
-    )
-    value = ValueOperator(
-        in_keys=in_keys,
-        module=value_net,
-    )
-    model = nn.ModuleList([actor, qvalue, value]).to(device)
-
-    # init nets
-    with torch.no_grad(), set_exploration_type(ExplorationType.RANDOM):
-        td = proof_environment.reset()
-        td = td.to(device)
-        for net in model:
-            net(td)
-    del td
-
-    return model
-
-
 def make_redq_model(
     proof_environment: EnvBase,
     cfg: "DictConfig",  # noqa: F821
     device: DEVICE_TYPING = "cpu",
     in_keys: Optional[Sequence[str]] = None,
     actor_net_kwargs=None,
     qvalue_net_kwargs=None,
@@ -1824,119 +805,14 @@
     model_device: str = ""
     # Decay of the reward moving averaging
     exploration: str = "additive_gaussian"
     # One of "additive_gaussian", "ou_exploration" or ""
 
 
 @dataclass
-class PPOModelConfig:
-    """PPO model config struct."""
-
-    gSDE: bool = False
-    # if True, exploration is achieved using the gSDE technique.
-    tanh_loc: bool = False
-    # if True, uses a Tanh-Normal transform for the policy location of the form
-    # upscale * tanh(loc/upscale) (only available with TanhTransform and TruncatedGaussian distributions)
-    default_policy_scale: float = 1.0
-    # Default policy scale parameter
-    distribution: str = "tanh_normal"
-    # if True, uses a Tanh-Normal-Tanh distribution for the policy
-    lstm: bool = False
-    # if True, uses an LSTM for the policy.
-    shared_mapping: bool = False
-    # if True, the first layers of the actor-critic are shared.
-
-
-@dataclass
-class A2CModelConfig:
-    """A2C model config struct."""
-
-    gSDE: bool = False
-    # if True, exploration is achieved using the gSDE technique.
-    tanh_loc: bool = False
-    # if True, uses a Tanh-Normal transform for the policy location of the form
-    # upscale * tanh(loc/upscale) (only available with TanhTransform and TruncatedGaussian distributions)
-    default_policy_scale: float = 1.0
-    # Default policy scale parameter
-    distribution: str = "tanh_normal"
-    # if True, uses a Tanh-Normal-Tanh distribution for the policy
-    lstm: bool = False
-    # if True, uses an LSTM for the policy.
-    shared_mapping: bool = False
-    # if True, the first layers of the actor-critic are shared.
-
-
-@dataclass
-class SACModelConfig:
-    """SAC model config struct."""
-
-    annealing_frames: int = 1000000
-    # float of frames used for annealing of the OrnsteinUhlenbeckProcess. Default=1e6.
-    noisy: bool = False
-    # whether to use NoisyLinearLayers in the value network.
-    ou_exploration: bool = False
-    # wraps the policy in an OU exploration wrapper, similar to DDPG. SAC being designed for
-    # efficient entropy-based exploration, this should be left for experimentation only.
-    ou_sigma: float = 0.2
-    # Ornstein-Uhlenbeck sigma
-    ou_theta: float = 0.15
-    # Aimed at superseeding --ou_exploration.
-    distributional: bool = False
-    # whether a distributional loss should be used (TODO: not implemented yet).
-    atoms: int = 51
-    # number of atoms used for the distributional loss (TODO)
-    gSDE: bool = False
-    # if True, exploration is achieved using the gSDE technique.
-    tanh_loc: bool = False
-    # if True, uses a Tanh-Normal transform for the policy location of the form
-    # upscale * tanh(loc/upscale) (only available with TanhTransform and TruncatedGaussian distributions)
-    default_policy_scale: float = 1.0
-    # Default policy scale parameter
-    distribution: str = "tanh_normal"
-    # if True, uses a Tanh-Normal-Tanh distribution for the policy
-    actor_cells: int = 256
-    # cells of the actor
-    qvalue_cells: int = 256
-    # cells of the qvalue net
-    scale_lb: float = 0.1
-    # min value of scale
-    value_cells: int = 256
-    # cells of the value net
-    activation: str = "tanh"
-    # activation function, either relu or elu or tanh, Default=tanh
-    model_device: str = ""
-    # device where the model to be trained should sit
-
-
-@dataclass
-class DDPGModelConfig:
-    """DDPG model config struct."""
-
-    annealing_frames: int = 1000000
-    # float of frames used for annealing of the OrnsteinUhlenbeckProcess. Default=1e6.
-    noisy: bool = False
-    # whether to use NoisyLinearLayers in the value network.
-    ou_exploration: bool = False
-    # wraps the policy in an OU exploration wrapper, similar to DDPG. SAC being designed for
-    # efficient entropy-based exploration, this should be left for experimentation only.
-    ou_sigma: float = 0.2
-    # Ornstein-Uhlenbeck sigma
-    ou_theta: float = 0.15
-    # Aimed at superseeding --ou_exploration.
-    distributional: bool = False
-    # whether a distributional loss should be used (TODO: not implemented yet).
-    atoms: int = 51
-    # number of atoms used for the distributional loss (TODO)
-    gSDE: bool = False
-    # if True, exploration is achieved using the gSDE technique.
-    activation: str = "tanh"
-    # activation function, either relu or elu or tanh, Default=tanh
-
-
-@dataclass
 class REDQModelConfig:
     """REDQ model config struct."""
 
     annealing_frames: int = 1000000
     # float of frames used for annealing of the OrnsteinUhlenbeckProcess. Default=1e6.
     noisy: bool = False
     # whether to use NoisyLinearLayers in the value network.
```

## Comparing `torchrl_nightly-2023.6.2.dist-info/LICENSE` & `torchrl_nightly-2023.6.3.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `torchrl_nightly-2023.6.2.dist-info/METADATA` & `torchrl_nightly-2023.6.3.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: torchrl-nightly
-Version: 2023.6.2
+Version: 2023.6.3
 Summary: UNKNOWN
 Home-page: https://github.com/pytorch/rl
 Author: torchrl contributors
 Author-email: vmoens@fb.com
 License: BSD
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3.7
@@ -63,16 +63,16 @@
 <a href="https://pypi.org/project/torchrl-nightly"><img src="https://img.shields.io/pypi/v/torchrl-nightly?label=nightly" alt="pypi nightly version"></a>
 [![Downloads](https://static.pepy.tech/personalized-badge/torchrl?period=total&units=international_system&left_color=blue&right_color=orange&left_text=Downloads)](https://pepy.tech/project/torchrl)
 [![Downloads](https://static.pepy.tech/personalized-badge/torchrl-nightly?period=total&units=international_system&left_color=blue&right_color=orange&left_text=Downloads%20(nightly))](https://pepy.tech/project/torchrl-nightly)
 
 # TorchRL
 
 [**Documentation**](#documentation-and-knowledge-base) | [**TensorDict**](#writing-simplified-and-portable-rl-codebase-with-tensordict) |
-[**Features**](#features) | [**Examples, tutorials and demos**](#examples-tutorials-and-demos) | [**Installation**](#installation) |
-[**Asking a question**](#asking-a-question) | [**Citation**](#citation) | [**Contributing**](#contributing)
+[**Features**](#features) | [**Examples, tutorials and demos**](#examples-tutorials-and-demos) | [**Citation**](#citation) | [**Installation**](#installation) |
+[**Asking a question**](#asking-a-question) | [**Contributing**](#contributing)
 
 **TorchRL** is an open-source Reinforcement Learning (RL) library for PyTorch.
 
 It provides pytorch and **python-first**, low and high level abstractions for RL that are intended to be **efficient**, **modular**, **documented** and properly **tested**.
 The code is aimed at supporting research in RL. Most of it is written in python in a highly modular way, such that researchers can easily swap components, transform them or write new ones with little effort.
 
 This repo attempts to align with the existing pytorch ecosystem libraries in that it has a dataset pillar ([torchrl/envs](torchrl/envs)), [transforms](torchrl/envs/transforms), [models](torchrl/modules), data utilities (e.g. collectors and containers), etc.
@@ -554,15 +554,30 @@
 
 Check the [examples markdown](examples/EXAMPLES.md) directory for more details 
 about handling the various configuration settings.
 
 We also provide [tutorials and demos](https://pytorch.org/rl/#tutorials) that give a sense of
 what the library can do.
 
+## Citation
+
+If you're using TorchRL, please refer to this BibTeX entry to cite this work:
+```
+@misc{bou2023torchrl,
+      title={TorchRL: A data-driven decision-making library for PyTorch}, 
+      author={Albert Bou and Matteo Bettini and Sebastian Dittert and Vikash Kumar and Shagun Sodhani and Xiaomeng Yang and Gianni De Fabritiis and Vincent Moens},
+      year={2023},
+      eprint={2306.00577},
+      archivePrefix={arXiv},
+      primaryClass={cs.LG}
+}
+```
+
 ## Installation
+
 Create a conda environment where the packages will be installed.
 
 ```
 conda create --name torch_rl python=3.9
 conda activate torch_rl
 ```
 
@@ -685,27 +700,14 @@
 ## Asking a question
 
 If you spot a bug in the library, please raise an issue in this repo.
 
 If you have a more generic question regarding RL in PyTorch, post it on
 the [PyTorch forum](https://discuss.pytorch.org/c/reinforcement-learning/6).
 
-## Citation
-
-If you're using TorchRL, please refer to this BibTeX entry to cite this work:
-```
-@software{TorchRL,
-  author = {Moens, Vincent},
-  title = {{TorchRL: an open-source Reinforcement Learning (RL) library for PyTorch}},
-  url = {https://github.com/pytorch/rl},
-  version = {0.1.1},
-  year = {2023}
-}
-```
-
 ## Contributing
 
 Internal collaborations to torchrl are welcome! Feel free to fork, submit issues and PRs.
 You can checkout the detailed contribution guide [here](CONTRIBUTING.md).
 As mentioned above, a list of open contributions can be found in [here](https://github.com/pytorch/rl/issues/509).
 
 Contributors are recommended to install [pre-commit hooks](https://pre-commit.com/) (using `pre-commit install`). pre-commit will check for linting related issues when the code is commited locally. You can disable th check by appending `-n` to your commit command: `git commit -m <commit message> -n`
```

### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: torchrl-nightly Version: 2023.6.2 Summary: UNKNOWN
+Metadata-Version: 2.1 Name: torchrl-nightly Version: 2023.6.3 Summary: UNKNOWN
 Home-page: https://github.com/pytorch/rl Author: torchrl contributors Author-
 email: vmoens@fb.com License: BSD Platform: UNKNOWN Classifier: Programming
 Language :: Python :: 3.7 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9 Classifier: Programming
 Language :: Python :: 3.10 Classifier: License :: OSI Approved :: MIT License
 Classifier: Operating System :: OS Independent Classifier: Development Status
 :: 4 - Beta Classifier: Intended Audience :: Developers Classifier: Intended
@@ -42,16 +42,16 @@
 (https://pepy.tech/project/torchrl) [![Downloads](https://static.pepy.tech/
 personalized-badge/torchrl-
 nightly?period=total&units=international_system&left_color=blue&right_color=orange&left_text=Downloads%20
 (nightly))](https://pepy.tech/project/torchrl-nightly) # TorchRL
 [**Documentation**](#documentation-and-knowledge-base) | [**TensorDict**]
 (#writing-simplified-and-portable-rl-codebase-with-tensordict) | [**Features**]
 (#features) | [**Examples, tutorials and demos**](#examples-tutorials-and-
-demos) | [**Installation**](#installation) | [**Asking a question**](#asking-a-
-question) | [**Citation**](#citation) | [**Contributing**](#contributing)
+demos) | [**Citation**](#citation) | [**Installation**](#installation) |
+[**Asking a question**](#asking-a-question) | [**Contributing**](#contributing)
 **TorchRL** is an open-source Reinforcement Learning (RL) library for PyTorch.
 It provides pytorch and **python-first**, low and high level abstractions for
 RL that are intended to be **efficient**, **modular**, **documented** and
 properly **tested**. The code is aimed at supporting research in RL. Most of it
 is written in python in a highly modular way, such that researchers can easily
 swap components, transform them or write new ones with little effort. This repo
 attempts to align with the existing pytorch ecosystem libraries in that it has
@@ -268,36 +268,42 @@
 (examples/ddpg/ddpg.py) - [IQL](examples/iql/iql.py) - [TD3](examples/td3/
 td3.py) - [A2C](examples/a2c_old/a2c.py) - [PPO](examples/ppo/ppo.py) - [SAC]
 (examples/sac/sac.py) - [REDQ](examples/redq/redq.py) - [Dreamer](examples/
 dreamer/dreamer.py) and many more to come! Check the [examples markdown]
 (examples/EXAMPLES.md) directory for more details about handling the various
 configuration settings. We also provide [tutorials and demos](https://
 pytorch.org/rl/#tutorials) that give a sense of what the library can do. ##
-Installation Create a conda environment where the packages will be installed.
-``` conda create --name torch_rl python=3.9 conda activate torch_rl ```
-**PyTorch** Depending on the use of functorch that you want to make, you may
-want to install the latest (nightly) PyTorch release or the latest stable
-version of PyTorch. See [here](https://pytorch.org/get-started/locally/) for a
-detailed list of commands, including `pip3` or windows/OSX compatible
-installation commands. **Torchrl** You can install the **latest stable
-release** by using ``` pip3 install torchrl ``` This should work on linux and
-MacOs (not M1). For Windows and M1/M2 machines, one should install the library
-locally (see below). The **nightly build** can be installed via ``` pip install
-torchrl-nightly ``` To install extra dependencies, call ``` pip3 install
-"torchrl[atari,dm_control,gym_continuous,rendering,tests,utils]" ``` or a
-subset of these. Alternatively, as the library is at an early stage, it may be
-wise to install it in develop mode as this will make it possible to pull the
-latest changes and benefit from them immediately. Start by cloning the repo:
-``` git clone https://github.com/pytorch/rl ``` Go to the directory where you
-have cloned the torchrl repo and install it ``` cd /path/to/torchrl/ pip
-install -e . ``` On M1 machines, this should work out-of-the-box with the
-nightly build of PyTorch. If the generation of this artifact in MacOs M1
-doesn't work correctly or in the execution the message `(mach-o file, but is an
-incompatible architecture (have 'x86_64', need 'arm64e'))` appears, then try
-``` ARCHFLAGS="-arch arm64" python setup.py develop ``` To run a quick sanity
+Citation If you're using TorchRL, please refer to this BibTeX entry to cite
+this work: ``` @misc{bou2023torchrl, title={TorchRL: A data-driven decision-
+making library for PyTorch}, author={Albert Bou and Matteo Bettini and
+Sebastian Dittert and Vikash Kumar and Shagun Sodhani and Xiaomeng Yang and
+Gianni De Fabritiis and Vincent Moens}, year={2023}, eprint={2306.00577},
+archivePrefix={arXiv}, primaryClass={cs.LG} } ``` ## Installation Create a
+conda environment where the packages will be installed. ``` conda create --name
+torch_rl python=3.9 conda activate torch_rl ``` **PyTorch** Depending on the
+use of functorch that you want to make, you may want to install the latest
+(nightly) PyTorch release or the latest stable version of PyTorch. See [here]
+(https://pytorch.org/get-started/locally/) for a detailed list of commands,
+including `pip3` or windows/OSX compatible installation commands. **Torchrl**
+You can install the **latest stable release** by using ``` pip3 install torchrl
+``` This should work on linux and MacOs (not M1). For Windows and M1/M2
+machines, one should install the library locally (see below). The **nightly
+build** can be installed via ``` pip install torchrl-nightly ``` To install
+extra dependencies, call ``` pip3 install "torchrl
+[atari,dm_control,gym_continuous,rendering,tests,utils]" ``` or a subset of
+these. Alternatively, as the library is at an early stage, it may be wise to
+install it in develop mode as this will make it possible to pull the latest
+changes and benefit from them immediately. Start by cloning the repo: ``` git
+clone https://github.com/pytorch/rl ``` Go to the directory where you have
+cloned the torchrl repo and install it ``` cd /path/to/torchrl/ pip install -
+e . ``` On M1 machines, this should work out-of-the-box with the nightly build
+of PyTorch. If the generation of this artifact in MacOs M1 doesn't work
+correctly or in the execution the message `(mach-o file, but is an incompatible
+architecture (have 'x86_64', need 'arm64e'))` appears, then try ```
+ARCHFLAGS="-arch arm64" python setup.py develop ``` To run a quick sanity
 check, leave that directory (e.g. by executing `cd ~/`) and try to import the
 library. ``` python -c "import torchrl" ``` This should not return any warning
 or error. **Optional dependencies** The following libraries can be installed
 depending on the usage one wants to make of torchrl: ``` # diverse pip3 install
 tqdm tensorboard "hydra-core>=1.1" hydra-submitit-launcher # rendering pip3
 install moviepy # deepmind control suite pip3 install dm_control # gym, atari
 games pip3 install "gym[atari]" "gym[accept-rom-license]" pygame # tests pip3
@@ -317,19 +323,15 @@
 collect_env.py python collect_env.py ``` should display ``` OS: macOS ***
 (arm64) ``` and not ``` OS: macOS **** (x86_64) ``` Versioning issues can cause
 error message of the type ```undefined symbol``` and such. For these, refer to
 the [versioning issues document](knowledge_base/VERSIONING_ISSUES.md) for a
 complete explanation and proposed workarounds. ## Asking a question If you spot
 a bug in the library, please raise an issue in this repo. If you have a more
 generic question regarding RL in PyTorch, post it on the [PyTorch forum](https:
-//discuss.pytorch.org/c/reinforcement-learning/6). ## Citation If you're using
-TorchRL, please refer to this BibTeX entry to cite this work: ``` @software
-{TorchRL, author = {Moens, Vincent}, title = {{TorchRL: an open-source
-Reinforcement Learning (RL) library for PyTorch}}, url = {https://github.com/
-pytorch/rl}, version = {0.1.1}, year = {2023} } ``` ## Contributing Internal
+//discuss.pytorch.org/c/reinforcement-learning/6). ## Contributing Internal
 collaborations to torchrl are welcome! Feel free to fork, submit issues and
 PRs. You can checkout the detailed contribution guide [here](CONTRIBUTING.md).
 As mentioned above, a list of open contributions can be found in [here](https:/
 /github.com/pytorch/rl/issues/509). Contributors are recommended to install
 [pre-commit hooks](https://pre-commit.com/) (using `pre-commit install`). pre-
 commit will check for linting related issues when the code is commited locally.
 You can disable th check by appending `-n` to your commit command: `git commit
```

## Comparing `torchrl_nightly-2023.6.2.dist-info/RECORD` & `torchrl_nightly-2023.6.3.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -1,27 +1,27 @@
 build_tools/__init__.py,sha256=5xRGSM4YMr794wxfVj_SQly2cyHNyhXTdXdWUZJoM2M,183
 build_tools/setup_helpers/__init__.py,sha256=uwhUMyZq6dm1J5T_rjItc0aZb8-WSJFYd81pU3aJQsE,245
 build_tools/setup_helpers/extension.py,sha256=8u0VrdMo3WzXp1V9KrB_tq2EfkGkWncQ6G8yzW8gcyo,6085
 torchrl/__init__.py,sha256=cOWCLNeRq7gnOXqFzcFKPm93wefcLaa1LgWRe59xmUI,957
 torchrl/_extension.py,sha256=N01cFr5HT6P8G81alGHUWFGVWHHeTgJAJT88Lz6MZRk,868
-torchrl/_torchrl.pyd,sha256=tSC7yW9P7xc3XMU-0Xb10fdtUn_mHF-s8Hdcvjdn1-g,360448
+torchrl/_torchrl.pyd,sha256=5kE1wjG_Z3bwXBTS2GPnJHi3e_y0n51oyC3UuAmFlf0,360448
 torchrl/_utils.py,sha256=sPsco15Ls8RU-i3ubdGhg7eoNzfLev10v45phgu87X4,17746
-torchrl/version.py,sha256=3bzzGafDM4xkkdfD7YcvCj390daTOOtmZ2ZBAP1vjXg,84
+torchrl/version.py,sha256=fAj5f-shfd8-UgYFOLaRHSEQhobYz8TAz0DxnmU73oM,84
 torchrl/collectors/__init__.py,sha256=Wbyd0dYtj4Oe-XLTlT5VG4yl0cHhLjXjmxA-8L0bjp8,366
 torchrl/collectors/collectors.py,sha256=WsuesSKfaZ4YFjYbKdjGhJQATs2JOHVm6RA70RsTW-Y,97515
 torchrl/collectors/utils.py,sha256=D9ggquoMMQhvasgRNTMBvZicMpRvYKOzp2zwWVIviQo,3847
 torchrl/collectors/distributed/__init__.py,sha256=5qoBlS2wvrEFc9SRGwNMDcvCJ_XukVLyyvX7MwN9slU,412
 torchrl/collectors/distributed/default_configs.py,sha256=RkY8hmAzDttCXAjfoqdvAsCSwaXdtN4AGWJ70hSUmic,674
 torchrl/collectors/distributed/generic.py,sha256=QPJtNsB6niAbVkvhLoWwjGkNp6uKvew3tK1Y69NQgQM,34086
 torchrl/collectors/distributed/ray.py,sha256=j_9OOcoMsVSg11MhHMKqR5WR4XHuD_JRCzVq4_ZvNyE,27837
 torchrl/collectors/distributed/rpc.py,sha256=slSbSiVgMsfTd7eYOQ748ctF19hXnr3-frWrgd6PBSc,27697
 torchrl/collectors/distributed/sync.py,sha256=8_oTr7m0kkSNlAEbkhT7ZPqsf3nB8Fxg_Soju9yeBjk,20055
 torchrl/collectors/distributed/utils.py,sha256=M6s0Z2NBzukTnDKrcRtoIB2cj9xFU1f0git3cjnsaBU,6244
 torchrl/data/__init__.py,sha256=m41GQ37n_lKojqrhfJa2qdNPzccgSXA7wd92aZf2BBc,974
-torchrl/data/tensor_specs.py,sha256=anO1FwbMmE_sY3w21rhck161OsX7Q12de2jemdM_svE,122048
+torchrl/data/tensor_specs.py,sha256=rb0WtM2EN2CQlXs9Ze1fliyKv_wBczV7c4FotyQeZMs,122535
 torchrl/data/utils.py,sha256=7ng9E6GM--i0yeFFQSyCUAHZ5NMKCQ_x9c0ps_fw3MU,2061
 torchrl/data/datasets/__init__.py,sha256=JBXEjRRaZyPmX7dMKDp4hEHT84JscNSgt8eeI-LLc7w,84
 torchrl/data/datasets/d4rl.py,sha256=xi9p5cj440JL5D0sljcVGcIrK4LbyodkYyaIoF8_tUg,12415
 torchrl/data/datasets/openml.py,sha256=2gf11rEdVlj_28egD4WDuF83_JPDderPWfQtFjqkFgU,6333
 torchrl/data/postprocs/__init__.py,sha256=DG7YUtvdqTYkGfgqLaF-bneXlzvAM8h00WqhSjavWa4,219
 torchrl/data/postprocs/postprocs.py,sha256=Xh8HbkMM8kiULvjezygFSik6uv7WvlDnjxvrI-0pZMs,9361
 torchrl/data/replay_buffers/__init__.py,sha256=m5XxW0yXmPsXnvp2FWmHfkqGntJDC-cNBuFnQiJNHxQ,648
@@ -68,17 +68,17 @@
 torchrl/modules/models/models.py,sha256=3rJPNLJVwGj-zas8OISghI2cdxQ-1UahC53ZtmxFp6I,45664
 torchrl/modules/models/utils.py,sha256=TKji7lSi_1jgEqqC6kJ-TOOgNuVbbk2seM348uK7THk,4023
 torchrl/modules/planners/__init__.py,sha256=xxRLqxJcXo38KjFeZiNuSoD8l6u-kPsZ4PRsWCdzqZA,281
 torchrl/modules/planners/cem.py,sha256=vJWn7h0x1UcqhMNgo1k2YVVMF6IKerPy3yghE3PJaOs,9572
 torchrl/modules/planners/common.py,sha256=FbL8WstVp1QY8_YHlaH3EFpsd9I_CMn1JWx7z5SlbRk,2455
 torchrl/modules/planners/mppi.py,sha256=cKkB8ql7D82khYSdbSxxyHa8Ph3j2ciaPfazdolKHv4,10683
 torchrl/modules/tensordict_module/__init__.py,sha256=u50jmDz8JVqUpX-0M5jGXHskq_mzHOiDOLgtxMr8oaQ,882
-torchrl/modules/tensordict_module/actors.py,sha256=s4Ylw4uYmb4Df-FFomPnXnzNr3XL01UJV2l6vZLpJAs,78666
+torchrl/modules/tensordict_module/actors.py,sha256=sqWlsFj683kfnhUpSVdrk5Q-NHOqZFd0LALSaJbycjo,78665
 torchrl/modules/tensordict_module/common.py,sha256=l9sT0BWxqKWZ_OxzJ14aeWitz3ZTpzc6SWUBdkY1cWE,17209
-torchrl/modules/tensordict_module/exploration.py,sha256=8nXpmPYYATGOuEvDh9g4hVd6jIuJcaC7L5IHSwslbrE,24439
+torchrl/modules/tensordict_module/exploration.py,sha256=3xjCca8gBbxJM8-6j9NHrHMBZYLXBCQR8WNVCn5zYLs,24200
 torchrl/modules/tensordict_module/probabilistic.py,sha256=Ik9zisYTBrXynGgNwRJU_TJoMn56DCxGYwvZKT6UqDA,11468
 torchrl/modules/tensordict_module/rnn.py,sha256=u3psIBCz7Vv9y8_tJL8caPez6PLrT4QmuHgwqv52aAI,18299
 torchrl/modules/tensordict_module/sequence.py,sha256=VuNg8aMJhLQLy9HDw0xwBOozsNj_Fm5rSyMt2QntdQ8,5835
 torchrl/modules/tensordict_module/world_models.py,sha256=9qlJKd1GDKeVtkTNRg9eUpx6AGzKCiABOVX40jxUnqU,1360
 torchrl/modules/utils/__init__.py,sha256=YDiFNwjc0U8SD_WoTpTroQ-J2aaXuaTLnkR1sP8EXjk,3984
 torchrl/modules/utils/mappings.py,sha256=ZurH7TLCDroHVzRTFcBajy6btc-dECLNh_pP1iM0ne4,2435
 torchrl/modules/utils/utils.py,sha256=IOXtM_-Ru-wvmZCQNNZCeIJhwnYB40vtP2cLdFPHq5Q,1368
@@ -90,16 +90,16 @@
 torchrl/objectives/dqn.py,sha256=eT-rb9ysa8L1SAdX1u2kAbrBzfGjQIEVBXHAc5Qjw0M,22531
 torchrl/objectives/dreamer.py,sha256=3zhG_cni5VKcEo06ykHtje0FSdsCmaJUvGBdOEZKqmY,17407
 torchrl/objectives/functional.py,sha256=BzT7OBk-FOoIYPaXBx73eOHAeEc6LnjRJeL472AWMHI,2119
 torchrl/objectives/iql.py,sha256=lZZasTEe36D_VIKFMUOGOkJ7u4w_gG0lBMMDxhTye9U,12319
 torchrl/objectives/ppo.py,sha256=6gwF0K9WWUaJDNENCuzEpshxMYg_YVLg2stJx36JW_I,31728
 torchrl/objectives/redq.py,sha256=2l29qbJ51wrnRXurMNX0IjruiOX-nq0RiNL3-8Wge9s,16103
 torchrl/objectives/reinforce.py,sha256=OMM6ut16IwPb4ur-_tt9Y12WgwfBRDxX3CasP9FlP4I,9266
-torchrl/objectives/sac.py,sha256=pNMRAhmTW5iWPC53ntzRrz-pja0j9n8txxxmBpuRuE4,34866
-torchrl/objectives/td3.py,sha256=N_7Fhjt4QS2r5h2utPwOUVPH_ZmLJEpcajw5u3Nb1YQ,11303
+torchrl/objectives/sac.py,sha256=0yrV2g_kVyIGhkeadsy34i5xBC92ArkPq1RqM0vExQ0,34859
+torchrl/objectives/td3.py,sha256=fTXaQfZCIBm3EEWyTHR-swEu3l5Bn2eOD46YpAtd1EY,13144
 torchrl/objectives/utils.py,sha256=FzhZ9UQKr0w0GgfhMQqBrc28JIwfPqPDZ3LM_XW4KsE,16464
 torchrl/objectives/value/__init__.py,sha256=3YD7d9rTodAFMFj74ugS-25CaDU2b6mVv466Sx1iNiw,371
 torchrl/objectives/value/advantages.py,sha256=ER9AhiBKOHUBW9A6KrrXIgG-z1bnz0kpYFFILbsXk7Y,47539
 torchrl/objectives/value/functional.py,sha256=td0Sar1zBZjdt5cZJzBq0rZq7MMiGOKmUP-cH3dWPI4,40208
 torchrl/objectives/value/pg.py,sha256=fM3XbHBkiT1FTfENSF8RkkZ3ZTJOLaaXTA99E74KQlU,317
 torchrl/objectives/value/utils.py,sha256=ABpKGUMt2mBziM1z2wIuYUYM3RdPlQX3y0RYZ2uosv4,13251
 torchrl/objectives/value/vtrace.py,sha256=hEc0WuYrixYDM7c7w_P7yU0NVGEkOXcqBQaanGeRqX8,1724
@@ -110,20 +110,20 @@
 torchrl/record/loggers/csv.py,sha256=_43-GkHNKLwlDzFG_m1g-Ewqu_l0aP1gKJlmds1yBSI,4709
 torchrl/record/loggers/mlflow.py,sha256=iWZOI-yRzovF9_2MjCNsH1TFNmjcPcWSjC8fOJ_pkhs,4344
 torchrl/record/loggers/tensorboard.py,sha256=U1DHSGVC_yj0pxP7Db-Vzhn3BquymAOvbMktwSSpwDo,3412
 torchrl/record/loggers/utils.py,sha256=bOVmPOK3YPCHwajfkyyF7PNvjz4GnM09AYsgLw7oH3k,2248
 torchrl/record/loggers/wandb.py,sha256=omcjDccUzgARipO7uvI1zXeV4G-fMGefdf4OZm3Azzo,6025
 torchrl/trainers/__init__.py,sha256=f9u3CLeSN-LpSsS709rahhHGm4G1KK2bqB2l0F7NxME,467
 torchrl/trainers/trainers.py,sha256=Wn9C66KnBUoDnccCIN75B9kMkC_XESoJfObqzESi3r8,52042
-torchrl/trainers/helpers/__init__.py,sha256=olPgYXwiN0mmsE4DistxVdmnAXAHGm5RL5YeKLmmTUM,942
+torchrl/trainers/helpers/__init__.py,sha256=mBchxdnLP71roD_IgxDdsOCijRbN59RShBYeJLDZdaQ,734
 torchrl/trainers/helpers/collectors.py,sha256=bA0vf2EZk-CNOMSXV2N1Kqlfsr4EQPGceO_FaGM1iAY,19309
 torchrl/trainers/helpers/envs.py,sha256=hASZW8JL_QCwf3lTJtHPB5gHbKo6OENGWOl4Btl24_E,22620
 torchrl/trainers/helpers/logger.py,sha256=aRoSh9zpXy46MTqhqZBoCuKZko2LN-SePiAWgX9W5iA,1206
-torchrl/trainers/helpers/losses.py,sha256=w4ShEJJA-I4o6SJSBRcusYFku7GuX3Osx7MzXnX9Te4,11052
-torchrl/trainers/helpers/models.py,sha256=9_FjUU7FX90rcjalL4aqstJYsnP9mkISRP4AEVoQm1E,77459
+torchrl/trainers/helpers/losses.py,sha256=zEjIzUc1cLaZ7YTleLle-WBpn69zX4vq-VNFORPQk74,6920
+torchrl/trainers/helpers/models.py,sha256=BUZliY5Luj_Deqct8i-2V8WhdpXe9K7o9WUqyrmLjMs,32462
 torchrl/trainers/helpers/replay_buffer.py,sha256=R9kriPfZOJ1s-kaGkn2w4tcZH2iC-u22pWQwqqw3c-A,1939
 torchrl/trainers/helpers/trainers.py,sha256=H76tq76L5t-6S-z22F9RAYjxiOO2RSNvs4IXuPn22Ok,12076
-torchrl_nightly-2023.6.2.dist-info/LICENSE,sha256=PGO-oZsq4EzhE1-WQS2xGiEF3UCVb9YawfQ09cIMV_8,1119
-torchrl_nightly-2023.6.2.dist-info/METADATA,sha256=l7EisYwqhKklRYg-zVKUQyRNDFS0NkIu-Ui6R6dLNhM,28565
-torchrl_nightly-2023.6.2.dist-info/WHEEL,sha256=eep6QWEFiQfg2wcclssb_WY-D33AnLYLnEKGA9Rn-VU,100
-torchrl_nightly-2023.6.2.dist-info/top_level.txt,sha256=JeTJ1jV7QJwLcUS1nr21aPn_wb-XlAZ9c-z_EH472JA,20
-torchrl_nightly-2023.6.2.dist-info/RECORD,,
+torchrl_nightly-2023.6.3.dist-info/LICENSE,sha256=PGO-oZsq4EzhE1-WQS2xGiEF3UCVb9YawfQ09cIMV_8,1119
+torchrl_nightly-2023.6.3.dist-info/METADATA,sha256=NHudTOXDW8M02NhP1afkmR_ZBZErNE3wREMZOax8wH0,28718
+torchrl_nightly-2023.6.3.dist-info/WHEEL,sha256=eep6QWEFiQfg2wcclssb_WY-D33AnLYLnEKGA9Rn-VU,100
+torchrl_nightly-2023.6.3.dist-info/top_level.txt,sha256=JeTJ1jV7QJwLcUS1nr21aPn_wb-XlAZ9c-z_EH472JA,20
+torchrl_nightly-2023.6.3.dist-info/RECORD,,
```

